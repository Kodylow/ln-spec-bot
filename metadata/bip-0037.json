{
  "BIP": "37",
  "Layer": "Peer Services",
  "Title": "Connection Bloom filtering",
  "Author": "Mike Hearn <hearn@google.com>",
  "Comments-Summary": "No comments yet.",
  "Comments-URI": "https://github.com/bitcoin/bips/wiki/Comments:BIP-0037",
  "Status": "Final",
  "Type": "Standards Track",
  "Created": "2012-10-24",
  "License": "PD",
  "sections": [
    {
      "header": "Abstract",
      "content": "This BIP adds new support to the peer-to-peer protocol that allows peers\nto reduce the amount of transaction data they are sent. Peers have the\noption of setting *filters* on each connection they make after the\nversion handshake has completed. A filter is defined as a [Bloom\nfilter](http://en.wikipedia.org/wiki/Bloom_filter) on data derived from\ntransactions. A Bloom filter is a probabilistic data structure which\nallows for testing set membership - they can have false positives but\nnot false negatives.\n\nThis document will not go into the details of how Bloom filters work and\nthe reader is referred to Wikipedia for an introduction to the topic."
    },
    {
      "header": "Motivation",
      "content": "As Bitcoin grows in usage the amount of bandwidth needed to download\nblocks and transaction broadcasts increases. Clients implementing\n*simplified payment verification* do not attempt to fully verify the\nblock chain, instead just checking that block headers connect together\ncorrectly and trusting that the transactions in a chain of high\ndifficulty are in fact valid. See the Bitcoin paper for more detail on\nthis mode.\n\nToday,\n[SPV](https://bitcoin.org/en/developer-guide#simplified-payment-verification-spv \"wikilink\")\nclients have to download the entire contents of blocks and all broadcast\ntransactions, only to throw away the vast majority of the transactions\nthat are not relevant to their wallets. This slows down their\nsynchronization process, wastes users bandwidth (which on phones is\noften metered) and increases memory usage. All three problems are\ntriggering real user complaints for the Android \\\"Bitcoin Wallet\\\" app\nwhich implements SPV mode. In order to make chain synchronization fast,\ncheap and able to run on older phones with limited memory we want to\nhave remote peers throw away irrelevant transactions before sending them\nacross the network."
    },
    {
      "header": "Design rationale {#design_rationale}",
      "content": "The most obvious way to implement the stated goal would be for clients\nto upload lists of their keys to the remote node. We take a more complex\napproach for the following reasons:\n\n-   Privacy: Because Bloom filters are probabilistic, with the false\npositive rate chosen by the client, nodes can trade off precision vs\nbandwidth usage. A node with access to lots of bandwidth may choose\nto have a high FP rate, meaning the remote peer cannot accurately\nknow which transactions belong to the client and which don\\'t. A\nnode with very little bandwidth may choose to use a very accurate\nfilter meaning that they only get sent transactions actually\nrelevant to their wallet, but remote peers may be able to correlate\ntransactions with IP addresses (and each other).\n-   Bloom filters are compact and testing membership in them is fast.\nThis results in satisfying performance characteristics with minimal\nrisk of opening up potential for DoS attacks."
    },
    {
      "header": "Specification",
      "content": "### New messages {#new_messages}\n\nWe start by adding three new messages to the protocol:\n\n-   `filterload`, which sets the current Bloom filter on the connection\n-   `filteradd`, which adds the given data element to the connections\ncurrent filter without requiring a completely new one to be set\n-   `filterclear`, which deletes the current filter and goes back to\nregular pre-BIP37 usage.\n\nNote that there is no filterremove command because by their nature,\nBloom filters are append-only data structures. Once an element is added\nit cannot be removed again without rebuilding the entire structure from\nscratch.\n\nThe `filterload` command is defined as follows:\n\nField Size   Description   Data type     Comments\n------------ ------------- ------------- -----------------------------------------------------------------------------------------------------------\n?            filter        uint8_t\\[\\]   The filter itself is simply a bit field of arbitrary byte-aligned size. The maximum size is 36,000 bytes.\n4            nHashFuncs    uint32_t      The number of hash functions to use in this filter. The maximum value allowed in this field is 50.\n4            nTweak        uint32_t      A random value to add to the seed value in the hash function used by the bloom filter.\n1            nFlags        uint8_t       A set of flags that control how matched items are added to the filter.\n\nSee below for a description of the Bloom filter algorithm and how to\nselect nHashFuncs and filter size for a desired false positive rate.\n\nUpon receiving a `filterload` command, the remote peer will immediately\nrestrict the broadcast transactions it announces (in inv packets) to\ntransactions matching the filter, where the matching algorithm is\nspecified below. The flags control the update behaviour of the matching\nalgorithm.\n\nThe `filteradd` command is defined as follows:\n\nField Size   Description   Data type     Comments\n------------ ------------- ------------- ------------------------------------------------\n?            data          uint8_t\\[\\]   The data element to add to the current filter.\n\nThe data field must be smaller than or equal to 520 bytes in size (the\nmaximum size of any potentially matched object).\n\nThe given data element will be added to the Bloom filter. A filter must\nhave been previously provided using `filterload`. This command is useful\nif a new key or script is added to a clients wallet whilst it has\nconnections to the network open, it avoids the need to re-calculate and\nsend an entirely new filter to every peer (though doing so is usually\nadvisable to maintain anonymity).\n\nThe `filterclear` command has no arguments at all.\n\nAfter a filter has been set, nodes don\\'t merely stop announcing\nnon-matching transactions, they can also serve filtered blocks. A\nfiltered block is defined by the `merkleblock` message and is defined\nlike this:\n\nField Size   Description          Data type     Comments\n------------ -------------------- ------------- ---------------------------------------------------------------------------------------------------------\n4            version              uint32_t      Block version information, based upon the software version creating this block\n32           prev_block           char\\[32\\]    The hash value of the previous block this particular block references\n32           merkle_root          char\\[32\\]    The reference to a Merkle tree collection which is a hash of all transactions related to this block\n4            timestamp            uint32_t      A timestamp recording when this block was created (Limited to 2106!)\n4            bits                 uint32_t      The calculated difficulty target being used for this block\n4            nonce                uint32_t      The nonce used to generate this block... to allow variations of the header and compute different hashes\n4            total_transactions   uint32_t      Number of transactions in the block (including unmatched ones)\n?            hashes               uint256\\[\\]   hashes in depth-first order (including standard varint size prefix)\n?            flags                byte\\[\\]      flag bits, packed per 8 in a byte, least significant bit first (including standard varint size prefix)\n\nSee below for the format of the partial merkle tree hashes and flags.\n\nThus, a `merkleblock` message is a block header, plus a part of a merkle\ntree which can be used to extract identifying information for\ntransactions that matched the filter and prove that the matching\ntransaction data really did appear in the solved block. Clients can use\nthis data to be sure that the remote node is not feeding them fake\ntransactions that never appeared in a real block, although lying through\nomission is still possible."
    },
    {
      "header": "Extensions to existing messages {#extensions_to_existing_messages}",
      "content": "The `version` command is extended with a new field:\n\nField Size   Description   Data type   Comments\n------------ ------------- ----------- ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n1 byte       fRelay        bool        If false then broadcast transactions will not be announced until a filter{load,add,clear} command is received. If missing or true, no change in protocol behaviour occurs.\n\nSPV clients that wish to use Bloom filtering would normally set fRelay\nto false in the version message, then set a filter based on their wallet\n(or a subset of it, if they are overlapping different peers). Being able\nto opt-out of inv messages until the filter is set prevents a client\nbeing flooded with traffic in the brief window of time between finishing\nversion handshaking and setting the filter.\n\nThe `getdata` command is extended to allow a new type in the `inv`\nsubmessage. The type field can now be `MSG_FILTERED_BLOCK (== 3)` rather\nthan `MSG_BLOCK`. If no filter has been set on the connection, a request\nfor filtered blocks is ignored. If a filter has been set, a\n`merkleblock` message is returned for the requested block hash. In\naddition, because a `merkleblock` message contains only a list of\ntransaction hashes, transactions matching the filter should also be sent\nin separate tx messages after the merkleblock is sent. This avoids a\nslow roundtrip that would otherwise be required (receive hashes, didn\\'t\nsee some of these transactions yet, ask for them). Note that because\nthere is currently no way to request transactions which are already in a\nblock from a node (aside from requesting the full block), the set of\nmatching transactions that the requesting node hasn\\'t either received\nor announced with an inv must be sent and any additional transactions\nwhich match the filter may also be sent. This allows for clients (such\nas the reference client) to limit the number of invs it must remember a\ngiven node to have announced while still providing nodes with, at a\nminimum, all the transactions it needs."
    },
    {
      "header": "Filter matching algorithm {#filter_matching_algorithm}",
      "content": "The filter can be tested against arbitrary pieces of data, to see if\nthat data was inserted by the client. Therefore the question arises of\nwhat pieces of data should be inserted/tested.\n\nTo determine if a transaction matches the filter, the following\nalgorithm is used. Once a match is found the algorithm aborts.\n\n1.  Test the hash of the transaction itself.\n2.  For each output, test each data element of the output script. This\nmeans each hash and key in the output script is tested\nindependently. **Important:** if an output matches whilst testing a\ntransaction, the node might need to update the filter by inserting\nthe serialized COutPoint structure. See below for more details.\n3.  For each input, test the serialized COutPoint structure.\n4.  For each input, test each data element of the input script (note:\ninput scripts only ever contain data elements).\n5.  Otherwise there is no match.\n\nIn this way addresses, keys and script hashes (for P2SH outputs) can all\nbe added to the filter. You can also match against classes of\ntransactions that are marked with well known data elements in either\ninputs or outputs, for example, to implement various forms of [Smart\nproperty](https://en.bitcoin.it/wiki/Smart_Property \"wikilink\").\n\nThe test for outpoints is there to ensure you can find transactions\nspending outputs in your wallet, even though you don\\'t know anything\nabout their form. As you can see, once set on a connection the filter is\n**not static** and can change throughout the connections lifetime. This\nis done to avoid the following race condition:\n\n1.  A client sets a filter matching a key in their wallet. They then\nstart downloading the block chain. The part of the chain that the\nclient is missing is requested using getblocks.\n2.  The first block is read from disk by the serving peer. It contains\nTX 1 which sends money to the clients key. It matches the filter and\nis thus sent to the client.\n3.  The second block is read from disk by the serving peer. It contains\nTX 2 which spends TX 1. However TX 2 does not contain any of the\nclients keys and is thus not sent. The client does not know the\nmoney they received was already spent.\n\nBy updating the bloom filter atomically in step 2 with the discovered\noutpoint, the filter will match against TX 2 in step 3 and the client\nwill learn about all relevant transactions, despite that there is no\npause between the node processing the first and second blocks.\n\nThe nFlags field of the filter controls the nodes precise update\nbehaviour and is a bit field.\n\n-   `BLOOM_UPDATE_NONE (0)` means the filter is not adjusted when a\nmatch is found.\n-   `BLOOM_UPDATE_ALL (1)` means if the filter matches any data element\nin a scriptPubKey the outpoint is serialized and inserted into the\nfilter.\n-   `BLOOM_UPDATE_P2PUBKEY_ONLY (2)` means the outpoint is inserted into\nthe filter only if a data element in the scriptPubKey is matched,\nand that script is of the standard \\\"pay to pubkey\\\" or \\\"pay to\nmultisig\\\" forms.\n\nThese distinctions are useful to avoid too-rapid degradation of the\nfilter due to an increasing false positive rate. We can observe that a\nwallet which expects to receive only payments of the standard\npay-to-address form doesn\\'t need automatic filter updates because any\ntransaction that spends one of its own outputs has a predictable data\nelement in the input (the pubkey that hashes to the address). If a\nwallet might receive pay-to-address outputs and also pay-to-pubkey or\npay-to-multisig outputs then BLOOM_UPDATE_P2PUBKEY_ONLY is appropriate,\nas it avoids unnecessary expansions of the filter for the most common\ntypes of output but still ensures correct behaviour with payments that\nexplicitly specify keys.\n\nObviously, nFlags == 1 or nFlags == 2 mean that the filter will get\ndirtier as more of the chain is scanned. Clients should monitor the\nobserved false positive rate and periodically refresh the filter with a\nclean one."
    },
    {
      "header": "Partial Merkle branch format {#partial_merkle_branch_format}",
      "content": "A *Merkle tree* is a way of arranging a set of items as leaf nodes of\ntree in which the interior nodes are hashes of the concatenations of\ntheir child hashes. The root node is called the *Merkle root*. Every\nBitcoin block contains a Merkle root of the tree formed from the blocks\ntransactions. By providing some elements of the trees interior nodes\n(called a *Merkle branch*) a proof is formed that the given transaction\nwas indeed in the block when it was being mined, but the size of the\nproof is much smaller than the size of the original block."
    },
    {
      "header": "Constructing a partial merkle tree object {#constructing_a_partial_merkle_tree_object}",
      "content": "-   Traverse the merkle tree from the root down, and for each\nencountered node:\n-   Check whether this node corresponds to a leaf node (transaction)\nthat is to be included OR any parent thereof:\n-   If so, append a \\'1\\' bit to the flag bits\n-   Otherwise, append a \\'0\\' bit.\n-   Check whether this node is a internal node (non-leaf) AND is the\nparent of an included leaf node:\n-   If so:\n-   Descend into its left child node, and process the\nsubtree beneath it entirely (depth-first).\n-   If this node has a right child node too, descend into it\nas well.\n-   Otherwise: append this node\\'s hash to the hash list."
    },
    {
      "header": "Parsing a partial merkle tree object {#parsing_a_partial_merkle_tree_object}",
      "content": "As the partial block message contains the number of transactions in the\nentire block, the shape of the merkle tree is known before hand. Again,\ntraverse this tree, computing traversed node\\'s hashes along the way:\n\n-   Read a bit from the flag bit list:\n-   If it is \\'0\\':\n-   Read a hash from the hashes list, and return it as this\nnode\\'s hash.\n-   If it is \\'1\\' and this is a leaf node:\n-   Read a hash from the hashes list, store it as a matched\ntxid, and return it as this node\\'s hash.\n-   If it is \\'1\\' and this is an internal node:\n-   Descend into its left child tree, and store its computed\nhash as L.\n-   If this node has a right child as well:\n-   Descend into its right child, and store its computed\nhash as R.\n-   If L == R, the partial merkle tree object is invalid.\n-   Return Hash(L \\|\\| R).\n-   If this node has no right child, return Hash(L \\|\\| L).\n\nThe partial merkle tree object is only valid if:\n\n-   All hashes in the hash list were consumed and no more.\n-   All bits in the flag bits list were consumed (except padding to make\nit into a full byte), and no more.\n-   The hash computed for the root node matches the block header\\'s\nmerkle root.\n-   The block header is valid, and matches its claimed proof of work.\n-   In two-child nodes, the hash of the left and right branches was\nnever equal."
    },
    {
      "header": "Bloom filter format {#bloom_filter_format}",
      "content": "A Bloom filter is a bit-field in which bits are set based on feeding the\ndata element to a set of different hash functions. The number of hash\nfunctions used is a parameter of the filter. In Bitcoin we use version 3\nof the 32-bit Murmur hash function. To get N \\\"different\\\" hash\nfunctions we simply initialize the Murmur algorithm with the following\nformula:\n\n`nHashNum * 0xFBA4C795 + nTweak`\n\ni.e. if the filter is initialized with 4 hash functions and a tweak of\n0x00000005, when the second function (index 1) is needed h1 would be\nequal to 4221880218.\n\nWhen loading a filter with the `filterload` command, there are two\nparameters that can be chosen. One is the size of the filter in bytes.\nThe other is the number of hash functions to use. To select the\nparameters you can use the following formulas:\n\nLet N be the number of elements you wish to insert into the set and P be\nthe probability of a false positive, where 1.0 is \\\"match everything\\\"\nand zero is unachievable.\n\nThe size S of the filter in bytes is given by\n`(-1 / pow(log(2), 2) * N * log(P)) / 8`. Of course you must ensure it\ndoes not go over the maximum size (36,000: selected as it represents a\nfilter of 20,000 items with false positive rate of \\< 0.1% or 10,000\nitems and a false positive rate of \\< 0.0001%).\n\nThe number of hash functions required is given by `S * 8 / N * log(2)`."
    },
    {
      "header": "Copyright",
      "content": "This document is placed in the public domain."
    }
  ]
}