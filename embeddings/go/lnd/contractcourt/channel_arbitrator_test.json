{
  "filepath": "../implementations/go/lnd/contractcourt/channel_arbitrator_test.go",
  "package": "contractcourt",
  "sections": [
    {
      "slug": "type mockArbitratorLog struct {",
      "content": "type mockArbitratorLog struct {\n\tstate           ArbitratorState\n\tnewStates       chan ArbitratorState\n\tfailLog         bool\n\tfailFetch       error\n\tfailCommit      bool\n\tfailCommitState ArbitratorState\n\tresolutions     *ContractResolutions\n\tresolvers       map[ContractResolver]struct{}\n\n\tcommitSet *CommitSet\n\n\tsync.Mutex\n}\n\n// A compile time check to ensure mockArbitratorLog meets the ArbitratorLog\n// interface.\nvar _ ArbitratorLog = (*mockArbitratorLog)(nil)\n",
      "length": 416,
      "tokens": 39,
      "embedding": []
    },
    {
      "slug": "func (b *mockArbitratorLog) CurrentState(kvdb.RTx) (ArbitratorState, error) {",
      "content": "func (b *mockArbitratorLog) CurrentState(kvdb.RTx) (ArbitratorState, error) {\n\treturn b.state, nil\n}\n",
      "length": 21,
      "tokens": 4,
      "embedding": []
    },
    {
      "slug": "func (b *mockArbitratorLog) CommitState(s ArbitratorState) error {",
      "content": "func (b *mockArbitratorLog) CommitState(s ArbitratorState) error {\n\tif b.failCommit && s == b.failCommitState {\n\t\treturn fmt.Errorf(\"intentional commit error at state %v\",\n\t\t\tb.failCommitState)\n\t}\n\tb.state = s\n\tb.newStates <- s\n\treturn nil\n}\n",
      "length": 167,
      "tokens": 25,
      "embedding": []
    },
    {
      "slug": "func (b *mockArbitratorLog) FetchUnresolvedContracts() ([]ContractResolver,",
      "content": "func (b *mockArbitratorLog) FetchUnresolvedContracts() ([]ContractResolver,\n\terror) {\n\n\tb.Lock()\n\tv := make([]ContractResolver, len(b.resolvers))\n\tidx := 0\n\tfor resolver := range b.resolvers {\n\t\tv[idx] = resolver\n\t\tidx++\n\t}\n\tb.Unlock()\n\n\treturn v, nil\n}\n",
      "length": 165,
      "tokens": 26,
      "embedding": []
    },
    {
      "slug": "func (b *mockArbitratorLog) InsertUnresolvedContracts(_ []*channeldb.ResolverReport,",
      "content": "func (b *mockArbitratorLog) InsertUnresolvedContracts(_ []*channeldb.ResolverReport,\n\tresolvers ...ContractResolver) error {\n\n\tb.Lock()\n\tfor _, resolver := range resolvers {\n\t\tresKey := resolver.ResolverKey()\n\t\tif resKey == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\tb.resolvers[resolver] = struct{}{}\n\t}\n\tb.Unlock()\n\treturn nil\n}\n",
      "length": 214,
      "tokens": 30,
      "embedding": []
    },
    {
      "slug": "func (b *mockArbitratorLog) SwapContract(oldContract,",
      "content": "func (b *mockArbitratorLog) SwapContract(oldContract,\n\tnewContract ContractResolver) error {\n\n\tb.Lock()\n\tdelete(b.resolvers, oldContract)\n\tb.resolvers[newContract] = struct{}{}\n\tb.Unlock()\n\n\treturn nil\n}\n",
      "length": 141,
      "tokens": 14,
      "embedding": []
    },
    {
      "slug": "func (b *mockArbitratorLog) ResolveContract(res ContractResolver) error {",
      "content": "func (b *mockArbitratorLog) ResolveContract(res ContractResolver) error {\n\tb.Lock()\n\tdelete(b.resolvers, res)\n\tb.Unlock()\n\n\treturn nil\n}\n",
      "length": 57,
      "tokens": 7,
      "embedding": []
    },
    {
      "slug": "func (b *mockArbitratorLog) LogContractResolutions(c *ContractResolutions) error {",
      "content": "func (b *mockArbitratorLog) LogContractResolutions(c *ContractResolutions) error {\n\tif b.failLog {\n\t\treturn fmt.Errorf(\"intentional log failure\")\n\t}\n\tb.resolutions = c\n\treturn nil\n}\n",
      "length": 93,
      "tokens": 14,
      "embedding": []
    },
    {
      "slug": "func (b *mockArbitratorLog) FetchContractResolutions() (*ContractResolutions, error) {",
      "content": "func (b *mockArbitratorLog) FetchContractResolutions() (*ContractResolutions, error) {\n\tif b.failFetch != nil {\n\t\treturn nil, b.failFetch\n\t}\n\n\treturn b.resolutions, nil\n}\n",
      "length": 78,
      "tokens": 13,
      "embedding": []
    },
    {
      "slug": "func (b *mockArbitratorLog) FetchChainActions() (ChainActionMap, error) {",
      "content": "func (b *mockArbitratorLog) FetchChainActions() (ChainActionMap, error) {\n\treturn nil, nil\n}\n",
      "length": 17,
      "tokens": 4,
      "embedding": []
    },
    {
      "slug": "func (b *mockArbitratorLog) InsertConfirmedCommitSet(c *CommitSet) error {",
      "content": "func (b *mockArbitratorLog) InsertConfirmedCommitSet(c *CommitSet) error {\n\tb.commitSet = c\n\treturn nil\n}\n",
      "length": 28,
      "tokens": 6,
      "embedding": []
    },
    {
      "slug": "func (b *mockArbitratorLog) FetchConfirmedCommitSet(kvdb.RTx) (*CommitSet, error) {",
      "content": "func (b *mockArbitratorLog) FetchConfirmedCommitSet(kvdb.RTx) (*CommitSet, error) {\n\treturn b.commitSet, nil\n}\n",
      "length": 25,
      "tokens": 4,
      "embedding": []
    },
    {
      "slug": "func (b *mockArbitratorLog) WipeHistory() error {",
      "content": "func (b *mockArbitratorLog) WipeHistory() error {\n\treturn nil\n}\n\n// testArbLog is a wrapper around an existing (ideally fully concrete\n// ArbitratorLog) that lets us intercept certain calls like transitioning to a\n// new state.",
      "length": 172,
      "tokens": 29,
      "embedding": []
    },
    {
      "slug": "type testArbLog struct {",
      "content": "type testArbLog struct {\n\tArbitratorLog\n\n\tnewStates chan ArbitratorState\n}\n",
      "length": 46,
      "tokens": 5,
      "embedding": []
    },
    {
      "slug": "func (t *testArbLog) CommitState(s ArbitratorState) error {",
      "content": "func (t *testArbLog) CommitState(s ArbitratorState) error {\n\tif err := t.ArbitratorLog.CommitState(s); err != nil {\n\t\treturn err\n\t}\n\n\tt.newStates <- s\n\n\treturn nil\n}\n",
      "length": 98,
      "tokens": 17,
      "embedding": []
    },
    {
      "slug": "type mockChainIO struct{}",
      "content": "type mockChainIO struct{}\n\nvar _ lnwallet.BlockChainIO = (*mockChainIO)(nil)\n",
      "length": 49,
      "tokens": 5,
      "embedding": []
    },
    {
      "slug": "func (*mockChainIO) GetBestBlock() (*chainhash.Hash, int32, error) {",
      "content": "func (*mockChainIO) GetBestBlock() (*chainhash.Hash, int32, error) {\n\treturn nil, 0, nil\n}\n",
      "length": 20,
      "tokens": 5,
      "embedding": []
    },
    {
      "slug": "func (*mockChainIO) GetUtxo(op *wire.OutPoint, _ []byte,",
      "content": "func (*mockChainIO) GetUtxo(op *wire.OutPoint, _ []byte,\n\theightHint uint32, _ <-chan struct{}) (*wire.TxOut, error) {\n\treturn nil, nil\n}\n",
      "length": 78,
      "tokens": 12,
      "embedding": []
    },
    {
      "slug": "func (*mockChainIO) GetBlockHash(blockHeight int64) (*chainhash.Hash, error) {",
      "content": "func (*mockChainIO) GetBlockHash(blockHeight int64) (*chainhash.Hash, error) {\n\treturn nil, nil\n}\n",
      "length": 17,
      "tokens": 4,
      "embedding": []
    },
    {
      "slug": "func (*mockChainIO) GetBlock(blockHash *chainhash.Hash) (*wire.MsgBlock, error) {",
      "content": "func (*mockChainIO) GetBlock(blockHash *chainhash.Hash) (*wire.MsgBlock, error) {\n\treturn nil, nil\n}\n",
      "length": 17,
      "tokens": 4,
      "embedding": []
    },
    {
      "slug": "type chanArbTestCtx struct {",
      "content": "type chanArbTestCtx struct {\n\tt *testing.T\n\n\tchanArb *ChannelArbitrator\n\n\tcleanUp func()\n\n\tresolvedChan chan struct{}\n\n\tincubationRequests chan struct{}\n\n\tresolutions chan []ResolutionMsg\n\n\tlog ArbitratorLog\n\n\tsweeper *mockSweeper\n\n\tbreachSubscribed     chan struct{}\n\tbreachResolutionChan chan struct{}\n\n\tfinalHtlcs map[uint64]bool\n}\n",
      "length": 285,
      "tokens": 28,
      "embedding": []
    },
    {
      "slug": "func (c *chanArbTestCtx) CleanUp() {",
      "content": "func (c *chanArbTestCtx) CleanUp() {\n\tif err := c.chanArb.Stop(); err != nil {\n\t\tc.t.Fatalf(\"unable to stop chan arb: %v\", err)\n\t}\n\n\tif c.cleanUp != nil {\n\t\tc.cleanUp()\n\t}\n}\n\n// AssertStateTransitions asserts that the state machine steps through the\n// passed states in order.",
      "length": 229,
      "tokens": 39,
      "embedding": []
    },
    {
      "slug": "func (c *chanArbTestCtx) AssertStateTransitions(expectedStates ...ArbitratorState) {",
      "content": "func (c *chanArbTestCtx) AssertStateTransitions(expectedStates ...ArbitratorState) {\n\tc.t.Helper()\n\n\tvar newStatesChan chan ArbitratorState\n\tswitch log := c.log.(type) {\n\tcase *mockArbitratorLog:\n\t\tnewStatesChan = log.newStates\n\n\tcase *testArbLog:\n\t\tnewStatesChan = log.newStates\n\n\tdefault:\n\t\tc.t.Fatalf(\"unable to assert state transitions with %T\", log)\n\t}\n\n\tfor _, exp := range expectedStates {\n\t\tvar state ArbitratorState\n\t\tselect {\n\t\tcase state = <-newStatesChan:\n\t\tcase <-time.After(defaultTimeout):\n\t\t\tc.t.Fatalf(\"new state not received\")\n\t\t}\n\n\t\tif state != exp {\n\t\t\tc.t.Fatalf(\"expected new state %v, got %v\", exp, state)\n\t\t}\n\t}\n}\n\n// AssertState checks that the ChannelArbitrator is in the state we expect it\n// to be.",
      "length": 612,
      "tokens": 85,
      "embedding": []
    },
    {
      "slug": "func (c *chanArbTestCtx) AssertState(expected ArbitratorState) {",
      "content": "func (c *chanArbTestCtx) AssertState(expected ArbitratorState) {\n\tif c.chanArb.state != expected {\n\t\tc.t.Fatalf(\"expected state %v, was %v\", expected, c.chanArb.state)\n\t}\n}\n\n// Restart simulates a clean restart of the channel arbitrator, forcing it to\n// walk through it's recovery logic. If this function returns nil, then a\n// restart was successful. Note that the restart process keeps the log in\n// place, in order to simulate proper persistence of the log. The caller can\n// optionally provide a restart closure which will be executed before the\n// resolver is started again, but after it is created.",
      "length": 530,
      "tokens": 89,
      "embedding": []
    },
    {
      "slug": "func (c *chanArbTestCtx) Restart(restartClosure func(*chanArbTestCtx)) (*chanArbTestCtx, error) {",
      "content": "func (c *chanArbTestCtx) Restart(restartClosure func(*chanArbTestCtx)) (*chanArbTestCtx, error) {\n\tif err := c.chanArb.Stop(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tnewCtx, err := createTestChannelArbitrator(c.t, c.log)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif restartClosure != nil {\n\t\trestartClosure(newCtx)\n\t}\n\n\tif err := newCtx.chanArb.Start(nil); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn newCtx, nil\n}\n\n// testChanArbOption applies custom settings to a channel arbitrator config for\n// testing purposes.",
      "length": 391,
      "tokens": 63,
      "embedding": []
    },
    {
      "slug": "type testChanArbOption func(cfg *ChannelArbitratorConfig)",
      "content": "type testChanArbOption func(cfg *ChannelArbitratorConfig)\n\n// remoteInitiatorOption sets the MarkChannelClosed function in the\n// Channel Arbitrator's config.",
      "length": 98,
      "tokens": 12,
      "embedding": []
    },
    {
      "slug": "func withMarkClosed(markClosed func(*channeldb.ChannelCloseSummary,",
      "content": "func withMarkClosed(markClosed func(*channeldb.ChannelCloseSummary,\n\t...channeldb.ChannelStatus) error) testChanArbOption {\n\n\treturn func(cfg *ChannelArbitratorConfig) {\n\t\tcfg.MarkChannelClosed = markClosed\n\t}\n}\n\n// createTestChannelArbitrator returns a channel arbitrator test context which\n// contains a channel arbitrator with default values. These values can be\n// changed by providing options which overwrite the default config.",
      "length": 356,
      "tokens": 44,
      "embedding": []
    },
    {
      "slug": "func createTestChannelArbitrator(t *testing.T, log ArbitratorLog,",
      "content": "func createTestChannelArbitrator(t *testing.T, log ArbitratorLog,\n\topts ...testChanArbOption) (*chanArbTestCtx, error) {\n\n\tchanArbCtx := &chanArbTestCtx{\n\t\tbreachSubscribed: make(chan struct{}),\n\t\tfinalHtlcs:       make(map[uint64]bool),\n\t}\n\n\tchanPoint := wire.OutPoint{}\n\tshortChanID := lnwire.ShortChannelID{}\n\tchanEvents := &ChainEventSubscription{\n\t\tRemoteUnilateralClosure: make(chan *RemoteUnilateralCloseInfo, 1),\n\t\tLocalUnilateralClosure:  make(chan *LocalUnilateralCloseInfo, 1),\n\t\tCooperativeClosure:      make(chan *CooperativeCloseInfo, 1),\n\t\tContractBreach:          make(chan *BreachCloseInfo, 1),\n\t}\n\n\tresolutionChan := make(chan []ResolutionMsg, 1)\n\tincubateChan := make(chan struct{})\n\n\tchainIO := &mockChainIO{}\n\tmockSweeper := newMockSweeper()\n\tchainArbCfg := ChainArbitratorConfig{\n\t\tChainIO: chainIO,\n\t\tPublishTx: func(*wire.MsgTx, string) error {\n\t\t\treturn nil\n\t\t},\n\t\tDeliverResolutionMsg: func(msgs ...ResolutionMsg) error {\n\t\t\tresolutionChan <- msgs\n\t\t\treturn nil\n\t\t},\n\t\tOutgoingBroadcastDelta: 5,\n\t\tIncomingBroadcastDelta: 5,\n\t\tNotifier: &mock.ChainNotifier{\n\t\t\tEpochChan: make(chan *chainntnfs.BlockEpoch),\n\t\t\tSpendChan: make(chan *chainntnfs.SpendDetail),\n\t\t\tConfChan:  make(chan *chainntnfs.TxConfirmation),\n\t\t},\n\t\tIncubateOutputs: func(wire.OutPoint,\n\t\t\t*lnwallet.OutgoingHtlcResolution,\n\t\t\t*lnwallet.IncomingHtlcResolution, uint32) error {\n\n\t\t\tincubateChan <- struct{}{}\n\t\t\treturn nil\n\t\t},\n\t\tOnionProcessor: &mockOnionProcessor{},\n\t\tIsForwardedHTLC: func(chanID lnwire.ShortChannelID,\n\t\t\thtlcIndex uint64) bool {\n\n\t\t\treturn true\n\t\t},\n\t\tSubscribeBreachComplete: func(op *wire.OutPoint,\n\t\t\tc chan struct{}) (bool, error) {\n\n\t\t\tchanArbCtx.breachResolutionChan = c\n\t\t\tchanArbCtx.breachSubscribed <- struct{}{}\n\t\t\treturn false, nil\n\t\t},\n\t\tClock:        clock.NewDefaultClock(),\n\t\tSweeper:      mockSweeper,\n\t\tHtlcNotifier: &mockHTLCNotifier{},\n\t\tPutFinalHtlcOutcome: func(chanId lnwire.ShortChannelID,\n\t\t\thtlcId uint64, settled bool) error {\n\n\t\t\tchanArbCtx.finalHtlcs[htlcId] = settled\n\n\t\t\treturn nil\n\t\t},\n\t}\n\n\t// We'll use the resolvedChan to synchronize on call to\n\t// MarkChannelResolved.\n\tresolvedChan := make(chan struct{}, 1)\n\n\t// Next we'll create the matching configuration struct that contains\n\t// all interfaces and methods the arbitrator needs to do its job.\n\tarbCfg := &ChannelArbitratorConfig{\n\t\tChanPoint:   chanPoint,\n\t\tShortChanID: shortChanID,\n\t\tMarkChannelResolved: func() error {\n\t\t\tresolvedChan <- struct{}{}\n\t\t\treturn nil\n\t\t},\n\t\tChannel: &mockChannel{},\n\t\tMarkCommitmentBroadcasted: func(_ *wire.MsgTx, _ bool) error {\n\t\t\treturn nil\n\t\t},\n\t\tMarkChannelClosed: func(*channeldb.ChannelCloseSummary,\n\t\t\t...channeldb.ChannelStatus) error {\n\n\t\t\treturn nil\n\t\t},\n\t\tIsPendingClose:        false,\n\t\tChainArbitratorConfig: chainArbCfg,\n\t\tChainEvents:           chanEvents,\n\t\tPutResolverReport: func(_ kvdb.RwTx,\n\t\t\t_ *channeldb.ResolverReport) error {\n\n\t\t\treturn nil\n\t\t},\n\t\tFetchHistoricalChannel: func() (*channeldb.OpenChannel, error) {\n\t\t\treturn &channeldb.OpenChannel{}, nil\n\t\t},\n\t}\n\n\t// Apply all custom options to the config struct.\n\tfor _, option := range opts {\n\t\toption(arbCfg)\n\t}\n\n\tvar cleanUp func()\n\tif log == nil {\n\t\tdbPath := filepath.Join(t.TempDir(), \"testdb\")\n\t\tdb, err := kvdb.Create(\n\t\t\tkvdb.BoltBackendName, dbPath, true,\n\t\t\tkvdb.DefaultDBTimeout,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tbackingLog, err := newBoltArbitratorLog(\n\t\t\tdb, *arbCfg, chainhash.Hash{}, chanPoint,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tcleanUp = func() {\n\t\t\tdb.Close()\n\t\t}\n\n\t\tlog = &testArbLog{\n\t\t\tArbitratorLog: backingLog,\n\t\t\tnewStates:     make(chan ArbitratorState),\n\t\t}\n\t}\n\n\thtlcSets := make(map[HtlcSetKey]htlcSet)\n\n\tchanArb := NewChannelArbitrator(*arbCfg, htlcSets, log)\n\n\tchanArbCtx.t = t\n\tchanArbCtx.chanArb = chanArb\n\tchanArbCtx.cleanUp = cleanUp\n\tchanArbCtx.resolvedChan = resolvedChan\n\tchanArbCtx.resolutions = resolutionChan\n\tchanArbCtx.log = log\n\tchanArbCtx.incubationRequests = incubateChan\n\tchanArbCtx.sweeper = mockSweeper\n\n\treturn chanArbCtx, nil\n}\n\n// TestChannelArbitratorCooperativeClose tests that the ChannelArbitertor\n// correctly marks the channel resolved in case a cooperative close is\n// confirmed.",
      "length": 3955,
      "tokens": 401,
      "embedding": []
    },
    {
      "slug": "func TestChannelArbitratorCooperativeClose(t *testing.T) {",
      "content": "func TestChannelArbitratorCooperativeClose(t *testing.T) {\n\tlog := &mockArbitratorLog{\n\t\tstate:     StateDefault,\n\t\tnewStates: make(chan ArbitratorState, 5),\n\t}\n\n\tchanArbCtx, err := createTestChannelArbitrator(t, log)\n\trequire.NoError(t, err, \"unable to create ChannelArbitrator\")\n\n\tif err := chanArbCtx.chanArb.Start(nil); err != nil {\n\t\tt.Fatalf(\"unable to start ChannelArbitrator: %v\", err)\n\t}\n\tt.Cleanup(func() {\n\t\trequire.NoError(t, chanArbCtx.chanArb.Stop())\n\t})\n\n\t// It should start out in the default state.\n\tchanArbCtx.AssertState(StateDefault)\n\n\t// We set up a channel to detect when MarkChannelClosed is called.\n\tcloseInfos := make(chan *channeldb.ChannelCloseSummary)\n\tchanArbCtx.chanArb.cfg.MarkChannelClosed = func(\n\t\tcloseInfo *channeldb.ChannelCloseSummary,\n\t\tstatuses ...channeldb.ChannelStatus) error {\n\n\t\tcloseInfos <- closeInfo\n\t\treturn nil\n\t}\n\n\t// Cooperative close should do trigger a MarkChannelClosed +\n\t// MarkChannelResolved.\n\tcloseInfo := &CooperativeCloseInfo{\n\t\t&channeldb.ChannelCloseSummary{},\n\t}\n\tchanArbCtx.chanArb.cfg.ChainEvents.CooperativeClosure <- closeInfo\n\n\tselect {\n\tcase c := <-closeInfos:\n\t\tif c.CloseType != channeldb.CooperativeClose {\n\t\t\tt.Fatalf(\"expected cooperative close, got %v\", c.CloseType)\n\t\t}\n\tcase <-time.After(defaultTimeout):\n\t\tt.Fatalf(\"timeout waiting for channel close\")\n\t}\n\n\t// It should mark the channel as resolved.\n\tselect {\n\tcase <-chanArbCtx.resolvedChan:\n\t\t// Expected.\n\tcase <-time.After(defaultTimeout):\n\t\tt.Fatalf(\"contract was not resolved\")\n\t}\n}\n\n// TestChannelArbitratorRemoteForceClose checks that the ChannelArbitrator goes\n// through the expected states if a remote force close is observed in the\n// chain.",
      "length": 1569,
      "tokens": 172,
      "embedding": []
    },
    {
      "slug": "func TestChannelArbitratorRemoteForceClose(t *testing.T) {",
      "content": "func TestChannelArbitratorRemoteForceClose(t *testing.T) {\n\tlog := &mockArbitratorLog{\n\t\tstate:     StateDefault,\n\t\tnewStates: make(chan ArbitratorState, 5),\n\t}\n\n\tchanArbCtx, err := createTestChannelArbitrator(t, log)\n\trequire.NoError(t, err, \"unable to create ChannelArbitrator\")\n\tchanArb := chanArbCtx.chanArb\n\n\tif err := chanArb.Start(nil); err != nil {\n\t\tt.Fatalf(\"unable to start ChannelArbitrator: %v\", err)\n\t}\n\tdefer chanArb.Stop()\n\n\t// It should start out in the default state.\n\tchanArbCtx.AssertState(StateDefault)\n\n\t// Send a remote force close event.\n\tcommitSpend := &chainntnfs.SpendDetail{\n\t\tSpenderTxHash: &chainhash.Hash{},\n\t}\n\n\tuniClose := &lnwallet.UnilateralCloseSummary{\n\t\tSpendDetail:     commitSpend,\n\t\tHtlcResolutions: &lnwallet.HtlcResolutions{},\n\t}\n\tchanArb.cfg.ChainEvents.RemoteUnilateralClosure <- &RemoteUnilateralCloseInfo{\n\t\tUnilateralCloseSummary: uniClose,\n\t\tCommitSet: CommitSet{\n\t\t\tConfCommitKey: &RemoteHtlcSet,\n\t\t\tHtlcSets:      make(map[HtlcSetKey][]channeldb.HTLC),\n\t\t},\n\t}\n\n\t// It should transition StateDefault -> StateContractClosed ->\n\t// StateFullyResolved.\n\tchanArbCtx.AssertStateTransitions(\n\t\tStateContractClosed, StateFullyResolved,\n\t)\n\n\t// It should also mark the channel as resolved.\n\tselect {\n\tcase <-chanArbCtx.resolvedChan:\n\t\t// Expected.\n\tcase <-time.After(defaultTimeout):\n\t\tt.Fatalf(\"contract was not resolved\")\n\t}\n}\n\n// TestChannelArbitratorLocalForceClose tests that the ChannelArbitrator goes\n// through the expected states in case we request it to force close the channel,\n// and the local force close event is observed in chain.",
      "length": 1478,
      "tokens": 155,
      "embedding": []
    },
    {
      "slug": "func TestChannelArbitratorLocalForceClose(t *testing.T) {",
      "content": "func TestChannelArbitratorLocalForceClose(t *testing.T) {\n\tlog := &mockArbitratorLog{\n\t\tstate:     StateDefault,\n\t\tnewStates: make(chan ArbitratorState, 5),\n\t}\n\n\tchanArbCtx, err := createTestChannelArbitrator(t, log)\n\trequire.NoError(t, err, \"unable to create ChannelArbitrator\")\n\tchanArb := chanArbCtx.chanArb\n\n\tif err := chanArb.Start(nil); err != nil {\n\t\tt.Fatalf(\"unable to start ChannelArbitrator: %v\", err)\n\t}\n\tdefer chanArb.Stop()\n\n\t// It should start out in the default state.\n\tchanArbCtx.AssertState(StateDefault)\n\n\t// We create a channel we can use to pause the ChannelArbitrator at the\n\t// point where it broadcasts the close tx, and check its state.\n\tstateChan := make(chan ArbitratorState)\n\tchanArb.cfg.PublishTx = func(*wire.MsgTx, string) error {\n\t\t// When the force close tx is being broadcasted, check that the\n\t\t// state is correct at that point.\n\t\tselect {\n\t\tcase stateChan <- chanArb.state:\n\t\tcase <-chanArb.quit:\n\t\t\treturn fmt.Errorf(\"exiting\")\n\t\t}\n\t\treturn nil\n\t}\n\n\terrChan := make(chan error, 1)\n\trespChan := make(chan *wire.MsgTx, 1)\n\n\t// With the channel found, and the request crafted, we'll send over a\n\t// force close request to the arbitrator that watches this channel.\n\tchanArb.forceCloseReqs <- &forceCloseReq{\n\t\terrResp: errChan,\n\t\tcloseTx: respChan,\n\t}\n\n\t// It should transition to StateBroadcastCommit.\n\tchanArbCtx.AssertStateTransitions(StateBroadcastCommit)\n\n\t// When it is broadcasting the force close, its state should be\n\t// StateBroadcastCommit.\n\tselect {\n\tcase state := <-stateChan:\n\t\tif state != StateBroadcastCommit {\n\t\t\tt.Fatalf(\"state during PublishTx was %v\", state)\n\t\t}\n\tcase <-time.After(stateTimeout):\n\t\tt.Fatalf(\"did not get state update\")\n\t}\n\n\t// After broadcasting, transition should be to\n\t// StateCommitmentBroadcasted.\n\tchanArbCtx.AssertStateTransitions(StateCommitmentBroadcasted)\n\n\tselect {\n\tcase <-respChan:\n\tcase <-time.After(defaultTimeout):\n\t\tt.Fatalf(\"no response received\")\n\t}\n\n\tselect {\n\tcase err := <-errChan:\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"error force closing channel: %v\", err)\n\t\t}\n\tcase <-time.After(defaultTimeout):\n\t\tt.Fatalf(\"no response received\")\n\t}\n\n\t// After broadcasting the close tx, it should be in state\n\t// StateCommitmentBroadcasted.\n\tchanArbCtx.AssertState(StateCommitmentBroadcasted)\n\n\t// Now notify about the local force close getting confirmed.\n\tchanArb.cfg.ChainEvents.LocalUnilateralClosure <- &LocalUnilateralCloseInfo{\n\t\tSpendDetail: &chainntnfs.SpendDetail{},\n\t\tLocalForceCloseSummary: &lnwallet.LocalForceCloseSummary{\n\t\t\tCloseTx:         &wire.MsgTx{},\n\t\t\tHtlcResolutions: &lnwallet.HtlcResolutions{},\n\t\t},\n\t\tChannelCloseSummary: &channeldb.ChannelCloseSummary{},\n\t}\n\n\t// It should transition StateContractClosed -> StateFullyResolved.\n\tchanArbCtx.AssertStateTransitions(StateContractClosed, StateFullyResolved)\n\n\t// It should also mark the channel as resolved.\n\tselect {\n\tcase <-chanArbCtx.resolvedChan:\n\t\t// Expected.\n\tcase <-time.After(defaultTimeout):\n\t\tt.Fatalf(\"contract was not resolved\")\n\t}\n}\n\n// TestChannelArbitratorBreachClose tests that the ChannelArbitrator goes\n// through the expected states in case we notice a breach in the chain, and\n// is able to properly progress the breachResolver and anchorResolver to a\n// successful resolution.",
      "length": 3083,
      "tokens": 361,
      "embedding": []
    },
    {
      "slug": "func TestChannelArbitratorBreachClose(t *testing.T) {",
      "content": "func TestChannelArbitratorBreachClose(t *testing.T) {\n\tlog := &mockArbitratorLog{\n\t\tstate:     StateDefault,\n\t\tnewStates: make(chan ArbitratorState, 5),\n\t\tresolvers: make(map[ContractResolver]struct{}),\n\t}\n\n\tchanArbCtx, err := createTestChannelArbitrator(t, log)\n\trequire.NoError(t, err, \"unable to create ChannelArbitrator\")\n\tchanArb := chanArbCtx.chanArb\n\tchanArb.cfg.PreimageDB = newMockWitnessBeacon()\n\tchanArb.cfg.Registry = &mockRegistry{}\n\n\tif err := chanArb.Start(nil); err != nil {\n\t\tt.Fatalf(\"unable to start ChannelArbitrator: %v\", err)\n\t}\n\tt.Cleanup(func() {\n\t\trequire.NoError(t, chanArb.Stop())\n\t})\n\n\t// It should start out in the default state.\n\tchanArbCtx.AssertState(StateDefault)\n\n\t// We create two HTLCs, one incoming and one outgoing. We will later\n\t// assert that we only receive a ResolutionMsg for the outgoing HTLC.\n\toutgoingIdx := uint64(2)\n\n\trHash1 := [lntypes.PreimageSize]byte{1, 2, 3}\n\thtlc1 := channeldb.HTLC{\n\t\tRHash:       rHash1,\n\t\tOutputIndex: 2,\n\t\tIncoming:    false,\n\t\tHtlcIndex:   outgoingIdx,\n\t\tLogIndex:    2,\n\t}\n\n\trHash2 := [lntypes.PreimageSize]byte{2, 2, 2}\n\thtlc2 := channeldb.HTLC{\n\t\tRHash:       rHash2,\n\t\tOutputIndex: 3,\n\t\tIncoming:    true,\n\t\tHtlcIndex:   3,\n\t\tLogIndex:    3,\n\t}\n\n\tanchorRes := &lnwallet.AnchorResolution{\n\t\tAnchorSignDescriptor: input.SignDescriptor{\n\t\t\tOutput: &wire.TxOut{Value: 1},\n\t\t},\n\t}\n\n\t// Create the BreachCloseInfo that the chain_watcher would normally\n\t// send to the channel_arbitrator.\n\tbreachInfo := &BreachCloseInfo{\n\t\tBreachResolution: &BreachResolution{\n\t\t\tFundingOutPoint: wire.OutPoint{},\n\t\t},\n\t\tAnchorResolution: anchorRes,\n\t\tCommitSet: CommitSet{\n\t\t\tConfCommitKey: &RemoteHtlcSet,\n\t\t\tHtlcSets: map[HtlcSetKey][]channeldb.HTLC{\n\t\t\t\tRemoteHtlcSet: {htlc1, htlc2},\n\t\t\t},\n\t\t},\n\t\tCommitHash: chainhash.Hash{},\n\t}\n\n\t// Send a breach close event.\n\tchanArb.cfg.ChainEvents.ContractBreach <- breachInfo\n\n\t// It should transition StateDefault -> StateContractClosed.\n\tchanArbCtx.AssertStateTransitions(StateContractClosed)\n\n\t// We should receive one ResolutionMsg as there was only one outgoing\n\t// HTLC at the time of the breach.\n\tselect {\n\tcase res := <-chanArbCtx.resolutions:\n\t\trequire.Equal(t, 1, len(res))\n\t\trequire.Equal(t, outgoingIdx, res[0].HtlcIndex)\n\tcase <-time.After(5 * time.Second):\n\t\tt.Fatal(\"expected to receive a resolution msg\")\n\t}\n\n\t// We should now transition from StateContractClosed to\n\t// StateWaitingFullResolution.\n\tchanArbCtx.AssertStateTransitions(StateWaitingFullResolution)\n\n\t// One of the resolvers should be an anchor resolver and the other\n\t// should be a breach resolver.\n\trequire.Equal(t, 2, len(chanArb.activeResolvers))\n\n\tvar anchorExists, breachExists bool\n\tfor _, resolver := range chanArb.activeResolvers {\n\t\tswitch resolver.(type) {\n\t\tcase *anchorResolver:\n\t\t\tanchorExists = true\n\t\tcase *breachResolver:\n\t\t\tbreachExists = true\n\t\tdefault:\n\t\t\tt.Fatalf(\"did not expect resolver %T\", resolver)\n\t\t}\n\t}\n\trequire.True(t, anchorExists && breachExists)\n\n\t// The anchor resolver is expected to re-offer the anchor input to the\n\t// sweeper.\n\t<-chanArbCtx.sweeper.sweptInputs\n\n\t// Wait for SubscribeBreachComplete to be called.\n\t<-chanArbCtx.breachSubscribed\n\n\t// We'll now close the breach channel so that the state transitions to\n\t// StateFullyResolved.\n\tclose(chanArbCtx.breachResolutionChan)\n\n\tchanArbCtx.AssertStateTransitions(StateFullyResolved)\n\n\t// It should also mark the channel as resolved.\n\tselect {\n\tcase <-chanArbCtx.resolvedChan:\n\t\t// Expected.\n\tcase <-time.After(defaultTimeout):\n\t\tt.Fatalf(\"contract was not resolved\")\n\t}\n}\n\n// TestChannelArbitratorLocalForceClosePendingHtlc tests that the\n// ChannelArbitrator goes through the expected states in case we request it to\n// force close a channel that still has an HTLC pending.",
      "length": 3567,
      "tokens": 399,
      "embedding": []
    },
    {
      "slug": "func TestChannelArbitratorLocalForceClosePendingHtlc(t *testing.T) {",
      "content": "func TestChannelArbitratorLocalForceClosePendingHtlc(t *testing.T) {\n\t// We create a new test context for this channel arb, notice that we\n\t// pass in a nil ArbitratorLog which means that a default one backed by\n\t// a real DB will be created. We need this for our test as we want to\n\t// test proper restart recovery and resolver population.\n\tchanArbCtx, err := createTestChannelArbitrator(t, nil)\n\trequire.NoError(t, err, \"unable to create ChannelArbitrator\")\n\tchanArb := chanArbCtx.chanArb\n\tchanArb.cfg.PreimageDB = newMockWitnessBeacon()\n\tchanArb.cfg.Registry = &mockRegistry{}\n\n\tif err := chanArb.Start(nil); err != nil {\n\t\tt.Fatalf(\"unable to start ChannelArbitrator: %v\", err)\n\t}\n\tdefer chanArb.Stop()\n\n\tsignals := &ContractSignals{\n\t\tShortChanID: lnwire.ShortChannelID{},\n\t}\n\tchanArb.UpdateContractSignals(signals)\n\n\t// Add HTLC to channel arbitrator.\n\thtlcAmt := 10000\n\thtlc := channeldb.HTLC{\n\t\tIncoming:  false,\n\t\tAmt:       lnwire.MilliSatoshi(htlcAmt),\n\t\tHtlcIndex: 99,\n\t}\n\n\toutgoingDustHtlc := channeldb.HTLC{\n\t\tIncoming:    false,\n\t\tAmt:         100,\n\t\tHtlcIndex:   100,\n\t\tOutputIndex: -1,\n\t}\n\n\tincomingDustHtlc := channeldb.HTLC{\n\t\tIncoming:    true,\n\t\tAmt:         105,\n\t\tHtlcIndex:   101,\n\t\tOutputIndex: -1,\n\t}\n\n\thtlcSet := []channeldb.HTLC{\n\t\thtlc, outgoingDustHtlc, incomingDustHtlc,\n\t}\n\n\tnewUpdate := &ContractUpdate{\n\t\tHtlcKey: LocalHtlcSet,\n\t\tHtlcs:   htlcSet,\n\t}\n\tchanArb.notifyContractUpdate(newUpdate)\n\n\terrChan := make(chan error, 1)\n\trespChan := make(chan *wire.MsgTx, 1)\n\n\t// With the channel found, and the request crafted, we'll send over a\n\t// force close request to the arbitrator that watches this channel.\n\tchanArb.forceCloseReqs <- &forceCloseReq{\n\t\terrResp: errChan,\n\t\tcloseTx: respChan,\n\t}\n\n\t// The force close request should trigger broadcast of the commitment\n\t// transaction.\n\tchanArbCtx.AssertStateTransitions(\n\t\tStateBroadcastCommit,\n\t\tStateCommitmentBroadcasted,\n\t)\n\tselect {\n\tcase <-respChan:\n\tcase <-time.After(defaultTimeout):\n\t\tt.Fatalf(\"no response received\")\n\t}\n\n\tselect {\n\tcase err := <-errChan:\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"error force closing channel: %v\", err)\n\t\t}\n\tcase <-time.After(defaultTimeout):\n\t\tt.Fatalf(\"no response received\")\n\t}\n\n\t// Now notify about the local force close getting confirmed.\n\tcloseTx := &wire.MsgTx{\n\t\tTxIn: []*wire.TxIn{\n\t\t\t{\n\t\t\t\tPreviousOutPoint: wire.OutPoint{},\n\t\t\t\tWitness: [][]byte{\n\t\t\t\t\t{0x1},\n\t\t\t\t\t{0x2},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\thtlcOp := wire.OutPoint{\n\t\tHash:  closeTx.TxHash(),\n\t\tIndex: 0,\n\t}\n\n\t// Set up the outgoing resolution. Populate SignedTimeoutTx because our\n\t// commitment transaction got confirmed.\n\toutgoingRes := lnwallet.OutgoingHtlcResolution{\n\t\tExpiry: 10,\n\t\tSweepSignDesc: input.SignDescriptor{\n\t\t\tOutput: &wire.TxOut{},\n\t\t},\n\t\tSignedTimeoutTx: &wire.MsgTx{\n\t\t\tTxIn: []*wire.TxIn{\n\t\t\t\t{\n\t\t\t\t\tPreviousOutPoint: htlcOp,\n\t\t\t\t\tWitness:          [][]byte{{}},\n\t\t\t\t},\n\t\t\t},\n\t\t\tTxOut: []*wire.TxOut{\n\t\t\t\t{},\n\t\t\t},\n\t\t},\n\t}\n\n\tchanArb.cfg.ChainEvents.LocalUnilateralClosure <- &LocalUnilateralCloseInfo{\n\t\tSpendDetail: &chainntnfs.SpendDetail{},\n\t\tLocalForceCloseSummary: &lnwallet.LocalForceCloseSummary{\n\t\t\tCloseTx: closeTx,\n\t\t\tHtlcResolutions: &lnwallet.HtlcResolutions{\n\t\t\t\tOutgoingHTLCs: []lnwallet.OutgoingHtlcResolution{\n\t\t\t\t\toutgoingRes,\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\tChannelCloseSummary: &channeldb.ChannelCloseSummary{},\n\t\tCommitSet: CommitSet{\n\t\t\tConfCommitKey: &LocalHtlcSet,\n\t\t\tHtlcSets: map[HtlcSetKey][]channeldb.HTLC{\n\t\t\t\tLocalHtlcSet: htlcSet,\n\t\t\t},\n\t\t},\n\t}\n\n\tchanArbCtx.AssertStateTransitions(\n\t\tStateContractClosed,\n\t\tStateWaitingFullResolution,\n\t)\n\n\t// We expect an immediate resolution message for the outgoing dust htlc.\n\t// It is not resolvable on-chain.\n\tselect {\n\tcase msgs := <-chanArbCtx.resolutions:\n\t\tif len(msgs) != 1 {\n\t\t\tt.Fatalf(\"expected 1 message, instead got %v\", len(msgs))\n\t\t}\n\n\t\tif msgs[0].HtlcIndex != outgoingDustHtlc.HtlcIndex {\n\t\t\tt.Fatalf(\"wrong htlc index: expected %v, got %v\",\n\t\t\t\toutgoingDustHtlc.HtlcIndex, msgs[0].HtlcIndex)\n\t\t}\n\tcase <-time.After(defaultTimeout):\n\t\tt.Fatalf(\"resolution msgs not sent\")\n\t}\n\n\t// We'll grab the old notifier here as our resolvers are still holding\n\t// a reference to this instance, and a new one will be created when we\n\t// restart the channel arb below.\n\toldNotifier := chanArb.cfg.Notifier.(*mock.ChainNotifier)\n\n\t// At this point, in order to simulate a restart, we'll re-create the\n\t// channel arbitrator. We do this to ensure that all information\n\t// required to properly resolve this HTLC are populated.\n\tif err := chanArb.Stop(); err != nil {\n\t\tt.Fatalf(\"unable to stop chan arb: %v\", err)\n\t}\n\n\t// Assert that a final resolution was stored for the incoming dust htlc.\n\texpectedFinalHtlcs := map[uint64]bool{\n\t\tincomingDustHtlc.HtlcIndex: false,\n\t}\n\trequire.Equal(t, expectedFinalHtlcs, chanArbCtx.finalHtlcs)\n\n\t// We'll no re-create the resolver, notice that we use the existing\n\t// arbLog so it carries over the same on-disk state.\n\tchanArbCtxNew, err := chanArbCtx.Restart(nil)\n\trequire.NoError(t, err, \"unable to create ChannelArbitrator\")\n\tchanArb = chanArbCtxNew.chanArb\n\tdefer chanArbCtxNew.CleanUp()\n\n\t// Post restart, it should be the case that our resolver was properly\n\t// supplemented, and we only have a single resolver in the final set.\n\tif len(chanArb.activeResolvers) != 1 {\n\t\tt.Fatalf(\"expected single resolver, instead got: %v\",\n\t\t\tlen(chanArb.activeResolvers))\n\t}\n\n\t// We'll now examine the in-memory state of the active resolvers to\n\t// ensure t hey were populated properly.\n\tresolver := chanArb.activeResolvers[0]\n\toutgoingResolver, ok := resolver.(*htlcOutgoingContestResolver)\n\tif !ok {\n\t\tt.Fatalf(\"expected outgoing contest resolver, got %vT\",\n\t\t\tresolver)\n\t}\n\n\t// The resolver should have its htlc amt field populated as it.\n\tif int64(outgoingResolver.htlc.Amt) != int64(htlcAmt) {\n\t\tt.Fatalf(\"wrong htlc amount: expected %v, got %v,\",\n\t\t\thtlcAmt, int64(outgoingResolver.htlc.Amt))\n\t}\n\n\t// htlcOutgoingContestResolver is now active and waiting for the HTLC to\n\t// expire. It should not yet have passed it on for incubation.\n\tselect {\n\tcase <-chanArbCtx.incubationRequests:\n\t\tt.Fatalf(\"contract should not be incubated yet\")\n\tdefault:\n\t}\n\n\t// Send a notification that the expiry height has been reached.\n\toldNotifier.EpochChan <- &chainntnfs.BlockEpoch{Height: 10}\n\n\t// htlcOutgoingContestResolver is now transforming into a\n\t// htlcTimeoutResolver and should send the contract off for incubation.\n\tselect {\n\tcase <-chanArbCtx.incubationRequests:\n\tcase <-time.After(defaultTimeout):\n\t\tt.Fatalf(\"no response received\")\n\t}\n\n\t// Notify resolver that the HTLC output of the commitment has been\n\t// spent.\n\toldNotifier.SpendChan <- &chainntnfs.SpendDetail{SpendingTx: closeTx}\n\n\t// Finally, we should also receive a resolution message instructing the\n\t// switch to cancel back the HTLC.\n\tselect {\n\tcase msgs := <-chanArbCtx.resolutions:\n\t\tif len(msgs) != 1 {\n\t\t\tt.Fatalf(\"expected 1 message, instead got %v\", len(msgs))\n\t\t}\n\n\t\tif msgs[0].HtlcIndex != htlc.HtlcIndex {\n\t\t\tt.Fatalf(\"wrong htlc index: expected %v, got %v\",\n\t\t\t\thtlc.HtlcIndex, msgs[0].HtlcIndex)\n\t\t}\n\tcase <-time.After(defaultTimeout):\n\t\tt.Fatalf(\"resolution msgs not sent\")\n\t}\n\n\t// As this is our own commitment transaction, the HTLC will go through\n\t// to the second level. Channel arbitrator should still not be marked\n\t// as resolved.\n\tselect {\n\tcase <-chanArbCtxNew.resolvedChan:\n\t\tt.Fatalf(\"channel resolved prematurely\")\n\tdefault:\n\t}\n\n\t// Notify resolver that the second level transaction is spent.\n\toldNotifier.SpendChan <- &chainntnfs.SpendDetail{SpendingTx: closeTx}\n\n\t// At this point channel should be marked as resolved.\n\tchanArbCtxNew.AssertStateTransitions(StateFullyResolved)\n\tselect {\n\tcase <-chanArbCtxNew.resolvedChan:\n\tcase <-time.After(defaultTimeout):\n\t\tt.Fatalf(\"contract was not resolved\")\n\t}\n}\n\n// TestChannelArbitratorLocalForceCloseRemoteConfiremd tests that the\n// ChannelArbitrator behaves as expected in the case where we request a local\n// force close, but a remote commitment ends up being confirmed in chain.",
      "length": 7660,
      "tokens": 925,
      "embedding": []
    },
    {
      "slug": "func TestChannelArbitratorLocalForceCloseRemoteConfirmed(t *testing.T) {",
      "content": "func TestChannelArbitratorLocalForceCloseRemoteConfirmed(t *testing.T) {\n\tlog := &mockArbitratorLog{\n\t\tstate:     StateDefault,\n\t\tnewStates: make(chan ArbitratorState, 5),\n\t}\n\n\tchanArbCtx, err := createTestChannelArbitrator(t, log)\n\trequire.NoError(t, err, \"unable to create ChannelArbitrator\")\n\tchanArb := chanArbCtx.chanArb\n\n\tif err := chanArb.Start(nil); err != nil {\n\t\tt.Fatalf(\"unable to start ChannelArbitrator: %v\", err)\n\t}\n\tdefer chanArb.Stop()\n\n\t// It should start out in the default state.\n\tchanArbCtx.AssertState(StateDefault)\n\n\t// Create a channel we can use to assert the state when it publishes\n\t// the close tx.\n\tstateChan := make(chan ArbitratorState)\n\tchanArb.cfg.PublishTx = func(*wire.MsgTx, string) error {\n\t\t// When the force close tx is being broadcasted, check that the\n\t\t// state is correct at that point.\n\t\tselect {\n\t\tcase stateChan <- chanArb.state:\n\t\tcase <-chanArb.quit:\n\t\t\treturn fmt.Errorf(\"exiting\")\n\t\t}\n\t\treturn nil\n\t}\n\n\terrChan := make(chan error, 1)\n\trespChan := make(chan *wire.MsgTx, 1)\n\n\t// With the channel found, and the request crafted, we'll send over a\n\t// force close request to the arbitrator that watches this channel.\n\tchanArb.forceCloseReqs <- &forceCloseReq{\n\t\terrResp: errChan,\n\t\tcloseTx: respChan,\n\t}\n\n\t// It should transition to StateBroadcastCommit.\n\tchanArbCtx.AssertStateTransitions(StateBroadcastCommit)\n\n\t// We expect it to be in state StateBroadcastCommit when publishing\n\t// the force close.\n\tselect {\n\tcase state := <-stateChan:\n\t\tif state != StateBroadcastCommit {\n\t\t\tt.Fatalf(\"state during PublishTx was %v\", state)\n\t\t}\n\tcase <-time.After(stateTimeout):\n\t\tt.Fatalf(\"no state update received\")\n\t}\n\n\t// After broadcasting, transition should be to\n\t// StateCommitmentBroadcasted.\n\tchanArbCtx.AssertStateTransitions(StateCommitmentBroadcasted)\n\n\t// Wait for a response to the force close.\n\tselect {\n\tcase <-respChan:\n\tcase <-time.After(defaultTimeout):\n\t\tt.Fatalf(\"no response received\")\n\t}\n\n\tselect {\n\tcase err := <-errChan:\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"error force closing channel: %v\", err)\n\t\t}\n\tcase <-time.After(defaultTimeout):\n\t\tt.Fatalf(\"no response received\")\n\t}\n\n\t// The state should be StateCommitmentBroadcasted.\n\tchanArbCtx.AssertState(StateCommitmentBroadcasted)\n\n\t// Now notify about the _REMOTE_ commitment getting confirmed.\n\tcommitSpend := &chainntnfs.SpendDetail{\n\t\tSpenderTxHash: &chainhash.Hash{},\n\t}\n\tuniClose := &lnwallet.UnilateralCloseSummary{\n\t\tSpendDetail:     commitSpend,\n\t\tHtlcResolutions: &lnwallet.HtlcResolutions{},\n\t}\n\tchanArb.cfg.ChainEvents.RemoteUnilateralClosure <- &RemoteUnilateralCloseInfo{\n\t\tUnilateralCloseSummary: uniClose,\n\t}\n\n\t// It should transition StateContractClosed -> StateFullyResolved.\n\tchanArbCtx.AssertStateTransitions(StateContractClosed, StateFullyResolved)\n\n\t// It should resolve.\n\tselect {\n\tcase <-chanArbCtx.resolvedChan:\n\t\t// Expected.\n\tcase <-time.After(stateTimeout):\n\t\tt.Fatalf(\"contract was not resolved\")\n\t}\n}\n\n// TestChannelArbitratorLocalForceCloseDoubleSpend tests that the\n// ChannelArbitrator behaves as expected in the case where we request a local\n// force close, but we fail broadcasting our commitment because a remote\n// commitment has already been published.",
      "length": 3017,
      "tokens": 353,
      "embedding": []
    },
    {
      "slug": "func TestChannelArbitratorLocalForceDoubleSpend(t *testing.T) {",
      "content": "func TestChannelArbitratorLocalForceDoubleSpend(t *testing.T) {\n\tlog := &mockArbitratorLog{\n\t\tstate:     StateDefault,\n\t\tnewStates: make(chan ArbitratorState, 5),\n\t}\n\n\tchanArbCtx, err := createTestChannelArbitrator(t, log)\n\trequire.NoError(t, err, \"unable to create ChannelArbitrator\")\n\tchanArb := chanArbCtx.chanArb\n\n\tif err := chanArb.Start(nil); err != nil {\n\t\tt.Fatalf(\"unable to start ChannelArbitrator: %v\", err)\n\t}\n\tdefer chanArb.Stop()\n\n\t// It should start out in the default state.\n\tchanArbCtx.AssertState(StateDefault)\n\n\t// Return ErrDoubleSpend when attempting to publish the tx.\n\tstateChan := make(chan ArbitratorState)\n\tchanArb.cfg.PublishTx = func(*wire.MsgTx, string) error {\n\t\t// When the force close tx is being broadcasted, check that the\n\t\t// state is correct at that point.\n\t\tselect {\n\t\tcase stateChan <- chanArb.state:\n\t\tcase <-chanArb.quit:\n\t\t\treturn fmt.Errorf(\"exiting\")\n\t\t}\n\t\treturn lnwallet.ErrDoubleSpend\n\t}\n\n\terrChan := make(chan error, 1)\n\trespChan := make(chan *wire.MsgTx, 1)\n\n\t// With the channel found, and the request crafted, we'll send over a\n\t// force close request to the arbitrator that watches this channel.\n\tchanArb.forceCloseReqs <- &forceCloseReq{\n\t\terrResp: errChan,\n\t\tcloseTx: respChan,\n\t}\n\n\t// It should transition to StateBroadcastCommit.\n\tchanArbCtx.AssertStateTransitions(StateBroadcastCommit)\n\n\t// We expect it to be in state StateBroadcastCommit when publishing\n\t// the force close.\n\tselect {\n\tcase state := <-stateChan:\n\t\tif state != StateBroadcastCommit {\n\t\t\tt.Fatalf(\"state during PublishTx was %v\", state)\n\t\t}\n\tcase <-time.After(stateTimeout):\n\t\tt.Fatalf(\"no state update received\")\n\t}\n\n\t// After broadcasting, transition should be to\n\t// StateCommitmentBroadcasted.\n\tchanArbCtx.AssertStateTransitions(StateCommitmentBroadcasted)\n\n\t// Wait for a response to the force close.\n\tselect {\n\tcase <-respChan:\n\tcase <-time.After(defaultTimeout):\n\t\tt.Fatalf(\"no response received\")\n\t}\n\n\tselect {\n\tcase err := <-errChan:\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"error force closing channel: %v\", err)\n\t\t}\n\tcase <-time.After(defaultTimeout):\n\t\tt.Fatalf(\"no response received\")\n\t}\n\n\t// The state should be StateCommitmentBroadcasted.\n\tchanArbCtx.AssertState(StateCommitmentBroadcasted)\n\n\t// Now notify about the _REMOTE_ commitment getting confirmed.\n\tcommitSpend := &chainntnfs.SpendDetail{\n\t\tSpenderTxHash: &chainhash.Hash{},\n\t}\n\tuniClose := &lnwallet.UnilateralCloseSummary{\n\t\tSpendDetail:     commitSpend,\n\t\tHtlcResolutions: &lnwallet.HtlcResolutions{},\n\t}\n\tchanArb.cfg.ChainEvents.RemoteUnilateralClosure <- &RemoteUnilateralCloseInfo{\n\t\tUnilateralCloseSummary: uniClose,\n\t}\n\n\t// It should transition StateContractClosed -> StateFullyResolved.\n\tchanArbCtx.AssertStateTransitions(StateContractClosed, StateFullyResolved)\n\n\t// It should resolve.\n\tselect {\n\tcase <-chanArbCtx.resolvedChan:\n\t\t// Expected.\n\tcase <-time.After(stateTimeout):\n\t\tt.Fatalf(\"contract was not resolved\")\n\t}\n}\n\n// TestChannelArbitratorPersistence tests that the ChannelArbitrator is able to\n// keep advancing the state machine from various states after restart.",
      "length": 2905,
      "tokens": 328,
      "embedding": []
    },
    {
      "slug": "func TestChannelArbitratorPersistence(t *testing.T) {",
      "content": "func TestChannelArbitratorPersistence(t *testing.T) {\n\t// Start out with a log that will fail writing the set of resolutions.\n\tlog := &mockArbitratorLog{\n\t\tstate:     StateDefault,\n\t\tnewStates: make(chan ArbitratorState, 5),\n\t\tfailLog:   true,\n\t}\n\n\tchanArbCtx, err := createTestChannelArbitrator(t, log)\n\trequire.NoError(t, err, \"unable to create ChannelArbitrator\")\n\n\tchanArb := chanArbCtx.chanArb\n\tif err := chanArb.Start(nil); err != nil {\n\t\tt.Fatalf(\"unable to start ChannelArbitrator: %v\", err)\n\t}\n\n\t// It should start in StateDefault.\n\tchanArbCtx.AssertState(StateDefault)\n\n\t// Send a remote force close event.\n\tcommitSpend := &chainntnfs.SpendDetail{\n\t\tSpenderTxHash: &chainhash.Hash{},\n\t}\n\n\tuniClose := &lnwallet.UnilateralCloseSummary{\n\t\tSpendDetail:     commitSpend,\n\t\tHtlcResolutions: &lnwallet.HtlcResolutions{},\n\t}\n\tchanArb.cfg.ChainEvents.RemoteUnilateralClosure <- &RemoteUnilateralCloseInfo{\n\t\tUnilateralCloseSummary: uniClose,\n\t}\n\n\t// Since writing the resolutions fail, the arbitrator should not\n\t// advance to the next state.\n\ttime.Sleep(100 * time.Millisecond)\n\tif log.state != StateDefault {\n\t\tt.Fatalf(\"expected to stay in StateDefault\")\n\t}\n\n\t// Restart the channel arb, this'll use the same long and prior\n\t// context.\n\tchanArbCtx, err = chanArbCtx.Restart(nil)\n\trequire.NoError(t, err, \"unable to restart channel arb\")\n\tchanArb = chanArbCtx.chanArb\n\n\t// Again, it should start up in the default state.\n\tchanArbCtx.AssertState(StateDefault)\n\n\t// Now we make the log succeed writing the resolutions, but fail when\n\t// attempting to close the channel.\n\tlog.failLog = false\n\tchanArb.cfg.MarkChannelClosed = func(*channeldb.ChannelCloseSummary,\n\t\t...channeldb.ChannelStatus) error {\n\n\t\treturn fmt.Errorf(\"intentional close error\")\n\t}\n\n\t// Send a new remote force close event.\n\tchanArb.cfg.ChainEvents.RemoteUnilateralClosure <- &RemoteUnilateralCloseInfo{\n\t\tUnilateralCloseSummary: uniClose,\n\t}\n\n\t// Since closing the channel failed, the arbitrator should stay in the\n\t// default state.\n\ttime.Sleep(100 * time.Millisecond)\n\tif log.state != StateDefault {\n\t\tt.Fatalf(\"expected to stay in StateDefault\")\n\t}\n\n\t// Restart once again to simulate yet another restart.\n\tchanArbCtx, err = chanArbCtx.Restart(nil)\n\trequire.NoError(t, err, \"unable to restart channel arb\")\n\tchanArb = chanArbCtx.chanArb\n\n\t// Starts out in StateDefault.\n\tchanArbCtx.AssertState(StateDefault)\n\n\t// Now make fetching the resolutions fail.\n\tlog.failFetch = fmt.Errorf(\"intentional fetch failure\")\n\tchanArb.cfg.ChainEvents.RemoteUnilateralClosure <- &RemoteUnilateralCloseInfo{\n\t\tUnilateralCloseSummary: uniClose,\n\t}\n\n\t// Since logging the resolutions and closing the channel now succeeds,\n\t// it should advance to StateContractClosed.\n\tchanArbCtx.AssertStateTransitions(StateContractClosed)\n\n\t// It should not advance further, however, as fetching resolutions\n\t// failed.\n\ttime.Sleep(100 * time.Millisecond)\n\tif log.state != StateContractClosed {\n\t\tt.Fatalf(\"expected to stay in StateContractClosed\")\n\t}\n\tchanArb.Stop()\n\n\t// Create a new arbitrator, and now make fetching resolutions succeed.\n\tlog.failFetch = nil\n\tchanArbCtx, err = chanArbCtx.Restart(nil)\n\trequire.NoError(t, err, \"unable to restart channel arb\")\n\tdefer chanArbCtx.CleanUp()\n\n\t// Finally it should advance to StateFullyResolved.\n\tchanArbCtx.AssertStateTransitions(StateFullyResolved)\n\n\t// It should also mark the channel as resolved.\n\tselect {\n\tcase <-chanArbCtx.resolvedChan:\n\t\t// Expected.\n\tcase <-time.After(defaultTimeout):\n\t\tt.Fatalf(\"contract was not resolved\")\n\t}\n}\n\n// TestChannelArbitratorForceCloseBreachedChannel tests that the channel\n// arbitrator is able to handle a channel in the process of being force closed\n// is breached by the remote node. In these cases we expect the\n// ChannelArbitrator to properly execute the breachResolver flow and then\n// gracefully exit once the breachResolver receives the signal from what would\n// normally be the breacharbiter.",
      "length": 3761,
      "tokens": 445,
      "embedding": []
    },
    {
      "slug": "func TestChannelArbitratorForceCloseBreachedChannel(t *testing.T) {",
      "content": "func TestChannelArbitratorForceCloseBreachedChannel(t *testing.T) {\n\tlog := &mockArbitratorLog{\n\t\tstate:     StateDefault,\n\t\tnewStates: make(chan ArbitratorState, 5),\n\t\tresolvers: make(map[ContractResolver]struct{}),\n\t}\n\n\tchanArbCtx, err := createTestChannelArbitrator(t, log)\n\trequire.NoError(t, err, \"unable to create ChannelArbitrator\")\n\n\tchanArb := chanArbCtx.chanArb\n\tif err := chanArb.Start(nil); err != nil {\n\t\tt.Fatalf(\"unable to start ChannelArbitrator: %v\", err)\n\t}\n\n\t// It should start in StateDefault.\n\tchanArbCtx.AssertState(StateDefault)\n\n\t// We start by attempting a local force close. We'll return an\n\t// unexpected publication error, causing the state machine to halt.\n\texpErr := errors.New(\"intentional publication error\")\n\tstateChan := make(chan ArbitratorState)\n\tchanArb.cfg.PublishTx = func(*wire.MsgTx, string) error {\n\t\t// When the force close tx is being broadcasted, check that the\n\t\t// state is correct at that point.\n\t\tselect {\n\t\tcase stateChan <- chanArb.state:\n\t\tcase <-chanArb.quit:\n\t\t\treturn fmt.Errorf(\"exiting\")\n\t\t}\n\t\treturn expErr\n\t}\n\n\terrChan := make(chan error, 1)\n\trespChan := make(chan *wire.MsgTx, 1)\n\n\t// With the channel found, and the request crafted, we'll send over a\n\t// force close request to the arbitrator that watches this channel.\n\tchanArb.forceCloseReqs <- &forceCloseReq{\n\t\terrResp: errChan,\n\t\tcloseTx: respChan,\n\t}\n\n\t// It should transition to StateBroadcastCommit.\n\tchanArbCtx.AssertStateTransitions(StateBroadcastCommit)\n\n\t// We expect it to be in state StateBroadcastCommit when attempting\n\t// the force close.\n\tselect {\n\tcase state := <-stateChan:\n\t\tif state != StateBroadcastCommit {\n\t\t\tt.Fatalf(\"state during PublishTx was %v\", state)\n\t\t}\n\tcase <-time.After(stateTimeout):\n\t\tt.Fatalf(\"no state update received\")\n\t}\n\n\t// Make sure we get the expected error.\n\tselect {\n\tcase err := <-errChan:\n\t\tif err != expErr {\n\t\t\tt.Fatalf(\"unexpected error force closing channel: %v\",\n\t\t\t\terr)\n\t\t}\n\tcase <-time.After(defaultTimeout):\n\t\tt.Fatalf(\"no response received\")\n\t}\n\n\t// Before restarting, we'll need to modify the arbitrator log to have\n\t// a set of contract resolutions and a commit set.\n\tlog.resolutions = &ContractResolutions{\n\t\tBreachResolution: &BreachResolution{\n\t\t\tFundingOutPoint: wire.OutPoint{},\n\t\t},\n\t}\n\tlog.commitSet = &CommitSet{\n\t\tConfCommitKey: &RemoteHtlcSet,\n\t\tHtlcSets: map[HtlcSetKey][]channeldb.HTLC{\n\t\t\tRemoteHtlcSet: {},\n\t\t},\n\t}\n\n\t// We mimic that the channel is breached while the channel arbitrator\n\t// is down. This means that on restart it will be started with a\n\t// pending close channel, of type BreachClose.\n\tchanArbCtx, err = chanArbCtx.Restart(func(c *chanArbTestCtx) {\n\t\tc.chanArb.cfg.IsPendingClose = true\n\t\tc.chanArb.cfg.ClosingHeight = 100\n\t\tc.chanArb.cfg.CloseType = channeldb.BreachClose\n\t})\n\trequire.NoError(t, err, \"unable to create ChannelArbitrator\")\n\tdefer chanArbCtx.CleanUp()\n\n\t// We should transition to StateContractClosed.\n\tchanArbCtx.AssertStateTransitions(\n\t\tStateContractClosed, StateWaitingFullResolution,\n\t)\n\n\t// Wait for SubscribeBreachComplete to be called.\n\t<-chanArbCtx.breachSubscribed\n\n\t// We'll close the breachResolutionChan to cleanup the breachResolver\n\t// and make the state transition to StateFullyResolved.\n\tclose(chanArbCtx.breachResolutionChan)\n\n\tchanArbCtx.AssertStateTransitions(StateFullyResolved)\n\n\t// It should also mark the channel as resolved.\n\tselect {\n\tcase <-chanArbCtx.resolvedChan:\n\t\t// Expected.\n\tcase <-time.After(defaultTimeout):\n\t\tt.Fatalf(\"contract was not resolved\")\n\t}\n}\n\n// TestChannelArbitratorCommitFailure tests that the channel arbitrator is able\n// to recover from a failed CommitState call at restart.",
      "length": 3461,
      "tokens": 418,
      "embedding": []
    },
    {
      "slug": "func TestChannelArbitratorCommitFailure(t *testing.T) {",
      "content": "func TestChannelArbitratorCommitFailure(t *testing.T) {\n\ttestCases := []struct {\n\n\t\t// closeType is the type of channel close we want ot test.\n\t\tcloseType channeldb.ClosureType\n\n\t\t// sendEvent is a function that will send the event\n\t\t// corresponding to this test's closeType to the passed\n\t\t// ChannelArbitrator.\n\t\tsendEvent func(chanArb *ChannelArbitrator)\n\n\t\t// expectedStates is the states we expect the state machine to\n\t\t// go through after a restart and successful log commit.\n\t\texpectedStates []ArbitratorState\n\t}{\n\t\t{\n\t\t\tcloseType: channeldb.CooperativeClose,\n\t\t\tsendEvent: func(chanArb *ChannelArbitrator) {\n\t\t\t\tcloseInfo := &CooperativeCloseInfo{\n\t\t\t\t\t&channeldb.ChannelCloseSummary{},\n\t\t\t\t}\n\t\t\t\tchanArb.cfg.ChainEvents.CooperativeClosure <- closeInfo\n\t\t\t},\n\t\t\texpectedStates: []ArbitratorState{StateFullyResolved},\n\t\t},\n\t\t{\n\t\t\tcloseType: channeldb.RemoteForceClose,\n\t\t\tsendEvent: func(chanArb *ChannelArbitrator) {\n\t\t\t\tcommitSpend := &chainntnfs.SpendDetail{\n\t\t\t\t\tSpenderTxHash: &chainhash.Hash{},\n\t\t\t\t}\n\n\t\t\t\tuniClose := &lnwallet.UnilateralCloseSummary{\n\t\t\t\t\tSpendDetail:     commitSpend,\n\t\t\t\t\tHtlcResolutions: &lnwallet.HtlcResolutions{},\n\t\t\t\t}\n\t\t\t\tchanArb.cfg.ChainEvents.RemoteUnilateralClosure <- &RemoteUnilateralCloseInfo{\n\t\t\t\t\tUnilateralCloseSummary: uniClose,\n\t\t\t\t}\n\t\t\t},\n\t\t\texpectedStates: []ArbitratorState{StateContractClosed, StateFullyResolved},\n\t\t},\n\t\t{\n\t\t\tcloseType: channeldb.LocalForceClose,\n\t\t\tsendEvent: func(chanArb *ChannelArbitrator) {\n\t\t\t\tchanArb.cfg.ChainEvents.LocalUnilateralClosure <- &LocalUnilateralCloseInfo{\n\t\t\t\t\tSpendDetail: &chainntnfs.SpendDetail{},\n\t\t\t\t\tLocalForceCloseSummary: &lnwallet.LocalForceCloseSummary{\n\t\t\t\t\t\tCloseTx:         &wire.MsgTx{},\n\t\t\t\t\t\tHtlcResolutions: &lnwallet.HtlcResolutions{},\n\t\t\t\t\t},\n\t\t\t\t\tChannelCloseSummary: &channeldb.ChannelCloseSummary{},\n\t\t\t\t}\n\t\t\t},\n\t\t\texpectedStates: []ArbitratorState{StateContractClosed, StateFullyResolved},\n\t\t},\n\t}\n\n\tfor _, test := range testCases {\n\t\ttest := test\n\n\t\tlog := &mockArbitratorLog{\n\t\t\tstate:      StateDefault,\n\t\t\tnewStates:  make(chan ArbitratorState, 5),\n\t\t\tfailCommit: true,\n\n\t\t\t// Set the log to fail on the first expected state\n\t\t\t// after state machine progress for this test case.\n\t\t\tfailCommitState: test.expectedStates[0],\n\t\t}\n\n\t\tchanArbCtx, err := createTestChannelArbitrator(t, log)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to create ChannelArbitrator: %v\", err)\n\t\t}\n\n\t\tchanArb := chanArbCtx.chanArb\n\t\tif err := chanArb.Start(nil); err != nil {\n\t\t\tt.Fatalf(\"unable to start ChannelArbitrator: %v\", err)\n\t\t}\n\n\t\t// It should start in StateDefault.\n\t\tchanArbCtx.AssertState(StateDefault)\n\n\t\tclosed := make(chan struct{})\n\t\tchanArb.cfg.MarkChannelClosed = func(\n\t\t\t*channeldb.ChannelCloseSummary,\n\t\t\t...channeldb.ChannelStatus) error {\n\n\t\t\tclose(closed)\n\t\t\treturn nil\n\t\t}\n\n\t\t// Send the test event to trigger the state machine.\n\t\ttest.sendEvent(chanArb)\n\n\t\tselect {\n\t\tcase <-closed:\n\t\tcase <-time.After(defaultTimeout):\n\t\t\tt.Fatalf(\"channel was not marked closed\")\n\t\t}\n\n\t\t// Since the channel was marked closed in the database, but the\n\t\t// commit to the next state failed, the state should still be\n\t\t// StateDefault.\n\t\ttime.Sleep(100 * time.Millisecond)\n\t\tif log.state != StateDefault {\n\t\t\tt.Fatalf(\"expected to stay in StateDefault, instead \"+\n\t\t\t\t\"has %v\", log.state)\n\t\t}\n\t\tchanArb.Stop()\n\n\t\t// Start the arbitrator again, with IsPendingClose reporting\n\t\t// the channel closed in the database.\n\t\tlog.failCommit = false\n\t\tchanArbCtx, err = chanArbCtx.Restart(func(c *chanArbTestCtx) {\n\t\t\tc.chanArb.cfg.IsPendingClose = true\n\t\t\tc.chanArb.cfg.ClosingHeight = 100\n\t\t\tc.chanArb.cfg.CloseType = test.closeType\n\t\t})\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to create ChannelArbitrator: %v\", err)\n\t\t}\n\n\t\t// Since the channel is marked closed in the database, it\n\t\t// should advance to the expected states.\n\t\tchanArbCtx.AssertStateTransitions(test.expectedStates...)\n\n\t\t// It should also mark the channel as resolved.\n\t\tselect {\n\t\tcase <-chanArbCtx.resolvedChan:\n\t\t\t// Expected.\n\t\tcase <-time.After(defaultTimeout):\n\t\t\tt.Fatalf(\"contract was not resolved\")\n\t\t}\n\t}\n}\n\n// TestChannelArbitratorEmptyResolutions makes sure that a channel that is\n// pending close in the database, but haven't had any resolutions logged will\n// not be marked resolved. This situation must be handled to avoid closing\n// channels from earlier versions of the ChannelArbitrator, which didn't have a\n// proper handoff from the ChainWatcher, and we could risk ending up in a state\n// where the channel was closed in the DB, but the resolutions weren't properly\n// written.",
      "length": 4358,
      "tokens": 482,
      "embedding": []
    },
    {
      "slug": "func TestChannelArbitratorEmptyResolutions(t *testing.T) {",
      "content": "func TestChannelArbitratorEmptyResolutions(t *testing.T) {\n\t// Start out with a log that will fail writing the set of resolutions.\n\tlog := &mockArbitratorLog{\n\t\tstate:     StateDefault,\n\t\tnewStates: make(chan ArbitratorState, 5),\n\t\tfailFetch: errNoResolutions,\n\t}\n\n\tchanArbCtx, err := createTestChannelArbitrator(t, log)\n\trequire.NoError(t, err, \"unable to create ChannelArbitrator\")\n\n\tchanArb := chanArbCtx.chanArb\n\tchanArb.cfg.IsPendingClose = true\n\tchanArb.cfg.ClosingHeight = 100\n\tchanArb.cfg.CloseType = channeldb.RemoteForceClose\n\n\tif err := chanArb.Start(nil); err != nil {\n\t\tt.Fatalf(\"unable to start ChannelArbitrator: %v\", err)\n\t}\n\n\t// It should not advance its state beyond StateContractClosed, since\n\t// fetching resolutions fails.\n\tchanArbCtx.AssertStateTransitions(StateContractClosed)\n\n\t// It should not advance further, however, as fetching resolutions\n\t// failed.\n\ttime.Sleep(100 * time.Millisecond)\n\tif log.state != StateContractClosed {\n\t\tt.Fatalf(\"expected to stay in StateContractClosed\")\n\t}\n\tchanArb.Stop()\n}\n\n// TestChannelArbitratorAlreadyForceClosed ensures that we cannot force close a\n// channel that is already in the process of doing so.",
      "length": 1074,
      "tokens": 127,
      "embedding": []
    },
    {
      "slug": "func TestChannelArbitratorAlreadyForceClosed(t *testing.T) {",
      "content": "func TestChannelArbitratorAlreadyForceClosed(t *testing.T) {\n\tt.Parallel()\n\n\t// We'll create the arbitrator and its backing log to signal that it's\n\t// already in the process of being force closed.\n\tlog := &mockArbitratorLog{\n\t\tstate: StateCommitmentBroadcasted,\n\t}\n\tchanArbCtx, err := createTestChannelArbitrator(t, log)\n\trequire.NoError(t, err, \"unable to create ChannelArbitrator\")\n\tchanArb := chanArbCtx.chanArb\n\tif err := chanArb.Start(nil); err != nil {\n\t\tt.Fatalf(\"unable to start ChannelArbitrator: %v\", err)\n\t}\n\tdefer chanArb.Stop()\n\n\t// Then, we'll create a request to signal a force close request to the\n\t// channel arbitrator.\n\terrChan := make(chan error, 1)\n\trespChan := make(chan *wire.MsgTx, 1)\n\n\tselect {\n\tcase chanArb.forceCloseReqs <- &forceCloseReq{\n\t\tcloseTx: respChan,\n\t\terrResp: errChan,\n\t}:\n\tcase <-chanArb.quit:\n\t}\n\n\t// Finally, we should ensure that we are not able to do so by seeing\n\t// the expected errAlreadyForceClosed error.\n\tselect {\n\tcase err = <-errChan:\n\t\tif err != errAlreadyForceClosed {\n\t\t\tt.Fatalf(\"expected errAlreadyForceClosed, got %v\", err)\n\t\t}\n\tcase <-time.After(time.Second):\n\t\tt.Fatal(\"expected to receive error response\")\n\t}\n}\n\n// TestChannelArbitratorDanglingCommitForceClose tests that if there're HTLCs\n// on the remote party's commitment, but not ours, and they're about to time\n// out, then we'll go on chain so we can cancel back the HTLCs on the incoming\n// commitment.",
      "length": 1319,
      "tokens": 187,
      "embedding": []
    },
    {
      "slug": "func TestChannelArbitratorDanglingCommitForceClose(t *testing.T) {",
      "content": "func TestChannelArbitratorDanglingCommitForceClose(t *testing.T) {\n\tt.Parallel()\n\n\ttype testCase struct {\n\t\thtlcExpired       bool\n\t\tremotePendingHTLC bool\n\t\tconfCommit        HtlcSetKey\n\t}\n\tvar testCases []testCase\n\n\ttestOptions := []bool{true, false}\n\tconfOptions := []HtlcSetKey{\n\t\tLocalHtlcSet, RemoteHtlcSet, RemotePendingHtlcSet,\n\t}\n\tfor _, htlcExpired := range testOptions {\n\t\tfor _, remotePendingHTLC := range testOptions {\n\t\t\tfor _, commitConf := range confOptions {\n\t\t\t\tswitch {\n\t\t\t\t// If the HTLC is on the remote commitment, and\n\t\t\t\t// that one confirms, then there's no special\n\t\t\t\t// behavior, we should play all the HTLCs on\n\t\t\t\t// that remote commitment as normal.\n\t\t\t\tcase !remotePendingHTLC && commitConf == RemoteHtlcSet:\n\t\t\t\t\tfallthrough\n\n\t\t\t\t// If the HTLC is on the remote pending, and\n\t\t\t\t// that confirms, then we don't have any\n\t\t\t\t// special actions.\n\t\t\t\tcase remotePendingHTLC && commitConf == RemotePendingHtlcSet:\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\ttestCases = append(testCases, testCase{\n\t\t\t\t\thtlcExpired:       htlcExpired,\n\t\t\t\t\tremotePendingHTLC: remotePendingHTLC,\n\t\t\t\t\tconfCommit:        commitConf,\n\t\t\t\t})\n\t\t\t}\n\t\t}\n\t}\n\n\tfor _, testCase := range testCases {\n\t\ttestCase := testCase\n\t\ttestName := fmt.Sprintf(\"testCase: htlcExpired=%v,\"+\n\t\t\t\"remotePendingHTLC=%v,remotePendingCommitConf=%v\",\n\t\t\ttestCase.htlcExpired, testCase.remotePendingHTLC,\n\t\t\ttestCase.confCommit)\n\n\t\tt.Run(testName, func(t *testing.T) {\n\t\t\tt.Parallel()\n\n\t\t\tarbLog := &mockArbitratorLog{\n\t\t\t\tstate:     StateDefault,\n\t\t\t\tnewStates: make(chan ArbitratorState, 5),\n\t\t\t\tresolvers: make(map[ContractResolver]struct{}),\n\t\t\t}\n\n\t\t\tchanArbCtx, err := createTestChannelArbitrator(\n\t\t\t\tt, arbLog,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"unable to create ChannelArbitrator: %v\", err)\n\t\t\t}\n\t\t\tchanArb := chanArbCtx.chanArb\n\t\t\tif err := chanArb.Start(nil); err != nil {\n\t\t\t\tt.Fatalf(\"unable to start ChannelArbitrator: %v\", err)\n\t\t\t}\n\t\t\tdefer chanArb.Stop()\n\n\t\t\t// Now that our channel arb has started, we'll set up\n\t\t\t// its contract signals channel so we can send it\n\t\t\t// various HTLC updates for this test.\n\t\t\tsignals := &ContractSignals{\n\t\t\t\tShortChanID: lnwire.ShortChannelID{},\n\t\t\t}\n\t\t\tchanArb.UpdateContractSignals(signals)\n\n\t\t\thtlcKey := RemoteHtlcSet\n\t\t\tif testCase.remotePendingHTLC {\n\t\t\t\thtlcKey = RemotePendingHtlcSet\n\t\t\t}\n\n\t\t\t// Next, we'll send it a new HTLC that is set to expire\n\t\t\t// in 10 blocks, this HTLC will only appear on the\n\t\t\t// commitment transaction of the _remote_ party.\n\t\t\thtlcIndex := uint64(99)\n\t\t\thtlcExpiry := uint32(10)\n\t\t\tdanglingHTLC := channeldb.HTLC{\n\t\t\t\tIncoming:      false,\n\t\t\t\tAmt:           10000,\n\t\t\t\tHtlcIndex:     htlcIndex,\n\t\t\t\tRefundTimeout: htlcExpiry,\n\t\t\t}\n\t\t\tnewUpdate := &ContractUpdate{\n\t\t\t\tHtlcKey: htlcKey,\n\t\t\t\tHtlcs:   []channeldb.HTLC{danglingHTLC},\n\t\t\t}\n\t\t\tchanArb.notifyContractUpdate(newUpdate)\n\n\t\t\t// At this point, we now have a split commitment state\n\t\t\t// from the PoV of the channel arb. There's now an HTLC\n\t\t\t// that only exists on the commitment transaction of\n\t\t\t// the remote party.\n\t\t\terrChan := make(chan error, 1)\n\t\t\trespChan := make(chan *wire.MsgTx, 1)\n\t\t\tswitch {\n\t\t\t// If we want an HTLC expiration trigger, then We'll\n\t\t\t// now mine a block (height 5), which is 5 blocks away\n\t\t\t// (our grace delta) from the expiry of that HTLC.\n\t\t\tcase testCase.htlcExpired:\n\t\t\t\tchanArbCtx.chanArb.blocks <- 5\n\n\t\t\t// Otherwise, we'll just trigger a regular force close\n\t\t\t// request.\n\t\t\tcase !testCase.htlcExpired:\n\t\t\t\tchanArb.forceCloseReqs <- &forceCloseReq{\n\t\t\t\t\terrResp: errChan,\n\t\t\t\t\tcloseTx: respChan,\n\t\t\t\t}\n\n\t\t\t}\n\n\t\t\t// At this point, the resolver should now have\n\t\t\t// determined that it needs to go to chain in order to\n\t\t\t// block off the redemption path so it can cancel the\n\t\t\t// incoming HTLC.\n\t\t\tchanArbCtx.AssertStateTransitions(\n\t\t\t\tStateBroadcastCommit,\n\t\t\t\tStateCommitmentBroadcasted,\n\t\t\t)\n\n\t\t\t// Next we'll craft a fake commitment transaction to\n\t\t\t// send to signal that the channel has closed out on\n\t\t\t// chain.\n\t\t\tcloseTx := &wire.MsgTx{\n\t\t\t\tTxIn: []*wire.TxIn{\n\t\t\t\t\t{\n\t\t\t\t\t\tPreviousOutPoint: wire.OutPoint{},\n\t\t\t\t\t\tWitness: [][]byte{\n\t\t\t\t\t\t\t{0x9},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t}\n\n\t\t\t// We'll now signal to the channel arb that the HTLC\n\t\t\t// has fully closed on chain. Our local commit set\n\t\t\t// shows now HTLC on our commitment, but one on the\n\t\t\t// remote commitment. This should result in the HTLC\n\t\t\t// being canalled back. Also note that there're no HTLC\n\t\t\t// resolutions sent since we have none on our\n\t\t\t// commitment transaction.\n\t\t\tuniCloseInfo := &LocalUnilateralCloseInfo{\n\t\t\t\tSpendDetail: &chainntnfs.SpendDetail{},\n\t\t\t\tLocalForceCloseSummary: &lnwallet.LocalForceCloseSummary{\n\t\t\t\t\tCloseTx:         closeTx,\n\t\t\t\t\tHtlcResolutions: &lnwallet.HtlcResolutions{},\n\t\t\t\t},\n\t\t\t\tChannelCloseSummary: &channeldb.ChannelCloseSummary{},\n\t\t\t\tCommitSet: CommitSet{\n\t\t\t\t\tConfCommitKey: &testCase.confCommit,\n\t\t\t\t\tHtlcSets:      make(map[HtlcSetKey][]channeldb.HTLC),\n\t\t\t\t},\n\t\t\t}\n\n\t\t\t// If the HTLC was meant to expire, then we'll mark the\n\t\t\t// closing transaction at the proper expiry height\n\t\t\t// since our comparison \"need to timeout\" comparison is\n\t\t\t// based on the confirmation height.\n\t\t\tif testCase.htlcExpired {\n\t\t\t\tuniCloseInfo.SpendDetail.SpendingHeight = 5\n\t\t\t}\n\n\t\t\t// Depending on if we're testing the remote pending\n\t\t\t// commitment or not, we'll populate either a fake\n\t\t\t// dangling remote commitment, or a regular locked in\n\t\t\t// one.\n\t\t\thtlcs := []channeldb.HTLC{danglingHTLC}\n\t\t\tif testCase.remotePendingHTLC {\n\t\t\t\tuniCloseInfo.CommitSet.HtlcSets[RemotePendingHtlcSet] = htlcs\n\t\t\t} else {\n\t\t\t\tuniCloseInfo.CommitSet.HtlcSets[RemoteHtlcSet] = htlcs\n\t\t\t}\n\n\t\t\tchanArb.cfg.ChainEvents.LocalUnilateralClosure <- uniCloseInfo\n\n\t\t\t// The channel arb should now transition to waiting\n\t\t\t// until the HTLCs have been fully resolved.\n\t\t\tchanArbCtx.AssertStateTransitions(\n\t\t\t\tStateContractClosed,\n\t\t\t\tStateWaitingFullResolution,\n\t\t\t)\n\n\t\t\t// Now that we've sent this signal, we should have that\n\t\t\t// HTLC be canceled back immediately.\n\t\t\tselect {\n\t\t\tcase msgs := <-chanArbCtx.resolutions:\n\t\t\t\tif len(msgs) != 1 {\n\t\t\t\t\tt.Fatalf(\"expected 1 message, \"+\n\t\t\t\t\t\t\"instead got %v\", len(msgs))\n\t\t\t\t}\n\n\t\t\t\tif msgs[0].HtlcIndex != htlcIndex {\n\t\t\t\t\tt.Fatalf(\"wrong htlc index: expected %v, got %v\",\n\t\t\t\t\t\thtlcIndex, msgs[0].HtlcIndex)\n\t\t\t\t}\n\t\t\tcase <-time.After(defaultTimeout):\n\t\t\t\tt.Fatalf(\"resolution msgs not sent\")\n\t\t\t}\n\n\t\t\t// There's no contract to send a fully resolve message,\n\t\t\t// so instead, we'll mine another block which'll cause\n\t\t\t// it to re-examine its state and realize there're no\n\t\t\t// more HTLCs.\n\t\t\tchanArbCtx.chanArb.blocks <- 6\n\t\t\tchanArbCtx.AssertStateTransitions(StateFullyResolved)\n\t\t})\n\t}\n}\n\n// TestChannelArbitratorPendingExpiredHTLC tests that if we have pending htlc\n// that is expired we will only go to chain if we are running at least the\n// time defined in PaymentsExpirationGracePeriod.\n// During this time the remote party is expected to send his updates and cancel\n// The htlc.",
      "length": 6688,
      "tokens": 835,
      "embedding": []
    },
    {
      "slug": "func TestChannelArbitratorPendingExpiredHTLC(t *testing.T) {",
      "content": "func TestChannelArbitratorPendingExpiredHTLC(t *testing.T) {\n\tt.Parallel()\n\n\t// We'll create the arbitrator and its backing log in a default state.\n\tlog := &mockArbitratorLog{\n\t\tstate:     StateDefault,\n\t\tnewStates: make(chan ArbitratorState, 5),\n\t\tresolvers: make(map[ContractResolver]struct{}),\n\t}\n\tchanArbCtx, err := createTestChannelArbitrator(t, log)\n\trequire.NoError(t, err, \"unable to create ChannelArbitrator\")\n\tchanArb := chanArbCtx.chanArb\n\n\t// We'll inject a test clock implementation so we can control the uptime.\n\tstartTime := time.Date(2020, time.February, 3, 13, 0, 0, 0, time.UTC)\n\ttestClock := clock.NewTestClock(startTime)\n\tchanArb.cfg.Clock = testClock\n\n\t// We also configure the grace period and the IsForwardedHTLC to identify\n\t// the htlc as our initiated payment.\n\tchanArb.cfg.PaymentsExpirationGracePeriod = time.Second * 15\n\tchanArb.cfg.IsForwardedHTLC = func(chanID lnwire.ShortChannelID,\n\t\thtlcIndex uint64) bool {\n\n\t\treturn false\n\t}\n\n\tif err := chanArb.Start(nil); err != nil {\n\t\tt.Fatalf(\"unable to start ChannelArbitrator: %v\", err)\n\t}\n\tt.Cleanup(func() {\n\t\trequire.NoError(t, chanArb.Stop())\n\t})\n\n\t// Now that our channel arb has started, we'll set up\n\t// its contract signals channel so we can send it\n\t// various HTLC updates for this test.\n\tsignals := &ContractSignals{\n\t\tShortChanID: lnwire.ShortChannelID{},\n\t}\n\tchanArb.UpdateContractSignals(signals)\n\n\t// Next, we'll send it a new HTLC that is set to expire\n\t// in 10 blocks.\n\thtlcIndex := uint64(99)\n\thtlcExpiry := uint32(10)\n\tpendingHTLC := channeldb.HTLC{\n\t\tIncoming:      false,\n\t\tAmt:           10000,\n\t\tHtlcIndex:     htlcIndex,\n\t\tRefundTimeout: htlcExpiry,\n\t}\n\tnewUpdate := &ContractUpdate{\n\t\tHtlcKey: RemoteHtlcSet,\n\t\tHtlcs:   []channeldb.HTLC{pendingHTLC},\n\t}\n\tchanArb.notifyContractUpdate(newUpdate)\n\n\t// We will advance the uptime to 10 seconds which should be still within\n\t// the grace period and should not trigger going to chain.\n\ttestClock.SetTime(startTime.Add(time.Second * 10))\n\tchanArbCtx.chanArb.blocks <- 5\n\tchanArbCtx.AssertState(StateDefault)\n\n\t// We will advance the uptime to 16 seconds which should trigger going\n\t// to chain.\n\ttestClock.SetTime(startTime.Add(time.Second * 16))\n\tchanArbCtx.chanArb.blocks <- 6\n\tchanArbCtx.AssertStateTransitions(\n\t\tStateBroadcastCommit,\n\t\tStateCommitmentBroadcasted,\n\t)\n}\n\n// TestRemoteCloseInitiator tests the setting of close initiator statuses\n// for remote force closes and breaches.",
      "length": 2300,
      "tokens": 278,
      "embedding": []
    },
    {
      "slug": "func TestRemoteCloseInitiator(t *testing.T) {",
      "content": "func TestRemoteCloseInitiator(t *testing.T) {\n\t// getCloseSummary returns a unilateral close summary for the channel\n\t// provided.\n\tgetCloseSummary := func(channel *channeldb.OpenChannel) *RemoteUnilateralCloseInfo {\n\t\treturn &RemoteUnilateralCloseInfo{\n\t\t\tUnilateralCloseSummary: &lnwallet.UnilateralCloseSummary{\n\t\t\t\tSpendDetail: &chainntnfs.SpendDetail{\n\t\t\t\t\tSpenderTxHash: &chainhash.Hash{},\n\t\t\t\t\tSpendingTx: &wire.MsgTx{\n\t\t\t\t\t\tTxIn:  []*wire.TxIn{},\n\t\t\t\t\t\tTxOut: []*wire.TxOut{},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\tChannelCloseSummary: channeldb.ChannelCloseSummary{\n\t\t\t\t\tChanPoint:         channel.FundingOutpoint,\n\t\t\t\t\tRemotePub:         channel.IdentityPub,\n\t\t\t\t\tSettledBalance:    btcutil.Amount(500),\n\t\t\t\t\tTimeLockedBalance: btcutil.Amount(10000),\n\t\t\t\t\tIsPending:         false,\n\t\t\t\t},\n\t\t\t\tHtlcResolutions: &lnwallet.HtlcResolutions{},\n\t\t\t},\n\t\t}\n\t}\n\n\ttests := []struct {\n\t\tname string\n\n\t\t// notifyClose sends the appropriate chain event to indicate\n\t\t// that the channel has closed. The event subscription channel\n\t\t// is expected to be buffered, as is the default for test\n\t\t// channel arbitrators.\n\t\tnotifyClose func(sub *ChainEventSubscription,\n\t\t\tchannel *channeldb.OpenChannel)\n\n\t\t// expectedStates is the set of states we expect the arbitrator\n\t\t// to progress through.\n\t\texpectedStates []ArbitratorState\n\t}{\n\t\t{\n\t\t\tname: \"force close\",\n\t\t\tnotifyClose: func(sub *ChainEventSubscription,\n\t\t\t\tchannel *channeldb.OpenChannel) {\n\n\t\t\t\ts := getCloseSummary(channel)\n\t\t\t\tsub.RemoteUnilateralClosure <- s\n\t\t\t},\n\t\t\texpectedStates: []ArbitratorState{\n\t\t\t\tStateContractClosed, StateFullyResolved,\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\ttest := test\n\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tt.Parallel()\n\n\t\t\t// First, create alice's channel.\n\t\t\talice, _, err := lnwallet.CreateTestChannels(\n\t\t\t\tt, channeldb.SingleFunderTweaklessBit,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"unable to create test channels: %v\",\n\t\t\t\t\terr)\n\t\t\t}\n\n\t\t\t// Create a mock log which will not block the test's\n\t\t\t// expected number of transitions transitions, and has\n\t\t\t// no commit resolutions so that the channel will\n\t\t\t// resolve immediately.\n\t\t\tlog := &mockArbitratorLog{\n\t\t\t\tstate: StateDefault,\n\t\t\t\tnewStates: make(chan ArbitratorState,\n\t\t\t\t\tlen(test.expectedStates)),\n\t\t\t\tresolutions: &ContractResolutions{\n\t\t\t\t\tCommitHash:       chainhash.Hash{},\n\t\t\t\t\tCommitResolution: nil,\n\t\t\t\t},\n\t\t\t}\n\n\t\t\t// Mock marking the channel as closed, we only care\n\t\t\t// about setting of channel status.\n\t\t\tmockMarkClosed := func(_ *channeldb.ChannelCloseSummary,\n\t\t\t\tstatuses ...channeldb.ChannelStatus) error {\n\n\t\t\t\tfor _, status := range statuses {\n\t\t\t\t\terr := alice.State().ApplyChanStatus(status)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\tchanArbCtx, err := createTestChannelArbitrator(\n\t\t\t\tt, log, withMarkClosed(mockMarkClosed),\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"unable to create \"+\n\t\t\t\t\t\"ChannelArbitrator: %v\", err)\n\t\t\t}\n\t\t\tchanArb := chanArbCtx.chanArb\n\n\t\t\tif err := chanArb.Start(nil); err != nil {\n\t\t\t\tt.Fatalf(\"unable to start \"+\n\t\t\t\t\t\"ChannelArbitrator: %v\", err)\n\t\t\t}\n\t\t\tt.Cleanup(func() {\n\t\t\t\trequire.NoError(t, chanArb.Stop())\n\t\t\t})\n\n\t\t\t// It should start out in the default state.\n\t\t\tchanArbCtx.AssertState(StateDefault)\n\n\t\t\t// Notify the close event.\n\t\t\ttest.notifyClose(chanArb.cfg.ChainEvents, alice.State())\n\n\t\t\t// Check that the channel transitions as expected.\n\t\t\tchanArbCtx.AssertStateTransitions(\n\t\t\t\ttest.expectedStates...,\n\t\t\t)\n\n\t\t\t// It should also mark the channel as resolved.\n\t\t\tselect {\n\t\t\tcase <-chanArbCtx.resolvedChan:\n\t\t\t\t// Expected.\n\t\t\tcase <-time.After(defaultTimeout):\n\t\t\t\tt.Fatalf(\"contract was not resolved\")\n\t\t\t}\n\n\t\t\t// Check that alice has the status we expect.\n\t\t\tif !alice.State().HasChanStatus(\n\t\t\t\tchanneldb.ChanStatusRemoteCloseInitiator,\n\t\t\t) {\n\n\t\t\t\tt.Fatalf(\"expected remote close initiator, \"+\n\t\t\t\t\t\"got: %v\", alice.State().ChanStatus())\n\t\t\t}\n\t\t})\n\t}\n}\n\n// TestFindCommitmentDeadline tests the logic used to determine confirmation\n// deadline is implemented as expected.",
      "length": 3828,
      "tokens": 410,
      "embedding": []
    },
    {
      "slug": "func TestFindCommitmentDeadline(t *testing.T) {",
      "content": "func TestFindCommitmentDeadline(t *testing.T) {\n\t// Create a testing channel arbitrator.\n\tlog := &mockArbitratorLog{\n\t\tstate:     StateDefault,\n\t\tnewStates: make(chan ArbitratorState, 5),\n\t}\n\tchanArbCtx, err := createTestChannelArbitrator(t, log)\n\trequire.NoError(t, err, \"unable to create ChannelArbitrator\")\n\n\t// Add a dummy payment hash to the preimage lookup.\n\trHash := [lntypes.PreimageSize]byte{1, 2, 3}\n\tmockPreimageDB := newMockWitnessBeacon()\n\tmockPreimageDB.lookupPreimage[rHash] = rHash\n\n\t// Attack a mock PreimageDB and Registry to channel arbitrator.\n\tchanArb := chanArbCtx.chanArb\n\tchanArb.cfg.PreimageDB = mockPreimageDB\n\tchanArb.cfg.Registry = &mockRegistry{}\n\n\thtlcIndexBase := uint64(99)\n\theightHint := uint32(1000)\n\thtlcExpiryBase := heightHint + uint32(10)\n\n\t// Create four testing HTLCs.\n\thtlcDust := channeldb.HTLC{\n\t\tHtlcIndex:     htlcIndexBase + 1,\n\t\tRefundTimeout: htlcExpiryBase + 1,\n\t\tOutputIndex:   -1,\n\t}\n\thtlcSmallExipry := channeldb.HTLC{\n\t\tHtlcIndex:     htlcIndexBase + 2,\n\t\tRefundTimeout: htlcExpiryBase + 2,\n\t}\n\n\thtlcPreimage := channeldb.HTLC{\n\t\tHtlcIndex:     htlcIndexBase + 3,\n\t\tRefundTimeout: htlcExpiryBase + 3,\n\t\tRHash:         rHash,\n\t}\n\thtlcLargeExpiry := channeldb.HTLC{\n\t\tHtlcIndex:     htlcIndexBase + 4,\n\t\tRefundTimeout: htlcExpiryBase + 100,\n\t}\n\thtlcExpired := channeldb.HTLC{\n\t\tHtlcIndex:     htlcIndexBase + 5,\n\t\tRefundTimeout: heightHint,\n\t}\n\n\tmakeHTLCSet := func(incoming, outgoing channeldb.HTLC) htlcSet {\n\t\treturn htlcSet{\n\t\t\tincomingHTLCs: map[uint64]channeldb.HTLC{\n\t\t\t\tincoming.HtlcIndex: incoming,\n\t\t\t},\n\t\t\toutgoingHTLCs: map[uint64]channeldb.HTLC{\n\t\t\t\toutgoing.HtlcIndex: outgoing,\n\t\t\t},\n\t\t}\n\t}\n\n\ttestCases := []struct {\n\t\tname     string\n\t\thtlcs    htlcSet\n\t\terr      error\n\t\tdeadline uint32\n\t}{\n\t\t{\n\t\t\t// When we have no HTLCs, the default value should be\n\t\t\t// used.\n\t\t\tname:     \"use default conf target\",\n\t\t\thtlcs:    htlcSet{},\n\t\t\terr:      nil,\n\t\t\tdeadline: anchorSweepConfTarget,\n\t\t},\n\t\t{\n\t\t\t// When we have a preimage available in the local HTLC\n\t\t\t// set, its CLTV should be used.\n\t\t\tname:     \"use htlc with preimage available\",\n\t\t\thtlcs:    makeHTLCSet(htlcPreimage, htlcLargeExpiry),\n\t\t\terr:      nil,\n\t\t\tdeadline: htlcPreimage.RefundTimeout - heightHint,\n\t\t},\n\t\t{\n\t\t\t// When the HTLC in the local set is not preimage\n\t\t\t// available, we should not use its CLTV even its value\n\t\t\t// is smaller.\n\t\t\tname:     \"use htlc with no preimage available\",\n\t\t\thtlcs:    makeHTLCSet(htlcSmallExipry, htlcLargeExpiry),\n\t\t\terr:      nil,\n\t\t\tdeadline: htlcLargeExpiry.RefundTimeout - heightHint,\n\t\t},\n\t\t{\n\t\t\t// When we have dust HTLCs, their CLTVs should NOT be\n\t\t\t// used even the values are smaller.\n\t\t\tname:     \"ignore dust HTLCs\",\n\t\t\thtlcs:    makeHTLCSet(htlcPreimage, htlcDust),\n\t\t\terr:      nil,\n\t\t\tdeadline: htlcPreimage.RefundTimeout - heightHint,\n\t\t},\n\t\t{\n\t\t\t// When we've reached our deadline, use conf target of\n\t\t\t// 1 as our deadline.\n\t\t\tname:     \"use conf target 1\",\n\t\t\thtlcs:    makeHTLCSet(htlcPreimage, htlcExpired),\n\t\t\terr:      nil,\n\t\t\tdeadline: 1,\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\ttc := tc\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tt.Parallel()\n\t\t\tdeadline, err := chanArb.findCommitmentDeadline(\n\t\t\t\theightHint, tc.htlcs,\n\t\t\t)\n\n\t\t\trequire.Equal(t, tc.err, err)\n\t\t\trequire.Equal(t, tc.deadline, deadline)\n\t\t})\n\t}\n}\n\n// TestSweepAnchors checks the sweep transactions are created using the\n// expected deadlines for different anchor resolutions.",
      "length": 3266,
      "tokens": 394,
      "embedding": []
    },
    {
      "slug": "func TestSweepAnchors(t *testing.T) {",
      "content": "func TestSweepAnchors(t *testing.T) {\n\t// Create a testing channel arbitrator.\n\tlog := &mockArbitratorLog{\n\t\tstate:     StateDefault,\n\t\tnewStates: make(chan ArbitratorState, 5),\n\t}\n\tchanArbCtx, err := createTestChannelArbitrator(t, log)\n\trequire.NoError(t, err, \"unable to create ChannelArbitrator\")\n\n\t// Add a dummy payment hash to the preimage lookup.\n\trHash := [lntypes.PreimageSize]byte{1, 2, 3}\n\tmockPreimageDB := newMockWitnessBeacon()\n\tmockPreimageDB.lookupPreimage[rHash] = rHash\n\n\t// Attack a mock PreimageDB and Registry to channel arbitrator.\n\tchanArb := chanArbCtx.chanArb\n\tchanArb.cfg.PreimageDB = mockPreimageDB\n\tchanArb.cfg.Registry = &mockRegistry{}\n\n\t// Set current block height.\n\theightHint := uint32(1000)\n\tchanArbCtx.chanArb.blocks <- int32(heightHint)\n\n\thtlcIndexBase := uint64(99)\n\thtlcExpiryBase := heightHint + uint32(10)\n\n\t// Create three testing HTLCs.\n\thtlcDust := channeldb.HTLC{\n\t\tHtlcIndex:     htlcIndexBase + 1,\n\t\tRefundTimeout: htlcExpiryBase + 1,\n\t\tOutputIndex:   -1,\n\t}\n\thtlcWithPreimage := channeldb.HTLC{\n\t\tHtlcIndex:     htlcIndexBase + 2,\n\t\tRefundTimeout: htlcExpiryBase + 2,\n\t\tRHash:         rHash,\n\t}\n\thtlcSmallExipry := channeldb.HTLC{\n\t\tHtlcIndex:     htlcIndexBase + 3,\n\t\tRefundTimeout: htlcExpiryBase + 3,\n\t}\n\n\t// Setup our local HTLC set such that we will use the HTLC's CLTV from\n\t// the incoming HTLC set.\n\texpectedLocalDeadline := htlcWithPreimage.RefundTimeout - heightHint\n\tchanArb.activeHTLCs[LocalHtlcSet] = htlcSet{\n\t\tincomingHTLCs: map[uint64]channeldb.HTLC{\n\t\t\thtlcWithPreimage.HtlcIndex: htlcWithPreimage,\n\t\t},\n\t\toutgoingHTLCs: map[uint64]channeldb.HTLC{\n\t\t\thtlcDust.HtlcIndex: htlcDust,\n\t\t},\n\t}\n\tchanArb.unmergedSet[LocalHtlcSet] = htlcSet{\n\t\tincomingHTLCs: map[uint64]channeldb.HTLC{\n\t\t\thtlcWithPreimage.HtlcIndex: htlcWithPreimage,\n\t\t},\n\t\toutgoingHTLCs: map[uint64]channeldb.HTLC{\n\t\t\thtlcDust.HtlcIndex: htlcDust,\n\t\t},\n\t}\n\n\t// Setup our remote HTLC set such that no valid HTLCs can be used, thus\n\t// we default to anchorSweepConfTarget.\n\texpectedRemoteDeadline := anchorSweepConfTarget\n\tchanArb.activeHTLCs[RemoteHtlcSet] = htlcSet{\n\t\tincomingHTLCs: map[uint64]channeldb.HTLC{\n\t\t\thtlcSmallExipry.HtlcIndex: htlcSmallExipry,\n\t\t},\n\t\toutgoingHTLCs: map[uint64]channeldb.HTLC{\n\t\t\thtlcDust.HtlcIndex: htlcDust,\n\t\t},\n\t}\n\tchanArb.unmergedSet[RemoteHtlcSet] = htlcSet{\n\t\tincomingHTLCs: map[uint64]channeldb.HTLC{\n\t\t\thtlcSmallExipry.HtlcIndex: htlcSmallExipry,\n\t\t},\n\t\toutgoingHTLCs: map[uint64]channeldb.HTLC{\n\t\t\thtlcDust.HtlcIndex: htlcDust,\n\t\t},\n\t}\n\n\t// Setup out pending remote HTLC set such that we will use the HTLC's\n\t// CLTV from the outgoing HTLC set.\n\texpectedPendingDeadline := htlcSmallExipry.RefundTimeout - heightHint\n\tchanArb.activeHTLCs[RemotePendingHtlcSet] = htlcSet{\n\t\tincomingHTLCs: map[uint64]channeldb.HTLC{\n\t\t\thtlcDust.HtlcIndex: htlcDust,\n\t\t},\n\t\toutgoingHTLCs: map[uint64]channeldb.HTLC{\n\t\t\thtlcSmallExipry.HtlcIndex: htlcSmallExipry,\n\t\t},\n\t}\n\tchanArb.unmergedSet[RemotePendingHtlcSet] = htlcSet{\n\t\tincomingHTLCs: map[uint64]channeldb.HTLC{\n\t\t\thtlcDust.HtlcIndex: htlcDust,\n\t\t},\n\t\toutgoingHTLCs: map[uint64]channeldb.HTLC{\n\t\t\thtlcSmallExipry.HtlcIndex: htlcSmallExipry,\n\t\t},\n\t}\n\n\t// Create AnchorResolutions.\n\tanchors := &lnwallet.AnchorResolutions{\n\t\tLocal: &lnwallet.AnchorResolution{\n\t\t\tAnchorSignDescriptor: input.SignDescriptor{\n\t\t\t\tOutput: &wire.TxOut{Value: 1},\n\t\t\t},\n\t\t},\n\t\tRemote: &lnwallet.AnchorResolution{\n\t\t\tAnchorSignDescriptor: input.SignDescriptor{\n\t\t\t\tOutput: &wire.TxOut{Value: 1},\n\t\t\t},\n\t\t},\n\t\tRemotePending: &lnwallet.AnchorResolution{\n\t\t\tAnchorSignDescriptor: input.SignDescriptor{\n\t\t\t\tOutput: &wire.TxOut{Value: 1},\n\t\t\t},\n\t\t},\n\t}\n\n\t// Sweep anchors and check there's no error.\n\terr = chanArb.sweepAnchors(anchors, heightHint)\n\trequire.NoError(t, err)\n\n\t// Verify deadlines are used as expected.\n\tdeadlines := chanArbCtx.sweeper.deadlines\n\t// Since there's no guarantee of the deadline orders, we sort it here\n\t// so they can be compared.\n\tsort.Ints(deadlines) // [12, 13, 144]\n\trequire.EqualValues(\n\t\tt, expectedLocalDeadline, deadlines[0],\n\t\t\"local deadline not matched\",\n\t)\n\trequire.EqualValues(\n\t\tt, expectedPendingDeadline, deadlines[1],\n\t\t\"pending remote deadline not matched\",\n\t)\n\trequire.EqualValues(\n\t\tt, expectedRemoteDeadline, deadlines[2],\n\t\t\"remote deadline not matched\",\n\t)\n}\n\n// TestChannelArbitratorAnchors asserts that the commitment tx anchor is swept.",
      "length": 4184,
      "tokens": 410,
      "embedding": []
    },
    {
      "slug": "func TestChannelArbitratorAnchors(t *testing.T) {",
      "content": "func TestChannelArbitratorAnchors(t *testing.T) {\n\tlog := &mockArbitratorLog{\n\t\tstate:     StateDefault,\n\t\tnewStates: make(chan ArbitratorState, 5),\n\t}\n\n\tchanArbCtx, err := createTestChannelArbitrator(t, log)\n\trequire.NoError(t, err, \"unable to create ChannelArbitrator\")\n\n\t// Replace our mocked put report function with one which will push\n\t// reports into a channel for us to consume. We update this function\n\t// because our resolver will be created from the existing chanArb cfg.\n\treports := make(chan *channeldb.ResolverReport)\n\tchanArbCtx.chanArb.cfg.PutResolverReport = putResolverReportInChannel(\n\t\treports,\n\t)\n\n\t// Add a dummy payment hash to the preimage lookup.\n\trHash := [lntypes.PreimageSize]byte{1, 2, 3}\n\tmockPreimageDB := newMockWitnessBeacon()\n\tmockPreimageDB.lookupPreimage[rHash] = rHash\n\n\t// Attack a mock PreimageDB and Registry to channel arbitrator.\n\tchanArb := chanArbCtx.chanArb\n\tchanArb.cfg.PreimageDB = mockPreimageDB\n\tchanArb.cfg.Registry = &mockRegistry{}\n\n\t// Setup two pre-confirmation anchor resolutions on the mock channel.\n\tchanArb.cfg.Channel.(*mockChannel).anchorResolutions =\n\t\t&lnwallet.AnchorResolutions{\n\t\t\tLocal: &lnwallet.AnchorResolution{\n\t\t\t\tAnchorSignDescriptor: input.SignDescriptor{\n\t\t\t\t\tOutput: &wire.TxOut{Value: 1},\n\t\t\t\t},\n\t\t\t},\n\t\t\tRemote: &lnwallet.AnchorResolution{\n\t\t\t\tAnchorSignDescriptor: input.SignDescriptor{\n\t\t\t\t\tOutput: &wire.TxOut{Value: 1},\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\n\tif err := chanArb.Start(nil); err != nil {\n\t\tt.Fatalf(\"unable to start ChannelArbitrator: %v\", err)\n\t}\n\tt.Cleanup(func() {\n\t\trequire.NoError(t, chanArb.Stop())\n\t})\n\n\tsignals := &ContractSignals{\n\t\tShortChanID: lnwire.ShortChannelID{},\n\t}\n\tchanArb.UpdateContractSignals(signals)\n\n\t// Set current block height.\n\theightHint := uint32(1000)\n\tchanArbCtx.chanArb.blocks <- int32(heightHint)\n\n\t// Create testing HTLCs.\n\thtlcExpiryBase := heightHint + uint32(10)\n\thtlcWithPreimage := channeldb.HTLC{\n\t\tHtlcIndex:     99,\n\t\tRefundTimeout: htlcExpiryBase + 2,\n\t\tRHash:         rHash,\n\t\tIncoming:      true,\n\t}\n\thtlc := channeldb.HTLC{\n\t\tHtlcIndex:     100,\n\t\tRefundTimeout: htlcExpiryBase + 3,\n\t}\n\n\t// We now send two HTLC updates, one for local HTLC set and the other\n\t// for remote HTLC set.\n\tnewUpdate := &ContractUpdate{\n\t\tHtlcKey: LocalHtlcSet,\n\t\t// This will make the deadline of the local anchor resolution\n\t\t// to be htlcWithPreimage's CLTV minus heightHint since the\n\t\t// incoming HTLC (toLocalHTLCs) has a lower CLTV value and is\n\t\t// preimage available.\n\t\tHtlcs: []channeldb.HTLC{htlc, htlcWithPreimage},\n\t}\n\tchanArb.notifyContractUpdate(newUpdate)\n\n\tnewUpdate = &ContractUpdate{\n\t\tHtlcKey: RemoteHtlcSet,\n\t\t// This will make the deadline of the remote anchor resolution\n\t\t// to be htlcWithPreimage's CLTV minus heightHint because the\n\t\t// incoming HTLC (toRemoteHTLCs) has a lower CLTV.\n\t\tHtlcs: []channeldb.HTLC{htlc, htlcWithPreimage},\n\t}\n\tchanArb.notifyContractUpdate(newUpdate)\n\n\terrChan := make(chan error, 1)\n\trespChan := make(chan *wire.MsgTx, 1)\n\n\t// With the channel found, and the request crafted, we'll send over a\n\t// force close request to the arbitrator that watches this channel.\n\tchanArb.forceCloseReqs <- &forceCloseReq{\n\t\terrResp: errChan,\n\t\tcloseTx: respChan,\n\t}\n\n\t// The force close request should trigger broadcast of the commitment\n\t// transaction.\n\tchanArbCtx.AssertStateTransitions(\n\t\tStateBroadcastCommit,\n\t\tStateCommitmentBroadcasted,\n\t)\n\n\t// With the commitment tx still unconfirmed, we expect sweep attempts\n\t// for all three versions of the commitment transaction.\n\t<-chanArbCtx.sweeper.sweptInputs\n\t<-chanArbCtx.sweeper.sweptInputs\n\n\tselect {\n\tcase <-respChan:\n\tcase <-time.After(5 * time.Second):\n\t\tt.Fatalf(\"no response received\")\n\t}\n\n\tselect {\n\tcase err := <-errChan:\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"error force closing channel: %v\", err)\n\t\t}\n\tcase <-time.After(5 * time.Second):\n\t\tt.Fatalf(\"no response received\")\n\t}\n\n\t// Now notify about the local force close getting confirmed.\n\tcloseTx := &wire.MsgTx{\n\t\tTxIn: []*wire.TxIn{\n\t\t\t{\n\t\t\t\tPreviousOutPoint: wire.OutPoint{},\n\t\t\t\tWitness: [][]byte{\n\t\t\t\t\t{0x1},\n\t\t\t\t\t{0x2},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\tanchorResolution := &lnwallet.AnchorResolution{\n\t\tAnchorSignDescriptor: input.SignDescriptor{\n\t\t\tOutput: &wire.TxOut{\n\t\t\t\tValue: 1,\n\t\t\t},\n\t\t},\n\t}\n\n\tchanArb.cfg.ChainEvents.LocalUnilateralClosure <- &LocalUnilateralCloseInfo{\n\t\tSpendDetail: &chainntnfs.SpendDetail{},\n\t\tLocalForceCloseSummary: &lnwallet.LocalForceCloseSummary{\n\t\t\tCloseTx:          closeTx,\n\t\t\tHtlcResolutions:  &lnwallet.HtlcResolutions{},\n\t\t\tAnchorResolution: anchorResolution,\n\t\t},\n\t\tChannelCloseSummary: &channeldb.ChannelCloseSummary{},\n\t\tCommitSet: CommitSet{\n\t\t\tConfCommitKey: &LocalHtlcSet,\n\t\t\tHtlcSets:      map[HtlcSetKey][]channeldb.HTLC{},\n\t\t},\n\t}\n\n\tchanArbCtx.AssertStateTransitions(\n\t\tStateContractClosed,\n\t\tStateWaitingFullResolution,\n\t)\n\n\t// We expect to only have the anchor resolver active.\n\tif len(chanArb.activeResolvers) != 1 {\n\t\tt.Fatalf(\"expected single resolver, instead got: %v\",\n\t\t\tlen(chanArb.activeResolvers))\n\t}\n\n\tresolver := chanArb.activeResolvers[0]\n\t_, ok := resolver.(*anchorResolver)\n\tif !ok {\n\t\tt.Fatalf(\"expected anchor resolver, got %T\", resolver)\n\t}\n\n\t// The anchor resolver is expected to re-offer the anchor input to the\n\t// sweeper.\n\t<-chanArbCtx.sweeper.sweptInputs\n\n\t// The mock sweeper immediately signals success for that input. This\n\t// should transition the channel to the resolved state.\n\tchanArbCtx.AssertStateTransitions(StateFullyResolved)\n\tselect {\n\tcase <-chanArbCtx.resolvedChan:\n\tcase <-time.After(5 * time.Second):\n\t\tt.Fatalf(\"contract was not resolved\")\n\t}\n\n\tanchorAmt := btcutil.Amount(\n\t\tanchorResolution.AnchorSignDescriptor.Output.Value,\n\t)\n\tspendTx := chanArbCtx.sweeper.sweepTx.TxHash()\n\texpectedReport := &channeldb.ResolverReport{\n\t\tOutPoint:        anchorResolution.CommitAnchor,\n\t\tAmount:          anchorAmt,\n\t\tResolverType:    channeldb.ResolverTypeAnchor,\n\t\tResolverOutcome: channeldb.ResolverOutcomeClaimed,\n\t\tSpendTxID:       &spendTx,\n\t}\n\n\tassertResolverReport(t, reports, expectedReport)\n\n\t// We expect two anchor inputs, the local and the remote to be swept.\n\t// Thus we should expect there are two deadlines used, both are equal\n\t// to htlcWithPreimage's CLTV minus current block height.\n\trequire.Equal(t, 2, len(chanArbCtx.sweeper.deadlines))\n\trequire.EqualValues(t,\n\t\thtlcWithPreimage.RefundTimeout-heightHint,\n\t\tchanArbCtx.sweeper.deadlines[0],\n\t)\n\trequire.EqualValues(t,\n\t\thtlcWithPreimage.RefundTimeout-heightHint,\n\t\tchanArbCtx.sweeper.deadlines[1],\n\t)\n}\n\n// putResolverReportInChannel returns a put report function which will pipe\n// reports into the channel provided.",
      "length": 6340,
      "tokens": 675,
      "embedding": []
    },
    {
      "slug": "func putResolverReportInChannel(reports chan *channeldb.ResolverReport) func(",
      "content": "func putResolverReportInChannel(reports chan *channeldb.ResolverReport) func(\n\t_ kvdb.RwTx, report *channeldb.ResolverReport) error {\n\n\treturn func(_ kvdb.RwTx, report *channeldb.ResolverReport) error {\n\t\treports <- report\n\t\treturn nil\n\t}\n}\n\n// assertResolverReport checks that  a set of reports only contains a single\n// report, and that it is equal to the expected report passed in.",
      "length": 297,
      "tokens": 45,
      "embedding": []
    },
    {
      "slug": "func assertResolverReport(t *testing.T, reports chan *channeldb.ResolverReport,",
      "content": "func assertResolverReport(t *testing.T, reports chan *channeldb.ResolverReport,\n\texpected *channeldb.ResolverReport) {\n\n\tselect {\n\tcase report := <-reports:\n\t\tif !reflect.DeepEqual(report, expected) {\n\t\t\tt.Fatalf(\"expected: %v, got: %v\", expected, report)\n\t\t}\n\n\tcase <-time.After(defaultTimeout):\n\t\tt.Fatalf(\"no reports present\")\n\t}\n}\n",
      "length": 243,
      "tokens": 27,
      "embedding": []
    },
    {
      "slug": "type mockChannel struct {",
      "content": "type mockChannel struct {\n\tanchorResolutions *lnwallet.AnchorResolutions\n}\n",
      "length": 47,
      "tokens": 3,
      "embedding": []
    },
    {
      "slug": "func (m *mockChannel) NewAnchorResolutions() (*lnwallet.AnchorResolutions,",
      "content": "func (m *mockChannel) NewAnchorResolutions() (*lnwallet.AnchorResolutions,\n\terror) {\n\n\tif m.anchorResolutions != nil {\n\t\treturn m.anchorResolutions, nil\n\t}\n\n\treturn &lnwallet.AnchorResolutions{}, nil\n}\n",
      "length": 119,
      "tokens": 15,
      "embedding": []
    },
    {
      "slug": "func (m *mockChannel) ForceCloseChan() (*lnwallet.LocalForceCloseSummary, error) {",
      "content": "func (m *mockChannel) ForceCloseChan() (*lnwallet.LocalForceCloseSummary, error) {\n\tsummary := &lnwallet.LocalForceCloseSummary{\n\t\tCloseTx:         &wire.MsgTx{},\n\t\tHtlcResolutions: &lnwallet.HtlcResolutions{},\n\t}\n\treturn summary, nil\n}\n",
      "length": 148,
      "tokens": 12,
      "embedding": []
    }
  ]
}