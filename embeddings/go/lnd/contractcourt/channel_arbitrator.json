{
  "filepath": "../implementations/go/lnd/contractcourt/channel_arbitrator.go",
  "package": "contractcourt",
  "sections": [
    {
      "slug": "type WitnessSubscription struct {",
      "content": "type WitnessSubscription struct {\n\t// WitnessUpdates is a channel that newly discovered witnesses will be\n\t// sent over.\n\t//\n\t// TODO(roasbeef): couple with WitnessType?\n\tWitnessUpdates <-chan lntypes.Preimage\n\n\t// CancelSubscription is a function closure that should be used by a\n\t// client to cancel the subscription once they are no longer interested\n\t// in receiving new updates.\n\tCancelSubscription func()\n}\n\n// WitnessBeacon is a global beacon of witnesses. Contract resolvers will use\n// this interface to lookup witnesses (preimages typically) of contracts\n// they're trying to resolve, add new preimages they resolve, and finally\n// receive new updates each new time a preimage is discovered.\n//\n// TODO(roasbeef): need to delete the pre-images once we've used them\n// and have been sufficiently confirmed?",
      "length": 763,
      "tokens": 118,
      "embedding": []
    },
    {
      "slug": "type WitnessBeacon interface {",
      "content": "type WitnessBeacon interface {\n\t// SubscribeUpdates returns a channel that will be sent upon *each* time\n\t// a new preimage is discovered.\n\tSubscribeUpdates(chanID lnwire.ShortChannelID, htlc *channeldb.HTLC,\n\t\tpayload *hop.Payload,\n\t\tnextHopOnionBlob []byte) (*WitnessSubscription, error)\n\n\t// LookupPreImage attempts to lookup a preimage in the global cache.\n\t// True is returned for the second argument if the preimage is found.\n\tLookupPreimage(payhash lntypes.Hash) (lntypes.Preimage, bool)\n\n\t// AddPreimages adds a batch of newly discovered preimages to the global\n\t// cache, and also signals any subscribers of the newly discovered\n\t// witness.\n\tAddPreimages(preimages ...lntypes.Preimage) error\n}\n\n// ArbChannel is an abstraction that allows the channel arbitrator to interact\n// with an open channel.",
      "length": 760,
      "tokens": 102,
      "embedding": []
    },
    {
      "slug": "type ArbChannel interface {",
      "content": "type ArbChannel interface {\n\t// ForceCloseChan should force close the contract that this attendant\n\t// is watching over. We'll use this when we decide that we need to go\n\t// to chain. It should in addition tell the switch to remove the\n\t// corresponding link, such that we won't accept any new updates. The\n\t// returned summary contains all items needed to eventually resolve all\n\t// outputs on chain.\n\tForceCloseChan() (*lnwallet.LocalForceCloseSummary, error)\n\n\t// NewAnchorResolutions returns the anchor resolutions for currently\n\t// valid commitment transactions.\n\tNewAnchorResolutions() (*lnwallet.AnchorResolutions, error)\n}\n\n// ChannelArbitratorConfig contains all the functionality that the\n// ChannelArbitrator needs in order to properly arbitrate any contract dispute\n// on chain.",
      "length": 747,
      "tokens": 106,
      "embedding": []
    },
    {
      "slug": "type ChannelArbitratorConfig struct {",
      "content": "type ChannelArbitratorConfig struct {\n\t// ChanPoint is the channel point that uniquely identifies this\n\t// channel.\n\tChanPoint wire.OutPoint\n\n\t// Channel is the full channel data structure. For legacy channels, this\n\t// field may not always be set after a restart.\n\tChannel ArbChannel\n\n\t// ShortChanID describes the exact location of the channel within the\n\t// chain. We'll use this to address any messages that we need to send\n\t// to the switch during contract resolution.\n\tShortChanID lnwire.ShortChannelID\n\n\t// ChainEvents is an active subscription to the chain watcher for this\n\t// channel to be notified of any on-chain activity related to this\n\t// channel.\n\tChainEvents *ChainEventSubscription\n\n\t// MarkCommitmentBroadcasted should mark the channel as the commitment\n\t// being broadcast, and we are waiting for the commitment to confirm.\n\tMarkCommitmentBroadcasted func(*wire.MsgTx, bool) error\n\n\t// MarkChannelClosed marks the channel closed in the database, with the\n\t// passed close summary. After this method successfully returns we can\n\t// no longer expect to receive chain events for this channel, and must\n\t// be able to recover from a failure without getting the close event\n\t// again. It takes an optional channel status which will update the\n\t// channel status in the record that we keep of historical channels.\n\tMarkChannelClosed func(*channeldb.ChannelCloseSummary,\n\t\t...channeldb.ChannelStatus) error\n\n\t// IsPendingClose is a boolean indicating whether the channel is marked\n\t// as pending close in the database.\n\tIsPendingClose bool\n\n\t// ClosingHeight is the height at which the channel was closed. Note\n\t// that this value is only valid if IsPendingClose is true.\n\tClosingHeight uint32\n\n\t// CloseType is the type of the close event in case IsPendingClose is\n\t// true. Otherwise this value is unset.\n\tCloseType channeldb.ClosureType\n\n\t// MarkChannelResolved is a function closure that serves to mark a\n\t// channel as \"fully resolved\". A channel itself can be considered\n\t// fully resolved once all active contracts have individually been\n\t// fully resolved.\n\t//\n\t// TODO(roasbeef): need RPC's to combine for pendingchannels RPC\n\tMarkChannelResolved func() error\n\n\t// PutResolverReport records a resolver report for the channel. If the\n\t// transaction provided is nil, the function should write the report\n\t// in a new transaction.\n\tPutResolverReport func(tx kvdb.RwTx,\n\t\treport *channeldb.ResolverReport) error\n\n\t// FetchHistoricalChannel retrieves the historical state of a channel.\n\t// This is mostly used to supplement the ContractResolvers with\n\t// additional information required for proper contract resolution.\n\tFetchHistoricalChannel func() (*channeldb.OpenChannel, error)\n\n\tChainArbitratorConfig\n}\n\n// ReportOutputType describes the type of output that is being reported\n// on.",
      "length": 2701,
      "tokens": 395,
      "embedding": []
    },
    {
      "slug": "type ReportOutputType uint8",
      "content": "type ReportOutputType uint8\n\nconst (\n\t// ReportOutputIncomingHtlc is an incoming hash time locked contract on\n\t// the commitment tx.\n\tReportOutputIncomingHtlc ReportOutputType = iota\n\n\t// ReportOutputOutgoingHtlc is an outgoing hash time locked contract on\n\t// the commitment tx.\n\tReportOutputOutgoingHtlc\n\n\t// ReportOutputUnencumbered is an uncontested output on the commitment\n\t// transaction paying to us directly.\n\tReportOutputUnencumbered\n\n\t// ReportOutputAnchor is an anchor output on the commitment tx.\n\tReportOutputAnchor\n)\n\n// ContractReport provides a summary of a commitment tx output.",
      "length": 550,
      "tokens": 73,
      "embedding": []
    },
    {
      "slug": "type ContractReport struct {",
      "content": "type ContractReport struct {\n\t// Outpoint is the final output that will be swept back to the wallet.\n\tOutpoint wire.OutPoint\n\n\t// Type indicates the type of the reported output.\n\tType ReportOutputType\n\n\t// Amount is the final value that will be swept in back to the wallet.\n\tAmount btcutil.Amount\n\n\t// MaturityHeight is the absolute block height that this output will\n\t// mature at.\n\tMaturityHeight uint32\n\n\t// Stage indicates whether the htlc is in the CLTV-timeout stage (1) or\n\t// the CSV-delay stage (2). A stage 1 htlc's maturity height will be set\n\t// to its expiry height, while a stage 2 htlc's maturity height will be\n\t// set to its confirmation height plus the maturity requirement.\n\tStage uint32\n\n\t// LimboBalance is the total number of frozen coins within this\n\t// contract.\n\tLimboBalance btcutil.Amount\n\n\t// RecoveredBalance is the total value that has been successfully swept\n\t// back to the user's wallet.\n\tRecoveredBalance btcutil.Amount\n}\n\n// resolverReport creates a resolve report using some of the information in the\n// contract report.",
      "length": 998,
      "tokens": 164,
      "embedding": []
    },
    {
      "slug": "func (c *ContractReport) resolverReport(spendTx *chainhash.Hash,",
      "content": "func (c *ContractReport) resolverReport(spendTx *chainhash.Hash,\n\tresolverType channeldb.ResolverType,\n\toutcome channeldb.ResolverOutcome) *channeldb.ResolverReport {\n\n\treturn &channeldb.ResolverReport{\n\t\tOutPoint:        c.Outpoint,\n\t\tAmount:          c.Amount,\n\t\tResolverType:    resolverType,\n\t\tResolverOutcome: outcome,\n\t\tSpendTxID:       spendTx,\n\t}\n}\n\n// htlcSet represents the set of active HTLCs on a given commitment\n// transaction.",
      "length": 363,
      "tokens": 34,
      "embedding": []
    },
    {
      "slug": "type htlcSet struct {",
      "content": "type htlcSet struct {\n\t// incomingHTLCs is a map of all incoming HTLCs on the target\n\t// commitment transaction. We may potentially go onchain to claim the\n\t// funds sent to us within this set.\n\tincomingHTLCs map[uint64]channeldb.HTLC\n\n\t// outgoingHTLCs is a map of all outgoing HTLCs on the target\n\t// commitment transaction. We may potentially go onchain to reclaim the\n\t// funds that are currently in limbo.\n\toutgoingHTLCs map[uint64]channeldb.HTLC\n}\n\n// newHtlcSet constructs a new HTLC set from a slice of HTLC's.",
      "length": 485,
      "tokens": 78,
      "embedding": []
    },
    {
      "slug": "func newHtlcSet(htlcs []channeldb.HTLC) htlcSet {",
      "content": "func newHtlcSet(htlcs []channeldb.HTLC) htlcSet {\n\toutHTLCs := make(map[uint64]channeldb.HTLC)\n\tinHTLCs := make(map[uint64]channeldb.HTLC)\n\tfor _, htlc := range htlcs {\n\t\tif htlc.Incoming {\n\t\t\tinHTLCs[htlc.HtlcIndex] = htlc\n\t\t\tcontinue\n\t\t}\n\n\t\toutHTLCs[htlc.HtlcIndex] = htlc\n\t}\n\n\treturn htlcSet{\n\t\tincomingHTLCs: inHTLCs,\n\t\toutgoingHTLCs: outHTLCs,\n\t}\n}\n\n// HtlcSetKey is a two-tuple that uniquely identifies a set of HTLCs on a\n// commitment transaction.",
      "length": 387,
      "tokens": 50,
      "embedding": []
    },
    {
      "slug": "type HtlcSetKey struct {",
      "content": "type HtlcSetKey struct {\n\t// IsRemote denotes if the HTLCs are on the remote commitment\n\t// transaction.\n\tIsRemote bool\n\n\t// IsPending denotes if the commitment transaction that HTLCS are on\n\t// are pending (the higher of two unrevoked commitments).\n\tIsPending bool\n}\n\nvar (\n\t// LocalHtlcSet is the HtlcSetKey used for local commitments.\n\tLocalHtlcSet = HtlcSetKey{IsRemote: false, IsPending: false}\n\n\t// RemoteHtlcSet is the HtlcSetKey used for remote commitments.\n\tRemoteHtlcSet = HtlcSetKey{IsRemote: true, IsPending: false}\n\n\t// RemotePendingHtlcSet is the HtlcSetKey used for dangling remote\n\t// commitment transactions.\n\tRemotePendingHtlcSet = HtlcSetKey{IsRemote: true, IsPending: true}\n)\n\n// String returns a human readable string describing the target HtlcSetKey.",
      "length": 726,
      "tokens": 100,
      "embedding": []
    },
    {
      "slug": "func (h HtlcSetKey) String() string {",
      "content": "func (h HtlcSetKey) String() string {\n\tswitch h {\n\tcase LocalHtlcSet:\n\t\treturn \"LocalHtlcSet\"\n\tcase RemoteHtlcSet:\n\t\treturn \"RemoteHtlcSet\"\n\tcase RemotePendingHtlcSet:\n\t\treturn \"RemotePendingHtlcSet\"\n\tdefault:\n\t\treturn \"unknown HtlcSetKey\"\n\t}\n}\n\n// ChannelArbitrator is the on-chain arbitrator for a particular channel. The\n// struct will keep in sync with the current set of HTLCs on the commitment\n// transaction. The job of the attendant is to go on-chain to either settle or\n// cancel an HTLC as necessary iff: an HTLC times out, or we known the\n// pre-image to an HTLC, but it wasn't settled by the link off-chain. The\n// ChannelArbitrator will factor in an expected confirmation delta when\n// broadcasting to ensure that we avoid any possibility of race conditions, and\n// sweep the output(s) without contest.",
      "length": 758,
      "tokens": 120,
      "embedding": []
    },
    {
      "slug": "type ChannelArbitrator struct {",
      "content": "type ChannelArbitrator struct {\n\tstarted int32 // To be used atomically.\n\tstopped int32 // To be used atomically.\n\n\t// startTimestamp is the time when this ChannelArbitrator was started.\n\tstartTimestamp time.Time\n\n\t// log is a persistent log that the attendant will use to checkpoint\n\t// its next action, and the state of any unresolved contracts.\n\tlog ArbitratorLog\n\n\t// activeHTLCs is the set of active incoming/outgoing HTLC's on all\n\t// currently valid commitment transactions.\n\tactiveHTLCs map[HtlcSetKey]htlcSet\n\n\t// unmergedSet is used to update the activeHTLCs map in two callsites:\n\t// checkLocalChainActions and sweepAnchors. It contains the latest\n\t// updates from the link. It is not deleted from, its entries may be\n\t// replaced on subsequent calls to notifyContractUpdate.\n\tunmergedSet map[HtlcSetKey]htlcSet\n\tunmergedMtx sync.RWMutex\n\n\t// cfg contains all the functionality that the ChannelArbitrator requires\n\t// to do its duty.\n\tcfg ChannelArbitratorConfig\n\n\t// blocks is a channel that the arbitrator will receive new blocks on.\n\t// This channel should be buffered by so that it does not block the\n\t// sender.\n\tblocks chan int32\n\n\t// signalUpdates is a channel that any new live signals for the channel\n\t// we're watching over will be sent.\n\tsignalUpdates chan *signalUpdateMsg\n\n\t// activeResolvers is a slice of any active resolvers. This is used to\n\t// be able to signal them for shutdown in the case that we shutdown.\n\tactiveResolvers []ContractResolver\n\n\t// activeResolversLock prevents simultaneous read and write to the\n\t// resolvers slice.\n\tactiveResolversLock sync.RWMutex\n\n\t// resolutionSignal is a channel that will be sent upon by contract\n\t// resolvers once their contract has been fully resolved. With each\n\t// send, we'll check to see if the contract is fully resolved.\n\tresolutionSignal chan struct{}\n\n\t// forceCloseReqs is a channel that requests to forcibly close the\n\t// contract will be sent over.\n\tforceCloseReqs chan *forceCloseReq\n\n\t// state is the current state of the arbitrator. This state is examined\n\t// upon start up to decide which actions to take.\n\tstate ArbitratorState\n\n\twg   sync.WaitGroup\n\tquit chan struct{}\n}\n\n// NewChannelArbitrator returns a new instance of a ChannelArbitrator backed by\n// the passed config struct.",
      "length": 2180,
      "tokens": 335,
      "embedding": []
    },
    {
      "slug": "func NewChannelArbitrator(cfg ChannelArbitratorConfig,",
      "content": "func NewChannelArbitrator(cfg ChannelArbitratorConfig,\n\thtlcSets map[HtlcSetKey]htlcSet, log ArbitratorLog) *ChannelArbitrator {\n\n\t// Create a new map for unmerged HTLC's as we will overwrite the values\n\t// and want to avoid modifying activeHTLCs directly. This soft copying\n\t// is done to ensure that activeHTLCs isn't reset as an empty map later\n\t// on.\n\tunmerged := make(map[HtlcSetKey]htlcSet)\n\tunmerged[LocalHtlcSet] = htlcSets[LocalHtlcSet]\n\tunmerged[RemoteHtlcSet] = htlcSets[RemoteHtlcSet]\n\n\t// If the pending set exists, write that as well.\n\tif _, ok := htlcSets[RemotePendingHtlcSet]; ok {\n\t\tunmerged[RemotePendingHtlcSet] = htlcSets[RemotePendingHtlcSet]\n\t}\n\n\treturn &ChannelArbitrator{\n\t\tlog:              log,\n\t\tblocks:           make(chan int32, arbitratorBlockBufferSize),\n\t\tsignalUpdates:    make(chan *signalUpdateMsg),\n\t\tresolutionSignal: make(chan struct{}),\n\t\tforceCloseReqs:   make(chan *forceCloseReq),\n\t\tactiveHTLCs:      htlcSets,\n\t\tunmergedSet:      unmerged,\n\t\tcfg:              cfg,\n\t\tquit:             make(chan struct{}),\n\t}\n}\n\n// chanArbStartState contains the information from disk that we need to start\n// up a channel arbitrator.",
      "length": 1078,
      "tokens": 122,
      "embedding": []
    },
    {
      "slug": "type chanArbStartState struct {",
      "content": "type chanArbStartState struct {\n\tcurrentState ArbitratorState\n\tcommitSet    *CommitSet\n}\n\n// getStartState retrieves the information from disk that our channel arbitrator\n// requires to start.",
      "length": 155,
      "tokens": 20,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) getStartState(tx kvdb.RTx) (*chanArbStartState,",
      "content": "func (c *ChannelArbitrator) getStartState(tx kvdb.RTx) (*chanArbStartState,\n\terror) {\n\n\t// First, we'll read our last state from disk, so our internal state\n\t// machine can act accordingly.\n\tstate, err := c.log.CurrentState(tx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Next we'll fetch our confirmed commitment set. This will only exist\n\t// if the channel has been closed out on chain for modern nodes. For\n\t// older nodes, this won't be found at all, and will rely on the\n\t// existing written chain actions. Additionally, if this channel hasn't\n\t// logged any actions in the log, then this field won't be present.\n\tcommitSet, err := c.log.FetchConfirmedCommitSet(tx)\n\tif err != nil && err != errNoCommitSet && err != errScopeBucketNoExist {\n\t\treturn nil, err\n\t}\n\n\treturn &chanArbStartState{\n\t\tcurrentState: state,\n\t\tcommitSet:    commitSet,\n\t}, nil\n}\n\n// Start starts all the goroutines that the ChannelArbitrator needs to operate.\n// If takes a start state, which will be looked up on disk if it is not\n// provided.",
      "length": 916,
      "tokens": 157,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) Start(state *chanArbStartState) error {",
      "content": "func (c *ChannelArbitrator) Start(state *chanArbStartState) error {\n\tif !atomic.CompareAndSwapInt32(&c.started, 0, 1) {\n\t\treturn nil\n\t}\n\tc.startTimestamp = c.cfg.Clock.Now()\n\n\t// If the state passed in is nil, we look it up now.\n\tif state == nil {\n\t\tvar err error\n\t\tstate, err = c.getStartState(nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tlog.Debugf(\"Starting ChannelArbitrator(%v), htlc_set=%v, state=%v\",\n\t\tc.cfg.ChanPoint, newLogClosure(func() string {\n\t\t\treturn spew.Sdump(c.activeHTLCs)\n\t\t}), state.currentState,\n\t)\n\n\t// Set our state from our starting state.\n\tc.state = state.currentState\n\n\t_, bestHeight, err := c.cfg.ChainIO.GetBestBlock()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// If the channel has been marked pending close in the database, and we\n\t// haven't transitioned the state machine to StateContractClosed (or a\n\t// succeeding state), then a state transition most likely failed. We'll\n\t// try to recover from this by manually advancing the state by setting\n\t// the corresponding close trigger.\n\ttrigger := chainTrigger\n\ttriggerHeight := uint32(bestHeight)\n\tif c.cfg.IsPendingClose {\n\t\tswitch c.state {\n\t\tcase StateDefault:\n\t\t\tfallthrough\n\t\tcase StateBroadcastCommit:\n\t\t\tfallthrough\n\t\tcase StateCommitmentBroadcasted:\n\t\t\tswitch c.cfg.CloseType {\n\n\t\t\tcase channeldb.CooperativeClose:\n\t\t\t\ttrigger = coopCloseTrigger\n\n\t\t\tcase channeldb.BreachClose:\n\t\t\t\ttrigger = breachCloseTrigger\n\n\t\t\tcase channeldb.LocalForceClose:\n\t\t\t\ttrigger = localCloseTrigger\n\n\t\t\tcase channeldb.RemoteForceClose:\n\t\t\t\ttrigger = remoteCloseTrigger\n\t\t\t}\n\n\t\t\tlog.Warnf(\"ChannelArbitrator(%v): detected stalled \"+\n\t\t\t\t\"state=%v for closed channel\",\n\t\t\t\tc.cfg.ChanPoint, c.state)\n\t\t}\n\n\t\ttriggerHeight = c.cfg.ClosingHeight\n\t}\n\n\tlog.Infof(\"ChannelArbitrator(%v): starting state=%v, trigger=%v, \"+\n\t\t\"triggerHeight=%v\", c.cfg.ChanPoint, c.state, trigger,\n\t\ttriggerHeight)\n\n\t// We'll now attempt to advance our state forward based on the current\n\t// on-chain state, and our set of active contracts.\n\tstartingState := c.state\n\tnextState, _, err := c.advanceState(\n\t\ttriggerHeight, trigger, state.commitSet,\n\t)\n\tif err != nil {\n\t\tswitch err {\n\n\t\t// If we detect that we tried to fetch resolutions, but failed,\n\t\t// this channel was marked closed in the database before\n\t\t// resolutions successfully written. In this case there is not\n\t\t// much we can do, so we don't return the error.\n\t\tcase errScopeBucketNoExist:\n\t\t\tfallthrough\n\t\tcase errNoResolutions:\n\t\t\tlog.Warnf(\"ChannelArbitrator(%v): detected closed\"+\n\t\t\t\t\"channel with no contract resolutions written.\",\n\t\t\t\tc.cfg.ChanPoint)\n\n\t\tdefault:\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// If we start and ended at the awaiting full resolution state, then\n\t// we'll relaunch our set of unresolved contracts.\n\tif startingState == StateWaitingFullResolution &&\n\t\tnextState == StateWaitingFullResolution {\n\n\t\t// In order to relaunch the resolvers, we'll need to fetch the\n\t\t// set of HTLCs that were present in the commitment transaction\n\t\t// at the time it was confirmed. commitSet.ConfCommitKey can't\n\t\t// be nil at this point since we're in\n\t\t// StateWaitingFullResolution. We can only be in\n\t\t// StateWaitingFullResolution after we've transitioned from\n\t\t// StateContractClosed which can only be triggered by the local\n\t\t// or remote close trigger. This trigger is only fired when we\n\t\t// receive a chain event from the chain watcher than the\n\t\t// commitment has been confirmed on chain, and before we\n\t\t// advance our state step, we call InsertConfirmedCommitSet.\n\t\terr := c.relaunchResolvers(state.commitSet, triggerHeight)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tc.wg.Add(1)\n\tgo c.channelAttendant(bestHeight)\n\treturn nil\n}\n\n// relauchResolvers relaunches the set of resolvers for unresolved contracts in\n// order to provide them with information that's not immediately available upon\n// starting the ChannelArbitrator. This information should ideally be stored in\n// the database, so this only serves as a intermediate work-around to prevent a\n// migration.",
      "length": 3778,
      "tokens": 513,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) relaunchResolvers(commitSet *CommitSet,",
      "content": "func (c *ChannelArbitrator) relaunchResolvers(commitSet *CommitSet,\n\theightHint uint32) error {\n\n\t// We'll now query our log to see if there are any active unresolved\n\t// contracts. If this is the case, then we'll relaunch all contract\n\t// resolvers.\n\tunresolvedContracts, err := c.log.FetchUnresolvedContracts()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Retrieve the commitment tx hash from the log.\n\tcontractResolutions, err := c.log.FetchContractResolutions()\n\tif err != nil {\n\t\tlog.Errorf(\"unable to fetch contract resolutions: %v\",\n\t\t\terr)\n\t\treturn err\n\t}\n\tcommitHash := contractResolutions.CommitHash\n\n\t// In prior versions of lnd, the information needed to supplement the\n\t// resolvers (in most cases, the full amount of the HTLC) was found in\n\t// the chain action map, which is now deprecated.  As a result, if the\n\t// commitSet is nil (an older node with unresolved HTLCs at time of\n\t// upgrade), then we'll use the chain action information in place. The\n\t// chain actions may exclude some information, but we cannot recover it\n\t// for these older nodes at the moment.\n\tvar confirmedHTLCs []channeldb.HTLC\n\tif commitSet != nil {\n\t\tconfirmedHTLCs = commitSet.HtlcSets[*commitSet.ConfCommitKey]\n\t} else {\n\t\tchainActions, err := c.log.FetchChainActions()\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"unable to fetch chain actions: %v\", err)\n\t\t\treturn err\n\t\t}\n\t\tfor _, htlcs := range chainActions {\n\t\t\tconfirmedHTLCs = append(confirmedHTLCs, htlcs...)\n\t\t}\n\t}\n\n\t// Reconstruct the htlc outpoints and data from the chain action log.\n\t// The purpose of the constructed htlc map is to supplement to\n\t// resolvers restored from database with extra data. Ideally this data\n\t// is stored as part of the resolver in the log. This is a workaround\n\t// to prevent a db migration. We use all available htlc sets here in\n\t// order to ensure we have complete coverage.\n\thtlcMap := make(map[wire.OutPoint]*channeldb.HTLC)\n\tfor _, htlc := range confirmedHTLCs {\n\t\thtlc := htlc\n\t\toutpoint := wire.OutPoint{\n\t\t\tHash:  commitHash,\n\t\t\tIndex: uint32(htlc.OutputIndex),\n\t\t}\n\t\thtlcMap[outpoint] = &htlc\n\t}\n\n\t// We'll also fetch the historical state of this channel, as it should\n\t// have been marked as closed by now, and supplement it to each resolver\n\t// such that we can properly resolve our pending contracts.\n\tvar chanState *channeldb.OpenChannel\n\tchanState, err = c.cfg.FetchHistoricalChannel()\n\tswitch {\n\t// If we don't find this channel, then it may be the case that it\n\t// was closed before we started to retain the final state\n\t// information for open channels.\n\tcase err == channeldb.ErrNoHistoricalBucket:\n\t\tfallthrough\n\tcase err == channeldb.ErrChannelNotFound:\n\t\tlog.Warnf(\"ChannelArbitrator(%v): unable to fetch historical \"+\n\t\t\t\"state\", c.cfg.ChanPoint)\n\n\tcase err != nil:\n\t\treturn err\n\t}\n\n\tlog.Infof(\"ChannelArbitrator(%v): relaunching %v contract \"+\n\t\t\"resolvers\", c.cfg.ChanPoint, len(unresolvedContracts))\n\n\tfor _, resolver := range unresolvedContracts {\n\t\tif chanState != nil {\n\t\t\tresolver.SupplementState(chanState)\n\t\t}\n\n\t\thtlcResolver, ok := resolver.(htlcContractResolver)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\thtlcPoint := htlcResolver.HtlcPoint()\n\t\thtlc, ok := htlcMap[htlcPoint]\n\t\tif !ok {\n\t\t\treturn fmt.Errorf(\n\t\t\t\t\"htlc resolver %T unavailable\", resolver,\n\t\t\t)\n\t\t}\n\n\t\thtlcResolver.Supplement(*htlc)\n\t}\n\n\t// The anchor resolver is stateless and can always be re-instantiated.\n\tif contractResolutions.AnchorResolution != nil {\n\t\tanchorResolver := newAnchorResolver(\n\t\t\tcontractResolutions.AnchorResolution.AnchorSignDescriptor,\n\t\t\tcontractResolutions.AnchorResolution.CommitAnchor,\n\t\t\theightHint, c.cfg.ChanPoint,\n\t\t\tResolverConfig{\n\t\t\t\tChannelArbitratorConfig: c.cfg,\n\t\t\t},\n\t\t)\n\t\tunresolvedContracts = append(unresolvedContracts, anchorResolver)\n\t}\n\n\tc.launchResolvers(unresolvedContracts)\n\n\treturn nil\n}\n\n// Report returns htlc reports for the active resolvers.",
      "length": 3663,
      "tokens": 502,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) Report() []*ContractReport {",
      "content": "func (c *ChannelArbitrator) Report() []*ContractReport {\n\tc.activeResolversLock.RLock()\n\tdefer c.activeResolversLock.RUnlock()\n\n\tvar reports []*ContractReport\n\tfor _, resolver := range c.activeResolvers {\n\t\tr, ok := resolver.(reportingContractResolver)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\treport := r.report()\n\t\tif report == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\treports = append(reports, report)\n\t}\n\n\treturn reports\n}\n\n// Stop signals the ChannelArbitrator for a graceful shutdown.",
      "length": 385,
      "tokens": 49,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) Stop() error {",
      "content": "func (c *ChannelArbitrator) Stop() error {\n\tif !atomic.CompareAndSwapInt32(&c.stopped, 0, 1) {\n\t\treturn nil\n\t}\n\n\tlog.Debugf(\"Stopping ChannelArbitrator(%v)\", c.cfg.ChanPoint)\n\n\tif c.cfg.ChainEvents.Cancel != nil {\n\t\tgo c.cfg.ChainEvents.Cancel()\n\t}\n\n\tc.activeResolversLock.RLock()\n\tfor _, activeResolver := range c.activeResolvers {\n\t\tactiveResolver.Stop()\n\t}\n\tc.activeResolversLock.RUnlock()\n\n\tclose(c.quit)\n\tc.wg.Wait()\n\n\treturn nil\n}\n\n// transitionTrigger is an enum that denotes exactly *why* a state transition\n// was initiated. This is useful as depending on the initial trigger, we may\n// skip certain states as those actions are expected to have already taken\n// place as a result of the external trigger.",
      "length": 645,
      "tokens": 83,
      "embedding": []
    },
    {
      "slug": "type transitionTrigger uint8",
      "content": "type transitionTrigger uint8\n\nconst (\n\t// chainTrigger is a transition trigger that has been attempted due to\n\t// changing on-chain conditions such as a block which times out HTLC's\n\t// being attached.\n\tchainTrigger transitionTrigger = iota\n\n\t// userTrigger is a transition trigger driven by user action. Examples\n\t// of such a trigger include a user requesting a force closure of the\n\t// channel.\n\tuserTrigger\n\n\t// remoteCloseTrigger is a transition trigger driven by the remote\n\t// peer's commitment being confirmed.\n\tremoteCloseTrigger\n\n\t// localCloseTrigger is a transition trigger driven by our commitment\n\t// being confirmed.\n\tlocalCloseTrigger\n\n\t// coopCloseTrigger is a transition trigger driven by a cooperative\n\t// close transaction being confirmed.\n\tcoopCloseTrigger\n\n\t// breachCloseTrigger is a transition trigger driven by a remote breach\n\t// being confirmed. In this case the channel arbitrator will wait for\n\t// the breacharbiter to finish and then clean up gracefully.\n\tbreachCloseTrigger\n)\n\n// String returns a human readable string describing the passed\n// transitionTrigger.",
      "length": 1033,
      "tokens": 154,
      "embedding": []
    },
    {
      "slug": "func (t transitionTrigger) String() string {",
      "content": "func (t transitionTrigger) String() string {\n\tswitch t {\n\tcase chainTrigger:\n\t\treturn \"chainTrigger\"\n\n\tcase remoteCloseTrigger:\n\t\treturn \"remoteCloseTrigger\"\n\n\tcase userTrigger:\n\t\treturn \"userTrigger\"\n\n\tcase localCloseTrigger:\n\t\treturn \"localCloseTrigger\"\n\n\tcase coopCloseTrigger:\n\t\treturn \"coopCloseTrigger\"\n\n\tcase breachCloseTrigger:\n\t\treturn \"breachCloseTrigger\"\n\n\tdefault:\n\t\treturn \"unknown trigger\"\n\t}\n}\n\n// stateStep is a help method that examines our internal state, and attempts\n// the appropriate state transition if necessary. The next state we transition\n// to is returned, Additionally, if the next transition results in a commitment\n// broadcast, the commitment transaction itself is returned.",
      "length": 634,
      "tokens": 79,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) stateStep(",
      "content": "func (c *ChannelArbitrator) stateStep(\n\ttriggerHeight uint32, trigger transitionTrigger,\n\tconfCommitSet *CommitSet) (ArbitratorState, *wire.MsgTx, error) {\n\n\tvar (\n\t\tnextState ArbitratorState\n\t\tcloseTx   *wire.MsgTx\n\t)\n\tswitch c.state {\n\n\t// If we're in the default state, then we'll check our set of actions\n\t// to see if while we were down, conditions have changed.\n\tcase StateDefault:\n\t\tlog.Debugf(\"ChannelArbitrator(%v): new block (height=%v) \"+\n\t\t\t\"examining active HTLC's\", c.cfg.ChanPoint,\n\t\t\ttriggerHeight)\n\n\t\t// As a new block has been connected to the end of the main\n\t\t// chain, we'll check to see if we need to make any on-chain\n\t\t// claims on behalf of the channel contract that we're\n\t\t// arbitrating for. If a commitment has confirmed, then we'll\n\t\t// use the set snapshot from the chain, otherwise we'll use our\n\t\t// current set.\n\t\tvar htlcs map[HtlcSetKey]htlcSet\n\t\tif confCommitSet != nil {\n\t\t\thtlcs = confCommitSet.toActiveHTLCSets()\n\t\t} else {\n\t\t\t// Update the set of activeHTLCs so\n\t\t\t// checkLocalChainActions has an up-to-date view of the\n\t\t\t// commitments.\n\t\t\tc.updateActiveHTLCs()\n\t\t\thtlcs = c.activeHTLCs\n\t\t}\n\t\tchainActions, err := c.checkLocalChainActions(\n\t\t\ttriggerHeight, trigger, htlcs, false,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn StateDefault, nil, err\n\t\t}\n\n\t\t// If there are no actions to be made, then we'll remain in the\n\t\t// default state. If this isn't a self initiated event (we're\n\t\t// checking due to a chain update), then we'll exit now.\n\t\tif len(chainActions) == 0 && trigger == chainTrigger {\n\t\t\tlog.Debugf(\"ChannelArbitrator(%v): no actions for \"+\n\t\t\t\t\"chain trigger, terminating\", c.cfg.ChanPoint)\n\n\t\t\treturn StateDefault, closeTx, nil\n\t\t}\n\n\t\t// Otherwise, we'll log that we checked the HTLC actions as the\n\t\t// commitment transaction has already been broadcast.\n\t\tlog.Tracef(\"ChannelArbitrator(%v): logging chain_actions=%v\",\n\t\t\tc.cfg.ChanPoint,\n\t\t\tnewLogClosure(func() string {\n\t\t\t\treturn spew.Sdump(chainActions)\n\t\t\t}))\n\n\t\t// Depending on the type of trigger, we'll either \"tunnel\"\n\t\t// through to a farther state, or just proceed linearly to the\n\t\t// next state.\n\t\tswitch trigger {\n\n\t\t// If this is a chain trigger, then we'll go straight to the\n\t\t// next state, as we still need to broadcast the commitment\n\t\t// transaction.\n\t\tcase chainTrigger:\n\t\t\tfallthrough\n\t\tcase userTrigger:\n\t\t\tnextState = StateBroadcastCommit\n\n\t\t// If the trigger is a cooperative close being confirmed, then\n\t\t// we can go straight to StateFullyResolved, as there won't be\n\t\t// any contracts to resolve.\n\t\tcase coopCloseTrigger:\n\t\t\tnextState = StateFullyResolved\n\n\t\t// Otherwise, if this state advance was triggered by a\n\t\t// commitment being confirmed on chain, then we'll jump\n\t\t// straight to the state where the contract has already been\n\t\t// closed, and we will inspect the set of unresolved contracts.\n\t\tcase localCloseTrigger:\n\t\t\tlog.Errorf(\"ChannelArbitrator(%v): unexpected local \"+\n\t\t\t\t\"commitment confirmed while in StateDefault\",\n\t\t\t\tc.cfg.ChanPoint)\n\t\t\tfallthrough\n\t\tcase remoteCloseTrigger:\n\t\t\tnextState = StateContractClosed\n\n\t\tcase breachCloseTrigger:\n\t\t\tnextContractState, err := c.checkLegacyBreach()\n\t\t\tif nextContractState == StateError {\n\t\t\t\treturn nextContractState, nil, err\n\t\t\t}\n\n\t\t\tnextState = nextContractState\n\t\t}\n\n\t// If we're in this state, then we've decided to broadcast the\n\t// commitment transaction. We enter this state either due to an outside\n\t// sub-system, or because an on-chain action has been triggered.\n\tcase StateBroadcastCommit:\n\t\t// Under normal operation, we can only enter\n\t\t// StateBroadcastCommit via a user or chain trigger. On restart,\n\t\t// this state may be reexecuted after closing the channel, but\n\t\t// failing to commit to StateContractClosed or\n\t\t// StateFullyResolved. In that case, one of the four close\n\t\t// triggers will be presented, signifying that we should skip\n\t\t// rebroadcasting, and go straight to resolving the on-chain\n\t\t// contract or marking the channel resolved.\n\t\tswitch trigger {\n\t\tcase localCloseTrigger, remoteCloseTrigger:\n\t\t\tlog.Infof(\"ChannelArbitrator(%v): detected %s \"+\n\t\t\t\t\"close after closing channel, fast-forwarding \"+\n\t\t\t\t\"to %s to resolve contract\",\n\t\t\t\tc.cfg.ChanPoint, trigger, StateContractClosed)\n\t\t\treturn StateContractClosed, closeTx, nil\n\n\t\tcase breachCloseTrigger:\n\t\t\tnextContractState, err := c.checkLegacyBreach()\n\t\t\tif nextContractState == StateError {\n\t\t\t\tlog.Infof(\"ChannelArbitrator(%v): unable to \"+\n\t\t\t\t\t\"advance breach close resolution: %v\",\n\t\t\t\t\tc.cfg.ChanPoint, nextContractState)\n\t\t\t\treturn StateError, closeTx, err\n\t\t\t}\n\n\t\t\tlog.Infof(\"ChannelArbitrator(%v): detected %s close \"+\n\t\t\t\t\"after closing channel, fast-forwarding to %s\"+\n\t\t\t\t\" to resolve contract\", c.cfg.ChanPoint,\n\t\t\t\ttrigger, nextContractState)\n\n\t\t\treturn nextContractState, closeTx, nil\n\n\t\tcase coopCloseTrigger:\n\t\t\tlog.Infof(\"ChannelArbitrator(%v): detected %s \"+\n\t\t\t\t\"close after closing channel, fast-forwarding \"+\n\t\t\t\t\"to %s to resolve contract\",\n\t\t\t\tc.cfg.ChanPoint, trigger, StateFullyResolved)\n\t\t\treturn StateFullyResolved, closeTx, nil\n\t\t}\n\n\t\tlog.Infof(\"ChannelArbitrator(%v): force closing \"+\n\t\t\t\"chan\", c.cfg.ChanPoint)\n\n\t\t// Now that we have all the actions decided for the set of\n\t\t// HTLC's, we'll broadcast the commitment transaction, and\n\t\t// signal the link to exit.\n\n\t\t// We'll tell the switch that it should remove the link for\n\t\t// this channel, in addition to fetching the force close\n\t\t// summary needed to close this channel on chain.\n\t\tcloseSummary, err := c.cfg.Channel.ForceCloseChan()\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"ChannelArbitrator(%v): unable to \"+\n\t\t\t\t\"force close: %v\", c.cfg.ChanPoint, err)\n\t\t\treturn StateError, closeTx, err\n\t\t}\n\t\tcloseTx = closeSummary.CloseTx\n\n\t\t// Before publishing the transaction, we store it to the\n\t\t// database, such that we can re-publish later in case it\n\t\t// didn't propagate. We initiated the force close, so we\n\t\t// mark broadcast with local initiator set to true.\n\t\terr = c.cfg.MarkCommitmentBroadcasted(closeTx, true)\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"ChannelArbitrator(%v): unable to \"+\n\t\t\t\t\"mark commitment broadcasted: %v\",\n\t\t\t\tc.cfg.ChanPoint, err)\n\t\t\treturn StateError, closeTx, err\n\t\t}\n\n\t\t// With the close transaction in hand, broadcast the\n\t\t// transaction to the network, thereby entering the post\n\t\t// channel resolution state.\n\t\tlog.Infof(\"Broadcasting force close transaction %v, \"+\n\t\t\t\"ChannelPoint(%v): %v\", closeTx.TxHash(),\n\t\t\tc.cfg.ChanPoint,\n\t\t\tnewLogClosure(func() string {\n\t\t\t\treturn spew.Sdump(closeTx)\n\t\t\t}))\n\n\t\t// At this point, we'll now broadcast the commitment\n\t\t// transaction itself.\n\t\tlabel := labels.MakeLabel(\n\t\t\tlabels.LabelTypeChannelClose, &c.cfg.ShortChanID,\n\t\t)\n\n\t\tif err := c.cfg.PublishTx(closeTx, label); err != nil {\n\t\t\tlog.Errorf(\"ChannelArbitrator(%v): unable to broadcast \"+\n\t\t\t\t\"close tx: %v\", c.cfg.ChanPoint, err)\n\t\t\tif err != lnwallet.ErrDoubleSpend {\n\t\t\t\treturn StateError, closeTx, err\n\t\t\t}\n\t\t}\n\n\t\t// We go to the StateCommitmentBroadcasted state, where we'll\n\t\t// be waiting for the commitment to be confirmed.\n\t\tnextState = StateCommitmentBroadcasted\n\n\t// In this state we have broadcasted our own commitment, and will need\n\t// to wait for a commitment (not necessarily the one we broadcasted!)\n\t// to be confirmed.\n\tcase StateCommitmentBroadcasted:\n\t\tswitch trigger {\n\n\t\t// We are waiting for a commitment to be confirmed.\n\t\tcase chainTrigger, userTrigger:\n\t\t\t// The commitment transaction has been broadcast, but it\n\t\t\t// doesn't necessarily need to be the commitment\n\t\t\t// transaction version that is going to be confirmed. To\n\t\t\t// be sure that any of those versions can be anchored\n\t\t\t// down, we now submit all anchor resolutions to the\n\t\t\t// sweeper. The sweeper will keep trying to sweep all of\n\t\t\t// them.\n\t\t\t//\n\t\t\t// Note that the sweeper is idempotent. If we ever\n\t\t\t// happen to end up at this point in the code again, no\n\t\t\t// harm is done by re-offering the anchors to the\n\t\t\t// sweeper.\n\t\t\tanchors, err := c.cfg.Channel.NewAnchorResolutions()\n\t\t\tif err != nil {\n\t\t\t\treturn StateError, closeTx, err\n\t\t\t}\n\n\t\t\terr = c.sweepAnchors(anchors, triggerHeight)\n\t\t\tif err != nil {\n\t\t\t\treturn StateError, closeTx, err\n\t\t\t}\n\n\t\t\tnextState = StateCommitmentBroadcasted\n\n\t\t// If this state advance was triggered by any of the\n\t\t// commitments being confirmed, then we'll jump to the state\n\t\t// where the contract has been closed.\n\t\tcase localCloseTrigger, remoteCloseTrigger:\n\t\t\tnextState = StateContractClosed\n\n\t\t// If a coop close was confirmed, jump straight to the fully\n\t\t// resolved state.\n\t\tcase coopCloseTrigger:\n\t\t\tnextState = StateFullyResolved\n\n\t\tcase breachCloseTrigger:\n\t\t\tnextContractState, err := c.checkLegacyBreach()\n\t\t\tif nextContractState == StateError {\n\t\t\t\treturn nextContractState, closeTx, err\n\t\t\t}\n\n\t\t\tnextState = nextContractState\n\t\t}\n\n\t\tlog.Infof(\"ChannelArbitrator(%v): trigger %v moving from \"+\n\t\t\t\"state %v to %v\", c.cfg.ChanPoint, trigger, c.state,\n\t\t\tnextState)\n\n\t// If we're in this state, then the contract has been fully closed to\n\t// outside sub-systems, so we'll process the prior set of on-chain\n\t// contract actions and launch a set of resolvers.\n\tcase StateContractClosed:\n\t\t// First, we'll fetch our chain actions, and both sets of\n\t\t// resolutions so we can process them.\n\t\tcontractResolutions, err := c.log.FetchContractResolutions()\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"unable to fetch contract resolutions: %v\",\n\t\t\t\terr)\n\t\t\treturn StateError, closeTx, err\n\t\t}\n\n\t\t// If the resolution is empty, and we have no HTLCs at all to\n\t\t// send to, then we're done here. We don't need to launch any\n\t\t// resolvers, and can go straight to our final state.\n\t\tif contractResolutions.IsEmpty() && confCommitSet.IsEmpty() {\n\t\t\tlog.Infof(\"ChannelArbitrator(%v): contract \"+\n\t\t\t\t\"resolutions empty, marking channel as fully resolved!\",\n\t\t\t\tc.cfg.ChanPoint)\n\t\t\tnextState = StateFullyResolved\n\t\t\tbreak\n\t\t}\n\n\t\t// Now that we know we'll need to act, we'll process the htlc\n\t\t// actions, then create the structures we need to resolve all\n\t\t// outstanding contracts.\n\t\thtlcResolvers, pktsToSend, err := c.prepContractResolutions(\n\t\t\tcontractResolutions, triggerHeight, trigger,\n\t\t\tconfCommitSet,\n\t\t)\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"ChannelArbitrator(%v): unable to \"+\n\t\t\t\t\"resolve contracts: %v\", c.cfg.ChanPoint, err)\n\t\t\treturn StateError, closeTx, err\n\t\t}\n\n\t\tlog.Debugf(\"ChannelArbitrator(%v): sending resolution message=%v\",\n\t\t\tc.cfg.ChanPoint,\n\t\t\tnewLogClosure(func() string {\n\t\t\t\treturn spew.Sdump(pktsToSend)\n\t\t\t}))\n\n\t\t// With the commitment broadcast, we'll then send over all\n\t\t// messages we can send immediately.\n\t\tif len(pktsToSend) != 0 {\n\t\t\terr := c.cfg.DeliverResolutionMsg(pktsToSend...)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"unable to send pkts: %v\", err)\n\t\t\t\treturn StateError, closeTx, err\n\t\t\t}\n\t\t}\n\n\t\tlog.Debugf(\"ChannelArbitrator(%v): inserting %v contract \"+\n\t\t\t\"resolvers\", c.cfg.ChanPoint, len(htlcResolvers))\n\n\t\terr = c.log.InsertUnresolvedContracts(nil, htlcResolvers...)\n\t\tif err != nil {\n\t\t\treturn StateError, closeTx, err\n\t\t}\n\n\t\t// Finally, we'll launch all the required contract resolvers.\n\t\t// Once they're all resolved, we're no longer needed.\n\t\tc.launchResolvers(htlcResolvers)\n\n\t\tnextState = StateWaitingFullResolution\n\n\t// This is our terminal state. We'll keep returning this state until\n\t// all contracts are fully resolved.\n\tcase StateWaitingFullResolution:\n\t\tlog.Infof(\"ChannelArbitrator(%v): still awaiting contract \"+\n\t\t\t\"resolution\", c.cfg.ChanPoint)\n\n\t\tnumUnresolved, err := c.log.FetchUnresolvedContracts()\n\t\tif err != nil {\n\t\t\treturn StateError, closeTx, err\n\t\t}\n\n\t\t// If we still have unresolved contracts, then we'll stay alive\n\t\t// to oversee their resolution.\n\t\tif len(numUnresolved) != 0 {\n\t\t\tnextState = StateWaitingFullResolution\n\t\t\tbreak\n\t\t}\n\n\t\tnextState = StateFullyResolved\n\n\t// If we start as fully resolved, then we'll end as fully resolved.\n\tcase StateFullyResolved:\n\t\t// To ensure that the state of the contract in persistent\n\t\t// storage is properly reflected, we'll mark the contract as\n\t\t// fully resolved now.\n\t\tnextState = StateFullyResolved\n\n\t\tlog.Infof(\"ChannelPoint(%v) has been fully resolved \"+\n\t\t\t\"on-chain at height=%v\", c.cfg.ChanPoint, triggerHeight)\n\n\t\tif err := c.cfg.MarkChannelResolved(); err != nil {\n\t\t\tlog.Errorf(\"unable to mark channel resolved: %v\", err)\n\t\t\treturn StateError, closeTx, err\n\t\t}\n\t}\n\n\tlog.Tracef(\"ChannelArbitrator(%v): next_state=%v\", c.cfg.ChanPoint,\n\t\tnextState)\n\n\treturn nextState, closeTx, nil\n}\n\n// sweepAnchors offers all given anchor resolutions to the sweeper. It requests\n// sweeping at the minimum fee rate. This fee rate can be upped manually by the\n// user via the BumpFee rpc.",
      "length": 12206,
      "tokens": 1642,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) sweepAnchors(anchors *lnwallet.AnchorResolutions,",
      "content": "func (c *ChannelArbitrator) sweepAnchors(anchors *lnwallet.AnchorResolutions,\n\theightHint uint32) error {\n\n\t// Use the chan id as the exclusive group. This prevents any of the\n\t// anchors from being batched together.\n\texclusiveGroup := c.cfg.ShortChanID.ToUint64()\n\n\t// sweepWithDeadline is a helper closure that takes an anchor\n\t// resolution and sweeps it with its corresponding deadline.\n\tsweepWithDeadline := func(anchor *lnwallet.AnchorResolution,\n\t\thtlcs htlcSet, anchorPath string) error {\n\n\t\t// Find the deadline for this specific anchor.\n\t\tdeadline, err := c.findCommitmentDeadline(heightHint, htlcs)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tlog.Debugf(\"ChannelArbitrator(%v): pre-confirmation sweep of \"+\n\t\t\t\"anchor of %s commit tx %v\", c.cfg.ChanPoint,\n\t\t\tanchorPath, anchor.CommitAnchor)\n\n\t\t// Prepare anchor output for sweeping.\n\t\tanchorInput := input.MakeBaseInput(\n\t\t\t&anchor.CommitAnchor,\n\t\t\tinput.CommitmentAnchor,\n\t\t\t&anchor.AnchorSignDescriptor,\n\t\t\theightHint,\n\t\t\t&input.TxInfo{\n\t\t\t\tFee:    anchor.CommitFee,\n\t\t\t\tWeight: anchor.CommitWeight,\n\t\t\t},\n\t\t)\n\n\t\t// Sweep anchor output with a confirmation target fee\n\t\t// preference. Because this is a cpfp-operation, the anchor will\n\t\t// only be attempted to sweep when the current fee estimate for\n\t\t// the confirmation target exceeds the commit fee rate.\n\t\t//\n\t\t// Also signal that this is a force sweep, so that the anchor\n\t\t// will be swept even if it isn't economical purely based on the\n\t\t// anchor value.\n\t\t_, err = c.cfg.Sweeper.SweepInput(\n\t\t\t&anchorInput,\n\t\t\tsweep.Params{\n\t\t\t\tFee: sweep.FeePreference{\n\t\t\t\t\tConfTarget: deadline,\n\t\t\t\t},\n\t\t\t\tForce:          true,\n\t\t\t\tExclusiveGroup: &exclusiveGroup,\n\t\t\t},\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\treturn nil\n\t}\n\n\t// Update the set of activeHTLCs so that the sweeping routine has an\n\t// up-to-date view of the set of commitments.\n\tc.updateActiveHTLCs()\n\n\t// Sweep anchors based on different HTLC sets. Notice the HTLC sets may\n\t// differ across commitments, thus their deadline values could vary.\n\tfor htlcSet, htlcs := range c.activeHTLCs {\n\t\tswitch {\n\t\tcase htlcSet == LocalHtlcSet && anchors.Local != nil:\n\t\t\terr := sweepWithDeadline(anchors.Local, htlcs, \"local\")\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\tcase htlcSet == RemoteHtlcSet && anchors.Remote != nil:\n\t\t\terr := sweepWithDeadline(\n\t\t\t\tanchors.Remote, htlcs, \"remote\",\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\tcase htlcSet == RemotePendingHtlcSet &&\n\t\t\tanchors.RemotePending != nil:\n\n\t\t\terr := sweepWithDeadline(\n\t\t\t\tanchors.RemotePending, htlcs, \"remote pending\",\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// findCommitmentDeadline finds the deadline (relative block height) for a\n// commitment transaction by extracting the minimum CLTV from its HTLCs. From\n// our PoV, the deadline is defined to be the smaller of,\n//   - the least CLTV from outgoing HTLCs,  or,\n//   - the least CLTV from incoming HTLCs if the preimage is available.\n//\n// Note: when the deadline turns out to be 0 blocks, we will replace it with 1\n// block because our fee estimator doesn't allow a 0 conf target. This also\n// means we've left behind and should increase our fee to make the transaction\n// confirmed asap.",
      "length": 3026,
      "tokens": 441,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) findCommitmentDeadline(heightHint uint32,",
      "content": "func (c *ChannelArbitrator) findCommitmentDeadline(heightHint uint32,\n\thtlcs htlcSet) (uint32, error) {\n\n\tdeadlineMinHeight := uint32(math.MaxUint32)\n\n\t// First, iterate through the outgoingHTLCs to find the lowest CLTV\n\t// value.\n\tfor _, htlc := range htlcs.outgoingHTLCs {\n\t\t// Skip if the HTLC is dust.\n\t\tif htlc.OutputIndex < 0 {\n\t\t\tlog.Debugf(\"ChannelArbitrator(%v): skipped deadline \"+\n\t\t\t\t\"for dust htlc=%x\",\n\t\t\t\tc.cfg.ChanPoint, htlc.RHash[:])\n\n\t\t\tcontinue\n\t\t}\n\n\t\tif htlc.RefundTimeout < deadlineMinHeight {\n\t\t\tdeadlineMinHeight = htlc.RefundTimeout\n\t\t}\n\t}\n\n\t// Then going through the incomingHTLCs, and update the minHeight when\n\t// conditions met.\n\tfor _, htlc := range htlcs.incomingHTLCs {\n\t\t// Skip if the HTLC is dust.\n\t\tif htlc.OutputIndex < 0 {\n\t\t\tlog.Debugf(\"ChannelArbitrator(%v): skipped deadline \"+\n\t\t\t\t\"for dust htlc=%x\",\n\t\t\t\tc.cfg.ChanPoint, htlc.RHash[:])\n\n\t\t\tcontinue\n\t\t}\n\n\t\t// Since it's an HTLC sent to us, check if we have preimage for\n\t\t// this HTLC.\n\t\tpreimageAvailable, err := c.isPreimageAvailable(htlc.RHash)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\n\t\tif !preimageAvailable {\n\t\t\tcontinue\n\t\t}\n\n\t\tif htlc.RefundTimeout < deadlineMinHeight {\n\t\t\tdeadlineMinHeight = htlc.RefundTimeout\n\t\t}\n\t}\n\n\t// Calculate the deadline. There are two cases to be handled here,\n\t//   - when the deadlineMinHeight never gets updated, which could\n\t//     happen when we have no outgoing HTLCs, and, for incoming HTLCs,\n\t//       * either we have none, or,\n\t//       * none of the HTLCs are preimageAvailable.\n\t//   - when our deadlineMinHeight is no greater than the heightHint,\n\t//     which means we are behind our schedule.\n\tdeadline := deadlineMinHeight - heightHint\n\tswitch {\n\t// When we couldn't find a deadline height from our HTLCs, we will fall\n\t// back to the default value.\n\tcase deadlineMinHeight == math.MaxUint32:\n\t\tdeadline = anchorSweepConfTarget\n\n\t// When the deadline is passed, we will fall back to the smallest conf\n\t// target (1 block).\n\tcase deadlineMinHeight <= heightHint:\n\t\tlog.Warnf(\"ChannelArbitrator(%v): deadline is passed with \"+\n\t\t\t\"deadlineMinHeight=%d, heightHint=%d\",\n\t\t\tc.cfg.ChanPoint, deadlineMinHeight, heightHint)\n\t\tdeadline = 1\n\t}\n\n\tlog.Debugf(\"ChannelArbitrator(%v): calculated deadline: %d, \"+\n\t\t\"using deadlineMinHeight=%d, heightHint=%d\",\n\t\tc.cfg.ChanPoint, deadline, deadlineMinHeight, heightHint)\n\n\treturn deadline, nil\n}\n\n// launchResolvers updates the activeResolvers list and starts the resolvers.",
      "length": 2305,
      "tokens": 315,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) launchResolvers(resolvers []ContractResolver) {",
      "content": "func (c *ChannelArbitrator) launchResolvers(resolvers []ContractResolver) {\n\tc.activeResolversLock.Lock()\n\tdefer c.activeResolversLock.Unlock()\n\n\tc.activeResolvers = resolvers\n\tfor _, contract := range resolvers {\n\t\tc.wg.Add(1)\n\t\tgo c.resolveContract(contract)\n\t}\n}\n\n// advanceState is the main driver of our state machine. This method is an\n// iterative function which repeatedly attempts to advance the internal state\n// of the channel arbitrator. The state will be advanced until we reach a\n// redundant transition, meaning that the state transition is a noop. The final\n// param is a callback that allows the caller to execute an arbitrary action\n// after each state transition.",
      "length": 591,
      "tokens": 89,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) advanceState(",
      "content": "func (c *ChannelArbitrator) advanceState(\n\ttriggerHeight uint32, trigger transitionTrigger,\n\tconfCommitSet *CommitSet) (ArbitratorState, *wire.MsgTx, error) {\n\n\tvar (\n\t\tpriorState   ArbitratorState\n\t\tforceCloseTx *wire.MsgTx\n\t)\n\n\t// We'll continue to advance our state forward until the state we\n\t// transition to is that same state that we started at.\n\tfor {\n\t\tpriorState = c.state\n\t\tlog.Debugf(\"ChannelArbitrator(%v): attempting state step with \"+\n\t\t\t\"trigger=%v from state=%v\", c.cfg.ChanPoint, trigger,\n\t\t\tpriorState)\n\n\t\tnextState, closeTx, err := c.stateStep(\n\t\t\ttriggerHeight, trigger, confCommitSet,\n\t\t)\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"ChannelArbitrator(%v): unable to advance \"+\n\t\t\t\t\"state: %v\", c.cfg.ChanPoint, err)\n\t\t\treturn priorState, nil, err\n\t\t}\n\n\t\tif forceCloseTx == nil && closeTx != nil {\n\t\t\tforceCloseTx = closeTx\n\t\t}\n\n\t\t// Our termination transition is a noop transition. If we get\n\t\t// our prior state back as the next state, then we'll\n\t\t// terminate.\n\t\tif nextState == priorState {\n\t\t\tlog.Debugf(\"ChannelArbitrator(%v): terminating at \"+\n\t\t\t\t\"state=%v\", c.cfg.ChanPoint, nextState)\n\t\t\treturn nextState, forceCloseTx, nil\n\t\t}\n\n\t\t// As the prior state was successfully executed, we can now\n\t\t// commit the next state. This ensures that we will re-execute\n\t\t// the prior state if anything fails.\n\t\tif err := c.log.CommitState(nextState); err != nil {\n\t\t\tlog.Errorf(\"ChannelArbitrator(%v): unable to commit \"+\n\t\t\t\t\"next state(%v): %v\", c.cfg.ChanPoint,\n\t\t\t\tnextState, err)\n\t\t\treturn priorState, nil, err\n\t\t}\n\t\tc.state = nextState\n\t}\n}\n\n// ChainAction is an enum that encompasses all possible on-chain actions\n// we'll take for a set of HTLC's.",
      "length": 1569,
      "tokens": 216,
      "embedding": []
    },
    {
      "slug": "type ChainAction uint8",
      "content": "type ChainAction uint8\n\nconst (\n\t// NoAction is the min chainAction type, indicating that no action\n\t// needs to be taken for a given HTLC.\n\tNoAction ChainAction = 0\n\n\t// HtlcTimeoutAction indicates that the HTLC will timeout soon. As a\n\t// result, we should get ready to sweep it on chain after the timeout.\n\tHtlcTimeoutAction = 1\n\n\t// HtlcClaimAction indicates that we should claim the HTLC on chain\n\t// before its timeout period.\n\tHtlcClaimAction = 2\n\n\t// HtlcFailNowAction indicates that we should fail an outgoing HTLC\n\t// immediately by cancelling it backwards as it has no corresponding\n\t// output in our commitment transaction.\n\tHtlcFailNowAction = 3\n\n\t// HtlcOutgoingWatchAction indicates that we can't yet timeout this\n\t// HTLC, but we had to go to chain on order to resolve an existing\n\t// HTLC.  In this case, we'll either: time it out once it expires, or\n\t// will learn the pre-image if the remote party claims the output. In\n\t// this case, well add the pre-image to our global store.\n\tHtlcOutgoingWatchAction = 4\n\n\t// HtlcIncomingWatchAction indicates that we don't yet have the\n\t// pre-image to claim incoming HTLC, but we had to go to chain in order\n\t// to resolve and existing HTLC. In this case, we'll either: let the\n\t// other party time it out, or eventually learn of the pre-image, in\n\t// which case we'll claim on chain.\n\tHtlcIncomingWatchAction = 5\n\n\t// HtlcIncomingDustFinalAction indicates that we should mark an incoming\n\t// dust htlc as final because it can't be claimed on-chain.\n\tHtlcIncomingDustFinalAction = 6\n)\n\n// String returns a human readable string describing a chain action.",
      "length": 1551,
      "tokens": 263,
      "embedding": []
    },
    {
      "slug": "func (c ChainAction) String() string {",
      "content": "func (c ChainAction) String() string {\n\tswitch c {\n\tcase NoAction:\n\t\treturn \"NoAction\"\n\n\tcase HtlcTimeoutAction:\n\t\treturn \"HtlcTimeoutAction\"\n\n\tcase HtlcClaimAction:\n\t\treturn \"HtlcClaimAction\"\n\n\tcase HtlcFailNowAction:\n\t\treturn \"HtlcFailNowAction\"\n\n\tcase HtlcOutgoingWatchAction:\n\t\treturn \"HtlcOutgoingWatchAction\"\n\n\tcase HtlcIncomingWatchAction:\n\t\treturn \"HtlcIncomingWatchAction\"\n\n\tcase HtlcIncomingDustFinalAction:\n\t\treturn \"HtlcIncomingDustFinalAction\"\n\n\tdefault:\n\t\treturn \"<unknown action>\"\n\t}\n}\n\n// ChainActionMap is a map of a chain action, to the set of HTLC's that need to\n// be acted upon for a given action type. The channel",
      "length": 568,
      "tokens": 65,
      "embedding": []
    },
    {
      "slug": "type ChainActionMap map[ChainAction][]channeldb.HTLC",
      "content": "type ChainActionMap map[ChainAction][]channeldb.HTLC\n\n// Merge merges the passed chain actions with the target chain action map.",
      "length": 74,
      "tokens": 13,
      "embedding": []
    },
    {
      "slug": "func (c ChainActionMap) Merge(actions ChainActionMap) {",
      "content": "func (c ChainActionMap) Merge(actions ChainActionMap) {\n\tfor chainAction, htlcs := range actions {\n\t\tc[chainAction] = append(c[chainAction], htlcs...)\n\t}\n}\n\n// shouldGoOnChain takes into account the absolute timeout of the HTLC, if the\n// confirmation delta that we need is close, and returns a bool indicating if\n// we should go on chain to claim.  We do this rather than waiting up until the\n// last minute as we want to ensure that when we *need* (HTLC is timed out) to\n// sweep, the commitment is already confirmed.",
      "length": 454,
      "tokens": 81,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) shouldGoOnChain(htlc channeldb.HTLC,",
      "content": "func (c *ChannelArbitrator) shouldGoOnChain(htlc channeldb.HTLC,\n\tbroadcastDelta, currentHeight uint32) bool {\n\n\t// We'll calculate the broadcast cut off for this HTLC. This is the\n\t// height that (based on our current fee estimation) we should\n\t// broadcast in order to ensure the commitment transaction is confirmed\n\t// before the HTLC fully expires.\n\tbroadcastCutOff := htlc.RefundTimeout - broadcastDelta\n\n\tlog.Tracef(\"ChannelArbitrator(%v): examining outgoing contract: \"+\n\t\t\"expiry=%v, cutoff=%v, height=%v\", c.cfg.ChanPoint, htlc.RefundTimeout,\n\t\tbroadcastCutOff, currentHeight)\n\n\t// TODO(roasbeef): take into account default HTLC delta, don't need to\n\t// broadcast immediately\n\t//  * can then batch with SINGLE | ANYONECANPAY\n\n\t// We should on-chain for this HTLC, iff we're within out broadcast\n\t// cutoff window.\n\tif currentHeight < broadcastCutOff {\n\t\treturn false\n\t}\n\n\t// In case of incoming htlc we should go to chain.\n\tif htlc.Incoming {\n\t\treturn true\n\t}\n\n\t// For htlcs that are result of our initiated payments we give some grace\n\t// period before force closing the channel. During this time we expect\n\t// both nodes to connect and give a chance to the other node to send its\n\t// updates and cancel the htlc.\n\t// This shouldn't add any security risk as there is no incoming htlc to\n\t// fulfill at this case and the expectation is that when the channel is\n\t// active the other node will send update_fail_htlc to remove the htlc\n\t// without closing the channel. It is up to the user to force close the\n\t// channel if the peer misbehaves and doesn't send the update_fail_htlc.\n\t// It is useful when this node is most of the time not online and is\n\t// likely to miss the time slot where the htlc may be cancelled.\n\tisForwarded := c.cfg.IsForwardedHTLC(c.cfg.ShortChanID, htlc.HtlcIndex)\n\tupTime := c.cfg.Clock.Now().Sub(c.startTimestamp)\n\treturn isForwarded || upTime > c.cfg.PaymentsExpirationGracePeriod\n}\n\n// checkCommitChainActions is called for each new block connected to the end of\n// the main chain. Given the new block height, this new method will examine all\n// active HTLC's, and determine if we need to go on-chain to claim any of them.\n// A map of action -> []htlc is returned, detailing what action (if any) should\n// be performed for each HTLC. For timed out HTLC's, once the commitment has\n// been sufficiently confirmed, the HTLC's should be canceled backwards. For\n// redeemed HTLC's, we should send the pre-image back to the incoming link.",
      "length": 2355,
      "tokens": 380,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) checkCommitChainActions(height uint32,",
      "content": "func (c *ChannelArbitrator) checkCommitChainActions(height uint32,\n\ttrigger transitionTrigger, htlcs htlcSet) (ChainActionMap, error) {\n\n\t// TODO(roasbeef): would need to lock channel? channel totem?\n\t//  * race condition if adding and we broadcast, etc\n\t//  * or would make each instance sync?\n\n\tlog.Debugf(\"ChannelArbitrator(%v): checking commit chain actions at \"+\n\t\t\"height=%v, in_htlc_count=%v, out_htlc_count=%v\",\n\t\tc.cfg.ChanPoint, height,\n\t\tlen(htlcs.incomingHTLCs), len(htlcs.outgoingHTLCs))\n\n\tactionMap := make(ChainActionMap)\n\n\t// First, we'll make an initial pass over the set of incoming and\n\t// outgoing HTLC's to decide if we need to go on chain at all.\n\thaveChainActions := false\n\tfor _, htlc := range htlcs.outgoingHTLCs {\n\t\t// We'll need to go on-chain for an outgoing HTLC if it was\n\t\t// never resolved downstream, and it's \"close\" to timing out.\n\t\ttoChain := c.shouldGoOnChain(htlc, c.cfg.OutgoingBroadcastDelta,\n\t\t\theight,\n\t\t)\n\n\t\tif toChain {\n\t\t\tlog.Infof(\"ChannelArbitrator(%v): go to chain for \"+\n\t\t\t\t\"outgoing htlc %x: timeout=%v, \"+\n\t\t\t\t\"blocks_until_expiry=%v, broadcast_delta=%v\",\n\t\t\t\tc.cfg.ChanPoint, htlc.RHash[:],\n\t\t\t\thtlc.RefundTimeout, htlc.RefundTimeout-height,\n\t\t\t\tc.cfg.OutgoingBroadcastDelta,\n\t\t\t)\n\t\t}\n\n\t\thaveChainActions = haveChainActions || toChain\n\t}\n\n\tfor _, htlc := range htlcs.incomingHTLCs {\n\t\t// We'll need to go on-chain to pull an incoming HTLC iff we\n\t\t// know the pre-image and it's close to timing out. We need to\n\t\t// ensure that we claim the funds that are rightfully ours\n\t\t// on-chain.\n\t\tpreimageAvailable, err := c.isPreimageAvailable(htlc.RHash)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif !preimageAvailable {\n\t\t\tcontinue\n\t\t}\n\n\t\ttoChain := c.shouldGoOnChain(htlc, c.cfg.IncomingBroadcastDelta,\n\t\t\theight,\n\t\t)\n\n\t\tif toChain {\n\t\t\tlog.Infof(\"ChannelArbitrator(%v): go to chain for \"+\n\t\t\t\t\"incoming htlc %x: timeout=%v, \"+\n\t\t\t\t\"blocks_until_expiry=%v, broadcast_delta=%v\",\n\t\t\t\tc.cfg.ChanPoint, htlc.RHash[:],\n\t\t\t\thtlc.RefundTimeout, htlc.RefundTimeout-height,\n\t\t\t\tc.cfg.IncomingBroadcastDelta,\n\t\t\t)\n\t\t}\n\n\t\thaveChainActions = haveChainActions || toChain\n\t}\n\n\t// If we don't have any actions to make, then we'll return an empty\n\t// action map. We only do this if this was a chain trigger though, as\n\t// if we're going to broadcast the commitment (or the remote party did)\n\t// we're *forced* to act on each HTLC.\n\tif !haveChainActions && trigger == chainTrigger {\n\t\tlog.Tracef(\"ChannelArbitrator(%v): no actions to take at \"+\n\t\t\t\"height=%v\", c.cfg.ChanPoint, height)\n\t\treturn actionMap, nil\n\t}\n\n\t// Now that we know we'll need to go on-chain, we'll examine all of our\n\t// active outgoing HTLC's to see if we either need to: sweep them after\n\t// a timeout (then cancel backwards), cancel them backwards\n\t// immediately, or watch them as they're still active contracts.\n\tfor _, htlc := range htlcs.outgoingHTLCs {\n\t\tswitch {\n\t\t// If the HTLC is dust, then we can cancel it backwards\n\t\t// immediately as there's no matching contract to arbitrate\n\t\t// on-chain. We know the HTLC is dust, if the OutputIndex\n\t\t// negative.\n\t\tcase htlc.OutputIndex < 0:\n\t\t\tlog.Tracef(\"ChannelArbitrator(%v): immediately \"+\n\t\t\t\t\"failing dust htlc=%x\", c.cfg.ChanPoint,\n\t\t\t\thtlc.RHash[:])\n\n\t\t\tactionMap[HtlcFailNowAction] = append(\n\t\t\t\tactionMap[HtlcFailNowAction], htlc,\n\t\t\t)\n\n\t\t// If we don't need to immediately act on this HTLC, then we'll\n\t\t// mark it still \"live\". After we broadcast, we'll monitor it\n\t\t// until the HTLC times out to see if we can also redeem it\n\t\t// on-chain.\n\t\tcase !c.shouldGoOnChain(htlc, c.cfg.OutgoingBroadcastDelta,\n\t\t\theight,\n\t\t):\n\t\t\t// TODO(roasbeef): also need to be able to query\n\t\t\t// circuit map to see if HTLC hasn't been fully\n\t\t\t// resolved\n\t\t\t//\n\t\t\t//  * can't fail incoming until if outgoing not yet\n\t\t\t//  failed\n\n\t\t\tlog.Tracef(\"ChannelArbitrator(%v): watching chain to \"+\n\t\t\t\t\"decide action for outgoing htlc=%x\",\n\t\t\t\tc.cfg.ChanPoint, htlc.RHash[:])\n\n\t\t\tactionMap[HtlcOutgoingWatchAction] = append(\n\t\t\t\tactionMap[HtlcOutgoingWatchAction], htlc,\n\t\t\t)\n\n\t\t// Otherwise, we'll update our actionMap to mark that we need\n\t\t// to sweep this HTLC on-chain\n\t\tdefault:\n\t\t\tlog.Tracef(\"ChannelArbitrator(%v): going on-chain to \"+\n\t\t\t\t\"timeout htlc=%x\", c.cfg.ChanPoint, htlc.RHash[:])\n\n\t\t\tactionMap[HtlcTimeoutAction] = append(\n\t\t\t\tactionMap[HtlcTimeoutAction], htlc,\n\t\t\t)\n\t\t}\n\t}\n\n\t// Similarly, for each incoming HTLC, now that we need to go on-chain,\n\t// we'll either: sweep it immediately if we know the pre-image, or\n\t// observe the output on-chain if we don't In this last, case we'll\n\t// either learn of it eventually from the outgoing HTLC, or the sender\n\t// will timeout the HTLC.\n\tfor _, htlc := range htlcs.incomingHTLCs {\n\t\t// If the HTLC is dust, there is no action to be taken.\n\t\tif htlc.OutputIndex < 0 {\n\t\t\tlog.Debugf(\"ChannelArbitrator(%v): no resolution \"+\n\t\t\t\t\"needed for incoming dust htlc=%x\",\n\t\t\t\tc.cfg.ChanPoint, htlc.RHash[:])\n\n\t\t\tactionMap[HtlcIncomingDustFinalAction] = append(\n\t\t\t\tactionMap[HtlcIncomingDustFinalAction], htlc,\n\t\t\t)\n\n\t\t\tcontinue\n\t\t}\n\n\t\tlog.Tracef(\"ChannelArbitrator(%v): watching chain to decide \"+\n\t\t\t\"action for incoming htlc=%x\", c.cfg.ChanPoint,\n\t\t\thtlc.RHash[:])\n\n\t\tactionMap[HtlcIncomingWatchAction] = append(\n\t\t\tactionMap[HtlcIncomingWatchAction], htlc,\n\t\t)\n\t}\n\n\treturn actionMap, nil\n}\n\n// isPreimageAvailable returns whether the hash preimage is available in either\n// the preimage cache or the invoice database.",
      "length": 5194,
      "tokens": 699,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) isPreimageAvailable(hash lntypes.Hash) (bool,",
      "content": "func (c *ChannelArbitrator) isPreimageAvailable(hash lntypes.Hash) (bool,\n\terror) {\n\n\t// Start by checking the preimage cache for preimages of\n\t// forwarded HTLCs.\n\t_, preimageAvailable := c.cfg.PreimageDB.LookupPreimage(\n\t\thash,\n\t)\n\tif preimageAvailable {\n\t\treturn true, nil\n\t}\n\n\t// Then check if we have an invoice that can be settled by this HTLC.\n\t//\n\t// TODO(joostjager): Check that there are still more blocks remaining\n\t// than the invoice cltv delta. We don't want to go to chain only to\n\t// have the incoming contest resolver decide that we don't want to\n\t// settle this invoice.\n\tinvoice, err := c.cfg.Registry.LookupInvoice(hash)\n\tswitch err {\n\tcase nil:\n\tcase invoices.ErrInvoiceNotFound, invoices.ErrNoInvoicesCreated:\n\t\treturn false, nil\n\tdefault:\n\t\treturn false, err\n\t}\n\n\tpreimageAvailable = invoice.Terms.PaymentPreimage != nil\n\n\treturn preimageAvailable, nil\n}\n\n// checkLocalChainActions is similar to checkCommitChainActions, but it also\n// examines the set of HTLCs on the remote party's commitment. This allows us\n// to ensure we're able to satisfy the HTLC timeout constraints for incoming vs\n// outgoing HTLCs.",
      "length": 1024,
      "tokens": 154,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) checkLocalChainActions(",
      "content": "func (c *ChannelArbitrator) checkLocalChainActions(\n\theight uint32, trigger transitionTrigger,\n\tactiveHTLCs map[HtlcSetKey]htlcSet,\n\tcommitsConfirmed bool) (ChainActionMap, error) {\n\n\t// First, we'll check our local chain actions as normal. This will only\n\t// examine HTLCs on our local commitment (timeout or settle).\n\tlocalCommitActions, err := c.checkCommitChainActions(\n\t\theight, trigger, activeHTLCs[LocalHtlcSet],\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Next, we'll examine the remote commitment (and maybe a dangling one)\n\t// to see if the set difference of our HTLCs is non-empty. If so, then\n\t// we may need to cancel back some HTLCs if we decide go to chain.\n\tremoteDanglingActions := c.checkRemoteDanglingActions(\n\t\theight, activeHTLCs, commitsConfirmed,\n\t)\n\n\t// Finally, we'll merge the two set of chain actions.\n\tlocalCommitActions.Merge(remoteDanglingActions)\n\n\treturn localCommitActions, nil\n}\n\n// checkRemoteDanglingActions examines the set of remote commitments for any\n// HTLCs that are close to timing out. If we find any, then we'll return a set\n// of chain actions for HTLCs that are on our commitment, but not theirs to\n// cancel immediately.",
      "length": 1089,
      "tokens": 160,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) checkRemoteDanglingActions(",
      "content": "func (c *ChannelArbitrator) checkRemoteDanglingActions(\n\theight uint32, activeHTLCs map[HtlcSetKey]htlcSet,\n\tcommitsConfirmed bool) ChainActionMap {\n\n\tvar (\n\t\tpendingRemoteHTLCs []channeldb.HTLC\n\t\tlocalHTLCs         = make(map[uint64]struct{})\n\t\tremoteHTLCs        = make(map[uint64]channeldb.HTLC)\n\t\tactionMap          = make(ChainActionMap)\n\t)\n\n\t// First, we'll construct two sets of the outgoing HTLCs: those on our\n\t// local commitment, and those that are on the remote commitment(s).\n\tfor htlcSetKey, htlcs := range activeHTLCs {\n\t\tif htlcSetKey.IsRemote {\n\t\t\tfor _, htlc := range htlcs.outgoingHTLCs {\n\t\t\t\tremoteHTLCs[htlc.HtlcIndex] = htlc\n\t\t\t}\n\t\t} else {\n\t\t\tfor _, htlc := range htlcs.outgoingHTLCs {\n\t\t\t\tlocalHTLCs[htlc.HtlcIndex] = struct{}{}\n\t\t\t}\n\t\t}\n\t}\n\n\t// With both sets constructed, we'll now compute the set difference of\n\t// our two sets of HTLCs. This'll give us the HTLCs that exist on the\n\t// remote commitment transaction, but not on ours.\n\tfor htlcIndex, htlc := range remoteHTLCs {\n\t\tif _, ok := localHTLCs[htlcIndex]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tpendingRemoteHTLCs = append(pendingRemoteHTLCs, htlc)\n\t}\n\n\t// Finally, we'll examine all the pending remote HTLCs for those that\n\t// have expired. If we find any, then we'll recommend that they be\n\t// failed now so we can free up the incoming HTLC.\n\tfor _, htlc := range pendingRemoteHTLCs {\n\t\t// We'll now check if we need to go to chain in order to cancel\n\t\t// the incoming HTLC.\n\t\tgoToChain := c.shouldGoOnChain(htlc, c.cfg.OutgoingBroadcastDelta,\n\t\t\theight,\n\t\t)\n\n\t\t// If we don't need to go to chain, and no commitments have\n\t\t// been confirmed, then we can move on. Otherwise, if\n\t\t// commitments have been confirmed, then we need to cancel back\n\t\t// *all* of the pending remote HTLCS.\n\t\tif !goToChain && !commitsConfirmed {\n\t\t\tcontinue\n\t\t}\n\n\t\tlog.Infof(\"ChannelArbitrator(%v): immediately failing \"+\n\t\t\t\"htlc=%x from remote commitment\",\n\t\t\tc.cfg.ChanPoint, htlc.RHash[:])\n\n\t\tactionMap[HtlcFailNowAction] = append(\n\t\t\tactionMap[HtlcFailNowAction], htlc,\n\t\t)\n\t}\n\n\treturn actionMap\n}\n\n// checkRemoteChainActions examines the two possible remote commitment chains\n// and returns the set of chain actions we need to carry out if the remote\n// commitment (non pending) confirms. The pendingConf indicates if the pending\n// remote commitment confirmed. This is similar to checkCommitChainActions, but\n// we'll immediately fail any HTLCs on the pending remote commit, but not the\n// remote commit (or the other way around).",
      "length": 2361,
      "tokens": 343,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) checkRemoteChainActions(",
      "content": "func (c *ChannelArbitrator) checkRemoteChainActions(\n\theight uint32, trigger transitionTrigger,\n\tactiveHTLCs map[HtlcSetKey]htlcSet,\n\tpendingConf bool) (ChainActionMap, error) {\n\n\t// First, we'll examine all the normal chain actions on the remote\n\t// commitment that confirmed.\n\tconfHTLCs := activeHTLCs[RemoteHtlcSet]\n\tif pendingConf {\n\t\tconfHTLCs = activeHTLCs[RemotePendingHtlcSet]\n\t}\n\tremoteCommitActions, err := c.checkCommitChainActions(\n\t\theight, trigger, confHTLCs,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// With these actions computed, we'll now check the diff of the HTLCs on\n\t// the commitments, and cancel back any that are on the pending but not\n\t// the non-pending.\n\tremoteDiffActions := c.checkRemoteDiffActions(\n\t\theight, activeHTLCs, pendingConf,\n\t)\n\n\t// Finally, we'll merge all the chain actions and the final set of\n\t// chain actions.\n\tremoteCommitActions.Merge(remoteDiffActions)\n\treturn remoteCommitActions, nil\n}\n\n// checkRemoteDiffActions checks the set difference of the HTLCs on the remote\n// confirmed commit and remote dangling commit for HTLCS that we need to cancel\n// back. If we find any HTLCs on the remote pending but not the remote, then\n// we'll mark them to be failed immediately.",
      "length": 1135,
      "tokens": 163,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) checkRemoteDiffActions(height uint32,",
      "content": "func (c *ChannelArbitrator) checkRemoteDiffActions(height uint32,\n\tactiveHTLCs map[HtlcSetKey]htlcSet,\n\tpendingConf bool) ChainActionMap {\n\n\t// First, we'll partition the HTLCs into those that are present on the\n\t// confirmed commitment, and those on the dangling commitment.\n\tconfHTLCs := activeHTLCs[RemoteHtlcSet]\n\tdanglingHTLCs := activeHTLCs[RemotePendingHtlcSet]\n\tif pendingConf {\n\t\tconfHTLCs = activeHTLCs[RemotePendingHtlcSet]\n\t\tdanglingHTLCs = activeHTLCs[RemoteHtlcSet]\n\t}\n\n\t// Next, we'll create a set of all the HTLCs confirmed commitment.\n\tremoteHtlcs := make(map[uint64]struct{})\n\tfor _, htlc := range confHTLCs.outgoingHTLCs {\n\t\tremoteHtlcs[htlc.HtlcIndex] = struct{}{}\n\t}\n\n\t// With the remote HTLCs assembled, we'll mark any HTLCs only on the\n\t// remote dangling commitment to be failed asap.\n\tactionMap := make(ChainActionMap)\n\tfor _, htlc := range danglingHTLCs.outgoingHTLCs {\n\t\tif _, ok := remoteHtlcs[htlc.HtlcIndex]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tactionMap[HtlcFailNowAction] = append(\n\t\t\tactionMap[HtlcFailNowAction], htlc,\n\t\t)\n\n\t\tlog.Infof(\"ChannelArbitrator(%v): immediately failing \"+\n\t\t\t\"htlc=%x from remote commitment\",\n\t\t\tc.cfg.ChanPoint, htlc.RHash[:])\n\t}\n\n\treturn actionMap\n}\n\n// constructChainActions returns the set of actions that should be taken for\n// confirmed HTLCs at the specified height. Our actions will depend on the set\n// of HTLCs that were active across all channels at the time of channel\n// closure.",
      "length": 1334,
      "tokens": 172,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) constructChainActions(confCommitSet *CommitSet,",
      "content": "func (c *ChannelArbitrator) constructChainActions(confCommitSet *CommitSet,\n\theight uint32, trigger transitionTrigger) (ChainActionMap, error) {\n\n\t// If we've reached this point and have not confirmed commitment set,\n\t// then this is an older node that had a pending close channel before\n\t// the CommitSet was introduced. In this case, we'll just return the\n\t// existing ChainActionMap they had on disk.\n\tif confCommitSet == nil {\n\t\treturn c.log.FetchChainActions()\n\t}\n\n\t// Otherwise, we have the full commitment set written to disk, and can\n\t// proceed as normal.\n\thtlcSets := confCommitSet.toActiveHTLCSets()\n\tswitch *confCommitSet.ConfCommitKey {\n\n\t// If the local commitment transaction confirmed, then we'll examine\n\t// that as well as their commitments to the set of chain actions.\n\tcase LocalHtlcSet:\n\t\treturn c.checkLocalChainActions(\n\t\t\theight, trigger, htlcSets, true,\n\t\t)\n\n\t// If the remote commitment confirmed, then we'll grab all the chain\n\t// actions for the remote commit, and check the pending commit for any\n\t// HTLCS we need to handle immediately (dust).\n\tcase RemoteHtlcSet:\n\t\treturn c.checkRemoteChainActions(\n\t\t\theight, trigger, htlcSets, false,\n\t\t)\n\n\t// Otherwise, the remote pending commitment confirmed, so we'll examine\n\t// the HTLCs on that unrevoked dangling commitment.\n\tcase RemotePendingHtlcSet:\n\t\treturn c.checkRemoteChainActions(\n\t\t\theight, trigger, htlcSets, true,\n\t\t)\n\t}\n\n\treturn nil, fmt.Errorf(\"unable to locate chain actions\")\n}\n\n// prepContractResolutions is called either in the case that we decide we need\n// to go to chain, or the remote party goes to chain. Given a set of actions we\n// need to take for each HTLC, this method will return a set of contract\n// resolvers that will resolve the contracts on-chain if needed, and also a set\n// of packets to send to the htlcswitch in order to ensure all incoming HTLC's\n// are properly resolved.",
      "length": 1762,
      "tokens": 272,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) prepContractResolutions(",
      "content": "func (c *ChannelArbitrator) prepContractResolutions(\n\tcontractResolutions *ContractResolutions, height uint32,\n\ttrigger transitionTrigger,\n\tconfCommitSet *CommitSet) ([]ContractResolver, []ResolutionMsg, error) {\n\n\t// First, we'll reconstruct a fresh set of chain actions as the set of\n\t// actions we need to act on may differ based on if it was our\n\t// commitment, or they're commitment that hit the chain.\n\thtlcActions, err := c.constructChainActions(\n\t\tconfCommitSet, height, trigger,\n\t)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\t// We'll also fetch the historical state of this channel, as it should\n\t// have been marked as closed by now, and supplement it to each resolver\n\t// such that we can properly resolve our pending contracts.\n\tvar chanState *channeldb.OpenChannel\n\tchanState, err = c.cfg.FetchHistoricalChannel()\n\tswitch {\n\t// If we don't find this channel, then it may be the case that it\n\t// was closed before we started to retain the final state\n\t// information for open channels.\n\tcase err == channeldb.ErrNoHistoricalBucket:\n\t\tfallthrough\n\tcase err == channeldb.ErrChannelNotFound:\n\t\tlog.Warnf(\"ChannelArbitrator(%v): unable to fetch historical \"+\n\t\t\t\"state\", c.cfg.ChanPoint)\n\n\tcase err != nil:\n\t\treturn nil, nil, err\n\t}\n\n\t// There may be a class of HTLC's which we can fail back immediately,\n\t// for those we'll prepare a slice of packets to add to our outbox. Any\n\t// packets we need to send, will be cancels.\n\tvar (\n\t\tmsgsToSend []ResolutionMsg\n\t)\n\n\tincomingResolutions := contractResolutions.HtlcResolutions.IncomingHTLCs\n\toutgoingResolutions := contractResolutions.HtlcResolutions.OutgoingHTLCs\n\n\t// We'll use these two maps to quickly look up an active HTLC with its\n\t// matching HTLC resolution.\n\toutResolutionMap := make(map[wire.OutPoint]lnwallet.OutgoingHtlcResolution)\n\tinResolutionMap := make(map[wire.OutPoint]lnwallet.IncomingHtlcResolution)\n\tfor i := 0; i < len(incomingResolutions); i++ {\n\t\tinRes := incomingResolutions[i]\n\t\tinResolutionMap[inRes.HtlcPoint()] = inRes\n\t}\n\tfor i := 0; i < len(outgoingResolutions); i++ {\n\t\toutRes := outgoingResolutions[i]\n\t\toutResolutionMap[outRes.HtlcPoint()] = outRes\n\t}\n\n\t// We'll create the resolver kit that we'll be cloning for each\n\t// resolver so they each can do their duty.\n\tresolverCfg := ResolverConfig{\n\t\tChannelArbitratorConfig: c.cfg,\n\t\tCheckpoint: func(res ContractResolver,\n\t\t\treports ...*channeldb.ResolverReport) error {\n\n\t\t\treturn c.log.InsertUnresolvedContracts(reports, res)\n\t\t},\n\t}\n\n\tcommitHash := contractResolutions.CommitHash\n\tfailureMsg := &lnwire.FailPermanentChannelFailure{}\n\n\tvar htlcResolvers []ContractResolver\n\n\t// We instantiate an anchor resolver if the commitment tx has an\n\t// anchor.\n\tif contractResolutions.AnchorResolution != nil {\n\t\tanchorResolver := newAnchorResolver(\n\t\t\tcontractResolutions.AnchorResolution.AnchorSignDescriptor,\n\t\t\tcontractResolutions.AnchorResolution.CommitAnchor,\n\t\t\theight, c.cfg.ChanPoint, resolverCfg,\n\t\t)\n\t\thtlcResolvers = append(htlcResolvers, anchorResolver)\n\t}\n\n\t// If this is a breach close, we'll create a breach resolver, determine\n\t// the htlc's to fail back, and exit. This is done because the other\n\t// steps taken for non-breach-closes do not matter for breach-closes.\n\tif contractResolutions.BreachResolution != nil {\n\t\tbreachResolver := newBreachResolver(resolverCfg)\n\t\thtlcResolvers = append(htlcResolvers, breachResolver)\n\n\t\t// We'll use the CommitSet, we'll fail back all outgoing HTLC's\n\t\t// that exist on either of the remote commitments. The map is\n\t\t// used to deduplicate any shared htlc's.\n\t\tremoteOutgoing := make(map[uint64]channeldb.HTLC)\n\t\tfor htlcSetKey, htlcs := range confCommitSet.HtlcSets {\n\t\t\tif !htlcSetKey.IsRemote {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tfor _, htlc := range htlcs {\n\t\t\t\tif htlc.Incoming {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tremoteOutgoing[htlc.HtlcIndex] = htlc\n\t\t\t}\n\t\t}\n\n\t\t// Now we'll loop over the map and create ResolutionMsgs for\n\t\t// each of them.\n\t\tfor _, htlc := range remoteOutgoing {\n\t\t\tfailMsg := ResolutionMsg{\n\t\t\t\tSourceChan: c.cfg.ShortChanID,\n\t\t\t\tHtlcIndex:  htlc.HtlcIndex,\n\t\t\t\tFailure:    failureMsg,\n\t\t\t}\n\n\t\t\tmsgsToSend = append(msgsToSend, failMsg)\n\t\t}\n\n\t\treturn htlcResolvers, msgsToSend, nil\n\t}\n\n\t// For each HTLC, we'll either act immediately, meaning we'll instantly\n\t// fail the HTLC, or we'll act only once the transaction has been\n\t// confirmed, in which case we'll need an HTLC resolver.\n\tfor htlcAction, htlcs := range htlcActions {\n\t\tswitch htlcAction {\n\n\t\t// If we can fail an HTLC immediately (an outgoing HTLC with no\n\t\t// contract), then we'll assemble an HTLC fail packet to send.\n\t\tcase HtlcFailNowAction:\n\t\t\tfor _, htlc := range htlcs {\n\t\t\t\tfailMsg := ResolutionMsg{\n\t\t\t\t\tSourceChan: c.cfg.ShortChanID,\n\t\t\t\t\tHtlcIndex:  htlc.HtlcIndex,\n\t\t\t\t\tFailure:    failureMsg,\n\t\t\t\t}\n\n\t\t\t\tmsgsToSend = append(msgsToSend, failMsg)\n\t\t\t}\n\n\t\t// If we can claim this HTLC, we'll create an HTLC resolver to\n\t\t// claim the HTLC (second-level or directly), then add the pre\n\t\tcase HtlcClaimAction:\n\t\t\tfor _, htlc := range htlcs {\n\t\t\t\thtlc := htlc\n\n\t\t\t\thtlcOp := wire.OutPoint{\n\t\t\t\t\tHash:  commitHash,\n\t\t\t\t\tIndex: uint32(htlc.OutputIndex),\n\t\t\t\t}\n\n\t\t\t\tresolution, ok := inResolutionMap[htlcOp]\n\t\t\t\tif !ok {\n\t\t\t\t\t// TODO(roasbeef): panic?\n\t\t\t\t\tlog.Errorf(\"ChannelArbitrator(%v) unable to find \"+\n\t\t\t\t\t\t\"incoming resolution: %v\",\n\t\t\t\t\t\tc.cfg.ChanPoint, htlcOp)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tresolver := newSuccessResolver(\n\t\t\t\t\tresolution, height, htlc, resolverCfg,\n\t\t\t\t)\n\t\t\t\tif chanState != nil {\n\t\t\t\t\tresolver.SupplementState(chanState)\n\t\t\t\t}\n\t\t\t\thtlcResolvers = append(htlcResolvers, resolver)\n\t\t\t}\n\n\t\t// If we can timeout the HTLC directly, then we'll create the\n\t\t// proper resolver to do so, who will then cancel the packet\n\t\t// backwards.\n\t\tcase HtlcTimeoutAction:\n\t\t\tfor _, htlc := range htlcs {\n\t\t\t\thtlc := htlc\n\n\t\t\t\thtlcOp := wire.OutPoint{\n\t\t\t\t\tHash:  commitHash,\n\t\t\t\t\tIndex: uint32(htlc.OutputIndex),\n\t\t\t\t}\n\n\t\t\t\tresolution, ok := outResolutionMap[htlcOp]\n\t\t\t\tif !ok {\n\t\t\t\t\tlog.Errorf(\"ChannelArbitrator(%v) unable to find \"+\n\t\t\t\t\t\t\"outgoing resolution: %v\", c.cfg.ChanPoint, htlcOp)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tresolver := newTimeoutResolver(\n\t\t\t\t\tresolution, height, htlc, resolverCfg,\n\t\t\t\t)\n\t\t\t\tif chanState != nil {\n\t\t\t\t\tresolver.SupplementState(chanState)\n\t\t\t\t}\n\t\t\t\thtlcResolvers = append(htlcResolvers, resolver)\n\t\t\t}\n\n\t\t// If this is an incoming HTLC, but we can't act yet, then\n\t\t// we'll create an incoming resolver to redeem the HTLC if we\n\t\t// learn of the pre-image, or let the remote party time out.\n\t\tcase HtlcIncomingWatchAction:\n\t\t\tfor _, htlc := range htlcs {\n\t\t\t\thtlc := htlc\n\n\t\t\t\thtlcOp := wire.OutPoint{\n\t\t\t\t\tHash:  commitHash,\n\t\t\t\t\tIndex: uint32(htlc.OutputIndex),\n\t\t\t\t}\n\n\t\t\t\t// TODO(roasbeef): need to handle incoming dust...\n\n\t\t\t\t// TODO(roasbeef): can't be negative!!!\n\t\t\t\tresolution, ok := inResolutionMap[htlcOp]\n\t\t\t\tif !ok {\n\t\t\t\t\tlog.Errorf(\"ChannelArbitrator(%v) unable to find \"+\n\t\t\t\t\t\t\"incoming resolution: %v\",\n\t\t\t\t\t\tc.cfg.ChanPoint, htlcOp)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tresolver := newIncomingContestResolver(\n\t\t\t\t\tresolution, height, htlc,\n\t\t\t\t\tresolverCfg,\n\t\t\t\t)\n\t\t\t\tif chanState != nil {\n\t\t\t\t\tresolver.SupplementState(chanState)\n\t\t\t\t}\n\t\t\t\thtlcResolvers = append(htlcResolvers, resolver)\n\t\t\t}\n\n\t\t// We've lost an htlc because it isn't manifested on the\n\t\t// commitment transaction that closed the channel.\n\t\tcase HtlcIncomingDustFinalAction:\n\t\t\tfor _, htlc := range htlcs {\n\t\t\t\thtlc := htlc\n\n\t\t\t\tkey := models.CircuitKey{\n\t\t\t\t\tChanID: c.cfg.ShortChanID,\n\t\t\t\t\tHtlcID: htlc.HtlcIndex,\n\t\t\t\t}\n\n\t\t\t\t// Mark this dust htlc as final failed.\n\t\t\t\tchainArbCfg := c.cfg.ChainArbitratorConfig\n\t\t\t\terr := chainArbCfg.PutFinalHtlcOutcome(\n\t\t\t\t\tkey.ChanID, key.HtlcID, false,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, nil, err\n\t\t\t\t}\n\n\t\t\t\t// Send notification.\n\t\t\t\tchainArbCfg.HtlcNotifier.NotifyFinalHtlcEvent(\n\t\t\t\t\tkey,\n\t\t\t\t\tchanneldb.FinalHtlcInfo{\n\t\t\t\t\t\tSettled:  false,\n\t\t\t\t\t\tOffchain: false,\n\t\t\t\t\t},\n\t\t\t\t)\n\t\t\t}\n\n\t\t// Finally, if this is an outgoing HTLC we've sent, then we'll\n\t\t// launch a resolver to watch for the pre-image (and settle\n\t\t// backwards), or just timeout.\n\t\tcase HtlcOutgoingWatchAction:\n\t\t\tfor _, htlc := range htlcs {\n\t\t\t\thtlc := htlc\n\n\t\t\t\thtlcOp := wire.OutPoint{\n\t\t\t\t\tHash:  commitHash,\n\t\t\t\t\tIndex: uint32(htlc.OutputIndex),\n\t\t\t\t}\n\n\t\t\t\tresolution, ok := outResolutionMap[htlcOp]\n\t\t\t\tif !ok {\n\t\t\t\t\tlog.Errorf(\"ChannelArbitrator(%v) unable to find \"+\n\t\t\t\t\t\t\"outgoing resolution: %v\",\n\t\t\t\t\t\tc.cfg.ChanPoint, htlcOp)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tresolver := newOutgoingContestResolver(\n\t\t\t\t\tresolution, height, htlc, resolverCfg,\n\t\t\t\t)\n\t\t\t\tif chanState != nil {\n\t\t\t\t\tresolver.SupplementState(chanState)\n\t\t\t\t}\n\t\t\t\thtlcResolvers = append(htlcResolvers, resolver)\n\t\t\t}\n\t\t}\n\t}\n\n\t// If this is was an unilateral closure, then we'll also create a\n\t// resolver to sweep our commitment output (but only if it wasn't\n\t// trimmed).\n\tif contractResolutions.CommitResolution != nil {\n\t\tresolver := newCommitSweepResolver(\n\t\t\t*contractResolutions.CommitResolution, height,\n\t\t\tc.cfg.ChanPoint, resolverCfg,\n\t\t)\n\t\tif chanState != nil {\n\t\t\tresolver.SupplementState(chanState)\n\t\t}\n\t\thtlcResolvers = append(htlcResolvers, resolver)\n\t}\n\n\treturn htlcResolvers, msgsToSend, nil\n}\n\n// replaceResolver replaces a in the list of active resolvers. If the resolver\n// to be replaced is not found, it returns an error.",
      "length": 8940,
      "tokens": 1130,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) replaceResolver(oldResolver,",
      "content": "func (c *ChannelArbitrator) replaceResolver(oldResolver,\n\tnewResolver ContractResolver) error {\n\n\tc.activeResolversLock.Lock()\n\tdefer c.activeResolversLock.Unlock()\n\n\toldKey := oldResolver.ResolverKey()\n\tfor i, r := range c.activeResolvers {\n\t\tif bytes.Equal(r.ResolverKey(), oldKey) {\n\t\t\tc.activeResolvers[i] = newResolver\n\t\t\treturn nil\n\t\t}\n\t}\n\n\treturn errors.New(\"resolver to be replaced not found\")\n}\n\n// resolveContract is a goroutine tasked with fully resolving an unresolved\n// contract. Either the initial contract will be resolved after a single step,\n// or the contract will itself create another contract to be resolved. In\n// either case, one the contract has been fully resolved, we'll signal back to\n// the main goroutine so it can properly keep track of the set of unresolved\n// contracts.\n//\n// NOTE: This MUST be run as a goroutine.",
      "length": 768,
      "tokens": 114,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) resolveContract(currentContract ContractResolver) {",
      "content": "func (c *ChannelArbitrator) resolveContract(currentContract ContractResolver) {\n\tdefer c.wg.Done()\n\n\tlog.Debugf(\"ChannelArbitrator(%v): attempting to resolve %T\",\n\t\tc.cfg.ChanPoint, currentContract)\n\n\t// Until the contract is fully resolved, we'll continue to iteratively\n\t// resolve the contract one step at a time.\n\tfor !currentContract.IsResolved() {\n\t\tlog.Debugf(\"ChannelArbitrator(%v): contract %T not yet resolved\",\n\t\t\tc.cfg.ChanPoint, currentContract)\n\n\t\tselect {\n\n\t\t// If we've been signalled to quit, then we'll exit early.\n\t\tcase <-c.quit:\n\t\t\treturn\n\n\t\tdefault:\n\t\t\t// Otherwise, we'll attempt to resolve the current\n\t\t\t// contract.\n\t\t\tnextContract, err := currentContract.Resolve()\n\t\t\tif err != nil {\n\t\t\t\tif err == errResolverShuttingDown {\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tlog.Errorf(\"ChannelArbitrator(%v): unable to \"+\n\t\t\t\t\t\"progress %T: %v\",\n\t\t\t\t\tc.cfg.ChanPoint, currentContract, err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tswitch {\n\t\t\t// If this contract produced another, then this means\n\t\t\t// the current contract was only able to be partially\n\t\t\t// resolved in this step. So we'll do a contract swap\n\t\t\t// within our logs: the new contract will take the\n\t\t\t// place of the old one.\n\t\t\tcase nextContract != nil:\n\t\t\t\tlog.Debugf(\"ChannelArbitrator(%v): swapping \"+\n\t\t\t\t\t\"out contract %T for %T \",\n\t\t\t\t\tc.cfg.ChanPoint, currentContract,\n\t\t\t\t\tnextContract)\n\n\t\t\t\t// Swap contract in log.\n\t\t\t\terr := c.log.SwapContract(\n\t\t\t\t\tcurrentContract, nextContract,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Errorf(\"unable to add recurse \"+\n\t\t\t\t\t\t\"contract: %v\", err)\n\t\t\t\t}\n\n\t\t\t\t// Swap contract in resolvers list. This is to\n\t\t\t\t// make sure that reports are queried from the\n\t\t\t\t// new resolver.\n\t\t\t\terr = c.replaceResolver(\n\t\t\t\t\tcurrentContract, nextContract,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Errorf(\"unable to replace \"+\n\t\t\t\t\t\t\"contract: %v\", err)\n\t\t\t\t}\n\n\t\t\t\t// As this contract produced another, we'll\n\t\t\t\t// re-assign, so we can continue our resolution\n\t\t\t\t// loop.\n\t\t\t\tcurrentContract = nextContract\n\n\t\t\t// If this contract is actually fully resolved, then\n\t\t\t// we'll mark it as such within the database.\n\t\t\tcase currentContract.IsResolved():\n\t\t\t\tlog.Debugf(\"ChannelArbitrator(%v): marking \"+\n\t\t\t\t\t\"contract %T fully resolved\",\n\t\t\t\t\tc.cfg.ChanPoint, currentContract)\n\n\t\t\t\terr := c.log.ResolveContract(currentContract)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Errorf(\"unable to resolve contract: %v\",\n\t\t\t\t\t\terr)\n\t\t\t\t}\n\n\t\t\t\t// Now that the contract has been resolved,\n\t\t\t\t// well signal to the main goroutine.\n\t\t\t\tselect {\n\t\t\t\tcase c.resolutionSignal <- struct{}{}:\n\t\t\t\tcase <-c.quit:\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\t}\n\t}\n}\n\n// signalUpdateMsg is a struct that carries fresh signals to the\n// ChannelArbitrator. We need to receive a message like this each time the\n// channel becomes active, as it's internal state may change.",
      "length": 2605,
      "tokens": 351,
      "embedding": []
    },
    {
      "slug": "type signalUpdateMsg struct {",
      "content": "type signalUpdateMsg struct {\n\t// newSignals is the set of new active signals to be sent to the\n\t// arbitrator.\n\tnewSignals *ContractSignals\n\n\t// doneChan is a channel that will be closed on the arbitrator has\n\t// attached the new signals.\n\tdoneChan chan struct{}\n}\n\n// UpdateContractSignals updates the set of signals the ChannelArbitrator needs\n// to receive from a channel in real-time in order to keep in sync with the\n// latest state of the contract.",
      "length": 414,
      "tokens": 72,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) UpdateContractSignals(newSignals *ContractSignals) {",
      "content": "func (c *ChannelArbitrator) UpdateContractSignals(newSignals *ContractSignals) {\n\tdone := make(chan struct{})\n\n\tselect {\n\tcase c.signalUpdates <- &signalUpdateMsg{\n\t\tnewSignals: newSignals,\n\t\tdoneChan:   done,\n\t}:\n\tcase <-c.quit:\n\t}\n\n\tselect {\n\tcase <-done:\n\tcase <-c.quit:\n\t}\n}\n\n// notifyContractUpdate updates the ChannelArbitrator's unmerged mappings such\n// that it can later be merged with activeHTLCs when calling\n// checkLocalChainActions or sweepAnchors. These are the only two places that\n// activeHTLCs is used.",
      "length": 421,
      "tokens": 60,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) notifyContractUpdate(upd *ContractUpdate) {",
      "content": "func (c *ChannelArbitrator) notifyContractUpdate(upd *ContractUpdate) {\n\tc.unmergedMtx.Lock()\n\tdefer c.unmergedMtx.Unlock()\n\n\t// Update the mapping.\n\tc.unmergedSet[upd.HtlcKey] = newHtlcSet(upd.Htlcs)\n\n\tlog.Tracef(\"ChannelArbitrator(%v): fresh set of htlcs=%v\",\n\t\tc.cfg.ChanPoint,\n\t\tnewLogClosure(func() string {\n\t\t\treturn spew.Sdump(upd)\n\t\t}),\n\t)\n}\n\n// updateActiveHTLCs merges the unmerged set of HTLCs from the link with\n// activeHTLCs.",
      "length": 352,
      "tokens": 38,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) updateActiveHTLCs() {",
      "content": "func (c *ChannelArbitrator) updateActiveHTLCs() {\n\tc.unmergedMtx.RLock()\n\tdefer c.unmergedMtx.RUnlock()\n\n\t// Update the mapping.\n\tc.activeHTLCs[LocalHtlcSet] = c.unmergedSet[LocalHtlcSet]\n\tc.activeHTLCs[RemoteHtlcSet] = c.unmergedSet[RemoteHtlcSet]\n\n\t// If the pending set exists, update that as well.\n\tif _, ok := c.unmergedSet[RemotePendingHtlcSet]; ok {\n\t\tpendingSet := c.unmergedSet[RemotePendingHtlcSet]\n\t\tc.activeHTLCs[RemotePendingHtlcSet] = pendingSet\n\t}\n}\n\n// channelAttendant is the primary goroutine that acts at the judicial\n// arbitrator between our channel state, the remote channel peer, and the\n// blockchain (Our judge). This goroutine will ensure that we faithfully execute\n// all clauses of our contract in the case that we need to go on-chain for a\n// dispute. Currently, two such conditions warrant our intervention: when an\n// outgoing HTLC is about to timeout, and when we know the pre-image for an\n// incoming HTLC, but it hasn't yet been settled off-chain. In these cases,\n// we'll: broadcast our commitment, cancel/settle any HTLC's backwards after\n// sufficient confirmation, and finally send our set of outputs to the UTXO\n// Nursery for incubation, and ultimate sweeping.\n//\n// NOTE: This MUST be run as a goroutine.",
      "length": 1170,
      "tokens": 169,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) channelAttendant(bestHeight int32) {",
      "content": "func (c *ChannelArbitrator) channelAttendant(bestHeight int32) {\n\n\t// TODO(roasbeef): tell top chain arb we're done\n\tdefer func() {\n\t\tc.wg.Done()\n\t}()\n\n\tfor {\n\t\tselect {\n\n\t\t// A new block has arrived, we'll examine all the active HTLC's\n\t\t// to see if any of them have expired, and also update our\n\t\t// track of the best current height.\n\t\tcase blockHeight, ok := <-c.blocks:\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tbestHeight = blockHeight\n\n\t\t\t// If we're not in the default state, then we can\n\t\t\t// ignore this signal as we're waiting for contract\n\t\t\t// resolution.\n\t\t\tif c.state != StateDefault {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Now that a new block has arrived, we'll attempt to\n\t\t\t// advance our state forward.\n\t\t\tnextState, _, err := c.advanceState(\n\t\t\t\tuint32(bestHeight), chainTrigger, nil,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"Unable to advance state: %v\", err)\n\t\t\t}\n\n\t\t\t// If as a result of this trigger, the contract is\n\t\t\t// fully resolved, then well exit.\n\t\t\tif nextState == StateFullyResolved {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t// A new signal update was just sent. This indicates that the\n\t\t// channel under watch is now live, and may modify its internal\n\t\t// state, so we'll get the most up to date signals to we can\n\t\t// properly do our job.\n\t\tcase signalUpdate := <-c.signalUpdates:\n\t\t\tlog.Tracef(\"ChannelArbitrator(%v) got new signal \"+\n\t\t\t\t\"update!\", c.cfg.ChanPoint)\n\n\t\t\t// We'll update the ShortChannelID.\n\t\t\tc.cfg.ShortChanID = signalUpdate.newSignals.ShortChanID\n\n\t\t\t// Now that the signal has been updated, we'll now\n\t\t\t// close the done channel to signal to the caller we've\n\t\t\t// registered the new ShortChannelID.\n\t\t\tclose(signalUpdate.doneChan)\n\n\t\t// We've cooperatively closed the channel, so we're no longer\n\t\t// needed. We'll mark the channel as resolved and exit.\n\t\tcase closeInfo := <-c.cfg.ChainEvents.CooperativeClosure:\n\t\t\tlog.Infof(\"ChannelArbitrator(%v) marking channel \"+\n\t\t\t\t\"cooperatively closed\", c.cfg.ChanPoint)\n\n\t\t\terr := c.cfg.MarkChannelClosed(\n\t\t\t\tcloseInfo.ChannelCloseSummary,\n\t\t\t\tchanneldb.ChanStatusCoopBroadcasted,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"Unable to mark channel closed: \"+\n\t\t\t\t\t\"%v\", err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// We'll now advance our state machine until it reaches\n\t\t\t// a terminal state, and the channel is marked resolved.\n\t\t\t_, _, err = c.advanceState(\n\t\t\t\tcloseInfo.CloseHeight, coopCloseTrigger, nil,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"Unable to advance state: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t// We have broadcasted our commitment, and it is now confirmed\n\t\t// on-chain.\n\t\tcase closeInfo := <-c.cfg.ChainEvents.LocalUnilateralClosure:\n\t\t\tlog.Infof(\"ChannelArbitrator(%v): local on-chain \"+\n\t\t\t\t\"channel close\", c.cfg.ChanPoint)\n\n\t\t\tif c.state != StateCommitmentBroadcasted {\n\t\t\t\tlog.Errorf(\"ChannelArbitrator(%v): unexpected \"+\n\t\t\t\t\t\"local on-chain channel close\",\n\t\t\t\t\tc.cfg.ChanPoint)\n\t\t\t}\n\t\t\tcloseTx := closeInfo.CloseTx\n\n\t\t\tcontractRes := &ContractResolutions{\n\t\t\t\tCommitHash:       closeTx.TxHash(),\n\t\t\t\tCommitResolution: closeInfo.CommitResolution,\n\t\t\t\tHtlcResolutions:  *closeInfo.HtlcResolutions,\n\t\t\t\tAnchorResolution: closeInfo.AnchorResolution,\n\t\t\t}\n\n\t\t\t// When processing a unilateral close event, we'll\n\t\t\t// transition to the ContractClosed state. We'll log\n\t\t\t// out the set of resolutions such that they are\n\t\t\t// available to fetch in that state, we'll also write\n\t\t\t// the commit set so we can reconstruct our chain\n\t\t\t// actions on restart.\n\t\t\terr := c.log.LogContractResolutions(contractRes)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"Unable to write resolutions: %v\",\n\t\t\t\t\terr)\n\t\t\t\treturn\n\t\t\t}\n\t\t\terr = c.log.InsertConfirmedCommitSet(\n\t\t\t\t&closeInfo.CommitSet,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"Unable to write commit set: %v\",\n\t\t\t\t\terr)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// After the set of resolutions are successfully\n\t\t\t// logged, we can safely close the channel. After this\n\t\t\t// succeeds we won't be getting chain events anymore,\n\t\t\t// so we must make sure we can recover on restart after\n\t\t\t// it is marked closed. If the next state transition\n\t\t\t// fails, we'll start up in the prior state again, and\n\t\t\t// we won't be longer getting chain events. In this\n\t\t\t// case we must manually re-trigger the state\n\t\t\t// transition into StateContractClosed based on the\n\t\t\t// close status of the channel.\n\t\t\terr = c.cfg.MarkChannelClosed(\n\t\t\t\tcloseInfo.ChannelCloseSummary,\n\t\t\t\tchanneldb.ChanStatusLocalCloseInitiator,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"Unable to mark \"+\n\t\t\t\t\t\"channel closed: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// We'll now advance our state machine until it reaches\n\t\t\t// a terminal state.\n\t\t\t_, _, err = c.advanceState(\n\t\t\t\tuint32(closeInfo.SpendingHeight),\n\t\t\t\tlocalCloseTrigger, &closeInfo.CommitSet,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"Unable to advance state: %v\", err)\n\t\t\t}\n\n\t\t// The remote party has broadcast the commitment on-chain.\n\t\t// We'll examine our state to determine if we need to act at\n\t\t// all.\n\t\tcase uniClosure := <-c.cfg.ChainEvents.RemoteUnilateralClosure:\n\t\t\tlog.Infof(\"ChannelArbitrator(%v): remote party has \"+\n\t\t\t\t\"closed channel out on-chain\", c.cfg.ChanPoint)\n\n\t\t\t// If we don't have a self output, and there are no\n\t\t\t// active HTLC's, then we can immediately mark the\n\t\t\t// contract as fully resolved and exit.\n\t\t\tcontractRes := &ContractResolutions{\n\t\t\t\tCommitHash:       *uniClosure.SpenderTxHash,\n\t\t\t\tCommitResolution: uniClosure.CommitResolution,\n\t\t\t\tHtlcResolutions:  *uniClosure.HtlcResolutions,\n\t\t\t\tAnchorResolution: uniClosure.AnchorResolution,\n\t\t\t}\n\n\t\t\t// When processing a unilateral close event, we'll\n\t\t\t// transition to the ContractClosed state. We'll log\n\t\t\t// out the set of resolutions such that they are\n\t\t\t// available to fetch in that state, we'll also write\n\t\t\t// the commit set so we can reconstruct our chain\n\t\t\t// actions on restart.\n\t\t\terr := c.log.LogContractResolutions(contractRes)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"Unable to write resolutions: %v\",\n\t\t\t\t\terr)\n\t\t\t\treturn\n\t\t\t}\n\t\t\terr = c.log.InsertConfirmedCommitSet(\n\t\t\t\t&uniClosure.CommitSet,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"Unable to write commit set: %v\",\n\t\t\t\t\terr)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// After the set of resolutions are successfully\n\t\t\t// logged, we can safely close the channel. After this\n\t\t\t// succeeds we won't be getting chain events anymore,\n\t\t\t// so we must make sure we can recover on restart after\n\t\t\t// it is marked closed. If the next state transition\n\t\t\t// fails, we'll start up in the prior state again, and\n\t\t\t// we won't be longer getting chain events. In this\n\t\t\t// case we must manually re-trigger the state\n\t\t\t// transition into StateContractClosed based on the\n\t\t\t// close status of the channel.\n\t\t\tcloseSummary := &uniClosure.ChannelCloseSummary\n\t\t\terr = c.cfg.MarkChannelClosed(\n\t\t\t\tcloseSummary,\n\t\t\t\tchanneldb.ChanStatusRemoteCloseInitiator,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"Unable to mark channel closed: %v\",\n\t\t\t\t\terr)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// We'll now advance our state machine until it reaches\n\t\t\t// a terminal state.\n\t\t\t_, _, err = c.advanceState(\n\t\t\t\tuint32(uniClosure.SpendingHeight),\n\t\t\t\tremoteCloseTrigger, &uniClosure.CommitSet,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"Unable to advance state: %v\", err)\n\t\t\t}\n\n\t\t// The remote has breached the channel. As this is handled by\n\t\t// the ChainWatcher and BreachArbiter, we don't have to do\n\t\t// anything in particular, so just advance our state and\n\t\t// gracefully exit.\n\t\tcase breachInfo := <-c.cfg.ChainEvents.ContractBreach:\n\t\t\tlog.Infof(\"ChannelArbitrator(%v): remote party has \"+\n\t\t\t\t\"breached channel!\", c.cfg.ChanPoint)\n\n\t\t\t// In the breach case, we'll only have anchor and\n\t\t\t// breach resolutions.\n\t\t\tcontractRes := &ContractResolutions{\n\t\t\t\tCommitHash:       breachInfo.CommitHash,\n\t\t\t\tBreachResolution: breachInfo.BreachResolution,\n\t\t\t\tAnchorResolution: breachInfo.AnchorResolution,\n\t\t\t}\n\n\t\t\t// We'll transition to the ContractClosed state and log\n\t\t\t// the set of resolutions such that they can be turned\n\t\t\t// into resolvers later on. We'll also insert the\n\t\t\t// CommitSet of the latest set of commitments.\n\t\t\terr := c.log.LogContractResolutions(contractRes)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"Unable to write resolutions: %v\",\n\t\t\t\t\terr)\n\t\t\t\treturn\n\t\t\t}\n\t\t\terr = c.log.InsertConfirmedCommitSet(\n\t\t\t\t&breachInfo.CommitSet,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"Unable to write commit set: %v\",\n\t\t\t\t\terr)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// The channel is finally marked pending closed here as\n\t\t\t// the breacharbiter and channel arbitrator have\n\t\t\t// persisted the relevant states.\n\t\t\tcloseSummary := &breachInfo.CloseSummary\n\t\t\terr = c.cfg.MarkChannelClosed(\n\t\t\t\tcloseSummary,\n\t\t\t\tchanneldb.ChanStatusRemoteCloseInitiator,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"Unable to mark channel closed: %v\",\n\t\t\t\t\terr)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tlog.Infof(\"Breached channel=%v marked pending-closed\",\n\t\t\t\tbreachInfo.BreachResolution.FundingOutPoint)\n\n\t\t\t// We'll advance our state machine until it reaches a\n\t\t\t// terminal state.\n\t\t\t_, _, err = c.advanceState(\n\t\t\t\tuint32(bestHeight), breachCloseTrigger,\n\t\t\t\t&breachInfo.CommitSet,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"Unable to advance state: %v\", err)\n\t\t\t}\n\n\t\t// A new contract has just been resolved, we'll now check our\n\t\t// log to see if all contracts have been resolved. If so, then\n\t\t// we can exit as the contract is fully resolved.\n\t\tcase <-c.resolutionSignal:\n\t\t\tlog.Infof(\"ChannelArbitrator(%v): a contract has been \"+\n\t\t\t\t\"fully resolved!\", c.cfg.ChanPoint)\n\n\t\t\tnextState, _, err := c.advanceState(\n\t\t\t\tuint32(bestHeight), chainTrigger, nil,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"Unable to advance state: %v\", err)\n\t\t\t}\n\n\t\t\t// If we don't have anything further to do after\n\t\t\t// advancing our state, then we'll exit.\n\t\t\tif nextState == StateFullyResolved {\n\t\t\t\tlog.Infof(\"ChannelArbitrator(%v): all \"+\n\t\t\t\t\t\"contracts fully resolved, exiting\",\n\t\t\t\t\tc.cfg.ChanPoint)\n\n\t\t\t\treturn\n\t\t\t}\n\n\t\t// We've just received a request to forcibly close out the\n\t\t// channel. We'll\n\t\tcase closeReq := <-c.forceCloseReqs:\n\t\t\tif c.state != StateDefault {\n\t\t\t\tselect {\n\t\t\t\tcase closeReq.closeTx <- nil:\n\t\t\t\tcase <-c.quit:\n\t\t\t\t}\n\n\t\t\t\tselect {\n\t\t\t\tcase closeReq.errResp <- errAlreadyForceClosed:\n\t\t\t\tcase <-c.quit:\n\t\t\t\t}\n\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tnextState, closeTx, err := c.advanceState(\n\t\t\t\tuint32(bestHeight), userTrigger, nil,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"Unable to advance state: %v\", err)\n\t\t\t}\n\n\t\t\tselect {\n\t\t\tcase closeReq.closeTx <- closeTx:\n\t\t\tcase <-c.quit:\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tselect {\n\t\t\tcase closeReq.errResp <- err:\n\t\t\tcase <-c.quit:\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// If we don't have anything further to do after\n\t\t\t// advancing our state, then we'll exit.\n\t\t\tif nextState == StateFullyResolved {\n\t\t\t\tlog.Infof(\"ChannelArbitrator(%v): all \"+\n\t\t\t\t\t\"contracts resolved, exiting\",\n\t\t\t\t\tc.cfg.ChanPoint)\n\t\t\t\treturn\n\t\t\t}\n\n\t\tcase <-c.quit:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// checkLegacyBreach returns StateFullyResolved if the channel was closed with\n// a breach transaction before the channel arbitrator launched its own breach\n// resolver. StateContractClosed is returned if this is a modern breach close\n// with a breach resolver. StateError is returned if the log lookup failed.",
      "length": 10660,
      "tokens": 1425,
      "embedding": []
    },
    {
      "slug": "func (c *ChannelArbitrator) checkLegacyBreach() (ArbitratorState, error) {",
      "content": "func (c *ChannelArbitrator) checkLegacyBreach() (ArbitratorState, error) {\n\t// A previous version of the channel arbitrator would make the breach\n\t// close skip to StateFullyResolved. If there are no contract\n\t// resolutions in the bolt arbitrator log, then this is an older breach\n\t// close. Otherwise, if there are resolutions, the state should advance\n\t// to StateContractClosed.\n\t_, err := c.log.FetchContractResolutions()\n\tif err == errNoResolutions {\n\t\t// This is an older breach close still in the database.\n\t\treturn StateFullyResolved, nil\n\t} else if err != nil {\n\t\treturn StateError, err\n\t}\n\n\t// This is a modern breach close with resolvers.\n\treturn StateContractClosed, nil\n}\n",
      "length": 595,
      "tokens": 96,
      "embedding": []
    }
  ]
}