{
  "filepath": "../implementations/go/lnd/contractcourt/chain_arbitrator.go",
  "package": "contractcourt",
  "sections": [
    {
      "slug": "type ResolutionMsg struct {",
      "content": "type ResolutionMsg struct {\n\t// SourceChan identifies the channel that this message is being sent\n\t// from. This is the channel's short channel ID.\n\tSourceChan lnwire.ShortChannelID\n\n\t// HtlcIndex is the index of the contract within the original\n\t// commitment trace.\n\tHtlcIndex uint64\n\n\t// Failure will be non-nil if the incoming contract should be canceled\n\t// all together. This can happen if the outgoing contract was dust, if\n\t// if the outgoing HTLC timed out.\n\tFailure lnwire.FailureMessage\n\n\t// PreImage will be non-nil if the incoming contract can successfully\n\t// be redeemed. This can happen if we learn of the preimage from the\n\t// outgoing HTLC on-chain.\n\tPreImage *[32]byte\n}\n\n// ChainArbitratorConfig is a configuration struct that contains all the\n// function closures and interface that required to arbitrate on-chain\n// contracts for a particular chain.",
      "length": 822,
      "tokens": 130,
      "embedding": []
    },
    {
      "slug": "type ChainArbitratorConfig struct {",
      "content": "type ChainArbitratorConfig struct {\n\t// ChainHash is the chain that this arbitrator is to operate within.\n\tChainHash chainhash.Hash\n\n\t// IncomingBroadcastDelta is the delta that we'll use to decide when to\n\t// broadcast our commitment transaction if we have incoming htlcs. This\n\t// value should be set based on our current fee estimation of the\n\t// commitment transaction. We use this to determine when we should\n\t// broadcast instead of just the HTLC timeout, as we want to ensure\n\t// that the commitment transaction is already confirmed, by the time the\n\t// HTLC expires. Otherwise we may end up not settling the htlc on-chain\n\t// because the other party managed to time it out.\n\tIncomingBroadcastDelta uint32\n\n\t// OutgoingBroadcastDelta is the delta that we'll use to decide when to\n\t// broadcast our commitment transaction if there are active outgoing\n\t// htlcs. This value can be lower than the incoming broadcast delta.\n\tOutgoingBroadcastDelta uint32\n\n\t// NewSweepAddr is a function that returns a new address under control\n\t// by the wallet. We'll use this to sweep any no-delay outputs as a\n\t// result of unilateral channel closes.\n\t//\n\t// NOTE: This SHOULD return a p2wkh script.\n\tNewSweepAddr func() ([]byte, error)\n\n\t// PublishTx reliably broadcasts a transaction to the network. Once\n\t// this function exits without an error, then they transaction MUST\n\t// continually be rebroadcast if needed.\n\tPublishTx func(*wire.MsgTx, string) error\n\n\t// DeliverResolutionMsg is a function that will append an outgoing\n\t// message to the \"out box\" for a ChannelLink. This is used to cancel\n\t// backwards any HTLC's that are either dust, we're timing out, or\n\t// settling on-chain to the incoming link.\n\tDeliverResolutionMsg func(...ResolutionMsg) error\n\n\t// MarkLinkInactive is a function closure that the ChainArbitrator will\n\t// use to mark that active HTLC's shouldn't be attempted to be routed\n\t// over a particular channel. This function will be called in that a\n\t// ChannelArbitrator decides that it needs to go to chain in order to\n\t// resolve contracts.\n\t//\n\t// TODO(roasbeef): rename, routing based\n\tMarkLinkInactive func(wire.OutPoint) error\n\n\t// ContractBreach is a function closure that the ChainArbitrator will\n\t// use to notify the breachArbiter about a contract breach. It should\n\t// only return a non-nil error when the breachArbiter has preserved\n\t// the necessary breach info for this channel point. Once the breach\n\t// resolution is persisted in the channel arbitrator, it will be safe\n\t// to mark the channel closed.\n\tContractBreach func(wire.OutPoint, *lnwallet.BreachRetribution) error\n\n\t// IsOurAddress is a function that returns true if the passed address\n\t// is known to the underlying wallet. Otherwise, false should be\n\t// returned.\n\tIsOurAddress func(btcutil.Address) bool\n\n\t// IncubateOutput sends either an incoming HTLC, an outgoing HTLC, or\n\t// both to the utxo nursery. Once this function returns, the nursery\n\t// should have safely persisted the outputs to disk, and should start\n\t// the process of incubation. This is used when a resolver wishes to\n\t// pass off the output to the nursery as we're only waiting on an\n\t// absolute/relative item block.\n\tIncubateOutputs func(wire.OutPoint, *lnwallet.OutgoingHtlcResolution,\n\t\t*lnwallet.IncomingHtlcResolution, uint32) error\n\n\t// PreimageDB is a global store of all known pre-images. We'll use this\n\t// to decide if we should broadcast a commitment transaction to claim\n\t// an HTLC on-chain.\n\tPreimageDB WitnessBeacon\n\n\t// Notifier is an instance of a chain notifier we'll use to watch for\n\t// certain on-chain events.\n\tNotifier chainntnfs.ChainNotifier\n\n\t// Signer is a signer backed by the active lnd node. This should be\n\t// capable of producing a signature as specified by a valid\n\t// SignDescriptor.\n\tSigner input.Signer\n\n\t// FeeEstimator will be used to return fee estimates.\n\tFeeEstimator chainfee.Estimator\n\n\t// ChainIO allows us to query the state of the current main chain.\n\tChainIO lnwallet.BlockChainIO\n\n\t// DisableChannel disables a channel, resulting in it not being able to\n\t// forward payments.\n\tDisableChannel func(wire.OutPoint) error\n\n\t// Sweeper allows resolvers to sweep their final outputs.\n\tSweeper UtxoSweeper\n\n\t// Registry is the invoice database that is used by resolvers to lookup\n\t// preimages and settle invoices.\n\tRegistry Registry\n\n\t// NotifyClosedChannel is a function closure that the ChainArbitrator\n\t// will use to notify the ChannelNotifier about a newly closed channel.\n\tNotifyClosedChannel func(wire.OutPoint)\n\n\t// NotifyFullyResolvedChannel is a function closure that the\n\t// ChainArbitrator will use to notify the ChannelNotifier about a newly\n\t// resolved channel. The main difference to NotifyClosedChannel is that\n\t// in case of a local force close the NotifyClosedChannel is called when\n\t// the published commitment transaction confirms while\n\t// NotifyFullyResolvedChannel is only called when the channel is fully\n\t// resolved (which includes sweeping any time locked funds).\n\tNotifyFullyResolvedChannel func(point wire.OutPoint)\n\n\t// OnionProcessor is used to decode onion payloads for on-chain\n\t// resolution.\n\tOnionProcessor OnionProcessor\n\n\t// PaymentsExpirationGracePeriod indicates a time window we let the\n\t// other node to cancel an outgoing htlc that our node has initiated and\n\t// has timed out.\n\tPaymentsExpirationGracePeriod time.Duration\n\n\t// IsForwardedHTLC checks for a given htlc, identified by channel id and\n\t// htlcIndex, if it is a forwarded one.\n\tIsForwardedHTLC func(chanID lnwire.ShortChannelID, htlcIndex uint64) bool\n\n\t// Clock is the clock implementation that ChannelArbitrator uses.\n\t// It is useful for testing.\n\tClock clock.Clock\n\n\t// SubscribeBreachComplete is used by the breachResolver to register a\n\t// subscription that notifies when the breach resolution process is\n\t// complete.\n\tSubscribeBreachComplete func(op *wire.OutPoint, c chan struct{}) (\n\t\tbool, error)\n\n\t// PutFinalHtlcOutcome stores the final outcome of an htlc in the\n\t// database.\n\tPutFinalHtlcOutcome func(chanId lnwire.ShortChannelID,\n\t\thtlcId uint64, settled bool) error\n\n\t// HtlcNotifier is an interface that htlc events are sent to.\n\tHtlcNotifier HtlcNotifier\n}\n\n// ChainArbitrator is a sub-system that oversees the on-chain resolution of all\n// active, and channel that are in the \"pending close\" state. Within the\n// contractcourt package, the ChainArbitrator manages a set of active\n// ContractArbitrators. Each ContractArbitrators is responsible for watching\n// the chain for any activity that affects the state of the channel, and also\n// for monitoring each contract in order to determine if any on-chain activity is\n// required. Outside sub-systems interact with the ChainArbitrator in order to\n// forcibly exit a contract, update the set of live signals for each contract,\n// and to receive reports on the state of contract resolution.",
      "length": 6687,
      "tokens": 1006,
      "embedding": []
    },
    {
      "slug": "type ChainArbitrator struct {",
      "content": "type ChainArbitrator struct {\n\tstarted int32 // To be used atomically.\n\tstopped int32 // To be used atomically.\n\n\tsync.Mutex\n\n\t// activeChannels is a map of all the active contracts that are still\n\t// open, and not fully resolved.\n\tactiveChannels map[wire.OutPoint]*ChannelArbitrator\n\n\t// activeWatchers is a map of all the active chainWatchers for channels\n\t// that are still considered open.\n\tactiveWatchers map[wire.OutPoint]*chainWatcher\n\n\t// cfg is the config struct for the arbitrator that contains all\n\t// methods and interface it needs to operate.\n\tcfg ChainArbitratorConfig\n\n\t// chanSource will be used by the ChainArbitrator to fetch all the\n\t// active channels that it must still watch over.\n\tchanSource *channeldb.DB\n\n\tquit chan struct{}\n\n\twg sync.WaitGroup\n}\n\n// NewChainArbitrator returns a new instance of the ChainArbitrator using the\n// passed config struct, and backing persistent database.",
      "length": 851,
      "tokens": 126,
      "embedding": []
    },
    {
      "slug": "func NewChainArbitrator(cfg ChainArbitratorConfig,",
      "content": "func NewChainArbitrator(cfg ChainArbitratorConfig,\n\tdb *channeldb.DB) *ChainArbitrator {\n\n\treturn &ChainArbitrator{\n\t\tcfg:            cfg,\n\t\tactiveChannels: make(map[wire.OutPoint]*ChannelArbitrator),\n\t\tactiveWatchers: make(map[wire.OutPoint]*chainWatcher),\n\t\tchanSource:     db,\n\t\tquit:           make(chan struct{}),\n\t}\n}\n\n// arbChannel is a wrapper around an open channel that channel arbitrators\n// interact with.",
      "length": 354,
      "tokens": 34,
      "embedding": []
    },
    {
      "slug": "type arbChannel struct {",
      "content": "type arbChannel struct {\n\t// channel is the in-memory channel state.\n\tchannel *channeldb.OpenChannel\n\n\t// c references the chain arbitrator and is used by arbChannel\n\t// internally.\n\tc *ChainArbitrator\n}\n\n// NewAnchorResolutions returns the anchor resolutions for currently valid\n// commitment transactions.\n//\n// NOTE: Part of the ArbChannel interface.",
      "length": 317,
      "tokens": 45,
      "embedding": []
    },
    {
      "slug": "func (a *arbChannel) NewAnchorResolutions() (*lnwallet.AnchorResolutions,",
      "content": "func (a *arbChannel) NewAnchorResolutions() (*lnwallet.AnchorResolutions,\n\terror) {\n\n\t// Get a fresh copy of the database state to base the anchor resolutions\n\t// on. Unfortunately the channel instance that we have here isn't the\n\t// same instance that is used by the link.\n\tchanPoint := a.channel.FundingOutpoint\n\n\tchannel, err := a.c.chanSource.ChannelStateDB().FetchChannel(\n\t\tnil, chanPoint,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tchanMachine, err := lnwallet.NewLightningChannel(\n\t\ta.c.cfg.Signer, channel, nil,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn chanMachine.NewAnchorResolutions()\n}\n\n// ForceCloseChan should force close the contract that this attendant is\n// watching over. We'll use this when we decide that we need to go to chain. It\n// should in addition tell the switch to remove the corresponding link, such\n// that we won't accept any new updates. The returned summary contains all items\n// needed to eventually resolve all outputs on chain.\n//\n// NOTE: Part of the ArbChannel interface.",
      "length": 915,
      "tokens": 148,
      "embedding": []
    },
    {
      "slug": "func (a *arbChannel) ForceCloseChan() (*lnwallet.LocalForceCloseSummary, error) {",
      "content": "func (a *arbChannel) ForceCloseChan() (*lnwallet.LocalForceCloseSummary, error) {\n\t// First, we mark the channel as borked, this ensure\n\t// that no new state transitions can happen, and also\n\t// that the link won't be loaded into the switch.\n\tif err := a.channel.MarkBorked(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// With the channel marked as borked, we'll now remove\n\t// the link from the switch if its there. If the link\n\t// is active, then this method will block until it\n\t// exits.\n\tchanPoint := a.channel.FundingOutpoint\n\n\tif err := a.c.cfg.MarkLinkInactive(chanPoint); err != nil {\n\t\tlog.Errorf(\"unable to mark link inactive: %v\", err)\n\t}\n\n\t// Now that we know the link can't mutate the channel\n\t// state, we'll read the channel from disk the target\n\t// channel according to its channel point.\n\tchannel, err := a.c.chanSource.ChannelStateDB().FetchChannel(\n\t\tnil, chanPoint,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Finally, we'll force close the channel completing\n\t// the force close workflow.\n\tchanMachine, err := lnwallet.NewLightningChannel(\n\t\ta.c.cfg.Signer, channel, nil,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn chanMachine.ForceClose()\n}\n\n// newActiveChannelArbitrator creates a new instance of an active channel\n// arbitrator given the state of the target channel.",
      "length": 1172,
      "tokens": 191,
      "embedding": []
    },
    {
      "slug": "func newActiveChannelArbitrator(channel *channeldb.OpenChannel,",
      "content": "func newActiveChannelArbitrator(channel *channeldb.OpenChannel,\n\tc *ChainArbitrator, chanEvents *ChainEventSubscription) (*ChannelArbitrator, error) {\n\n\tlog.Tracef(\"Creating ChannelArbitrator for ChannelPoint(%v)\",\n\t\tchannel.FundingOutpoint)\n\n\t// TODO(roasbeef): fetch best height (or pass in) so can ensure block\n\t// epoch delivers all the notifications to\n\n\tchanPoint := channel.FundingOutpoint\n\n\t// Next we'll create the matching configuration struct that contains\n\t// all interfaces and methods the arbitrator needs to do its job.\n\tarbCfg := ChannelArbitratorConfig{\n\t\tChanPoint:   chanPoint,\n\t\tChannel:     c.getArbChannel(channel),\n\t\tShortChanID: channel.ShortChanID(),\n\n\t\tMarkCommitmentBroadcasted: channel.MarkCommitmentBroadcasted,\n\t\tMarkChannelClosed: func(summary *channeldb.ChannelCloseSummary,\n\t\t\tstatuses ...channeldb.ChannelStatus) error {\n\n\t\t\terr := channel.CloseChannel(summary, statuses...)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tc.cfg.NotifyClosedChannel(summary.ChanPoint)\n\t\t\treturn nil\n\t\t},\n\t\tIsPendingClose:        false,\n\t\tChainArbitratorConfig: c.cfg,\n\t\tChainEvents:           chanEvents,\n\t\tPutResolverReport: func(tx kvdb.RwTx,\n\t\t\treport *channeldb.ResolverReport) error {\n\n\t\t\treturn c.chanSource.PutResolverReport(\n\t\t\t\ttx, c.cfg.ChainHash, &channel.FundingOutpoint,\n\t\t\t\treport,\n\t\t\t)\n\t\t},\n\t\tFetchHistoricalChannel: func() (*channeldb.OpenChannel, error) {\n\t\t\tchanStateDB := c.chanSource.ChannelStateDB()\n\t\t\treturn chanStateDB.FetchHistoricalChannel(&chanPoint)\n\t\t},\n\t}\n\n\t// The final component needed is an arbitrator log that the arbitrator\n\t// will use to keep track of its internal state using a backed\n\t// persistent log.\n\t//\n\t// TODO(roasbeef); abstraction leak...\n\t//  * rework: adaptor method to set log scope w/ factory func\n\tchanLog, err := newBoltArbitratorLog(\n\t\tc.chanSource.Backend, arbCfg, c.cfg.ChainHash, chanPoint,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tarbCfg.MarkChannelResolved = func() error {\n\t\tif c.cfg.NotifyFullyResolvedChannel != nil {\n\t\t\tc.cfg.NotifyFullyResolvedChannel(chanPoint)\n\t\t}\n\n\t\treturn c.ResolveContract(chanPoint)\n\t}\n\n\t// Finally, we'll need to construct a series of htlc Sets based on all\n\t// currently known valid commitments.\n\thtlcSets := make(map[HtlcSetKey]htlcSet)\n\thtlcSets[LocalHtlcSet] = newHtlcSet(channel.LocalCommitment.Htlcs)\n\thtlcSets[RemoteHtlcSet] = newHtlcSet(channel.RemoteCommitment.Htlcs)\n\n\tpendingRemoteCommitment, err := channel.RemoteCommitChainTip()\n\tif err != nil && err != channeldb.ErrNoPendingCommit {\n\t\treturn nil, err\n\t}\n\tif pendingRemoteCommitment != nil {\n\t\thtlcSets[RemotePendingHtlcSet] = newHtlcSet(\n\t\t\tpendingRemoteCommitment.Commitment.Htlcs,\n\t\t)\n\t}\n\n\treturn NewChannelArbitrator(\n\t\tarbCfg, htlcSets, chanLog,\n\t), nil\n}\n\n// getArbChannel returns an open channel wrapper for use by channel arbitrators.",
      "length": 2657,
      "tokens": 277,
      "embedding": []
    },
    {
      "slug": "func (c *ChainArbitrator) getArbChannel(",
      "content": "func (c *ChainArbitrator) getArbChannel(\n\tchannel *channeldb.OpenChannel) *arbChannel {\n\n\treturn &arbChannel{\n\t\tchannel: channel,\n\t\tc:       c,\n\t}\n}\n\n// ResolveContract marks a contract as fully resolved within the database.\n// This is only to be done once all contracts which were live on the channel\n// before hitting the chain have been resolved.",
      "length": 298,
      "tokens": 47,
      "embedding": []
    },
    {
      "slug": "func (c *ChainArbitrator) ResolveContract(chanPoint wire.OutPoint) error {",
      "content": "func (c *ChainArbitrator) ResolveContract(chanPoint wire.OutPoint) error {\n\tlog.Infof(\"Marking ChannelPoint(%v) fully resolved\", chanPoint)\n\n\t// First, we'll we'll mark the channel as fully closed from the PoV of\n\t// the channel source.\n\terr := c.chanSource.ChannelStateDB().MarkChanFullyClosed(&chanPoint)\n\tif err != nil {\n\t\tlog.Errorf(\"ChainArbitrator: unable to mark ChannelPoint(%v) \"+\n\t\t\t\"fully closed: %v\", chanPoint, err)\n\t\treturn err\n\t}\n\n\t// Now that the channel has been marked as fully closed, we'll stop\n\t// both the channel arbitrator and chain watcher for this channel if\n\t// they're still active.\n\tvar arbLog ArbitratorLog\n\tc.Lock()\n\tchainArb := c.activeChannels[chanPoint]\n\tdelete(c.activeChannels, chanPoint)\n\n\tchainWatcher := c.activeWatchers[chanPoint]\n\tdelete(c.activeWatchers, chanPoint)\n\tc.Unlock()\n\n\tif chainArb != nil {\n\t\tarbLog = chainArb.log\n\n\t\tif err := chainArb.Stop(); err != nil {\n\t\t\tlog.Warnf(\"unable to stop ChannelArbitrator(%v): %v\",\n\t\t\t\tchanPoint, err)\n\t\t}\n\t}\n\tif chainWatcher != nil {\n\t\tif err := chainWatcher.Stop(); err != nil {\n\t\t\tlog.Warnf(\"unable to stop ChainWatcher(%v): %v\",\n\t\t\t\tchanPoint, err)\n\t\t}\n\t}\n\n\t// Once this has been marked as resolved, we'll wipe the log that the\n\t// channel arbitrator was using to store its persistent state. We do\n\t// this after marking the channel resolved, as otherwise, the\n\t// arbitrator would be re-created, and think it was starting from the\n\t// default state.\n\tif arbLog != nil {\n\t\tif err := arbLog.WipeHistory(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Start launches all goroutines that the ChainArbitrator needs to operate.",
      "length": 1492,
      "tokens": 218,
      "embedding": []
    },
    {
      "slug": "func (c *ChainArbitrator) Start() error {",
      "content": "func (c *ChainArbitrator) Start() error {\n\tif !atomic.CompareAndSwapInt32(&c.started, 0, 1) {\n\t\treturn nil\n\t}\n\n\tlog.Info(\"ChainArbitrator starting\")\n\n\t// First, we'll fetch all the channels that are still open, in order to\n\t// collect them within our set of active contracts.\n\topenChannels, err := c.chanSource.ChannelStateDB().FetchAllChannels()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif len(openChannels) > 0 {\n\t\tlog.Infof(\"Creating ChannelArbitrators for %v active channels\",\n\t\t\tlen(openChannels))\n\t}\n\n\t// For each open channel, we'll configure then launch a corresponding\n\t// ChannelArbitrator.\n\tfor _, channel := range openChannels {\n\t\tchanPoint := channel.FundingOutpoint\n\t\tchannel := channel\n\n\t\t// First, we'll create an active chainWatcher for this channel\n\t\t// to ensure that we detect any relevant on chain events.\n\t\tbreachClosure := func(ret *lnwallet.BreachRetribution) error {\n\t\t\treturn c.cfg.ContractBreach(chanPoint, ret)\n\t\t}\n\n\t\tchainWatcher, err := newChainWatcher(\n\t\t\tchainWatcherConfig{\n\t\t\t\tchanState:           channel,\n\t\t\t\tnotifier:            c.cfg.Notifier,\n\t\t\t\tsigner:              c.cfg.Signer,\n\t\t\t\tisOurAddr:           c.cfg.IsOurAddress,\n\t\t\t\tcontractBreach:      breachClosure,\n\t\t\t\textractStateNumHint: lnwallet.GetStateNumHint,\n\t\t\t},\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tc.activeWatchers[chanPoint] = chainWatcher\n\t\tchannelArb, err := newActiveChannelArbitrator(\n\t\t\tchannel, c, chainWatcher.SubscribeChannelEvents(),\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tc.activeChannels[chanPoint] = channelArb\n\n\t\t// Republish any closing transactions for this channel.\n\t\terr = c.publishClosingTxs(channel)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// In addition to the channels that we know to be open, we'll also\n\t// launch arbitrators to finishing resolving any channels that are in\n\t// the pending close state.\n\tclosingChannels, err := c.chanSource.ChannelStateDB().FetchClosedChannels(\n\t\ttrue,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif len(closingChannels) > 0 {\n\t\tlog.Infof(\"Creating ChannelArbitrators for %v closing channels\",\n\t\t\tlen(closingChannels))\n\t}\n\n\t// Next, for each channel is the closing state, we'll launch a\n\t// corresponding more restricted resolver, as we don't have to watch\n\t// the chain any longer, only resolve the contracts on the confirmed\n\t// commitment.\n\tfor _, closeChanInfo := range closingChannels {\n\t\t// We can leave off the CloseContract and ForceCloseChan\n\t\t// methods as the channel is already closed at this point.\n\t\tchanPoint := closeChanInfo.ChanPoint\n\t\tarbCfg := ChannelArbitratorConfig{\n\t\t\tChanPoint:             chanPoint,\n\t\t\tShortChanID:           closeChanInfo.ShortChanID,\n\t\t\tChainArbitratorConfig: c.cfg,\n\t\t\tChainEvents:           &ChainEventSubscription{},\n\t\t\tIsPendingClose:        true,\n\t\t\tClosingHeight:         closeChanInfo.CloseHeight,\n\t\t\tCloseType:             closeChanInfo.CloseType,\n\t\t\tPutResolverReport: func(tx kvdb.RwTx,\n\t\t\t\treport *channeldb.ResolverReport) error {\n\n\t\t\t\treturn c.chanSource.PutResolverReport(\n\t\t\t\t\ttx, c.cfg.ChainHash, &chanPoint, report,\n\t\t\t\t)\n\t\t\t},\n\t\t\tFetchHistoricalChannel: func() (*channeldb.OpenChannel, error) {\n\t\t\t\tchanStateDB := c.chanSource.ChannelStateDB()\n\t\t\t\treturn chanStateDB.FetchHistoricalChannel(&chanPoint)\n\t\t\t},\n\t\t}\n\t\tchanLog, err := newBoltArbitratorLog(\n\t\t\tc.chanSource.Backend, arbCfg, c.cfg.ChainHash, chanPoint,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tarbCfg.MarkChannelResolved = func() error {\n\t\t\tif c.cfg.NotifyFullyResolvedChannel != nil {\n\t\t\t\tc.cfg.NotifyFullyResolvedChannel(chanPoint)\n\t\t\t}\n\n\t\t\treturn c.ResolveContract(chanPoint)\n\t\t}\n\n\t\t// We create an empty map of HTLC's here since it's possible\n\t\t// that the channel is in StateDefault and updateActiveHTLCs is\n\t\t// called. We want to avoid writing to an empty map. Since the\n\t\t// channel is already in the process of being resolved, no new\n\t\t// HTLCs will be added.\n\t\tc.activeChannels[chanPoint] = NewChannelArbitrator(\n\t\t\tarbCfg, make(map[HtlcSetKey]htlcSet), chanLog,\n\t\t)\n\t}\n\n\t// Now, we'll start all chain watchers in parallel to shorten start up\n\t// duration. In neutrino mode, this allows spend registrations to take\n\t// advantage of batch spend reporting, instead of doing a single rescan\n\t// per chain watcher.\n\t//\n\t// NOTE: After this point, we Stop the chain arb to ensure that any\n\t// lingering goroutines are cleaned up before exiting.\n\twatcherErrs := make(chan error, len(c.activeWatchers))\n\tvar wg sync.WaitGroup\n\tfor _, watcher := range c.activeWatchers {\n\t\twg.Add(1)\n\t\tgo func(w *chainWatcher) {\n\t\t\tdefer wg.Done()\n\t\t\tselect {\n\t\t\tcase watcherErrs <- w.Start():\n\t\t\tcase <-c.quit:\n\t\t\t\twatcherErrs <- ErrChainArbExiting\n\t\t\t}\n\t\t}(watcher)\n\t}\n\n\t// Once all chain watchers have been started, seal the err chan to\n\t// signal the end of the err stream.\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(watcherErrs)\n\t}()\n\n\t// stopAndLog is a helper function which shuts down the chain arb and\n\t// logs errors if they occur.\n\tstopAndLog := func() {\n\t\tif err := c.Stop(); err != nil {\n\t\t\tlog.Errorf(\"ChainArbitrator could not shutdown: %v\", err)\n\t\t}\n\t}\n\n\t// Handle all errors returned from spawning our chain watchers. If any\n\t// of them failed, we will stop the chain arb to shutdown any active\n\t// goroutines.\n\tfor err := range watcherErrs {\n\t\tif err != nil {\n\t\t\tstopAndLog()\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Before we start all of our arbitrators, we do a preliminary state\n\t// lookup so that we can combine all of these lookups in a single db\n\t// transaction.\n\tvar startStates map[wire.OutPoint]*chanArbStartState\n\n\terr = kvdb.View(c.chanSource, func(tx walletdb.ReadTx) error {\n\t\tfor _, arbitrator := range c.activeChannels {\n\t\t\tstartState, err := arbitrator.getStartState(tx)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tstartStates[arbitrator.cfg.ChanPoint] = startState\n\t\t}\n\n\t\treturn nil\n\t}, func() {\n\t\tstartStates = make(\n\t\t\tmap[wire.OutPoint]*chanArbStartState,\n\t\t\tlen(c.activeChannels),\n\t\t)\n\t})\n\tif err != nil {\n\t\tstopAndLog()\n\t\treturn err\n\t}\n\n\t// Launch all the goroutines for each arbitrator so they can carry out\n\t// their duties.\n\tfor _, arbitrator := range c.activeChannels {\n\t\tstartState, ok := startStates[arbitrator.cfg.ChanPoint]\n\t\tif !ok {\n\t\t\tstopAndLog()\n\t\t\treturn fmt.Errorf(\"arbitrator: %v has no start state\",\n\t\t\t\tarbitrator.cfg.ChanPoint)\n\t\t}\n\n\t\tif err := arbitrator.Start(startState); err != nil {\n\t\t\tstopAndLog()\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Subscribe to a single stream of block epoch notifications that we\n\t// will dispatch to all active arbitrators.\n\tblockEpoch, err := c.cfg.Notifier.RegisterBlockEpochNtfn(nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Start our goroutine which will dispatch blocks to each arbitrator.\n\tc.wg.Add(1)\n\tgo func() {\n\t\tdefer c.wg.Done()\n\t\tc.dispatchBlocks(blockEpoch)\n\t}()\n\n\t// TODO(roasbeef): eventually move all breach watching here\n\n\treturn nil\n}\n\n// blockRecipient contains the information we need to dispatch a block to a\n// channel arbitrator.",
      "length": 6589,
      "tokens": 867,
      "embedding": []
    },
    {
      "slug": "type blockRecipient struct {",
      "content": "type blockRecipient struct {\n\t// chanPoint is the funding outpoint of the channel.\n\tchanPoint wire.OutPoint\n\n\t// blocks is the channel that new block heights are sent into. This\n\t// channel should be sufficiently buffered as to not block the sender.\n\tblocks chan<- int32\n\n\t// quit is closed if the receiving entity is shutting down.\n\tquit chan struct{}\n}\n\n// dispatchBlocks consumes a block epoch notification stream and dispatches\n// blocks to each of the chain arb's active channel arbitrators. This function\n// must be run in a goroutine.",
      "length": 499,
      "tokens": 84,
      "embedding": []
    },
    {
      "slug": "func (c *ChainArbitrator) dispatchBlocks(",
      "content": "func (c *ChainArbitrator) dispatchBlocks(\n\tblockEpoch *chainntnfs.BlockEpochEvent) {\n\n\t// getRecipients is a helper function which acquires the chain arb\n\t// lock and returns a set of block recipients which can be used to\n\t// dispatch blocks.\n\tgetRecipients := func() []blockRecipient {\n\t\tc.Lock()\n\t\tblocks := make([]blockRecipient, 0, len(c.activeChannels))\n\t\tfor _, channel := range c.activeChannels {\n\t\t\tblocks = append(blocks, blockRecipient{\n\t\t\t\tchanPoint: channel.cfg.ChanPoint,\n\t\t\t\tblocks:    channel.blocks,\n\t\t\t\tquit:      channel.quit,\n\t\t\t})\n\t\t}\n\t\tc.Unlock()\n\n\t\treturn blocks\n\t}\n\n\t// On exit, cancel our blocks subscription and close each block channel\n\t// so that the arbitrators know they will no longer be receiving blocks.\n\tdefer func() {\n\t\tblockEpoch.Cancel()\n\n\t\trecipients := getRecipients()\n\t\tfor _, recipient := range recipients {\n\t\t\tclose(recipient.blocks)\n\t\t}\n\t}()\n\n\t// Consume block epochs until we receive the instruction to shutdown.\n\tfor {\n\t\tselect {\n\t\t// Consume block epochs, exiting if our subscription is\n\t\t// terminated.\n\t\tcase block, ok := <-blockEpoch.Epochs:\n\t\t\tif !ok {\n\t\t\t\tlog.Trace(\"dispatchBlocks block epoch \" +\n\t\t\t\t\t\"cancelled\")\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Get the set of currently active channels block\n\t\t\t// subscription channels and dispatch the block to\n\t\t\t// each.\n\t\t\tfor _, recipient := range getRecipients() {\n\t\t\t\tselect {\n\t\t\t\t// Deliver the block to the arbitrator.\n\t\t\t\tcase recipient.blocks <- block.Height:\n\n\t\t\t\t// If the recipient is shutting down, exit\n\t\t\t\t// without delivering the block. This may be\n\t\t\t\t// the case when two blocks are mined in quick\n\t\t\t\t// succession, and the arbitrator resolves\n\t\t\t\t// after the first block, and does not need to\n\t\t\t\t// consume the second block.\n\t\t\t\tcase <-recipient.quit:\n\t\t\t\t\tlog.Debugf(\"channel: %v exit without \"+\n\t\t\t\t\t\t\"receiving block: %v\",\n\t\t\t\t\t\trecipient.chanPoint,\n\t\t\t\t\t\tblock.Height)\n\n\t\t\t\t// If the chain arb is shutting down, we don't\n\t\t\t\t// need to deliver any more blocks (everything\n\t\t\t\t// will be shutting down).\n\t\t\t\tcase <-c.quit:\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\t// Exit if the chain arbitrator is shutting down.\n\t\tcase <-c.quit:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// publishClosingTxs will load any stored cooperative or unilater closing\n// transactions and republish them. This helps ensure propagation of the\n// transactions in the event that prior publications failed.",
      "length": 2227,
      "tokens": 320,
      "embedding": []
    },
    {
      "slug": "func (c *ChainArbitrator) publishClosingTxs(",
      "content": "func (c *ChainArbitrator) publishClosingTxs(\n\tchannel *channeldb.OpenChannel) error {\n\n\t// If the channel has had its unilateral close broadcasted already,\n\t// republish it in case it didn't propagate.\n\tif channel.HasChanStatus(channeldb.ChanStatusCommitBroadcasted) {\n\t\terr := c.rebroadcast(\n\t\t\tchannel, channeldb.ChanStatusCommitBroadcasted,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// If the channel has had its cooperative close broadcasted\n\t// already, republish it in case it didn't propagate.\n\tif channel.HasChanStatus(channeldb.ChanStatusCoopBroadcasted) {\n\t\terr := c.rebroadcast(\n\t\t\tchannel, channeldb.ChanStatusCoopBroadcasted,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// rebroadcast is a helper method which will republish the unilateral or\n// cooperative close transaction or a channel in a particular state.\n//\n// NOTE: There is no risk to calling this method if the channel isn't in either\n// CommitmentBroadcasted or CoopBroadcasted, but the logs will be misleading.",
      "length": 927,
      "tokens": 131,
      "embedding": []
    },
    {
      "slug": "func (c *ChainArbitrator) rebroadcast(channel *channeldb.OpenChannel,",
      "content": "func (c *ChainArbitrator) rebroadcast(channel *channeldb.OpenChannel,\n\tstate channeldb.ChannelStatus) error {\n\n\tchanPoint := channel.FundingOutpoint\n\n\tvar (\n\t\tcloseTx *wire.MsgTx\n\t\tkind    string\n\t\terr     error\n\t)\n\tswitch state {\n\tcase channeldb.ChanStatusCommitBroadcasted:\n\t\tkind = \"force\"\n\t\tcloseTx, err = channel.BroadcastedCommitment()\n\n\tcase channeldb.ChanStatusCoopBroadcasted:\n\t\tkind = \"coop\"\n\t\tcloseTx, err = channel.BroadcastedCooperative()\n\n\tdefault:\n\t\treturn fmt.Errorf(\"unknown closing state: %v\", state)\n\t}\n\n\tswitch {\n\t// This can happen for channels that had their closing tx published\n\t// before we started storing it to disk.\n\tcase err == channeldb.ErrNoCloseTx:\n\t\tlog.Warnf(\"Channel %v is in state %v, but no %s closing tx \"+\n\t\t\t\"to re-publish...\", chanPoint, state, kind)\n\t\treturn nil\n\n\tcase err != nil:\n\t\treturn err\n\t}\n\n\tlog.Infof(\"Re-publishing %s close tx(%v) for channel %v\",\n\t\tkind, closeTx.TxHash(), chanPoint)\n\n\tlabel := labels.MakeLabel(\n\t\tlabels.LabelTypeChannelClose, &channel.ShortChannelID,\n\t)\n\terr = c.cfg.PublishTx(closeTx, label)\n\tif err != nil && err != lnwallet.ErrDoubleSpend {\n\t\tlog.Warnf(\"Unable to broadcast %s close tx(%v): %v\",\n\t\t\tkind, closeTx.TxHash(), err)\n\t}\n\n\treturn nil\n}\n\n// Stop signals the ChainArbitrator to trigger a graceful shutdown. Any active\n// channel arbitrators will be signalled to exit, and this method will block\n// until they've all exited.",
      "length": 1285,
      "tokens": 170,
      "embedding": []
    },
    {
      "slug": "func (c *ChainArbitrator) Stop() error {",
      "content": "func (c *ChainArbitrator) Stop() error {\n\tif !atomic.CompareAndSwapInt32(&c.stopped, 0, 1) {\n\t\treturn nil\n\t}\n\n\tlog.Info(\"ChainArbitrator shutting down\")\n\n\tclose(c.quit)\n\n\tvar (\n\t\tactiveWatchers = make(map[wire.OutPoint]*chainWatcher)\n\t\tactiveChannels = make(map[wire.OutPoint]*ChannelArbitrator)\n\t)\n\n\t// Copy the current set of active watchers and arbitrators to shutdown.\n\t// We don't want to hold the lock when shutting down each watcher or\n\t// arbitrator individually, as they may need to acquire this mutex.\n\tc.Lock()\n\tfor chanPoint, watcher := range c.activeWatchers {\n\t\tactiveWatchers[chanPoint] = watcher\n\t}\n\tfor chanPoint, arbitrator := range c.activeChannels {\n\t\tactiveChannels[chanPoint] = arbitrator\n\t}\n\tc.Unlock()\n\n\tfor chanPoint, watcher := range activeWatchers {\n\t\tlog.Tracef(\"Attempting to stop ChainWatcher(%v)\",\n\t\t\tchanPoint)\n\n\t\tif err := watcher.Stop(); err != nil {\n\t\t\tlog.Errorf(\"unable to stop watcher for \"+\n\t\t\t\t\"ChannelPoint(%v): %v\", chanPoint, err)\n\t\t}\n\t}\n\tfor chanPoint, arbitrator := range activeChannels {\n\t\tlog.Tracef(\"Attempting to stop ChannelArbitrator(%v)\",\n\t\t\tchanPoint)\n\n\t\tif err := arbitrator.Stop(); err != nil {\n\t\t\tlog.Errorf(\"unable to stop arbitrator for \"+\n\t\t\t\t\"ChannelPoint(%v): %v\", chanPoint, err)\n\t\t}\n\t}\n\n\tc.wg.Wait()\n\n\treturn nil\n}\n\n// ContractUpdate is a message packages the latest set of active HTLCs on a\n// commitment, and also identifies which commitment received a new set of\n// HTLCs.",
      "length": 1346,
      "tokens": 178,
      "embedding": []
    },
    {
      "slug": "type ContractUpdate struct {",
      "content": "type ContractUpdate struct {\n\t// HtlcKey identifies which commitment the HTLCs below are present on.\n\tHtlcKey HtlcSetKey\n\n\t// Htlcs are the of active HTLCs on the commitment identified by the\n\t// above HtlcKey.\n\tHtlcs []channeldb.HTLC\n}\n\n// ContractSignals is used by outside subsystems to notify a channel arbitrator\n// of its ShortChannelID.",
      "length": 305,
      "tokens": 48,
      "embedding": []
    },
    {
      "slug": "type ContractSignals struct {",
      "content": "type ContractSignals struct {\n\t// ShortChanID is the up to date short channel ID for a contract. This\n\t// can change either if when the contract was added it didn't yet have\n\t// a stable identifier, or in the case of a reorg.\n\tShortChanID lnwire.ShortChannelID\n}\n\n// UpdateContractSignals sends a set of active, up to date contract signals to\n// the ChannelArbitrator which is has been assigned to the channel infield by\n// the passed channel point.",
      "length": 411,
      "tokens": 73,
      "embedding": []
    },
    {
      "slug": "func (c *ChainArbitrator) UpdateContractSignals(chanPoint wire.OutPoint,",
      "content": "func (c *ChainArbitrator) UpdateContractSignals(chanPoint wire.OutPoint,\n\tsignals *ContractSignals) error {\n\n\tlog.Infof(\"Attempting to update ContractSignals for ChannelPoint(%v)\",\n\t\tchanPoint)\n\n\tc.Lock()\n\tarbitrator, ok := c.activeChannels[chanPoint]\n\tc.Unlock()\n\tif !ok {\n\t\treturn fmt.Errorf(\"unable to find arbitrator\")\n\t}\n\n\tarbitrator.UpdateContractSignals(signals)\n\n\treturn nil\n}\n\n// NotifyContractUpdate lets a channel arbitrator know that a new\n// ContractUpdate is available. This calls the ChannelArbitrator's internal\n// method NotifyContractUpdate which waits for a response on a done chan before\n// returning. This method will return an error if the ChannelArbitrator is not\n// in the activeChannels map. However, this only happens if the arbitrator is\n// resolved and the related link would already be shut down.",
      "length": 730,
      "tokens": 99,
      "embedding": []
    },
    {
      "slug": "func (c *ChainArbitrator) NotifyContractUpdate(chanPoint wire.OutPoint,",
      "content": "func (c *ChainArbitrator) NotifyContractUpdate(chanPoint wire.OutPoint,\n\tupdate *ContractUpdate) error {\n\n\tc.Lock()\n\tarbitrator, ok := c.activeChannels[chanPoint]\n\tc.Unlock()\n\tif !ok {\n\t\treturn fmt.Errorf(\"can't find arbitrator for %v\", chanPoint)\n\t}\n\n\tarbitrator.notifyContractUpdate(update)\n\treturn nil\n}\n\n// GetChannelArbitrator safely returns the channel arbitrator for a given\n// channel outpoint.",
      "length": 316,
      "tokens": 38,
      "embedding": []
    },
    {
      "slug": "func (c *ChainArbitrator) GetChannelArbitrator(chanPoint wire.OutPoint) (",
      "content": "func (c *ChainArbitrator) GetChannelArbitrator(chanPoint wire.OutPoint) (\n\t*ChannelArbitrator, error) {\n\n\tc.Lock()\n\tarbitrator, ok := c.activeChannels[chanPoint]\n\tc.Unlock()\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"unable to find arbitrator\")\n\t}\n\n\treturn arbitrator, nil\n}\n\n// forceCloseReq is a request sent from an outside sub-system to the arbitrator\n// that watches a particular channel to broadcast the commitment transaction,\n// and enter the resolution phase of the channel.",
      "length": 388,
      "tokens": 56,
      "embedding": []
    },
    {
      "slug": "type forceCloseReq struct {",
      "content": "type forceCloseReq struct {\n\t// errResp is a channel that will be sent upon either in the case of\n\t// force close success (nil error), or in the case on an error.\n\t//\n\t// NOTE; This channel MUST be buffered.\n\terrResp chan error\n\n\t// closeTx is a channel that carries the transaction which ultimately\n\t// closed out the channel.\n\tcloseTx chan *wire.MsgTx\n}\n\n// ForceCloseContract attempts to force close the channel infield by the passed\n// channel point. A force close will immediately terminate the contract,\n// causing it to enter the resolution phase. If the force close was successful,\n// then the force close transaction itself will be returned.\n//\n// TODO(roasbeef): just return the summary itself?",
      "length": 660,
      "tokens": 114,
      "embedding": []
    },
    {
      "slug": "func (c *ChainArbitrator) ForceCloseContract(chanPoint wire.OutPoint) (*wire.MsgTx, error) {",
      "content": "func (c *ChainArbitrator) ForceCloseContract(chanPoint wire.OutPoint) (*wire.MsgTx, error) {\n\tc.Lock()\n\tarbitrator, ok := c.activeChannels[chanPoint]\n\tc.Unlock()\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"unable to find arbitrator\")\n\t}\n\n\tlog.Infof(\"Attempting to force close ChannelPoint(%v)\", chanPoint)\n\n\t// Before closing, we'll attempt to send a disable update for the\n\t// channel. We do so before closing the channel as otherwise the current\n\t// edge policy won't be retrievable from the graph.\n\tif err := c.cfg.DisableChannel(chanPoint); err != nil {\n\t\tlog.Warnf(\"Unable to disable channel %v on \"+\n\t\t\t\"close: %v\", chanPoint, err)\n\t}\n\n\terrChan := make(chan error, 1)\n\trespChan := make(chan *wire.MsgTx, 1)\n\n\t// With the channel found, and the request crafted, we'll send over a\n\t// force close request to the arbitrator that watches this channel.\n\tselect {\n\tcase arbitrator.forceCloseReqs <- &forceCloseReq{\n\t\terrResp: errChan,\n\t\tcloseTx: respChan,\n\t}:\n\tcase <-c.quit:\n\t\treturn nil, ErrChainArbExiting\n\t}\n\n\t// We'll await two responses: the error response, and the transaction\n\t// that closed out the channel.\n\tselect {\n\tcase err := <-errChan:\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\tcase <-c.quit:\n\t\treturn nil, ErrChainArbExiting\n\t}\n\n\tvar closeTx *wire.MsgTx\n\tselect {\n\tcase closeTx = <-respChan:\n\tcase <-c.quit:\n\t\treturn nil, ErrChainArbExiting\n\t}\n\n\treturn closeTx, nil\n}\n\n// WatchNewChannel sends the ChainArbitrator a message to create a\n// ChannelArbitrator tasked with watching over a new channel. Once a new\n// channel has finished its final funding flow, it should be registered with\n// the ChainArbitrator so we can properly react to any on-chain events.",
      "length": 1517,
      "tokens": 231,
      "embedding": []
    },
    {
      "slug": "func (c *ChainArbitrator) WatchNewChannel(newChan *channeldb.OpenChannel) error {",
      "content": "func (c *ChainArbitrator) WatchNewChannel(newChan *channeldb.OpenChannel) error {\n\tc.Lock()\n\tdefer c.Unlock()\n\n\tlog.Infof(\"Creating new ChannelArbitrator for ChannelPoint(%v)\",\n\t\tnewChan.FundingOutpoint)\n\n\t// If we're already watching this channel, then we'll ignore this\n\t// request.\n\tchanPoint := newChan.FundingOutpoint\n\tif _, ok := c.activeChannels[chanPoint]; ok {\n\t\treturn nil\n\t}\n\n\t// First, also create an active chainWatcher for this channel to ensure\n\t// that we detect any relevant on chain events.\n\tchainWatcher, err := newChainWatcher(\n\t\tchainWatcherConfig{\n\t\t\tchanState: newChan,\n\t\t\tnotifier:  c.cfg.Notifier,\n\t\t\tsigner:    c.cfg.Signer,\n\t\t\tisOurAddr: c.cfg.IsOurAddress,\n\t\t\tcontractBreach: func(\n\t\t\t\tretInfo *lnwallet.BreachRetribution) error {\n\n\t\t\t\treturn c.cfg.ContractBreach(\n\t\t\t\t\tchanPoint, retInfo,\n\t\t\t\t)\n\t\t\t},\n\t\t\textractStateNumHint: lnwallet.GetStateNumHint,\n\t\t},\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tc.activeWatchers[newChan.FundingOutpoint] = chainWatcher\n\n\t// We'll also create a new channel arbitrator instance using this new\n\t// channel, and our internal state.\n\tchannelArb, err := newActiveChannelArbitrator(\n\t\tnewChan, c, chainWatcher.SubscribeChannelEvents(),\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// With the arbitrator created, we'll add it to our set of active\n\t// arbitrators, then launch it.\n\tc.activeChannels[chanPoint] = channelArb\n\n\tif err := channelArb.Start(nil); err != nil {\n\t\treturn err\n\t}\n\n\treturn chainWatcher.Start()\n}\n\n// SubscribeChannelEvents returns a new active subscription for the set of\n// possible on-chain events for a particular channel. The struct can be used by\n// callers to be notified whenever an event that changes the state of the\n// channel on-chain occurs.",
      "length": 1580,
      "tokens": 208,
      "embedding": []
    },
    {
      "slug": "func (c *ChainArbitrator) SubscribeChannelEvents(",
      "content": "func (c *ChainArbitrator) SubscribeChannelEvents(\n\tchanPoint wire.OutPoint) (*ChainEventSubscription, error) {\n\n\t// First, we'll attempt to look up the active watcher for this channel.\n\t// If we can't find it, then we'll return an error back to the caller.\n\twatcher, ok := c.activeWatchers[chanPoint]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"unable to find watcher for: %v\",\n\t\t\tchanPoint)\n\t}\n\n\t// With the watcher located, we'll request for it to create a new chain\n\t// event subscription client.\n\treturn watcher.SubscribeChannelEvents(), nil\n}\n\n// TODO(roasbeef): arbitration reports\n//  * types: contested, waiting for success conf, etc\n",
      "length": 567,
      "tokens": 85,
      "embedding": []
    }
  ]
}