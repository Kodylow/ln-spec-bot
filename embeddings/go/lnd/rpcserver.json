{
  "filepath": "../implementations/go/lnd/rpcserver.go",
  "package": "lnd",
  "sections": [
    {
      "slug": "func stringInSlice(a string, slice []string) bool {",
      "content": "func stringInSlice(a string, slice []string) bool {\n\tfor _, b := range slice {\n\t\tif b == a {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// calculateFeeRate uses either satPerByte or satPerVByte, but not both, from a\n// request to calculate the fee rate. It provides compatibility for the\n// deprecated field, satPerByte. Once the field is safe to be removed, the\n// check can then be deleted.",
      "length": 325,
      "tokens": 62,
      "embedding": []
    },
    {
      "slug": "func calculateFeeRate(satPerByte, satPerVByte uint64, targetConf uint32,",
      "content": "func calculateFeeRate(satPerByte, satPerVByte uint64, targetConf uint32,\n\testimator chainfee.Estimator) (chainfee.SatPerKWeight, error) {\n\n\tvar feeRate chainfee.SatPerKWeight\n\n\t// We only allow using either the deprecated field or the new field.\n\tif satPerByte != 0 && satPerVByte != 0 {\n\t\treturn feeRate, fmt.Errorf(\"either SatPerByte or \" +\n\t\t\t\"SatPerVByte should be set, but not both\")\n\t}\n\n\t// Default to satPerVByte, and overwrite it if satPerByte is set.\n\tsatPerKw := chainfee.SatPerKVByte(satPerVByte * 1000).FeePerKWeight()\n\tif satPerByte != 0 {\n\t\tsatPerKw = chainfee.SatPerKVByte(\n\t\t\tsatPerByte * 1000,\n\t\t).FeePerKWeight()\n\t}\n\n\t// Based on the passed fee related parameters, we'll determine an\n\t// appropriate fee rate for this transaction.\n\tfeeRate, err := sweep.DetermineFeePerKw(\n\t\testimator, sweep.FeePreference{\n\t\t\tConfTarget: targetConf,\n\t\t\tFeeRate:    satPerKw,\n\t\t},\n\t)\n\tif err != nil {\n\t\treturn feeRate, err\n\t}\n\n\treturn feeRate, nil\n}\n\n// GetAllPermissions returns all the permissions required to interact with lnd.",
      "length": 925,
      "tokens": 128,
      "embedding": []
    },
    {
      "slug": "func GetAllPermissions() []bakery.Op {",
      "content": "func GetAllPermissions() []bakery.Op {\n\tallPerms := make([]bakery.Op, 0)\n\n\t// The map will help keep track of which specific permission pairs have\n\t// already been added to the slice.\n\tallPermsMap := make(map[string]map[string]struct{})\n\n\tfor _, perms := range MainRPCServerPermissions() {\n\t\tfor _, perm := range perms {\n\t\t\tentity := perm.Entity\n\t\t\taction := perm.Action\n\n\t\t\t// If this specific entity-action permission pair isn't\n\t\t\t// in the map yet. Add it to map, and the permission\n\t\t\t// slice.\n\t\t\tif acts, ok := allPermsMap[entity]; ok {\n\t\t\t\tif _, ok := acts[action]; !ok {\n\t\t\t\t\tallPermsMap[entity][action] = struct{}{}\n\n\t\t\t\t\tallPerms = append(\n\t\t\t\t\t\tallPerms, perm,\n\t\t\t\t\t)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tallPermsMap[entity] = make(map[string]struct{})\n\t\t\t\tallPermsMap[entity][action] = struct{}{}\n\t\t\t\tallPerms = append(allPerms, perm)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn allPerms\n}\n\n// MainRPCServerPermissions returns a mapping of the main RPC server calls to\n// the permissions they require.",
      "length": 903,
      "tokens": 129,
      "embedding": []
    },
    {
      "slug": "func MainRPCServerPermissions() map[string][]bakery.Op {",
      "content": "func MainRPCServerPermissions() map[string][]bakery.Op {\n\treturn map[string][]bakery.Op{\n\t\t\"/lnrpc.Lightning/SendCoins\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ListUnspent\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SendMany\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/NewAddress\": {{\n\t\t\tEntity: \"address\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SignMessage\": {{\n\t\t\tEntity: \"message\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/VerifyMessage\": {{\n\t\t\tEntity: \"message\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ConnectPeer\": {{\n\t\t\tEntity: \"peers\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/DisconnectPeer\": {{\n\t\t\tEntity: \"peers\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/OpenChannel\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"write\",\n\t\t}, {\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/BatchOpenChannel\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"write\",\n\t\t}, {\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/OpenChannelSync\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"write\",\n\t\t}, {\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/CloseChannel\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"write\",\n\t\t}, {\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/AbandonChannel\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/GetInfo\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/GetRecoveryInfo\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ListPeers\": {{\n\t\t\tEntity: \"peers\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/WalletBalance\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/EstimateFee\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ChannelBalance\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/PendingChannels\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ListChannels\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SubscribeChannelEvents\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ClosedChannels\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SendPayment\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SendPaymentSync\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SendToRoute\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SendToRouteSync\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/AddInvoice\": {{\n\t\t\tEntity: \"invoices\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/LookupInvoice\": {{\n\t\t\tEntity: \"invoices\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ListInvoices\": {{\n\t\t\tEntity: \"invoices\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SubscribeInvoices\": {{\n\t\t\tEntity: \"invoices\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SubscribeTransactions\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/GetTransactions\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/DescribeGraph\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/GetNodeMetrics\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/GetChanInfo\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/GetNodeInfo\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/QueryRoutes\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/GetNetworkInfo\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/StopDaemon\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SubscribeChannelGraph\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ListPayments\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/DeletePayment\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/DeleteAllPayments\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/DebugLevel\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/DecodePayReq\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/FeeReport\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/UpdateChannelPolicy\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ForwardingHistory\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/RestoreChannelBackups\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ExportChannelBackup\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/VerifyChanBackup\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ExportAllChannelBackups\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SubscribeChannelBackups\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ChannelAcceptor\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"write\",\n\t\t}, {\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/BakeMacaroon\": {{\n\t\t\tEntity: \"macaroon\",\n\t\t\tAction: \"generate\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ListMacaroonIDs\": {{\n\t\t\tEntity: \"macaroon\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/DeleteMacaroonID\": {{\n\t\t\tEntity: \"macaroon\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ListPermissions\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/CheckMacaroonPermissions\": {{\n\t\t\tEntity: \"macaroon\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SubscribePeerEvents\": {{\n\t\t\tEntity: \"peers\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/FundingStateStep\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"write\",\n\t\t}, {\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\tlnrpc.RegisterRPCMiddlewareURI: {{\n\t\t\tEntity: \"macaroon\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SendCustomMessage\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SubscribeCustomMessages\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/LookupHtlcResolution\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ListAliases\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t}\n}\n\n// rpcServer is a gRPC, RPC front end to the lnd daemon.\n// TODO(roasbeef): pagination support for the list-style calls",
      "length": 6036,
      "tokens": 529,
      "embedding": []
    },
    {
      "slug": "type rpcServer struct {",
      "content": "type rpcServer struct {\n\tstarted  int32 // To be used atomically.\n\tshutdown int32 // To be used atomically.\n\n\t// Required by the grpc-gateway/v2 library for forward compatibility.\n\t// Must be after the atomically used variables to not break struct\n\t// alignment.\n\tlnrpc.UnimplementedLightningServer\n\n\tserver *server\n\n\tcfg *Config\n\n\t// subServers are a set of sub-RPC servers that use the same gRPC and\n\t// listening sockets as the main RPC server, but which maintain their\n\t// own independent service. This allows us to expose a set of\n\t// micro-service like abstractions to the outside world for users to\n\t// consume.\n\tsubServers      []lnrpc.SubServer\n\tsubGrpcHandlers []lnrpc.GrpcHandler\n\n\t// routerBackend contains the backend implementation of the router\n\t// rpc sub server.\n\trouterBackend *routerrpc.RouterBackend\n\n\t// chanPredicate is used in the bidirectional ChannelAcceptor streaming\n\t// method.\n\tchanPredicate chanacceptor.MultiplexAcceptor\n\n\tquit chan struct{}\n\n\t// macService is the macaroon service that we need to mint new\n\t// macaroons.\n\tmacService *macaroons.Service\n\n\t// selfNode is our own pubkey.\n\tselfNode route.Vertex\n\n\t// interceptorChain is the interceptor added to our gRPC server.\n\tinterceptorChain *rpcperms.InterceptorChain\n\n\t// implCfg is the configuration for some of the interfaces that can be\n\t// provided externally.\n\timplCfg *ImplementationCfg\n\n\t// interceptor is used to be able to request a shutdown\n\tinterceptor signal.Interceptor\n\n\tgraphCache        sync.RWMutex\n\tdescribeGraphResp *lnrpc.ChannelGraph\n\tgraphCacheEvictor *time.Timer\n}\n\n// A compile time check to ensure that rpcServer fully implements the\n// LightningServer gRPC service.\nvar _ lnrpc.LightningServer = (*rpcServer)(nil)\n\n// newRPCServer creates and returns a new instance of the rpcServer. Before\n// dependencies are added, this will be an non-functioning RPC server only to\n// be used to register the LightningService with the gRPC server.",
      "length": 1863,
      "tokens": 259,
      "embedding": []
    },
    {
      "slug": "func newRPCServer(cfg *Config, interceptorChain *rpcperms.InterceptorChain,",
      "content": "func newRPCServer(cfg *Config, interceptorChain *rpcperms.InterceptorChain,\n\timplCfg *ImplementationCfg, interceptor signal.Interceptor) *rpcServer {\n\n\t// We go trhough the list of registered sub-servers, and create a gRPC\n\t// handler for each. These are used to register with the gRPC server\n\t// before all dependencies are available.\n\tregisteredSubServers := lnrpc.RegisteredSubServers()\n\n\tvar subServerHandlers []lnrpc.GrpcHandler\n\tfor _, subServer := range registeredSubServers {\n\t\tsubServerHandlers = append(\n\t\t\tsubServerHandlers, subServer.NewGrpcHandler(),\n\t\t)\n\t}\n\n\treturn &rpcServer{\n\t\tcfg:              cfg,\n\t\tsubGrpcHandlers:  subServerHandlers,\n\t\tinterceptorChain: interceptorChain,\n\t\timplCfg:          implCfg,\n\t\tquit:             make(chan struct{}, 1),\n\t\tinterceptor:      interceptor,\n\t}\n}\n\n// addDeps populates all dependencies needed by the RPC server, and any\n// of the sub-servers that it maintains. When this is done, the RPC server can\n// be started, and start accepting RPC calls.",
      "length": 900,
      "tokens": 111,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) addDeps(s *server, macService *macaroons.Service,",
      "content": "func (r *rpcServer) addDeps(s *server, macService *macaroons.Service,\n\tsubServerCgs *subRPCServerConfigs, atpl *autopilot.Manager,\n\tinvoiceRegistry *invoices.InvoiceRegistry, tower *watchtower.Standalone,\n\tchanPredicate chanacceptor.MultiplexAcceptor) error {\n\n\t// Set up router rpc backend.\n\tselfNode, err := s.graphDB.SourceNode()\n\tif err != nil {\n\t\treturn err\n\t}\n\tgraph := s.graphDB\n\n\trouterBackend := &routerrpc.RouterBackend{\n\t\tSelfNode: selfNode.PubKeyBytes,\n\t\tFetchChannelCapacity: func(chanID uint64) (btcutil.Amount,\n\t\t\terror) {\n\n\t\t\tinfo, _, _, err := graph.FetchChannelEdgesByID(chanID)\n\t\t\tif err != nil {\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\treturn info.Capacity, nil\n\t\t},\n\t\tFetchAmountPairCapacity: func(nodeFrom, nodeTo route.Vertex,\n\t\t\tamount lnwire.MilliSatoshi) (btcutil.Amount, error) {\n\n\t\t\troutingGraph, err := routing.NewCachedGraph(\n\t\t\t\tselfNode, graph,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\tdefer func() {\n\t\t\t\tcloseErr := routingGraph.Close()\n\t\t\t\tif closeErr != nil {\n\t\t\t\t\trpcsLog.Errorf(\"not able to close \"+\n\t\t\t\t\t\t\"routing graph tx: %v\",\n\t\t\t\t\t\tcloseErr)\n\t\t\t\t}\n\t\t\t}()\n\n\t\t\treturn routingGraph.FetchAmountPairCapacity(\n\t\t\t\tnodeFrom, nodeTo, amount,\n\t\t\t)\n\t\t},\n\t\tFetchChannelEndpoints: func(chanID uint64) (route.Vertex,\n\t\t\troute.Vertex, error) {\n\n\t\t\tinfo, _, _, err := graph.FetchChannelEdgesByID(\n\t\t\t\tchanID,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn route.Vertex{}, route.Vertex{},\n\t\t\t\t\tfmt.Errorf(\"unable to fetch channel \"+\n\t\t\t\t\t\t\"edges by channel ID %d: %v\",\n\t\t\t\t\t\tchanID, err)\n\t\t\t}\n\n\t\t\treturn info.NodeKey1Bytes, info.NodeKey2Bytes, nil\n\t\t},\n\t\tFindRoute:              s.chanRouter.FindRoute,\n\t\tMissionControl:         s.missionControl,\n\t\tActiveNetParams:        r.cfg.ActiveNetParams.Params,\n\t\tTower:                  s.controlTower,\n\t\tMaxTotalTimelock:       r.cfg.MaxOutgoingCltvExpiry,\n\t\tDefaultFinalCltvDelta:  uint16(r.cfg.Bitcoin.TimeLockDelta),\n\t\tSubscribeHtlcEvents:    s.htlcNotifier.SubscribeHtlcEvents,\n\t\tInterceptableForwarder: s.interceptableSwitch,\n\t\tSetChannelEnabled: func(outpoint wire.OutPoint) error {\n\t\t\treturn s.chanStatusMgr.RequestEnable(outpoint, true)\n\t\t},\n\t\tSetChannelDisabled: func(outpoint wire.OutPoint) error {\n\t\t\treturn s.chanStatusMgr.RequestDisable(outpoint, true)\n\t\t},\n\t\tSetChannelAuto: s.chanStatusMgr.RequestAuto,\n\t}\n\n\tgenInvoiceFeatures := func() *lnwire.FeatureVector {\n\t\treturn s.featureMgr.Get(feature.SetInvoice)\n\t}\n\tgenAmpInvoiceFeatures := func() *lnwire.FeatureVector {\n\t\treturn s.featureMgr.Get(feature.SetInvoiceAmp)\n\t}\n\n\tgetNodeAnnouncement := func() (lnwire.NodeAnnouncement, error) {\n\t\treturn s.genNodeAnnouncement(false)\n\t}\n\n\tparseAddr := func(addr string) (net.Addr, error) {\n\t\treturn parseAddr(addr, r.cfg.net)\n\t}\n\n\tvar (\n\t\tsubServers     []lnrpc.SubServer\n\t\tsubServerPerms []lnrpc.MacaroonPerms\n\t)\n\n\t// Before we create any of the sub-servers, we need to ensure that all\n\t// the dependencies they need are properly populated within each sub\n\t// server configuration struct.\n\t//\n\t// TODO(roasbeef): extend sub-sever config to have both (local vs remote) DB\n\terr = subServerCgs.PopulateDependencies(\n\t\tr.cfg, s.cc, r.cfg.networkDir, macService, atpl, invoiceRegistry,\n\t\ts.htlcSwitch, r.cfg.ActiveNetParams.Params, s.chanRouter,\n\t\trouterBackend, s.nodeSigner, s.graphDB, s.chanStateDB,\n\t\ts.sweeper, tower, s.towerClient, s.anchorTowerClient,\n\t\tr.cfg.net.ResolveTCPAddr, genInvoiceFeatures,\n\t\tgenAmpInvoiceFeatures, getNodeAnnouncement,\n\t\ts.updateAndBrodcastSelfNode, parseAddr, rpcsLog,\n\t\ts.aliasMgr.GetPeerAlias,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Now that the sub-servers have all their dependencies in place, we\n\t// can create each sub-server!\n\tfor _, subServerInstance := range r.subGrpcHandlers {\n\t\tsubServer, macPerms, err := subServerInstance.CreateSubServer(\n\t\t\tsubServerCgs,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// We'll collect the sub-server, and also the set of\n\t\t// permissions it needs for macaroons so we can apply the\n\t\t// interceptors below.\n\t\tsubServers = append(subServers, subServer)\n\t\tsubServerPerms = append(subServerPerms, macPerms)\n\t}\n\n\t// Next, we need to merge the set of sub server macaroon permissions\n\t// with the main RPC server permissions so we can unite them under a\n\t// single set of interceptors.\n\tfor m, ops := range MainRPCServerPermissions() {\n\t\terr := r.interceptorChain.AddPermission(m, ops)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tfor _, subServerPerm := range subServerPerms {\n\t\tfor method, ops := range subServerPerm {\n\t\t\terr := r.interceptorChain.AddPermission(method, ops)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\t// External subserver possibly need to register their own permissions\n\t// and macaroon validator.\n\tfor method, ops := range r.implCfg.ExternalValidator.Permissions() {\n\t\terr := r.interceptorChain.AddPermission(method, ops)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Give the external subservers the possibility to also use\n\t\t// their own validator to check any macaroons attached to calls\n\t\t// to this method. This allows them to have their own root key\n\t\t// ID database and permission entities.\n\t\terr = macService.RegisterExternalValidator(\n\t\t\tmethod, r.implCfg.ExternalValidator,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"could not register external \"+\n\t\t\t\t\"macaroon validator: %v\", err)\n\t\t}\n\t}\n\n\t// Finally, with all the set up complete, add the last dependencies to\n\t// the rpc server.\n\tr.server = s\n\tr.subServers = subServers\n\tr.routerBackend = routerBackend\n\tr.chanPredicate = chanPredicate\n\tr.macService = macService\n\tr.selfNode = selfNode.PubKeyBytes\n\n\tgraphCacheDuration := r.cfg.Caches.RPCGraphCacheDuration\n\tif graphCacheDuration != 0 {\n\t\tr.graphCacheEvictor = time.AfterFunc(graphCacheDuration, func() {\n\t\t\t// Grab the mutex and purge the current populated\n\t\t\t// describe graph response.\n\t\t\tr.graphCache.Lock()\n\t\t\tdefer r.graphCache.Unlock()\n\n\t\t\tr.describeGraphResp = nil\n\n\t\t\t// Reset ourselves as well at the end so we run again\n\t\t\t// after the duration.\n\t\t\tr.graphCacheEvictor.Reset(graphCacheDuration)\n\t\t})\n\t}\n\n\treturn nil\n}\n\n// RegisterWithGrpcServer registers the rpcServer and any subservers with the\n// root gRPC server.",
      "length": 5817,
      "tokens": 670,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) RegisterWithGrpcServer(grpcServer *grpc.Server) error {",
      "content": "func (r *rpcServer) RegisterWithGrpcServer(grpcServer *grpc.Server) error {\n\t// Register the main RPC server.\n\tlnrpc.RegisterLightningServer(grpcServer, r)\n\n\t// Now the main RPC server has been registered, we'll iterate through\n\t// all the sub-RPC servers and register them to ensure that requests\n\t// are properly routed towards them.\n\tfor _, subServer := range r.subGrpcHandlers {\n\t\terr := subServer.RegisterWithRootServer(grpcServer)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to register \"+\n\t\t\t\t\"sub-server with root: %v\", err)\n\t\t}\n\t}\n\n\t// Before actually listening on the gRPC listener, give external\n\t// subservers the chance to register to our gRPC server. Those external\n\t// subservers (think GrUB) are responsible for starting/stopping on\n\t// their own, we just let them register their services to the same\n\t// server instance so all of them can be exposed on the same\n\t// port/listener.\n\terr := r.implCfg.RegisterGrpcSubserver(grpcServer)\n\tif err != nil {\n\t\trpcsLog.Errorf(\"error registering external gRPC \"+\n\t\t\t\"subserver: %v\", err)\n\t}\n\n\treturn nil\n}\n\n// Start launches any helper goroutines required for the rpcServer to function.",
      "length": 1041,
      "tokens": 156,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) Start() error {",
      "content": "func (r *rpcServer) Start() error {\n\tif atomic.AddInt32(&r.started, 1) != 1 {\n\t\treturn nil\n\t}\n\n\t// First, we'll start all the sub-servers to ensure that they're ready\n\t// to take new requests in.\n\t//\n\t// TODO(roasbeef): some may require that the entire daemon be started\n\t// at that point\n\tfor _, subServer := range r.subServers {\n\t\trpcsLog.Debugf(\"Starting sub RPC server: %v\", subServer.Name())\n\n\t\tif err := subServer.Start(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// RegisterWithRestProxy registers the RPC server and any subservers with the\n// given REST proxy.",
      "length": 520,
      "tokens": 86,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) RegisterWithRestProxy(restCtx context.Context,",
      "content": "func (r *rpcServer) RegisterWithRestProxy(restCtx context.Context,\n\trestMux *proxy.ServeMux, restDialOpts []grpc.DialOption,\n\trestProxyDest string) error {\n\n\t// With our custom REST proxy mux created, register our main RPC and\n\t// give all subservers a chance to register as well.\n\terr := lnrpc.RegisterLightningHandlerFromEndpoint(\n\t\trestCtx, restMux, restProxyDest, restDialOpts,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tfor _, subServer := range r.subGrpcHandlers {\n\t\terr := subServer.RegisterWithRestServer(\n\t\t\trestCtx, restMux, restProxyDest, restDialOpts,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to register REST sub-server \"+\n\t\t\t\t\"with root: %v\", err)\n\t\t}\n\t}\n\n\t// Before listening on any of the interfaces, we also want to give the\n\t// external subservers a chance to register their own REST proxy stub\n\t// with our mux instance.\n\terr = r.implCfg.RegisterRestSubserver(\n\t\trestCtx, restMux, restProxyDest, restDialOpts,\n\t)\n\tif err != nil {\n\t\trpcsLog.Errorf(\"error registering external REST subserver: %v\",\n\t\t\terr)\n\t}\n\treturn nil\n}\n\n// Stop signals any active goroutines for a graceful closure.",
      "length": 1005,
      "tokens": 145,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) Stop() error {",
      "content": "func (r *rpcServer) Stop() error {\n\tif atomic.AddInt32(&r.shutdown, 1) != 1 {\n\t\treturn nil\n\t}\n\n\trpcsLog.Infof(\"Stopping RPC Server\")\n\n\tclose(r.quit)\n\n\t// After we've signalled all of our active goroutines to exit, we'll\n\t// then do the same to signal a graceful shutdown of all the sub\n\t// servers.\n\tfor _, subServer := range r.subServers {\n\t\trpcsLog.Infof(\"Stopping %v Sub-RPC Server\",\n\t\t\tsubServer.Name())\n\n\t\tif err := subServer.Stop(); err != nil {\n\t\t\trpcsLog.Errorf(\"unable to stop sub-server %v: %v\",\n\t\t\t\tsubServer.Name(), err)\n\t\t\tcontinue\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// addrPairsToOutputs converts a map describing a set of outputs to be created,\n// the outputs themselves. The passed map pairs up an address, to a desired\n// output value amount. Each address is converted to its corresponding pkScript\n// to be used within the constructed output(s).",
      "length": 788,
      "tokens": 122,
      "embedding": []
    },
    {
      "slug": "func addrPairsToOutputs(addrPairs map[string]int64,",
      "content": "func addrPairsToOutputs(addrPairs map[string]int64,\n\tparams *chaincfg.Params) ([]*wire.TxOut, error) {\n\n\toutputs := make([]*wire.TxOut, 0, len(addrPairs))\n\tfor addr, amt := range addrPairs {\n\t\taddr, err := btcutil.DecodeAddress(addr, params)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tpkscript, err := txscript.PayToAddrScript(addr)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\toutputs = append(outputs, wire.NewTxOut(amt, pkscript))\n\t}\n\n\treturn outputs, nil\n}\n\n// allowCORS wraps the given http.Handler with a function that adds the\n// Access-Control-Allow-Origin header to the response.",
      "length": 515,
      "tokens": 72,
      "embedding": []
    },
    {
      "slug": "func allowCORS(handler http.Handler, origins []string) http.Handler {",
      "content": "func allowCORS(handler http.Handler, origins []string) http.Handler {\n\tallowHeaders := \"Access-Control-Allow-Headers\"\n\tallowMethods := \"Access-Control-Allow-Methods\"\n\tallowOrigin := \"Access-Control-Allow-Origin\"\n\n\t// If the user didn't supply any origins that means CORS is disabled\n\t// and we should return the original handler.\n\tif len(origins) == 0 {\n\t\treturn handler\n\t}\n\n\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\torigin := r.Header.Get(\"Origin\")\n\n\t\t// Skip everything if the browser doesn't send the Origin field.\n\t\tif origin == \"\" {\n\t\t\thandler.ServeHTTP(w, r)\n\t\t\treturn\n\t\t}\n\n\t\t// Set the static header fields first.\n\t\tw.Header().Set(\n\t\t\tallowHeaders,\n\t\t\t\"Content-Type, Accept, Grpc-Metadata-Macaroon\",\n\t\t)\n\t\tw.Header().Set(allowMethods, \"GET, POST, DELETE\")\n\n\t\t// Either we allow all origins or the incoming request matches\n\t\t// a specific origin in our list of allowed origins.\n\t\tfor _, allowedOrigin := range origins {\n\t\t\tif allowedOrigin == \"*\" || origin == allowedOrigin {\n\t\t\t\t// Only set allowed origin to requested origin.\n\t\t\t\tw.Header().Set(allowOrigin, origin)\n\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\t// For a pre-flight request we only need to send the headers\n\t\t// back. No need to call the rest of the chain.\n\t\tif r.Method == \"OPTIONS\" {\n\t\t\treturn\n\t\t}\n\n\t\t// Everything's prepared now, we can pass the request along the\n\t\t// chain of handlers.\n\t\thandler.ServeHTTP(w, r)\n\t})\n}\n\n// sendCoinsOnChain makes an on-chain transaction in or to send coins to one or\n// more addresses specified in the passed payment map. The payment map maps an\n// address to a specified output value to be sent to that address.",
      "length": 1511,
      "tokens": 224,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) sendCoinsOnChain(paymentMap map[string]int64,",
      "content": "func (r *rpcServer) sendCoinsOnChain(paymentMap map[string]int64,\n\tfeeRate chainfee.SatPerKWeight, minConfs int32,\n\tlabel string) (*chainhash.Hash, error) {\n\n\toutputs, err := addrPairsToOutputs(paymentMap, r.cfg.ActiveNetParams.Params)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// We first do a dry run, to sanity check we won't spend our wallet\n\t// balance below the reserved amount.\n\tauthoredTx, err := r.server.cc.Wallet.CreateSimpleTx(\n\t\toutputs, feeRate, minConfs, true,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Check the authored transaction and use the explicitly set change index\n\t// to make sure that the wallet reserved balance is not invalidated.\n\t_, err = r.server.cc.Wallet.CheckReservedValueTx(\n\t\tlnwallet.CheckReservedValueTxReq{\n\t\t\tTx:          authoredTx.Tx,\n\t\t\tChangeIndex: &authoredTx.ChangeIndex,\n\t\t},\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// If that checks out, we're fairly confident that creating sending to\n\t// these outputs will keep the wallet balance above the reserve.\n\ttx, err := r.server.cc.Wallet.SendOutputs(\n\t\toutputs, feeRate, minConfs, label,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ttxHash := tx.TxHash()\n\treturn &txHash, nil\n}\n\n// ListUnspent returns useful information about each unspent output owned by\n// the wallet, as reported by the underlying `ListUnspentWitness`; the\n// information returned is: outpoint, amount in satoshis, address, address\n// type, scriptPubKey in hex and number of confirmations.  The result is\n// filtered to contain outputs whose number of confirmations is between a\n// minimum and maximum number of confirmations specified by the user, with\n// 0 meaning unconfirmed.",
      "length": 1531,
      "tokens": 225,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) ListUnspent(ctx context.Context,",
      "content": "func (r *rpcServer) ListUnspent(ctx context.Context,\n\tin *lnrpc.ListUnspentRequest) (*lnrpc.ListUnspentResponse, error) {\n\n\t// Validate the confirmation arguments.\n\tminConfs, maxConfs, err := lnrpc.ParseConfs(in.MinConfs, in.MaxConfs)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// With our arguments validated, we'll query the internal wallet for\n\t// the set of UTXOs that match our query.\n\t//\n\t// We'll acquire the global coin selection lock to ensure there aren't\n\t// any other concurrent processes attempting to lock any UTXOs which may\n\t// be shown available to us.\n\tvar utxos []*lnwallet.Utxo\n\terr = r.server.cc.Wallet.WithCoinSelectLock(func() error {\n\t\tutxos, err = r.server.cc.Wallet.ListUnspentWitness(\n\t\t\tminConfs, maxConfs, in.Account,\n\t\t)\n\t\treturn err\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trpcUtxos, err := lnrpc.MarshalUtxos(utxos, r.cfg.ActiveNetParams.Params)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tmaxStr := \"\"\n\tif maxConfs != math.MaxInt32 {\n\t\tmaxStr = \" max=\" + fmt.Sprintf(\"%d\", maxConfs)\n\t}\n\n\trpcsLog.Debugf(\"[listunspent] min=%v%v, generated utxos: %v\", minConfs,\n\t\tmaxStr, utxos)\n\n\treturn &lnrpc.ListUnspentResponse{\n\t\tUtxos: rpcUtxos,\n\t}, nil\n}\n\n// EstimateFee handles a request for estimating the fee for sending a\n// transaction spending to multiple specified outputs in parallel.",
      "length": 1212,
      "tokens": 170,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) EstimateFee(ctx context.Context,",
      "content": "func (r *rpcServer) EstimateFee(ctx context.Context,\n\tin *lnrpc.EstimateFeeRequest) (*lnrpc.EstimateFeeResponse, error) {\n\n\t// Create the list of outputs we are spending to.\n\toutputs, err := addrPairsToOutputs(in.AddrToAmount, r.cfg.ActiveNetParams.Params)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Query the fee estimator for the fee rate for the given confirmation\n\t// target.\n\ttarget := in.TargetConf\n\tfeePerKw, err := sweep.DetermineFeePerKw(\n\t\tr.server.cc.FeeEstimator, sweep.FeePreference{\n\t\t\tConfTarget: uint32(target),\n\t\t},\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Then, we'll extract the minimum number of confirmations that each\n\t// output we use to fund the transaction should satisfy.\n\tminConfs, err := lnrpc.ExtractMinConfs(\n\t\tin.GetMinConfs(), in.GetSpendUnconfirmed(),\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// We will ask the wallet to create a tx using this fee rate. We set\n\t// dryRun=true to avoid inflating the change addresses in the db.\n\tvar tx *txauthor.AuthoredTx\n\twallet := r.server.cc.Wallet\n\terr = wallet.WithCoinSelectLock(func() error {\n\t\ttx, err = wallet.CreateSimpleTx(outputs, feePerKw, minConfs, true)\n\t\treturn err\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Use the created tx to calculate the total fee.\n\ttotalOutput := int64(0)\n\tfor _, out := range tx.Tx.TxOut {\n\t\ttotalOutput += out.Value\n\t}\n\ttotalFee := int64(tx.TotalInput) - totalOutput\n\n\tresp := &lnrpc.EstimateFeeResponse{\n\t\tFeeSat:      totalFee,\n\t\tSatPerVbyte: uint64(feePerKw.FeePerKVByte() / 1000),\n\n\t\t// Deprecated field.\n\t\tFeerateSatPerByte: int64(feePerKw.FeePerKVByte() / 1000),\n\t}\n\n\trpcsLog.Debugf(\"[estimatefee] fee estimate for conf target %d: %v\",\n\t\ttarget, resp)\n\n\treturn resp, nil\n}\n\n// SendCoins executes a request to send coins to a particular address. Unlike\n// SendMany, this RPC call only allows creating a single output at a time.",
      "length": 1739,
      "tokens": 247,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) SendCoins(ctx context.Context,",
      "content": "func (r *rpcServer) SendCoins(ctx context.Context,\n\tin *lnrpc.SendCoinsRequest) (*lnrpc.SendCoinsResponse, error) {\n\n\t// Calculate an appropriate fee rate for this transaction.\n\tfeePerKw, err := calculateFeeRate(\n\t\tuint64(in.SatPerByte), in.SatPerVbyte, // nolint:staticcheck\n\t\tuint32(in.TargetConf), r.server.cc.FeeEstimator,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Then, we'll extract the minimum number of confirmations that each\n\t// output we use to fund the transaction should satisfy.\n\tminConfs, err := lnrpc.ExtractMinConfs(in.MinConfs, in.SpendUnconfirmed)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trpcsLog.Infof(\"[sendcoins] addr=%v, amt=%v, sat/kw=%v, min_confs=%v, \"+\n\t\t\"send_all=%v\",\n\t\tin.Addr, btcutil.Amount(in.Amount), int64(feePerKw), minConfs,\n\t\tin.SendAll)\n\n\t// Decode the address receiving the coins, we need to check whether the\n\t// address is valid for this network.\n\ttargetAddr, err := btcutil.DecodeAddress(\n\t\tin.Addr, r.cfg.ActiveNetParams.Params,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Make the check on the decoded address according to the active network.\n\tif !targetAddr.IsForNet(r.cfg.ActiveNetParams.Params) {\n\t\treturn nil, fmt.Errorf(\"address: %v is not valid for this \"+\n\t\t\t\"network: %v\", targetAddr.String(),\n\t\t\tr.cfg.ActiveNetParams.Params.Name)\n\t}\n\n\t// If the destination address parses to a valid pubkey, we assume the user\n\t// accidentally tried to send funds to a bare pubkey address. This check is\n\t// here to prevent unintended transfers.\n\tdecodedAddr, _ := hex.DecodeString(in.Addr)\n\t_, err = btcec.ParsePubKey(decodedAddr)\n\tif err == nil {\n\t\treturn nil, fmt.Errorf(\"cannot send coins to pubkeys\")\n\t}\n\n\tlabel, err := labels.ValidateAPI(in.Label)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar txid *chainhash.Hash\n\n\twallet := r.server.cc.Wallet\n\n\t// If the send all flag is active, then we'll attempt to sweep all the\n\t// coins in the wallet in a single transaction (if possible),\n\t// otherwise, we'll respect the amount, and attempt a regular 2-output\n\t// send.\n\tif in.SendAll {\n\t\t// At this point, the amount shouldn't be set since we've been\n\t\t// instructed to sweep all the coins from the wallet.\n\t\tif in.Amount != 0 {\n\t\t\treturn nil, fmt.Errorf(\"amount set while SendAll is \" +\n\t\t\t\t\"active\")\n\t\t}\n\n\t\t_, bestHeight, err := r.server.cc.ChainIO.GetBestBlock()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// With the sweeper instance created, we can now generate a\n\t\t// transaction that will sweep ALL outputs from the wallet in a\n\t\t// single transaction. This will be generated in a concurrent\n\t\t// safe manner, so no need to worry about locking. The tx will\n\t\t// pay to the change address created above if we needed to\n\t\t// reserve any value, the rest will go to targetAddr.\n\t\tsweepTxPkg, err := sweep.CraftSweepAllTx(\n\t\t\tfeePerKw, uint32(bestHeight), nil, targetAddr, wallet,\n\t\t\twallet, wallet.WalletController,\n\t\t\tr.server.cc.FeeEstimator, r.server.cc.Signer,\n\t\t\tminConfs,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// Before we publish the transaction we make sure it won't\n\t\t// violate our reserved wallet value.\n\t\tvar reservedVal btcutil.Amount\n\t\terr = wallet.WithCoinSelectLock(func() error {\n\t\t\tvar err error\n\t\t\treservedVal, err = wallet.CheckReservedValueTx(\n\t\t\t\tlnwallet.CheckReservedValueTxReq{\n\t\t\t\t\tTx: sweepTxPkg.SweepTx,\n\t\t\t\t},\n\t\t\t)\n\t\t\treturn err\n\t\t})\n\n\t\t// If sending everything to this address would invalidate our\n\t\t// reserved wallet balance, we create a new sweep tx, where\n\t\t// we'll send the reserved value back to our wallet.\n\t\tif err == lnwallet.ErrReservedValueInvalidated {\n\t\t\tsweepTxPkg.CancelSweepAttempt()\n\n\t\t\trpcsLog.Debugf(\"Reserved value %v not satisfied after \"+\n\t\t\t\t\"send_all, trying with change output\",\n\t\t\t\treservedVal)\n\n\t\t\t// We'll request a change address from the wallet,\n\t\t\t// where we'll send this reserved value back to. This\n\t\t\t// ensures this is an address the wallet knows about,\n\t\t\t// allowing us to pass the reserved value check.\n\t\t\tchangeAddr, err := r.server.cc.Wallet.NewAddress(\n\t\t\t\tlnwallet.TaprootPubkey, true,\n\t\t\t\tlnwallet.DefaultAccountName,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\t// Send the reserved value to this change address, the\n\t\t\t// remaining funds will go to the targetAddr.\n\t\t\toutputs := []sweep.DeliveryAddr{\n\t\t\t\t{\n\t\t\t\t\tAddr: changeAddr,\n\t\t\t\t\tAmt:  reservedVal,\n\t\t\t\t},\n\t\t\t}\n\n\t\t\tsweepTxPkg, err = sweep.CraftSweepAllTx(\n\t\t\t\tfeePerKw, uint32(bestHeight), outputs,\n\t\t\t\ttargetAddr, wallet, wallet,\n\t\t\t\twallet.WalletController,\n\t\t\t\tr.server.cc.FeeEstimator, r.server.cc.Signer,\n\t\t\t\tminConfs,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\t// Sanity check the new tx by re-doing the check.\n\t\t\terr = wallet.WithCoinSelectLock(func() error {\n\t\t\t\t_, err := wallet.CheckReservedValueTx(\n\t\t\t\t\tlnwallet.CheckReservedValueTxReq{\n\t\t\t\t\t\tTx: sweepTxPkg.SweepTx,\n\t\t\t\t\t},\n\t\t\t\t)\n\t\t\t\treturn err\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\tsweepTxPkg.CancelSweepAttempt()\n\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t} else if err != nil {\n\t\t\tsweepTxPkg.CancelSweepAttempt()\n\n\t\t\treturn nil, err\n\t\t}\n\n\t\trpcsLog.Debugf(\"Sweeping all coins from wallet to addr=%v, \"+\n\t\t\t\"with tx=%v\", in.Addr, spew.Sdump(sweepTxPkg.SweepTx))\n\n\t\t// As our sweep transaction was created, successfully, we'll\n\t\t// now attempt to publish it, cancelling the sweep pkg to\n\t\t// return all outputs if it fails.\n\t\terr = wallet.PublishTransaction(sweepTxPkg.SweepTx, label)\n\t\tif err != nil {\n\t\t\tsweepTxPkg.CancelSweepAttempt()\n\n\t\t\treturn nil, fmt.Errorf(\"unable to broadcast sweep \"+\n\t\t\t\t\"transaction: %v\", err)\n\t\t}\n\n\t\tsweepTXID := sweepTxPkg.SweepTx.TxHash()\n\t\ttxid = &sweepTXID\n\t} else {\n\n\t\t// We'll now construct out payment map, and use the wallet's\n\t\t// coin selection synchronization method to ensure that no coin\n\t\t// selection (funding, sweep alls, other sends) can proceed\n\t\t// while we instruct the wallet to send this transaction.\n\t\tpaymentMap := map[string]int64{targetAddr.String(): in.Amount}\n\t\terr := wallet.WithCoinSelectLock(func() error {\n\t\t\tnewTXID, err := r.sendCoinsOnChain(\n\t\t\t\tpaymentMap, feePerKw, minConfs, label,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\ttxid = newTXID\n\n\t\t\treturn nil\n\t\t})\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\trpcsLog.Infof(\"[sendcoins] spend generated txid: %v\", txid.String())\n\n\treturn &lnrpc.SendCoinsResponse{Txid: txid.String()}, nil\n}\n\n// SendMany handles a request for a transaction create multiple specified\n// outputs in parallel.",
      "length": 6091,
      "tokens": 831,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) SendMany(ctx context.Context,",
      "content": "func (r *rpcServer) SendMany(ctx context.Context,\n\tin *lnrpc.SendManyRequest) (*lnrpc.SendManyResponse, error) {\n\n\t// Calculate an appropriate fee rate for this transaction.\n\tfeePerKw, err := calculateFeeRate(\n\t\tuint64(in.SatPerByte), in.SatPerVbyte, // nolint:staticcheck\n\t\tuint32(in.TargetConf), r.server.cc.FeeEstimator,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Then, we'll extract the minimum number of confirmations that each\n\t// output we use to fund the transaction should satisfy.\n\tminConfs, err := lnrpc.ExtractMinConfs(in.MinConfs, in.SpendUnconfirmed)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlabel, err := labels.ValidateAPI(in.Label)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trpcsLog.Infof(\"[sendmany] outputs=%v, sat/kw=%v\",\n\t\tspew.Sdump(in.AddrToAmount), int64(feePerKw))\n\n\tvar txid *chainhash.Hash\n\n\t// We'll attempt to send to the target set of outputs, ensuring that we\n\t// synchronize with any other ongoing coin selection attempts which\n\t// happen to also be concurrently executing.\n\twallet := r.server.cc.Wallet\n\terr = wallet.WithCoinSelectLock(func() error {\n\t\tsendManyTXID, err := r.sendCoinsOnChain(\n\t\t\tin.AddrToAmount, feePerKw, minConfs, label,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\ttxid = sendManyTXID\n\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trpcsLog.Infof(\"[sendmany] spend generated txid: %v\", txid.String())\n\n\treturn &lnrpc.SendManyResponse{Txid: txid.String()}, nil\n}\n\n// NewAddress creates a new address under control of the local wallet.",
      "length": 1391,
      "tokens": 184,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) NewAddress(ctx context.Context,",
      "content": "func (r *rpcServer) NewAddress(ctx context.Context,\n\tin *lnrpc.NewAddressRequest) (*lnrpc.NewAddressResponse, error) {\n\n\t// Always use the default wallet account unless one was specified.\n\taccount := lnwallet.DefaultAccountName\n\tif in.Account != \"\" {\n\t\taccount = in.Account\n\t}\n\n\t// Translate the gRPC proto address type to the wallet controller's\n\t// available address types.\n\tvar (\n\t\taddr btcutil.Address\n\t\terr  error\n\t)\n\tswitch in.Type {\n\tcase lnrpc.AddressType_WITNESS_PUBKEY_HASH:\n\t\taddr, err = r.server.cc.Wallet.NewAddress(\n\t\t\tlnwallet.WitnessPubKey, false, account,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\tcase lnrpc.AddressType_NESTED_PUBKEY_HASH:\n\t\taddr, err = r.server.cc.Wallet.NewAddress(\n\t\t\tlnwallet.NestedWitnessPubKey, false, account,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\tcase lnrpc.AddressType_TAPROOT_PUBKEY:\n\t\taddr, err = r.server.cc.Wallet.NewAddress(\n\t\t\tlnwallet.TaprootPubkey, false, account,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\tcase lnrpc.AddressType_UNUSED_WITNESS_PUBKEY_HASH:\n\t\taddr, err = r.server.cc.Wallet.LastUnusedAddress(\n\t\t\tlnwallet.WitnessPubKey, account,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\tcase lnrpc.AddressType_UNUSED_NESTED_PUBKEY_HASH:\n\t\taddr, err = r.server.cc.Wallet.LastUnusedAddress(\n\t\t\tlnwallet.NestedWitnessPubKey, account,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\tcase lnrpc.AddressType_UNUSED_TAPROOT_PUBKEY:\n\t\taddr, err = r.server.cc.Wallet.LastUnusedAddress(\n\t\t\tlnwallet.TaprootPubkey, account,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown address type: %v\", in.Type)\n\t}\n\n\trpcsLog.Debugf(\"[newaddress] account=%v type=%v addr=%v\", account,\n\t\tin.Type, addr.String())\n\treturn &lnrpc.NewAddressResponse{Address: addr.String()}, nil\n}\n\nvar (\n\t// signedMsgPrefix is a special prefix that we'll prepend to any\n\t// messages we sign/verify. We do this to ensure that we don't\n\t// accidentally sign a sighash, or other sensitive material. By\n\t// prepending this fragment, we mind message signing to our particular\n\t// context.\n\tsignedMsgPrefix = []byte(\"Lightning Signed Message:\")\n)\n\n// SignMessage signs a message with the resident node's private key. The\n// returned signature string is zbase32 encoded and pubkey recoverable, meaning\n// that only the message digest and signature are needed for verification.",
      "length": 2207,
      "tokens": 274,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) SignMessage(_ context.Context,",
      "content": "func (r *rpcServer) SignMessage(_ context.Context,\n\tin *lnrpc.SignMessageRequest) (*lnrpc.SignMessageResponse, error) {\n\n\tif in.Msg == nil {\n\t\treturn nil, fmt.Errorf(\"need a message to sign\")\n\t}\n\n\tin.Msg = append(signedMsgPrefix, in.Msg...)\n\tsigBytes, err := r.server.nodeSigner.SignMessageCompact(\n\t\tin.Msg, !in.SingleHash,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tsig := zbase32.EncodeToString(sigBytes)\n\treturn &lnrpc.SignMessageResponse{Signature: sig}, nil\n}\n\n// VerifyMessage verifies a signature over a msg. The signature must be zbase32\n// encoded and signed by an active node in the resident node's channel\n// database. In addition to returning the validity of the signature,\n// VerifyMessage also returns the recovered pubkey from the signature.",
      "length": 685,
      "tokens": 93,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) VerifyMessage(ctx context.Context,",
      "content": "func (r *rpcServer) VerifyMessage(ctx context.Context,\n\tin *lnrpc.VerifyMessageRequest) (*lnrpc.VerifyMessageResponse, error) {\n\n\tif in.Msg == nil {\n\t\treturn nil, fmt.Errorf(\"need a message to verify\")\n\t}\n\n\t// The signature should be zbase32 encoded\n\tsig, err := zbase32.DecodeString(in.Signature)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to decode signature: %v\", err)\n\t}\n\n\t// The signature is over the double-sha256 hash of the message.\n\tin.Msg = append(signedMsgPrefix, in.Msg...)\n\tdigest := chainhash.DoubleHashB(in.Msg)\n\n\t// RecoverCompact both recovers the pubkey and validates the signature.\n\tpubKey, _, err := ecdsa.RecoverCompact(sig, digest)\n\tif err != nil {\n\t\treturn &lnrpc.VerifyMessageResponse{Valid: false}, nil\n\t}\n\tpubKeyHex := hex.EncodeToString(pubKey.SerializeCompressed())\n\n\tvar pub [33]byte\n\tcopy(pub[:], pubKey.SerializeCompressed())\n\n\t// Query the channel graph to ensure a node in the network with active\n\t// channels signed the message.\n\t//\n\t// TODO(phlip9): Require valid nodes to have capital in active channels.\n\tgraph := r.server.graphDB\n\t_, active, err := graph.HasLightningNode(pub)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to query graph: %v\", err)\n\t}\n\n\treturn &lnrpc.VerifyMessageResponse{\n\t\tValid:  active,\n\t\tPubkey: pubKeyHex,\n\t}, nil\n}\n\n// ConnectPeer attempts to establish a connection to a remote peer.",
      "length": 1260,
      "tokens": 168,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) ConnectPeer(ctx context.Context,",
      "content": "func (r *rpcServer) ConnectPeer(ctx context.Context,\n\tin *lnrpc.ConnectPeerRequest) (*lnrpc.ConnectPeerResponse, error) {\n\n\t// The server hasn't yet started, so it won't be able to service any of\n\t// our requests, so we'll bail early here.\n\tif !r.server.Started() {\n\t\treturn nil, ErrServerNotActive\n\t}\n\n\tif in.Addr == nil {\n\t\treturn nil, fmt.Errorf(\"need: lnc pubkeyhash@hostname\")\n\t}\n\n\tpubkeyHex, err := hex.DecodeString(in.Addr.Pubkey)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpubKey, err := btcec.ParsePubKey(pubkeyHex)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Connections to ourselves are disallowed for obvious reasons.\n\tif pubKey.IsEqual(r.server.identityECDH.PubKey()) {\n\t\treturn nil, fmt.Errorf(\"cannot make connection to self\")\n\t}\n\n\taddr, err := parseAddr(in.Addr.Host, r.cfg.net)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tpeerAddr := &lnwire.NetAddress{\n\t\tIdentityKey: pubKey,\n\t\tAddress:     addr,\n\t\tChainNet:    r.cfg.ActiveNetParams.Net,\n\t}\n\n\trpcsLog.Debugf(\"[connectpeer] requested connection to %x@%s\",\n\t\tpeerAddr.IdentityKey.SerializeCompressed(), peerAddr.Address)\n\n\t// By default, we will use the global connection timeout value.\n\ttimeout := r.cfg.ConnectionTimeout\n\n\t// Check if the connection timeout is set. If set, we will use it in our\n\t// request.\n\tif in.Timeout != 0 {\n\t\ttimeout = time.Duration(in.Timeout) * time.Second\n\t\trpcsLog.Debugf(\n\t\t\t\"[connectpeer] connection timeout is set to %v\",\n\t\t\ttimeout,\n\t\t)\n\t}\n\n\tif err := r.server.ConnectToPeer(\n\t\tpeerAddr, in.Perm, timeout,\n\t); err != nil {\n\t\trpcsLog.Errorf(\n\t\t\t\"[connectpeer]: error connecting to peer: %v\", err,\n\t\t)\n\t\treturn nil, err\n\t}\n\n\trpcsLog.Debugf(\"Connected to peer: %v\", peerAddr.String())\n\treturn &lnrpc.ConnectPeerResponse{}, nil\n}\n\n// DisconnectPeer attempts to disconnect one peer from another identified by a\n// given pubKey. In the case that we currently have a pending or active channel\n// with the target peer, this action will be disallowed.",
      "length": 1813,
      "tokens": 247,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) DisconnectPeer(ctx context.Context,",
      "content": "func (r *rpcServer) DisconnectPeer(ctx context.Context,\n\tin *lnrpc.DisconnectPeerRequest) (*lnrpc.DisconnectPeerResponse, error) {\n\n\trpcsLog.Debugf(\"[disconnectpeer] from peer(%s)\", in.PubKey)\n\n\tif !r.server.Started() {\n\t\treturn nil, ErrServerNotActive\n\t}\n\n\t// First we'll validate the string passed in within the request to\n\t// ensure that it's a valid hex-string, and also a valid compressed\n\t// public key.\n\tpubKeyBytes, err := hex.DecodeString(in.PubKey)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to decode pubkey bytes: %v\", err)\n\t}\n\tpeerPubKey, err := btcec.ParsePubKey(pubKeyBytes)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to parse pubkey: %v\", err)\n\t}\n\n\t// Next, we'll fetch the pending/active channels we have with a\n\t// particular peer.\n\tnodeChannels, err := r.server.chanStateDB.FetchOpenChannels(peerPubKey)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to fetch channels for peer: %v\", err)\n\t}\n\n\t// In order to avoid erroneously disconnecting from a peer that we have\n\t// an active channel with, if we have any channels active with this\n\t// peer, then we'll disallow disconnecting from them.\n\tif len(nodeChannels) > 0 && !r.cfg.UnsafeDisconnect {\n\t\treturn nil, fmt.Errorf(\"cannot disconnect from peer(%x), \"+\n\t\t\t\"all active channels with the peer need to be closed \"+\n\t\t\t\"first\", pubKeyBytes)\n\t}\n\n\t// With all initial validation complete, we'll now request that the\n\t// server disconnects from the peer.\n\tif err := r.server.DisconnectPeer(peerPubKey); err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to disconnect peer: %v\", err)\n\t}\n\n\treturn &lnrpc.DisconnectPeerResponse{}, nil\n}\n\n// newFundingShimAssembler returns a new fully populated\n// chanfunding.CannedAssembler using a FundingShim obtained from an RPC caller.",
      "length": 1648,
      "tokens": 231,
      "embedding": []
    },
    {
      "slug": "func newFundingShimAssembler(chanPointShim *lnrpc.ChanPointShim, initiator bool,",
      "content": "func newFundingShimAssembler(chanPointShim *lnrpc.ChanPointShim, initiator bool,\n\tkeyRing keychain.KeyRing) (chanfunding.Assembler, error) {\n\n\t// Perform some basic sanity checks to ensure that all the expected\n\t// fields are populated.\n\tswitch {\n\tcase chanPointShim.RemoteKey == nil:\n\t\treturn nil, fmt.Errorf(\"remote key not set\")\n\n\tcase chanPointShim.LocalKey == nil:\n\t\treturn nil, fmt.Errorf(\"local key desc not set\")\n\n\tcase chanPointShim.LocalKey.RawKeyBytes == nil:\n\t\treturn nil, fmt.Errorf(\"local raw key bytes not set\")\n\n\tcase chanPointShim.LocalKey.KeyLoc == nil:\n\t\treturn nil, fmt.Errorf(\"local key loc not set\")\n\n\tcase chanPointShim.ChanPoint == nil:\n\t\treturn nil, fmt.Errorf(\"chan point not set\")\n\n\tcase len(chanPointShim.PendingChanId) != 32:\n\t\treturn nil, fmt.Errorf(\"pending chan ID not set\")\n\t}\n\n\t// First, we'll map the RPC's channel point to one we can actually use.\n\tindex := chanPointShim.ChanPoint.OutputIndex\n\ttxid, err := lnrpc.GetChanPointFundingTxid(chanPointShim.ChanPoint)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tchanPoint := wire.NewOutPoint(txid, index)\n\n\t// Next we'll parse out the remote party's funding key, as well as our\n\t// full key descriptor.\n\tremoteKey, err := btcec.ParsePubKey(chanPointShim.RemoteKey)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tshimKeyDesc := chanPointShim.LocalKey\n\tlocalKey, err := btcec.ParsePubKey(shimKeyDesc.RawKeyBytes)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tlocalKeyDesc := keychain.KeyDescriptor{\n\t\tPubKey: localKey,\n\t\tKeyLocator: keychain.KeyLocator{\n\t\t\tFamily: keychain.KeyFamily(\n\t\t\t\tshimKeyDesc.KeyLoc.KeyFamily,\n\t\t\t),\n\t\t\tIndex: uint32(shimKeyDesc.KeyLoc.KeyIndex),\n\t\t},\n\t}\n\n\t// Verify that if we re-derive this key according to the passed\n\t// KeyLocator, that we get the exact same key back. Otherwise, we may\n\t// end up in a situation where we aren't able to actually sign for this\n\t// newly created channel.\n\tderivedKey, err := keyRing.DeriveKey(localKeyDesc.KeyLocator)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif !derivedKey.PubKey.IsEqual(localKey) {\n\t\treturn nil, fmt.Errorf(\"KeyLocator does not match attached \" +\n\t\t\t\"raw pubkey\")\n\t}\n\n\t// With all the parts assembled, we can now make the canned assembler\n\t// to pass into the wallet.\n\treturn chanfunding.NewCannedAssembler(\n\t\tchanPointShim.ThawHeight, *chanPoint,\n\t\tbtcutil.Amount(chanPointShim.Amt), &localKeyDesc,\n\t\tremoteKey, initiator,\n\t), nil\n}\n\n// newFundingShimAssembler returns a new fully populated\n// chanfunding.PsbtAssembler using a FundingShim obtained from an RPC caller.",
      "length": 2350,
      "tokens": 304,
      "embedding": []
    },
    {
      "slug": "func newPsbtAssembler(req *lnrpc.OpenChannelRequest, normalizedMinConfs int32,",
      "content": "func newPsbtAssembler(req *lnrpc.OpenChannelRequest, normalizedMinConfs int32,\n\tpsbtShim *lnrpc.PsbtShim, netParams *chaincfg.Params) (\n\tchanfunding.Assembler, error) {\n\n\tvar (\n\t\tpacket *psbt.Packet\n\t\terr    error\n\t)\n\n\t// Perform some basic sanity checks to ensure that all the expected\n\t// fields are populated and none of the incompatible fields are.\n\tif len(psbtShim.PendingChanId) != 32 {\n\t\treturn nil, fmt.Errorf(\"pending chan ID not set\")\n\t}\n\tif normalizedMinConfs != 1 {\n\t\treturn nil, fmt.Errorf(\"setting non-default values for \" +\n\t\t\t\"minimum confirmation is not supported for PSBT \" +\n\t\t\t\"funding\")\n\t}\n\tif req.SatPerByte != 0 || req.SatPerVbyte != 0 || req.TargetConf != 0 { // nolint:staticcheck\n\t\treturn nil, fmt.Errorf(\"specifying fee estimation parameters \" +\n\t\t\t\"is not supported for PSBT funding\")\n\t}\n\n\t// The base PSBT is optional. But if it's set, it has to be a valid,\n\t// binary serialized PSBT.\n\tif len(psbtShim.BasePsbt) > 0 {\n\t\tpacket, err = psbt.NewFromRawBytes(\n\t\t\tbytes.NewReader(psbtShim.BasePsbt), false,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"error parsing base PSBT: %v\",\n\t\t\t\terr)\n\t\t}\n\t}\n\n\t// With all the parts assembled, we can now make the canned assembler\n\t// to pass into the wallet.\n\treturn chanfunding.NewPsbtAssembler(\n\t\tbtcutil.Amount(req.LocalFundingAmount), packet, netParams,\n\t\t!psbtShim.NoPublish,\n\t), nil\n}\n\n// canOpenChannel returns an error if the necessary subsystems for channel\n// funding are not ready.",
      "length": 1338,
      "tokens": 196,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) canOpenChannel() error {",
      "content": "func (r *rpcServer) canOpenChannel() error {\n\t// We can't open a channel until the main server has started.\n\tif !r.server.Started() {\n\t\treturn ErrServerNotActive\n\t}\n\n\t// Creation of channels before the wallet syncs up is currently\n\t// disallowed.\n\tisSynced, _, err := r.server.cc.Wallet.IsSynced()\n\tif err != nil {\n\t\treturn err\n\t}\n\tif !isSynced {\n\t\treturn errors.New(\"channels cannot be created before the \" +\n\t\t\t\"wallet is fully synced\")\n\t}\n\n\treturn nil\n}\n\n// parseOpenChannelReq parses an OpenChannelRequest message into an InitFundingMsg\n// struct. The logic is abstracted so that it can be shared between OpenChannel\n// and OpenChannelSync.",
      "length": 578,
      "tokens": 90,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) parseOpenChannelReq(in *lnrpc.OpenChannelRequest,",
      "content": "func (r *rpcServer) parseOpenChannelReq(in *lnrpc.OpenChannelRequest,\n\tisSync bool) (*funding.InitFundingMsg, error) {\n\n\trpcsLog.Debugf(\"[openchannel] request to NodeKey(%x) \"+\n\t\t\"allocation(us=%v, them=%v)\", in.NodePubkey,\n\t\tin.LocalFundingAmount, in.PushSat)\n\n\tlocalFundingAmt := btcutil.Amount(in.LocalFundingAmount)\n\tremoteInitialBalance := btcutil.Amount(in.PushSat)\n\tminHtlcIn := lnwire.MilliSatoshi(in.MinHtlcMsat)\n\tremoteCsvDelay := uint16(in.RemoteCsvDelay)\n\tmaxValue := lnwire.MilliSatoshi(in.RemoteMaxValueInFlightMsat)\n\tmaxHtlcs := uint16(in.RemoteMaxHtlcs)\n\tremoteChanReserve := btcutil.Amount(in.RemoteChanReserveSat)\n\n\tglobalFeatureSet := r.server.featureMgr.Get(feature.SetNodeAnn)\n\n\t// Ensure that a local funding amount has been specified.\n\tif localFundingAmt == 0 {\n\t\treturn nil, fmt.Errorf(\"local funding amount must be non-zero\")\n\t}\n\n\t// Ensure that the initial balance of the remote party (if pushing\n\t// satoshis) does not exceed the amount the local party has requested\n\t// for funding.\n\t//\n\tif remoteInitialBalance >= localFundingAmt {\n\t\treturn nil, fmt.Errorf(\"amount pushed to remote peer for \" +\n\t\t\t\"initial state must be below the local funding amount\")\n\t}\n\n\t// Determine if the user provided channel fees\n\t// and if so pass them on to the funding workflow.\n\tvar channelBaseFee, channelFeeRate *uint64\n\tif in.UseBaseFee {\n\t\tchannelBaseFee = &in.BaseFee\n\t}\n\tif in.UseFeeRate {\n\t\tchannelFeeRate = &in.FeeRate\n\t}\n\n\t// Ensure that the remote channel reserve does not exceed 20% of the\n\t// channel capacity.\n\tif remoteChanReserve >= localFundingAmt/5 {\n\t\treturn nil, fmt.Errorf(\"remote channel reserve must be less \" +\n\t\t\t\"than the %%20 of the channel capacity\")\n\t}\n\n\t// Ensure that the user doesn't exceed the current soft-limit for\n\t// channel size. If the funding amount is above the soft-limit, then\n\t// we'll reject the request.\n\twumboEnabled := globalFeatureSet.HasFeature(\n\t\tlnwire.WumboChannelsOptional,\n\t)\n\tif !wumboEnabled && localFundingAmt > MaxFundingAmount {\n\t\treturn nil, fmt.Errorf(\"funding amount is too large, the max \"+\n\t\t\t\"channel size is: %v\", MaxFundingAmount)\n\t}\n\n\t// Restrict the size of the channel we'll actually open. At a later\n\t// level, we'll ensure that the output we create after accounting for\n\t// fees that a dust output isn't created.\n\tif localFundingAmt < funding.MinChanFundingSize {\n\t\treturn nil, fmt.Errorf(\"channel is too small, the minimum \"+\n\t\t\t\"channel size is: %v SAT\", int64(funding.MinChanFundingSize))\n\t}\n\n\t// Prevent users from submitting a max-htlc value that would exceed the\n\t// protocol maximum.\n\tif maxHtlcs > input.MaxHTLCNumber/2 {\n\t\treturn nil, fmt.Errorf(\"remote-max-htlcs (%v) cannot be \"+\n\t\t\t\"greater than %v\", maxHtlcs, input.MaxHTLCNumber/2)\n\t}\n\n\t// Then, we'll extract the minimum number of confirmations that each\n\t// output we use to fund the channel's funding transaction should\n\t// satisfy.\n\tminConfs, err := lnrpc.ExtractMinConfs(in.MinConfs, in.SpendUnconfirmed)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// TODO(roasbeef): also return channel ID?\n\n\tvar nodePubKey *btcec.PublicKey\n\n\t// Parse the remote pubkey the NodePubkey field of the request. If it's\n\t// not present, we'll fallback to the deprecated version that parses the\n\t// key from a hex string if this is for REST for backwards compatibility.\n\tswitch {\n\t// Parse the raw bytes of the node key into a pubkey object so we can\n\t// easily manipulate it.\n\tcase len(in.NodePubkey) > 0:\n\t\tnodePubKey, err = btcec.ParsePubKey(in.NodePubkey)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t// Decode the provided target node's public key, parsing it into a pub\n\t// key object. For all sync call, byte slices are expected to be encoded\n\t// as hex strings.\n\tcase isSync:\n\t\tkeyBytes, err := hex.DecodeString(in.NodePubkeyString) // nolint:staticcheck\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tnodePubKey, err = btcec.ParsePubKey(keyBytes)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"NodePubkey is not set\")\n\t}\n\n\t// Making a channel to ourselves wouldn't be of any use, so we\n\t// explicitly disallow them.\n\tif nodePubKey.IsEqual(r.server.identityECDH.PubKey()) {\n\t\treturn nil, fmt.Errorf(\"cannot open channel to self\")\n\t}\n\n\t// Calculate an appropriate fee rate for this transaction.\n\tfeeRate, err := calculateFeeRate(\n\t\tuint64(in.SatPerByte), in.SatPerVbyte, // nolint:staticcheck\n\t\tuint32(in.TargetConf), r.server.cc.FeeEstimator,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trpcsLog.Debugf(\"[openchannel]: using fee of %v sat/kw for funding tx\",\n\t\tint64(feeRate))\n\n\tscript, err := chancloser.ParseUpfrontShutdownAddress(\n\t\tin.CloseAddress, r.cfg.ActiveNetParams.Params,\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error parsing upfront shutdown: %v\",\n\t\t\terr)\n\t}\n\n\tvar channelType *lnwire.ChannelType\n\tswitch in.CommitmentType {\n\tcase lnrpc.CommitmentType_UNKNOWN_COMMITMENT_TYPE:\n\t\tif in.ZeroConf {\n\t\t\treturn nil, fmt.Errorf(\"use anchors for zero-conf\")\n\t\t}\n\n\tcase lnrpc.CommitmentType_LEGACY:\n\t\tchannelType = new(lnwire.ChannelType)\n\t\t*channelType = lnwire.ChannelType(*lnwire.NewRawFeatureVector())\n\n\tcase lnrpc.CommitmentType_STATIC_REMOTE_KEY:\n\t\tchannelType = new(lnwire.ChannelType)\n\t\t*channelType = lnwire.ChannelType(*lnwire.NewRawFeatureVector(\n\t\t\tlnwire.StaticRemoteKeyRequired,\n\t\t))\n\n\tcase lnrpc.CommitmentType_ANCHORS:\n\t\tchannelType = new(lnwire.ChannelType)\n\t\tfv := lnwire.NewRawFeatureVector(\n\t\t\tlnwire.StaticRemoteKeyRequired,\n\t\t\tlnwire.AnchorsZeroFeeHtlcTxRequired,\n\t\t)\n\n\t\tif in.ZeroConf {\n\t\t\tfv.Set(lnwire.ZeroConfRequired)\n\t\t}\n\n\t\tif in.ScidAlias {\n\t\t\tfv.Set(lnwire.ScidAliasRequired)\n\t\t}\n\n\t\t*channelType = lnwire.ChannelType(*fv)\n\n\tcase lnrpc.CommitmentType_SCRIPT_ENFORCED_LEASE:\n\t\tchannelType = new(lnwire.ChannelType)\n\t\tfv := lnwire.NewRawFeatureVector(\n\t\t\tlnwire.StaticRemoteKeyRequired,\n\t\t\tlnwire.AnchorsZeroFeeHtlcTxRequired,\n\t\t\tlnwire.ScriptEnforcedLeaseRequired,\n\t\t)\n\n\t\tif in.ZeroConf {\n\t\t\tfv.Set(lnwire.ZeroConfRequired)\n\t\t}\n\n\t\tif in.ScidAlias {\n\t\t\tfv.Set(lnwire.ScidAliasRequired)\n\t\t}\n\n\t\t*channelType = lnwire.ChannelType(*fv)\n\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unhandled request channel type %v\",\n\t\t\tin.CommitmentType)\n\t}\n\n\t// Instruct the server to trigger the necessary events to attempt to\n\t// open a new channel. A stream is returned in place, this stream will\n\t// be used to consume updates of the state of the pending channel.\n\treturn &funding.InitFundingMsg{\n\t\tTargetPubkey:      nodePubKey,\n\t\tChainHash:         *r.cfg.ActiveNetParams.GenesisHash,\n\t\tLocalFundingAmt:   localFundingAmt,\n\t\tBaseFee:           channelBaseFee,\n\t\tFeeRate:           channelFeeRate,\n\t\tPushAmt:           lnwire.NewMSatFromSatoshis(remoteInitialBalance),\n\t\tMinHtlcIn:         minHtlcIn,\n\t\tFundingFeePerKw:   feeRate,\n\t\tPrivate:           in.Private,\n\t\tRemoteCsvDelay:    remoteCsvDelay,\n\t\tRemoteChanReserve: remoteChanReserve,\n\t\tMinConfs:          minConfs,\n\t\tShutdownScript:    script,\n\t\tMaxValueInFlight:  maxValue,\n\t\tMaxHtlcs:          maxHtlcs,\n\t\tMaxLocalCsv:       uint16(in.MaxLocalCsv),\n\t\tChannelType:       channelType,\n\t}, nil\n}\n\n// OpenChannel attempts to open a singly funded channel specified in the\n// request to a remote peer.",
      "length": 6833,
      "tokens": 808,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) OpenChannel(in *lnrpc.OpenChannelRequest,",
      "content": "func (r *rpcServer) OpenChannel(in *lnrpc.OpenChannelRequest,\n\tupdateStream lnrpc.Lightning_OpenChannelServer) error {\n\n\tif err := r.canOpenChannel(); err != nil {\n\t\treturn err\n\t}\n\n\treq, err := r.parseOpenChannelReq(in, false)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// If the user has provided a shim, then we'll now augment the based\n\t// open channel request with this additional logic.\n\tif in.FundingShim != nil {\n\t\tswitch {\n\t\t// If we have a chan point shim, then this means the funding\n\t\t// transaction was crafted externally. In this case we only\n\t\t// need to hand a channel point down into the wallet.\n\t\tcase in.FundingShim.GetChanPointShim() != nil:\n\t\t\tchanPointShim := in.FundingShim.GetChanPointShim()\n\n\t\t\t// Map the channel point shim into a new\n\t\t\t// chanfunding.CannedAssembler that the wallet will use\n\t\t\t// to obtain the channel point details.\n\t\t\tcopy(req.PendingChanID[:], chanPointShim.PendingChanId)\n\t\t\treq.ChanFunder, err = newFundingShimAssembler(\n\t\t\t\tchanPointShim, true, r.server.cc.KeyRing,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t// If we have a PSBT shim, then this means the funding\n\t\t// transaction will be crafted outside of the wallet, once the\n\t\t// funding multisig output script is known. We'll create an\n\t\t// intent that will supervise the multi-step process.\n\t\tcase in.FundingShim.GetPsbtShim() != nil:\n\t\t\tpsbtShim := in.FundingShim.GetPsbtShim()\n\n\t\t\t// Instruct the wallet to use the new\n\t\t\t// chanfunding.PsbtAssembler to construct the funding\n\t\t\t// transaction.\n\t\t\tcopy(req.PendingChanID[:], psbtShim.PendingChanId)\n\t\t\treq.ChanFunder, err = newPsbtAssembler(\n\t\t\t\tin, req.MinConfs, psbtShim,\n\t\t\t\t&r.server.cc.Wallet.Cfg.NetParams,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\tupdateChan, errChan := r.server.OpenChannel(req)\n\n\tvar outpoint wire.OutPoint\nout:\n\tfor {\n\t\tselect {\n\t\tcase err := <-errChan:\n\t\t\trpcsLog.Errorf(\"unable to open channel to NodeKey(%x): %v\",\n\t\t\t\treq.TargetPubkey.SerializeCompressed(), err)\n\t\t\treturn err\n\t\tcase fundingUpdate := <-updateChan:\n\t\t\trpcsLog.Tracef(\"[openchannel] sending update: %v\",\n\t\t\t\tfundingUpdate)\n\t\t\tif err := updateStream.Send(fundingUpdate); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t// If a final channel open update is being sent, then\n\t\t\t// we can break out of our recv loop as we no longer\n\t\t\t// need to process any further updates.\n\t\t\tupdate, ok := fundingUpdate.Update.(*lnrpc.OpenStatusUpdate_ChanOpen)\n\t\t\tif ok {\n\t\t\t\tchanPoint := update.ChanOpen.ChannelPoint\n\t\t\t\ttxid, err := lnrpc.GetChanPointFundingTxid(chanPoint)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\toutpoint = wire.OutPoint{\n\t\t\t\t\tHash:  *txid,\n\t\t\t\t\tIndex: chanPoint.OutputIndex,\n\t\t\t\t}\n\n\t\t\t\tbreak out\n\t\t\t}\n\t\tcase <-r.quit:\n\t\t\treturn nil\n\t\t}\n\t}\n\n\trpcsLog.Tracef(\"[openchannel] success NodeKey(%x), ChannelPoint(%v)\",\n\t\treq.TargetPubkey.SerializeCompressed(), outpoint)\n\treturn nil\n}\n\n// OpenChannelSync is a synchronous version of the OpenChannel RPC call. This\n// call is meant to be consumed by clients to the REST proxy. As with all other\n// sync calls, all byte slices are instead to be populated as hex encoded\n// strings.",
      "length": 2919,
      "tokens": 395,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) OpenChannelSync(ctx context.Context,",
      "content": "func (r *rpcServer) OpenChannelSync(ctx context.Context,\n\tin *lnrpc.OpenChannelRequest) (*lnrpc.ChannelPoint, error) {\n\n\tif err := r.canOpenChannel(); err != nil {\n\t\treturn nil, err\n\t}\n\n\treq, err := r.parseOpenChannelReq(in, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tupdateChan, errChan := r.server.OpenChannel(req)\n\tselect {\n\t// If an error occurs them immediately return the error to the client.\n\tcase err := <-errChan:\n\t\trpcsLog.Errorf(\"unable to open channel to NodeKey(%x): %v\",\n\t\t\treq.TargetPubkey.SerializeCompressed(), err)\n\t\treturn nil, err\n\n\t// Otherwise, wait for the first channel update. The first update sent\n\t// is when the funding transaction is broadcast to the network.\n\tcase fundingUpdate := <-updateChan:\n\t\trpcsLog.Tracef(\"[openchannel] sending update: %v\",\n\t\t\tfundingUpdate)\n\n\t\t// Parse out the txid of the pending funding transaction. The\n\t\t// sync client can use this to poll against the list of\n\t\t// PendingChannels.\n\t\topenUpdate := fundingUpdate.Update.(*lnrpc.OpenStatusUpdate_ChanPending)\n\t\tchanUpdate := openUpdate.ChanPending\n\n\t\treturn &lnrpc.ChannelPoint{\n\t\t\tFundingTxid: &lnrpc.ChannelPoint_FundingTxidBytes{\n\t\t\t\tFundingTxidBytes: chanUpdate.Txid,\n\t\t\t},\n\t\t\tOutputIndex: chanUpdate.OutputIndex,\n\t\t}, nil\n\tcase <-r.quit:\n\t\treturn nil, nil\n\t}\n}\n\n// BatchOpenChannel attempts to open multiple single-funded channels in a\n// single transaction in an atomic way. This means either all channel open\n// requests succeed at once or all attempts are aborted if any of them fail.\n// This is the safer variant of using PSBTs to manually fund a batch of\n// channels through the OpenChannel RPC.",
      "length": 1513,
      "tokens": 206,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) BatchOpenChannel(ctx context.Context,",
      "content": "func (r *rpcServer) BatchOpenChannel(ctx context.Context,\n\tin *lnrpc.BatchOpenChannelRequest) (*lnrpc.BatchOpenChannelResponse,\n\terror) {\n\n\tif err := r.canOpenChannel(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// We need the wallet kit server to do the heavy lifting on the PSBT\n\t// part. If we didn't rely on re-using the wallet kit server's logic we\n\t// would need to re-implement everything here. Since we deliver lnd with\n\t// the wallet kit server enabled by default we can assume it's okay to\n\t// make this functionality dependent on that server being active.\n\tvar walletKitServer walletrpc.WalletKitServer\n\tfor _, subServer := range r.subServers {\n\t\tif subServer.Name() == walletrpc.SubServerName {\n\t\t\twalletKitServer = subServer.(walletrpc.WalletKitServer)\n\t\t}\n\t}\n\tif walletKitServer == nil {\n\t\treturn nil, fmt.Errorf(\"batch channel open is only possible \" +\n\t\t\t\"if walletrpc subserver is active\")\n\t}\n\n\trpcsLog.Debugf(\"[batchopenchannel] request to open batch of %d \"+\n\t\t\"channels\", len(in.Channels))\n\n\t// Make sure there is at least one channel to open. We could say we want\n\t// at least two channels for a batch. But maybe it's nice if developers\n\t// can use the same API for a single channel as well as a batch of\n\t// channels.\n\tif len(in.Channels) == 0 {\n\t\treturn nil, fmt.Errorf(\"specify at least one channel\")\n\t}\n\n\t// In case we remove a pending channel from the database, we need to set\n\t// a close height, so we'll just use the current best known height.\n\t_, bestHeight, err := r.server.cc.ChainIO.GetBestBlock()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error fetching best block: %v\", err)\n\t}\n\n\t// So far everything looks good and we can now start the heavy lifting\n\t// that's done in the funding package.\n\trequestParser := func(req *lnrpc.OpenChannelRequest) (\n\t\t*funding.InitFundingMsg, error) {\n\n\t\treturn r.parseOpenChannelReq(req, false)\n\t}\n\tchannelAbandoner := func(point *wire.OutPoint) error {\n\t\treturn r.abandonChan(point, uint32(bestHeight))\n\t}\n\tbatcher := funding.NewBatcher(&funding.BatchConfig{\n\t\tRequestParser:    requestParser,\n\t\tChannelAbandoner: channelAbandoner,\n\t\tChannelOpener:    r.server.OpenChannel,\n\t\tWalletKitServer:  walletKitServer,\n\t\tWallet:           r.server.cc.Wallet,\n\t\tNetParams:        &r.server.cc.Wallet.Cfg.NetParams,\n\t\tQuit:             r.quit,\n\t})\n\trpcPoints, err := batcher.BatchFund(ctx, in)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"batch funding failed: %v\", err)\n\t}\n\n\t// Now all that's left to do is send back the response with the channel\n\t// points we created.\n\treturn &lnrpc.BatchOpenChannelResponse{\n\t\tPendingChannels: rpcPoints,\n\t}, nil\n}\n\n// CloseChannel attempts to close an active channel identified by its channel\n// point. The actions of this method can additionally be augmented to attempt\n// a force close after a timeout period in the case of an inactive peer.",
      "length": 2699,
      "tokens": 386,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) CloseChannel(in *lnrpc.CloseChannelRequest,",
      "content": "func (r *rpcServer) CloseChannel(in *lnrpc.CloseChannelRequest,\n\tupdateStream lnrpc.Lightning_CloseChannelServer) error {\n\n\tif !r.server.Started() {\n\t\treturn ErrServerNotActive\n\t}\n\n\t// If the user didn't specify a channel point, then we'll reject this\n\t// request all together.\n\tif in.GetChannelPoint() == nil {\n\t\treturn fmt.Errorf(\"must specify channel point in close channel\")\n\t}\n\n\t// If force closing a channel, the fee set in the commitment transaction\n\t// is used.\n\tif in.Force && (in.SatPerByte != 0 || in.SatPerVbyte != 0 || // nolint:staticcheck\n\t\tin.TargetConf != 0) {\n\n\t\treturn fmt.Errorf(\"force closing a channel uses a pre-defined fee\")\n\t}\n\n\tforce := in.Force\n\tindex := in.ChannelPoint.OutputIndex\n\ttxid, err := lnrpc.GetChanPointFundingTxid(in.GetChannelPoint())\n\tif err != nil {\n\t\trpcsLog.Errorf(\"[closechannel] unable to get funding txid: %v\", err)\n\t\treturn err\n\t}\n\tchanPoint := wire.NewOutPoint(txid, index)\n\n\trpcsLog.Tracef(\"[closechannel] request for ChannelPoint(%v), force=%v\",\n\t\tchanPoint, force)\n\n\tvar (\n\t\tupdateChan chan interface{}\n\t\terrChan    chan error\n\t)\n\n\t// TODO(roasbeef): if force and peer online then don't force?\n\n\t// First, we'll fetch the channel as is, as we'll need to examine it\n\t// regardless of if this is a force close or not.\n\tchannel, err := r.server.chanStateDB.FetchChannel(nil, *chanPoint)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// We can't coop or force close restored channels or channels that have\n\t// experienced local data loss. Normally we would detect this in the\n\t// channel arbitrator if the channel has the status\n\t// ChanStatusLocalDataLoss after connecting to its peer. But if no\n\t// connection can be established, the channel arbitrator doesn't know it\n\t// can't be force closed yet.\n\tif channel.HasChanStatus(channeldb.ChanStatusRestored) ||\n\t\tchannel.HasChanStatus(channeldb.ChanStatusLocalDataLoss) {\n\n\t\treturn fmt.Errorf(\"cannot close channel with state: %v\",\n\t\t\tchannel.ChanStatus())\n\t}\n\n\t// Retrieve the best height of the chain, which we'll use to complete\n\t// either closing flow.\n\t_, bestHeight, err := r.server.cc.ChainIO.GetBestBlock()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// If a force closure was requested, then we'll handle all the details\n\t// around the creation and broadcast of the unilateral closure\n\t// transaction here rather than going to the switch as we don't require\n\t// interaction from the peer.\n\tif force {\n\n\t\t// As we're force closing this channel, as a precaution, we'll\n\t\t// ensure that the switch doesn't continue to see this channel\n\t\t// as eligible for forwarding HTLC's. If the peer is online,\n\t\t// then we'll also purge all of its indexes.\n\t\tremotePub := channel.IdentityPub\n\t\tif peer, err := r.server.FindPeer(remotePub); err == nil {\n\t\t\t// TODO(roasbeef): actually get the active channel\n\t\t\t// instead too?\n\t\t\t//  * so only need to grab from database\n\t\t\tpeer.WipeChannel(&channel.FundingOutpoint)\n\t\t} else {\n\t\t\tchanID := lnwire.NewChanIDFromOutPoint(&channel.FundingOutpoint)\n\t\t\tr.server.htlcSwitch.RemoveLink(chanID)\n\t\t}\n\n\t\t// With the necessary indexes cleaned up, we'll now force close\n\t\t// the channel.\n\t\tchainArbitrator := r.server.chainArb\n\t\tclosingTx, err := chainArbitrator.ForceCloseContract(\n\t\t\t*chanPoint,\n\t\t)\n\t\tif err != nil {\n\t\t\trpcsLog.Errorf(\"unable to force close transaction: %v\", err)\n\t\t\treturn err\n\t\t}\n\n\t\tclosingTxid := closingTx.TxHash()\n\n\t\t// With the transaction broadcast, we send our first update to\n\t\t// the client.\n\t\tupdateChan = make(chan interface{}, 2)\n\t\tupdateChan <- &peer.PendingUpdate{\n\t\t\tTxid: closingTxid[:],\n\t\t}\n\n\t\terrChan = make(chan error, 1)\n\t\tnotifier := r.server.cc.ChainNotifier\n\t\tgo peer.WaitForChanToClose(uint32(bestHeight), notifier, errChan, chanPoint,\n\t\t\t&closingTxid, closingTx.TxOut[0].PkScript, func() {\n\t\t\t\t// Respond to the local subsystem which\n\t\t\t\t// requested the channel closure.\n\t\t\t\tupdateChan <- &peer.ChannelCloseUpdate{\n\t\t\t\t\tClosingTxid: closingTxid[:],\n\t\t\t\t\tSuccess:     true,\n\t\t\t\t}\n\t\t\t})\n\t} else {\n\t\t// If this is a frozen channel, then we only allow the co-op\n\t\t// close to proceed if we were the responder to this channel if\n\t\t// the absolute thaw height has not been met.\n\t\tif channel.IsInitiator {\n\t\t\tabsoluteThawHeight, err := channel.AbsoluteThawHeight()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif uint32(bestHeight) < absoluteThawHeight {\n\t\t\t\treturn fmt.Errorf(\"cannot co-op close frozen \"+\n\t\t\t\t\t\"channel as initiator until height=%v, \"+\n\t\t\t\t\t\"(current_height=%v)\",\n\t\t\t\t\tabsoluteThawHeight, bestHeight)\n\t\t\t}\n\t\t}\n\n\t\t// If the link is not known by the switch, we cannot gracefully close\n\t\t// the channel.\n\t\tchannelID := lnwire.NewChanIDFromOutPoint(chanPoint)\n\t\tif _, err := r.server.htlcSwitch.GetLink(channelID); err != nil {\n\t\t\trpcsLog.Debugf(\"Trying to non-force close offline channel with \"+\n\t\t\t\t\"chan_point=%v\", chanPoint)\n\t\t\treturn fmt.Errorf(\"unable to gracefully close channel while peer \"+\n\t\t\t\t\"is offline (try force closing it instead): %v\", err)\n\t\t}\n\n\t\t// Based on the passed fee related parameters, we'll determine\n\t\t// an appropriate fee rate for the cooperative closure\n\t\t// transaction.\n\t\tfeeRate, err := calculateFeeRate(\n\t\t\tuint64(in.SatPerByte), in.SatPerVbyte, // nolint:staticcheck\n\t\t\tuint32(in.TargetConf), r.server.cc.FeeEstimator,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\trpcsLog.Debugf(\"Target sat/kw for closing transaction: %v\",\n\t\t\tint64(feeRate))\n\n\t\t// Before we attempt the cooperative channel closure, we'll\n\t\t// examine the channel to ensure that it doesn't have a\n\t\t// lingering HTLC.\n\t\tif len(channel.ActiveHtlcs()) != 0 {\n\t\t\treturn fmt.Errorf(\"cannot co-op close channel \" +\n\t\t\t\t\"with active htlcs\")\n\t\t}\n\n\t\t// Otherwise, the caller has requested a regular interactive\n\t\t// cooperative channel closure. So we'll forward the request to\n\t\t// the htlc switch which will handle the negotiation and\n\t\t// broadcast details.\n\n\t\tvar deliveryScript lnwire.DeliveryAddress\n\n\t\t// If a delivery address to close out to was specified, decode it.\n\t\tif len(in.DeliveryAddress) > 0 {\n\t\t\t// Decode the address provided.\n\t\t\taddr, err := btcutil.DecodeAddress(\n\t\t\t\tin.DeliveryAddress, r.cfg.ActiveNetParams.Params,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"invalid delivery address: %v\", err)\n\t\t\t}\n\n\t\t\t// Create a script to pay out to the address provided.\n\t\t\tdeliveryScript, err = txscript.PayToAddrScript(addr)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\tmaxFee := chainfee.SatPerKVByte(\n\t\t\tin.MaxFeePerVbyte * 1000,\n\t\t).FeePerKWeight()\n\t\tupdateChan, errChan = r.server.htlcSwitch.CloseLink(\n\t\t\tchanPoint, contractcourt.CloseRegular, feeRate,\n\t\t\tmaxFee, deliveryScript,\n\t\t)\n\t}\nout:\n\tfor {\n\t\tselect {\n\t\tcase err := <-errChan:\n\t\t\trpcsLog.Errorf(\"[closechannel] unable to close \"+\n\t\t\t\t\"ChannelPoint(%v): %v\", chanPoint, err)\n\t\t\treturn err\n\t\tcase closingUpdate := <-updateChan:\n\t\t\trpcClosingUpdate, err := createRPCCloseUpdate(\n\t\t\t\tclosingUpdate,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\trpcsLog.Tracef(\"[closechannel] sending update: %v\",\n\t\t\t\trpcClosingUpdate)\n\n\t\t\tif err := updateStream.Send(rpcClosingUpdate); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t// If a final channel closing updates is being sent,\n\t\t\t// then we can break out of our dispatch loop as we no\n\t\t\t// longer need to process any further updates.\n\t\t\tswitch closeUpdate := closingUpdate.(type) {\n\t\t\tcase *peer.ChannelCloseUpdate:\n\t\t\t\th, _ := chainhash.NewHash(closeUpdate.ClosingTxid)\n\t\t\t\trpcsLog.Infof(\"[closechannel] close completed: \"+\n\t\t\t\t\t\"txid(%v)\", h)\n\t\t\t\tbreak out\n\t\t\t}\n\t\tcase <-r.quit:\n\t\t\treturn nil\n\t\t}\n\t}\n\n\treturn nil\n}\n",
      "length": 7182,
      "tokens": 965,
      "embedding": []
    },
    {
      "slug": "func createRPCCloseUpdate(update interface{}) (",
      "content": "func createRPCCloseUpdate(update interface{}) (\n\t*lnrpc.CloseStatusUpdate, error) {\n\n\tswitch u := update.(type) {\n\tcase *peer.ChannelCloseUpdate:\n\t\treturn &lnrpc.CloseStatusUpdate{\n\t\t\tUpdate: &lnrpc.CloseStatusUpdate_ChanClose{\n\t\t\t\tChanClose: &lnrpc.ChannelCloseUpdate{\n\t\t\t\t\tClosingTxid: u.ClosingTxid,\n\t\t\t\t\tSuccess:     u.Success,\n\t\t\t\t},\n\t\t\t},\n\t\t}, nil\n\tcase *peer.PendingUpdate:\n\t\treturn &lnrpc.CloseStatusUpdate{\n\t\t\tUpdate: &lnrpc.CloseStatusUpdate_ClosePending{\n\t\t\t\tClosePending: &lnrpc.PendingUpdate{\n\t\t\t\t\tTxid:        u.Txid,\n\t\t\t\t\tOutputIndex: u.OutputIndex,\n\t\t\t\t},\n\t\t\t},\n\t\t}, nil\n\t}\n\n\treturn nil, errors.New(\"unknown close status update\")\n}\n\n// abandonChanFromGraph attempts to remove a channel from the channel graph. If\n// we can't find the chanID in the graph, then we assume it has already been\n// removed, and will return a nop.",
      "length": 764,
      "tokens": 83,
      "embedding": []
    },
    {
      "slug": "func abandonChanFromGraph(chanGraph *channeldb.ChannelGraph,",
      "content": "func abandonChanFromGraph(chanGraph *channeldb.ChannelGraph,\n\tchanPoint *wire.OutPoint) error {\n\n\t// First, we'll obtain the channel ID. If we can't locate this, then\n\t// it's the case that the channel may have already been removed from\n\t// the graph, so we'll return a nil error.\n\tchanID, err := chanGraph.ChannelID(chanPoint)\n\tswitch {\n\tcase err == channeldb.ErrEdgeNotFound:\n\t\treturn nil\n\tcase err != nil:\n\t\treturn err\n\t}\n\n\t// If the channel ID is still in the graph, then that means the channel\n\t// is still open, so we'll now move to purge it from the graph.\n\treturn chanGraph.DeleteChannelEdges(false, true, chanID)\n}\n\n// abandonChan removes a channel from the database, graph and contract court.",
      "length": 623,
      "tokens": 104,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) abandonChan(chanPoint *wire.OutPoint,",
      "content": "func (r *rpcServer) abandonChan(chanPoint *wire.OutPoint,\n\tbestHeight uint32) error {\n\n\t// Abandoning a channel is a three-step process: remove from the open\n\t// channel state, remove from the graph, remove from the contract\n\t// court. Between any step it's possible that the users restarts the\n\t// process all over again. As a result, each of the steps below are\n\t// intended to be idempotent.\n\terr := r.server.chanStateDB.AbandonChannel(chanPoint, bestHeight)\n\tif err != nil {\n\t\treturn err\n\t}\n\terr = abandonChanFromGraph(r.server.graphDB, chanPoint)\n\tif err != nil {\n\t\treturn err\n\t}\n\terr = r.server.chainArb.ResolveContract(*chanPoint)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// If this channel was in the process of being closed, but didn't fully\n\t// close, then it's possible that the nursery is hanging on to some\n\t// state. To err on the side of caution, we'll now attempt to wipe any\n\t// state for this channel from the nursery.\n\terr = r.server.utxoNursery.RemoveChannel(chanPoint)\n\tif err != nil && err != contractcourt.ErrContractNotFound {\n\t\treturn err\n\t}\n\n\t// Finally, notify the backup listeners that the channel can be removed\n\t// from any channel backups.\n\tr.server.channelNotifier.NotifyClosedChannelEvent(*chanPoint)\n\n\treturn nil\n}\n\n// AbandonChannel removes all channel state from the database except for a\n// close summary. This method can be used to get rid of permanently unusable\n// channels due to bugs fixed in newer versions of lnd.",
      "length": 1355,
      "tokens": 216,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) AbandonChannel(_ context.Context,",
      "content": "func (r *rpcServer) AbandonChannel(_ context.Context,\n\tin *lnrpc.AbandonChannelRequest) (*lnrpc.AbandonChannelResponse, error) {\n\n\t// If this isn't the dev build, then we won't allow the RPC to be\n\t// executed, as it's an advanced feature and won't be activated in\n\t// regular production/release builds except for the explicit case of\n\t// externally funded channels that are still pending. Due to repeated\n\t// requests, we also allow this requirement to be overwritten by a new\n\t// flag that attests to the user knowing what they're doing and the risk\n\t// associated with the command/RPC.\n\tif !in.IKnowWhatIAmDoing && !in.PendingFundingShimOnly &&\n\t\t!build.IsDevBuild() {\n\n\t\treturn nil, fmt.Errorf(\"AbandonChannel RPC call only \" +\n\t\t\t\"available in dev builds\")\n\t}\n\n\t// We'll parse out the arguments to we can obtain the chanPoint of the\n\t// target channel.\n\ttxid, err := lnrpc.GetChanPointFundingTxid(in.GetChannelPoint())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tindex := in.ChannelPoint.OutputIndex\n\tchanPoint := wire.NewOutPoint(txid, index)\n\n\t// When we remove the channel from the database, we need to set a close\n\t// height, so we'll just use the current best known height.\n\t_, bestHeight, err := r.server.cc.ChainIO.GetBestBlock()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tdbChan, err := r.server.chanStateDB.FetchChannel(nil, *chanPoint)\n\tswitch {\n\t// If the channel isn't found in the set of open channels, then we can\n\t// continue on as it can't be loaded into the link/peer.\n\tcase err == channeldb.ErrChannelNotFound:\n\t\tbreak\n\n\t// If the channel is still known to be open, then before we modify any\n\t// on-disk state, we'll remove the channel from the switch and peer\n\t// state if it's been loaded in.\n\tcase err == nil:\n\t\t// If the user requested the more safe version that only allows\n\t\t// the removal of externally (shim) funded channels that are\n\t\t// still pending, we enforce this option now that we know the\n\t\t// state of the channel.\n\t\t//\n\t\t// TODO(guggero): Properly store the funding type (wallet, shim,\n\t\t// PSBT) on the channel so we don't need to use the thaw height.\n\t\tisShimFunded := dbChan.ThawHeight > 0\n\t\tisPendingShimFunded := isShimFunded && dbChan.IsPending\n\t\tif !in.IKnowWhatIAmDoing && in.PendingFundingShimOnly &&\n\t\t\t!isPendingShimFunded {\n\n\t\t\treturn nil, fmt.Errorf(\"channel %v is not externally \"+\n\t\t\t\t\"funded or not pending\", chanPoint)\n\t\t}\n\n\t\t// We'll mark the channel as borked before we remove the state\n\t\t// from the switch/peer so it won't be loaded back in if the\n\t\t// peer reconnects.\n\t\tif err := dbChan.MarkBorked(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tremotePub := dbChan.IdentityPub\n\t\tif peer, err := r.server.FindPeer(remotePub); err == nil {\n\t\t\tpeer.WipeChannel(chanPoint)\n\t\t}\n\n\tdefault:\n\t\treturn nil, err\n\t}\n\n\t// Remove the channel from the graph, database and contract court.\n\tif err := r.abandonChan(chanPoint, uint32(bestHeight)); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &lnrpc.AbandonChannelResponse{}, nil\n}\n\n// GetInfo returns general information concerning the lightning node including\n// its identity pubkey, alias, the chains it is connected to, and information\n// concerning the number of open+pending channels.",
      "length": 3034,
      "tokens": 469,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) GetInfo(_ context.Context,",
      "content": "func (r *rpcServer) GetInfo(_ context.Context,\n\t_ *lnrpc.GetInfoRequest) (*lnrpc.GetInfoResponse, error) {\n\n\tserverPeers := r.server.Peers()\n\n\topenChannels, err := r.server.chanStateDB.FetchAllOpenChannels()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar activeChannels uint32\n\tfor _, channel := range openChannels {\n\t\tchanID := lnwire.NewChanIDFromOutPoint(&channel.FundingOutpoint)\n\t\tif r.server.htlcSwitch.HasActiveLink(chanID) {\n\t\t\tactiveChannels++\n\t\t}\n\t}\n\n\tinactiveChannels := uint32(len(openChannels)) - activeChannels\n\n\tpendingChannels, err := r.server.chanStateDB.FetchPendingChannels()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to get retrieve pending \"+\n\t\t\t\"channels: %v\", err)\n\t}\n\tnPendingChannels := uint32(len(pendingChannels))\n\n\tidPub := r.server.identityECDH.PubKey().SerializeCompressed()\n\tencodedIDPub := hex.EncodeToString(idPub)\n\n\tbestHash, bestHeight, err := r.server.cc.ChainIO.GetBestBlock()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to get best block info: %v\", err)\n\t}\n\n\tisSynced, bestHeaderTimestamp, err := r.server.cc.Wallet.IsSynced()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to sync PoV of the wallet \"+\n\t\t\t\"with current best block in the main chain: %v\", err)\n\t}\n\n\t// If the router does full channel validation, it has a lot of work to\n\t// do for each block. So it might be possible that it isn't yet up to\n\t// date with the most recent block, even if the wallet is. This can\n\t// happen in environments with high CPU load (such as parallel itests).\n\t// Since the `synced_to_chain` flag in the response of this call is used\n\t// by many wallets (and also our itests) to make sure everything's up to\n\t// date, we add the router's state to it. So the flag will only toggle\n\t// to true once the router was also able to catch up.\n\tif !r.cfg.Routing.AssumeChannelValid {\n\t\trouterHeight := r.server.chanRouter.SyncedHeight()\n\t\tisSynced = isSynced && uint32(bestHeight) == routerHeight\n\t}\n\n\tnetwork := lncfg.NormalizeNetwork(r.cfg.ActiveNetParams.Name)\n\tactiveChains := make([]*lnrpc.Chain, r.cfg.registeredChains.NumActiveChains())\n\tfor i, chain := range r.cfg.registeredChains.ActiveChains() {\n\t\tactiveChains[i] = &lnrpc.Chain{\n\t\t\tChain:   chain.String(),\n\t\t\tNetwork: network,\n\t\t}\n\t}\n\n\t// Check if external IP addresses were provided to lnd and use them\n\t// to set the URIs.\n\tnodeAnn, err := r.server.genNodeAnnouncement(false)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to retrieve current fully signed \"+\n\t\t\t\"node announcement: %v\", err)\n\t}\n\taddrs := nodeAnn.Addresses\n\turis := make([]string, len(addrs))\n\tfor i, addr := range addrs {\n\t\turis[i] = fmt.Sprintf(\"%s@%s\", encodedIDPub, addr.String())\n\t}\n\n\tisGraphSynced := r.server.authGossiper.SyncManager().IsGraphSynced()\n\n\tfeatures := make(map[uint32]*lnrpc.Feature)\n\tsets := r.server.featureMgr.ListSets()\n\n\tfor _, set := range sets {\n\t\t// Get the a list of lnrpc features for each set we support.\n\t\tfeatureVector := r.server.featureMgr.Get(set)\n\t\trpcFeatures := invoicesrpc.CreateRPCFeatures(featureVector)\n\n\t\t// Add the features to our map of features, allowing over writing of\n\t\t// existing values because features in different sets with the same bit\n\t\t// are duplicated across sets.\n\t\tfor bit, feature := range rpcFeatures {\n\t\t\tfeatures[bit] = feature\n\t\t}\n\t}\n\n\t// TODO(roasbeef): add synced height n stuff\n\n\tisTestNet := chainreg.IsTestnet(&r.cfg.ActiveNetParams)\n\tnodeColor := routing.EncodeHexColor(nodeAnn.RGBColor)\n\tversion := build.Version() + \" commit=\" + build.Commit\n\n\treturn &lnrpc.GetInfoResponse{\n\t\tIdentityPubkey:            encodedIDPub,\n\t\tNumPendingChannels:        nPendingChannels,\n\t\tNumActiveChannels:         activeChannels,\n\t\tNumInactiveChannels:       inactiveChannels,\n\t\tNumPeers:                  uint32(len(serverPeers)),\n\t\tBlockHeight:               uint32(bestHeight),\n\t\tBlockHash:                 bestHash.String(),\n\t\tSyncedToChain:             isSynced,\n\t\tTestnet:                   isTestNet,\n\t\tChains:                    activeChains,\n\t\tUris:                      uris,\n\t\tAlias:                     nodeAnn.Alias.String(),\n\t\tColor:                     nodeColor,\n\t\tBestHeaderTimestamp:       bestHeaderTimestamp,\n\t\tVersion:                   version,\n\t\tCommitHash:                build.CommitHash,\n\t\tSyncedToGraph:             isGraphSynced,\n\t\tFeatures:                  features,\n\t\tRequireHtlcInterceptor:    r.cfg.RequireInterceptor,\n\t\tStoreFinalHtlcResolutions: r.cfg.StoreFinalHtlcResolutions,\n\t}, nil\n}\n\n// GetRecoveryInfo returns a boolean indicating whether the wallet is started\n// in recovery mode, whether the recovery is finished, and the progress made\n// so far.",
      "length": 4440,
      "tokens": 506,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) GetRecoveryInfo(ctx context.Context,",
      "content": "func (r *rpcServer) GetRecoveryInfo(ctx context.Context,\n\tin *lnrpc.GetRecoveryInfoRequest) (*lnrpc.GetRecoveryInfoResponse, error) {\n\n\tisRecoveryMode, progress, err := r.server.cc.Wallet.GetRecoveryInfo()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to get wallet recovery info: %v\", err)\n\t}\n\n\trpcsLog.Debugf(\"[getrecoveryinfo] is recovery mode=%v, progress=%v\",\n\t\tisRecoveryMode, progress)\n\n\treturn &lnrpc.GetRecoveryInfoResponse{\n\t\tRecoveryMode:     isRecoveryMode,\n\t\tRecoveryFinished: progress == 1,\n\t\tProgress:         progress,\n\t}, nil\n}\n\n// ListPeers returns a verbose listing of all currently active peers.",
      "length": 545,
      "tokens": 57,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) ListPeers(ctx context.Context,",
      "content": "func (r *rpcServer) ListPeers(ctx context.Context,\n\tin *lnrpc.ListPeersRequest) (*lnrpc.ListPeersResponse, error) {\n\n\trpcsLog.Tracef(\"[listpeers] request\")\n\n\tserverPeers := r.server.Peers()\n\tresp := &lnrpc.ListPeersResponse{\n\t\tPeers: make([]*lnrpc.Peer, 0, len(serverPeers)),\n\t}\n\n\tfor _, serverPeer := range serverPeers {\n\t\tvar (\n\t\t\tsatSent int64\n\t\t\tsatRecv int64\n\t\t)\n\n\t\t// In order to display the total number of satoshis of outbound\n\t\t// (sent) and inbound (recv'd) satoshis that have been\n\t\t// transported through this peer, we'll sum up the sent/recv'd\n\t\t// values for each of the active channels we have with the\n\t\t// peer.\n\t\tchans := serverPeer.ChannelSnapshots()\n\t\tfor _, c := range chans {\n\t\t\tsatSent += int64(c.TotalMSatSent.ToSatoshis())\n\t\t\tsatRecv += int64(c.TotalMSatReceived.ToSatoshis())\n\t\t}\n\n\t\tnodePub := serverPeer.PubKey()\n\n\t\t// Retrieve the peer's sync type. If we don't currently have a\n\t\t// syncer for the peer, then we'll default to a passive sync.\n\t\t// This can happen if the RPC is called while a peer is\n\t\t// initializing.\n\t\tsyncer, ok := r.server.authGossiper.SyncManager().GossipSyncer(\n\t\t\tnodePub,\n\t\t)\n\n\t\tvar lnrpcSyncType lnrpc.Peer_SyncType\n\t\tif !ok {\n\t\t\trpcsLog.Warnf(\"Gossip syncer for peer=%x not found\",\n\t\t\t\tnodePub)\n\t\t\tlnrpcSyncType = lnrpc.Peer_UNKNOWN_SYNC\n\t\t} else {\n\t\t\tsyncType := syncer.SyncType()\n\t\t\tswitch syncType {\n\t\t\tcase discovery.ActiveSync:\n\t\t\t\tlnrpcSyncType = lnrpc.Peer_ACTIVE_SYNC\n\t\t\tcase discovery.PassiveSync:\n\t\t\t\tlnrpcSyncType = lnrpc.Peer_PASSIVE_SYNC\n\t\t\tcase discovery.PinnedSync:\n\t\t\t\tlnrpcSyncType = lnrpc.Peer_PINNED_SYNC\n\t\t\tdefault:\n\t\t\t\treturn nil, fmt.Errorf(\"unhandled sync type %v\",\n\t\t\t\t\tsyncType)\n\t\t\t}\n\t\t}\n\n\t\tfeatures := invoicesrpc.CreateRPCFeatures(\n\t\t\tserverPeer.RemoteFeatures(),\n\t\t)\n\n\t\trpcPeer := &lnrpc.Peer{\n\t\t\tPubKey:          hex.EncodeToString(nodePub[:]),\n\t\t\tAddress:         serverPeer.Conn().RemoteAddr().String(),\n\t\t\tInbound:         serverPeer.Inbound(),\n\t\t\tBytesRecv:       serverPeer.BytesReceived(),\n\t\t\tBytesSent:       serverPeer.BytesSent(),\n\t\t\tSatSent:         satSent,\n\t\t\tSatRecv:         satRecv,\n\t\t\tPingTime:        serverPeer.PingTime(),\n\t\t\tSyncType:        lnrpcSyncType,\n\t\t\tFeatures:        features,\n\t\t\tLastPingPayload: serverPeer.LastRemotePingPayload(),\n\t\t}\n\n\t\tvar peerErrors []interface{}\n\n\t\t// If we only want the most recent error, get the most recent\n\t\t// error from the buffer and add it to our list of errors if\n\t\t// it is non-nil. If we want all the stored errors, simply\n\t\t// add the full list to our set of errors.\n\t\tif in.LatestError {\n\t\t\tlatestErr := serverPeer.ErrorBuffer().Latest()\n\t\t\tif latestErr != nil {\n\t\t\t\tpeerErrors = []interface{}{latestErr}\n\t\t\t}\n\t\t} else {\n\t\t\tpeerErrors = serverPeer.ErrorBuffer().List()\n\t\t}\n\n\t\t// Add the relevant peer errors to our response.\n\t\tfor _, error := range peerErrors {\n\t\t\ttsError := error.(*peer.TimestampedError)\n\n\t\t\trpcErr := &lnrpc.TimestampedError{\n\t\t\t\tTimestamp: uint64(tsError.Timestamp.Unix()),\n\t\t\t\tError:     tsError.Error.Error(),\n\t\t\t}\n\n\t\t\trpcPeer.Errors = append(rpcPeer.Errors, rpcErr)\n\t\t}\n\n\t\t// If the server has started, we can query the event store\n\t\t// for our peer's flap count. If we do so when the server has\n\t\t// not started, the request will block.\n\t\tif r.server.Started() {\n\t\t\tvertex, err := route.NewVertexFromBytes(nodePub[:])\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tflap, ts, err := r.server.chanEventStore.FlapCount(\n\t\t\t\tvertex,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\t// If our timestamp is non-nil, we have values for our\n\t\t\t// peer's flap count, so we set them.\n\t\t\tif ts != nil {\n\t\t\t\trpcPeer.FlapCount = int32(flap)\n\t\t\t\trpcPeer.LastFlapNs = ts.UnixNano()\n\t\t\t}\n\t\t}\n\n\t\tresp.Peers = append(resp.Peers, rpcPeer)\n\t}\n\n\trpcsLog.Debugf(\"[listpeers] yielded %v peers\", serverPeers)\n\n\treturn resp, nil\n}\n\n// SubscribePeerEvents returns a uni-directional stream (server -> client)\n// for notifying the client of peer online and offline events.",
      "length": 3739,
      "tokens": 460,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) SubscribePeerEvents(req *lnrpc.PeerEventSubscription,",
      "content": "func (r *rpcServer) SubscribePeerEvents(req *lnrpc.PeerEventSubscription,\n\teventStream lnrpc.Lightning_SubscribePeerEventsServer) error {\n\n\tpeerEventSub, err := r.server.peerNotifier.SubscribePeerEvents()\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer peerEventSub.Cancel()\n\n\tfor {\n\t\tselect {\n\t\t// A new update has been sent by the peer notifier, we'll\n\t\t// marshal it into the form expected by the gRPC client, then\n\t\t// send it off to the client.\n\t\tcase e := <-peerEventSub.Updates():\n\t\t\tvar event *lnrpc.PeerEvent\n\n\t\t\tswitch peerEvent := e.(type) {\n\t\t\tcase peernotifier.PeerOfflineEvent:\n\t\t\t\tevent = &lnrpc.PeerEvent{\n\t\t\t\t\tPubKey: hex.EncodeToString(peerEvent.PubKey[:]),\n\t\t\t\t\tType:   lnrpc.PeerEvent_PEER_OFFLINE,\n\t\t\t\t}\n\n\t\t\tcase peernotifier.PeerOnlineEvent:\n\t\t\t\tevent = &lnrpc.PeerEvent{\n\t\t\t\t\tPubKey: hex.EncodeToString(peerEvent.PubKey[:]),\n\t\t\t\t\tType:   lnrpc.PeerEvent_PEER_ONLINE,\n\t\t\t\t}\n\n\t\t\tdefault:\n\t\t\t\treturn fmt.Errorf(\"unexpected peer event: %v\", event)\n\t\t\t}\n\n\t\t\tif err := eventStream.Send(event); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t// The response stream's context for whatever reason has been\n\t\t// closed. If context is closed by an exceeded deadline we will\n\t\t// return an error.\n\t\tcase <-eventStream.Context().Done():\n\t\t\tif errors.Is(eventStream.Context().Err(), context.Canceled) {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn eventStream.Context().Err()\n\n\t\tcase <-r.quit:\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\n// WalletBalance returns total unspent outputs(confirmed and unconfirmed), all\n// confirmed unspent outputs and all unconfirmed unspent outputs under control\n// by the wallet. This method can be modified by having the request specify\n// only witness outputs should be factored into the final output sum.\n// TODO(roasbeef): add async hooks into wallet balance changes.",
      "length": 1630,
      "tokens": 203,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) WalletBalance(ctx context.Context,",
      "content": "func (r *rpcServer) WalletBalance(ctx context.Context,\n\tin *lnrpc.WalletBalanceRequest) (*lnrpc.WalletBalanceResponse, error) {\n\n\t// Retrieve all existing wallet accounts. We'll compute the confirmed\n\t// and unconfirmed balance for each and tally them up.\n\taccounts, err := r.server.cc.Wallet.ListAccounts(\"\", nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar totalBalance, confirmedBalance, unconfirmedBalance btcutil.Amount\n\trpcAccountBalances := make(\n\t\tmap[string]*lnrpc.WalletAccountBalance, len(accounts),\n\t)\n\tfor _, account := range accounts {\n\t\t// There are two default accounts, one for NP2WKH outputs and\n\t\t// another for P2WKH outputs. The balance will be computed for\n\t\t// both given one call to ConfirmedBalance with the default\n\t\t// wallet and imported account, so we'll skip the second\n\t\t// instance to avoid inflating the balance.\n\t\tswitch account.AccountName {\n\t\tcase waddrmgr.ImportedAddrAccountName:\n\t\t\t// Omit the imported account from the response unless we\n\t\t\t// actually have any keys imported.\n\t\t\tif account.ImportedKeyCount == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tfallthrough\n\n\t\tcase lnwallet.DefaultAccountName:\n\t\t\tif _, ok := rpcAccountBalances[account.AccountName]; ok {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\tdefault:\n\t\t}\n\n\t\t// There now also are the accounts for the internal channel\n\t\t// related keys. We skip those as they'll never have any direct\n\t\t// balance.\n\t\tif account.KeyScope.Purpose == keychain.BIP0043Purpose {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Get total balance, from txs that have >= 0 confirmations.\n\t\ttotalBal, err := r.server.cc.Wallet.ConfirmedBalance(\n\t\t\t0, account.AccountName,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\ttotalBalance += totalBal\n\n\t\t// Get confirmed balance, from txs that have >= 1 confirmations.\n\t\t// TODO(halseth): get both unconfirmed and confirmed balance in\n\t\t// one call, as this is racy.\n\t\tconfirmedBal, err := r.server.cc.Wallet.ConfirmedBalance(\n\t\t\t1, account.AccountName,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tconfirmedBalance += confirmedBal\n\n\t\t// Get unconfirmed balance, from txs with 0 confirmations.\n\t\tunconfirmedBal := totalBal - confirmedBal\n\t\tunconfirmedBalance += unconfirmedBal\n\n\t\trpcAccountBalances[account.AccountName] = &lnrpc.WalletAccountBalance{\n\t\t\tConfirmedBalance:   int64(confirmedBal),\n\t\t\tUnconfirmedBalance: int64(unconfirmedBal),\n\t\t}\n\t}\n\n\t// Now that we have the base balance accounted for with each account,\n\t// we'll look at the set of locked UTXOs to tally that as well. If we\n\t// don't display this, then anytime we attempt a funding reservation,\n\t// the outputs will chose as being \"gone\" until they're confirmed on\n\t// chain.\n\tvar lockedBalance btcutil.Amount\n\tleases, err := r.server.cc.Wallet.ListLeasedOutputs()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, leasedOutput := range leases {\n\t\tutxoInfo, err := r.server.cc.Wallet.FetchInputInfo(\n\t\t\t&leasedOutput.Outpoint,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tlockedBalance += utxoInfo.Value\n\t}\n\n\t// Get the current number of non-private anchor channels.\n\tcurrentNumAnchorChans, err := r.server.cc.Wallet.CurrentNumAnchorChans()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Get the required reserve for the wallet.\n\trequiredReserve := r.server.cc.Wallet.RequiredReserve(\n\t\tuint32(currentNumAnchorChans),\n\t)\n\n\trpcsLog.Debugf(\"[walletbalance] Total balance=%v (confirmed=%v, \"+\n\t\t\"unconfirmed=%v)\", totalBalance, confirmedBalance,\n\t\tunconfirmedBalance)\n\n\treturn &lnrpc.WalletBalanceResponse{\n\t\tTotalBalance:              int64(totalBalance),\n\t\tConfirmedBalance:          int64(confirmedBalance),\n\t\tUnconfirmedBalance:        int64(unconfirmedBalance),\n\t\tLockedBalance:             int64(lockedBalance),\n\t\tReservedBalanceAnchorChan: int64(requiredReserve),\n\t\tAccountBalance:            rpcAccountBalances,\n\t}, nil\n}\n\n// ChannelBalance returns the total available channel flow across all open\n// channels in satoshis.",
      "length": 3672,
      "tokens": 454,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) ChannelBalance(ctx context.Context,",
      "content": "func (r *rpcServer) ChannelBalance(ctx context.Context,\n\tin *lnrpc.ChannelBalanceRequest) (\n\t*lnrpc.ChannelBalanceResponse, error) {\n\n\tvar (\n\t\tlocalBalance             lnwire.MilliSatoshi\n\t\tremoteBalance            lnwire.MilliSatoshi\n\t\tunsettledLocalBalance    lnwire.MilliSatoshi\n\t\tunsettledRemoteBalance   lnwire.MilliSatoshi\n\t\tpendingOpenLocalBalance  lnwire.MilliSatoshi\n\t\tpendingOpenRemoteBalance lnwire.MilliSatoshi\n\t)\n\n\topenChannels, err := r.server.chanStateDB.FetchAllOpenChannels()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor _, channel := range openChannels {\n\t\tc := channel.LocalCommitment\n\t\tlocalBalance += c.LocalBalance\n\t\tremoteBalance += c.RemoteBalance\n\n\t\t// Add pending htlc amount.\n\t\tfor _, htlc := range c.Htlcs {\n\t\t\tif htlc.Incoming {\n\t\t\t\tunsettledLocalBalance += htlc.Amt\n\t\t\t} else {\n\t\t\t\tunsettledRemoteBalance += htlc.Amt\n\t\t\t}\n\t\t}\n\t}\n\n\tpendingChannels, err := r.server.chanStateDB.FetchPendingChannels()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor _, channel := range pendingChannels {\n\t\tc := channel.LocalCommitment\n\t\tpendingOpenLocalBalance += c.LocalBalance\n\t\tpendingOpenRemoteBalance += c.RemoteBalance\n\t}\n\n\trpcsLog.Debugf(\"[channelbalance] local_balance=%v remote_balance=%v \"+\n\t\t\"unsettled_local_balance=%v unsettled_remote_balance=%v \"+\n\t\t\"pending_open_local_balance=%v pending_open_remote_balance=%v\",\n\t\tlocalBalance, remoteBalance, unsettledLocalBalance,\n\t\tunsettledRemoteBalance, pendingOpenLocalBalance,\n\t\tpendingOpenRemoteBalance)\n\n\treturn &lnrpc.ChannelBalanceResponse{\n\t\tLocalBalance: &lnrpc.Amount{\n\t\t\tSat:  uint64(localBalance.ToSatoshis()),\n\t\t\tMsat: uint64(localBalance),\n\t\t},\n\t\tRemoteBalance: &lnrpc.Amount{\n\t\t\tSat:  uint64(remoteBalance.ToSatoshis()),\n\t\t\tMsat: uint64(remoteBalance),\n\t\t},\n\t\tUnsettledLocalBalance: &lnrpc.Amount{\n\t\t\tSat:  uint64(unsettledLocalBalance.ToSatoshis()),\n\t\t\tMsat: uint64(unsettledLocalBalance),\n\t\t},\n\t\tUnsettledRemoteBalance: &lnrpc.Amount{\n\t\t\tSat:  uint64(unsettledRemoteBalance.ToSatoshis()),\n\t\t\tMsat: uint64(unsettledRemoteBalance),\n\t\t},\n\t\tPendingOpenLocalBalance: &lnrpc.Amount{\n\t\t\tSat:  uint64(pendingOpenLocalBalance.ToSatoshis()),\n\t\t\tMsat: uint64(pendingOpenLocalBalance),\n\t\t},\n\t\tPendingOpenRemoteBalance: &lnrpc.Amount{\n\t\t\tSat:  uint64(pendingOpenRemoteBalance.ToSatoshis()),\n\t\t\tMsat: uint64(pendingOpenRemoteBalance),\n\t\t},\n\n\t\t// Deprecated fields.\n\t\tBalance:            int64(localBalance.ToSatoshis()),\n\t\tPendingOpenBalance: int64(pendingOpenLocalBalance.ToSatoshis()),\n\t}, nil\n}\n",
      "length": 2323,
      "tokens": 176,
      "embedding": []
    },
    {
      "slug": "type (",
      "content": "type (\n\tpendingOpenChannels  []*lnrpc.PendingChannelsResponse_PendingOpenChannel\n\tpendingForceClose    []*lnrpc.PendingChannelsResponse_ForceClosedChannel\n\twaitingCloseChannels []*lnrpc.PendingChannelsResponse_WaitingCloseChannel\n)\n\n// fetchPendingOpenChannels queries the database for a list of channels that\n// have pending open state. The returned result is used in the response of the\n// PendingChannels RPC.",
      "length": 398,
      "tokens": 36,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) fetchPendingOpenChannels() (pendingOpenChannels, error) {",
      "content": "func (r *rpcServer) fetchPendingOpenChannels() (pendingOpenChannels, error) {\n\t// First, we'll populate the response with all the channels that are\n\t// soon to be opened. We can easily fetch this data from the database\n\t// and map the db struct to the proto response.\n\tchannels, err := r.server.chanStateDB.FetchPendingChannels()\n\tif err != nil {\n\t\trpcsLog.Errorf(\"unable to fetch pending channels: %v\", err)\n\t\treturn nil, err\n\t}\n\n\tresult := make(pendingOpenChannels, len(channels))\n\tfor i, pendingChan := range channels {\n\t\tpub := pendingChan.IdentityPub.SerializeCompressed()\n\n\t\t// As this is required for display purposes, we'll calculate\n\t\t// the weight of the commitment transaction. We also add on the\n\t\t// estimated weight of the witness to calculate the weight of\n\t\t// the transaction if it were to be immediately unilaterally\n\t\t// broadcast.\n\t\t// TODO(roasbeef): query for funding tx from wallet, display\n\t\t// that also?\n\t\tlocalCommitment := pendingChan.LocalCommitment\n\t\tutx := btcutil.NewTx(localCommitment.CommitTx)\n\t\tcommitBaseWeight := blockchain.GetTransactionWeight(utx)\n\t\tcommitWeight := commitBaseWeight + input.WitnessCommitmentTxWeight\n\n\t\tresult[i] = &lnrpc.PendingChannelsResponse_PendingOpenChannel{\n\t\t\tChannel: &lnrpc.PendingChannelsResponse_PendingChannel{\n\t\t\t\tRemoteNodePub:        hex.EncodeToString(pub),\n\t\t\t\tChannelPoint:         pendingChan.FundingOutpoint.String(),\n\t\t\t\tCapacity:             int64(pendingChan.Capacity),\n\t\t\t\tLocalBalance:         int64(localCommitment.LocalBalance.ToSatoshis()),\n\t\t\t\tRemoteBalance:        int64(localCommitment.RemoteBalance.ToSatoshis()),\n\t\t\t\tLocalChanReserveSat:  int64(pendingChan.LocalChanCfg.ChanReserve),\n\t\t\t\tRemoteChanReserveSat: int64(pendingChan.RemoteChanCfg.ChanReserve),\n\t\t\t\tInitiator:            rpcInitiator(pendingChan.IsInitiator),\n\t\t\t\tCommitmentType:       rpcCommitmentType(pendingChan.ChanType),\n\t\t\t\tPrivate:              isPrivate(pendingChan),\n\t\t\t},\n\t\t\tCommitWeight: commitWeight,\n\t\t\tCommitFee:    int64(localCommitment.CommitFee),\n\t\t\tFeePerKw:     int64(localCommitment.FeePerKw),\n\t\t\t// TODO(roasbeef): need to track confirmation height\n\t\t}\n\t}\n\n\treturn result, nil\n}\n\n// fetchPendingForceCloseChannels queries the database for a list of channels\n// that have their closing transactions confirmed but not fully resolved yet.\n// The returned result is used in the response of the PendingChannels RPC.",
      "length": 2256,
      "tokens": 221,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) fetchPendingForceCloseChannels() (pendingForceClose,",
      "content": "func (r *rpcServer) fetchPendingForceCloseChannels() (pendingForceClose,\n\tint64, error) {\n\n\t_, currentHeight, err := r.server.cc.ChainIO.GetBestBlock()\n\tif err != nil {\n\t\treturn nil, 0, err\n\t}\n\n\t// Next, we'll examine the channels that are soon to be closed so we\n\t// can populate these fields within the response.\n\tchannels, err := r.server.chanStateDB.FetchClosedChannels(true)\n\tif err != nil {\n\t\trpcsLog.Errorf(\"unable to fetch closed channels: %v\", err)\n\t\treturn nil, 0, err\n\t}\n\n\tresult := make(pendingForceClose, 0)\n\tlimboBalance := int64(0)\n\n\tfor _, pendingClose := range channels {\n\t\t// First construct the channel struct itself, this will be\n\t\t// needed regardless of how this channel was closed.\n\t\tpub := pendingClose.RemotePub.SerializeCompressed()\n\t\tchanPoint := pendingClose.ChanPoint\n\n\t\t// Create the pending channel. If this channel was closed before\n\t\t// we started storing historical channel data, we will not know\n\t\t// who initiated the channel, so we set the initiator field to\n\t\t// unknown.\n\t\tchannel := &lnrpc.PendingChannelsResponse_PendingChannel{\n\t\t\tRemoteNodePub:  hex.EncodeToString(pub),\n\t\t\tChannelPoint:   chanPoint.String(),\n\t\t\tCapacity:       int64(pendingClose.Capacity),\n\t\t\tLocalBalance:   int64(pendingClose.SettledBalance),\n\t\t\tCommitmentType: lnrpc.CommitmentType_UNKNOWN_COMMITMENT_TYPE,\n\t\t\tInitiator:      lnrpc.Initiator_INITIATOR_UNKNOWN,\n\t\t}\n\n\t\t// Lookup the channel in the historical channel bucket to obtain\n\t\t// initiator information. If the historical channel bucket was\n\t\t// not found, or the channel itself, this channel was closed\n\t\t// in a version before we started persisting historical\n\t\t// channels, so we silence the error.\n\t\thistorical, err := r.server.chanStateDB.FetchHistoricalChannel(\n\t\t\t&pendingClose.ChanPoint,\n\t\t)\n\t\tswitch err {\n\t\t// If the channel was closed in a version that did not record\n\t\t// historical channels, ignore the error.\n\t\tcase channeldb.ErrNoHistoricalBucket:\n\t\tcase channeldb.ErrChannelNotFound:\n\n\t\tcase nil:\n\t\t\tchannel.Initiator = rpcInitiator(historical.IsInitiator)\n\t\t\tchannel.CommitmentType = rpcCommitmentType(\n\t\t\t\thistorical.ChanType,\n\t\t\t)\n\n\t\t\t// Get the number of forwarding packages from the\n\t\t\t// historical channel.\n\t\t\tfwdPkgs, err := historical.LoadFwdPkgs()\n\t\t\tif err != nil {\n\t\t\t\trpcsLog.Errorf(\"unable to load forwarding \"+\n\t\t\t\t\t\"packages for channel:%s, %v\",\n\t\t\t\t\thistorical.ShortChannelID, err)\n\t\t\t\treturn nil, 0, err\n\t\t\t}\n\t\t\tchannel.NumForwardingPackages = int64(len(fwdPkgs))\n\n\t\t\tchannel.RemoteBalance = int64(\n\t\t\t\thistorical.LocalCommitment.RemoteBalance.ToSatoshis(),\n\t\t\t)\n\n\t\t\tchannel.Private = isPrivate(historical)\n\n\t\t// If the error is non-nil, and not due to older versions of lnd\n\t\t// not persisting historical channels, return it.\n\t\tdefault:\n\t\t\treturn nil, 0, err\n\t\t}\n\n\t\tcloseTXID := pendingClose.ClosingTXID.String()\n\n\t\tswitch pendingClose.CloseType {\n\n\t\t// A coop closed channel should never be in the \"pending close\"\n\t\t// state. If a node upgraded from an older lnd version in the\n\t\t// middle of a their channel confirming, it will be in this\n\t\t// state. We log a warning that the channel will not be included\n\t\t// in the now deprecated pending close channels field.\n\t\tcase channeldb.CooperativeClose:\n\t\t\trpcsLog.Warnf(\"channel %v cooperatively closed and \"+\n\t\t\t\t\"in pending close state\",\n\t\t\t\tpendingClose.ChanPoint)\n\n\t\t// If the channel was force closed, then we'll need to query\n\t\t// the utxoNursery for additional information.\n\t\t// TODO(halseth): distinguish remote and local case?\n\t\tcase channeldb.LocalForceClose, channeldb.RemoteForceClose:\n\t\t\tforceClose := &lnrpc.PendingChannelsResponse_ForceClosedChannel{\n\t\t\t\tChannel:     channel,\n\t\t\t\tClosingTxid: closeTXID,\n\t\t\t}\n\n\t\t\t// Fetch reports from both nursery and resolvers. At the\n\t\t\t// moment this is not an atomic snapshot. This is\n\t\t\t// planned to be resolved when the nursery is removed\n\t\t\t// and channel arbitrator will be the single source for\n\t\t\t// these kind of reports.\n\t\t\terr := r.nurseryPopulateForceCloseResp(\n\t\t\t\t&chanPoint, currentHeight, forceClose,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\trpcsLog.Errorf(\"unable to populate nursery \"+\n\t\t\t\t\t\"force close resp:%s, %v\",\n\t\t\t\t\tchanPoint, err)\n\t\t\t\treturn nil, 0, err\n\t\t\t}\n\n\t\t\terr = r.arbitratorPopulateForceCloseResp(\n\t\t\t\t&chanPoint, currentHeight, forceClose,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\trpcsLog.Errorf(\"unable to populate arbitrator \"+\n\t\t\t\t\t\"force close resp:%s, %v\",\n\t\t\t\t\tchanPoint, err)\n\t\t\t\treturn nil, 0, err\n\t\t\t}\n\n\t\t\tlimboBalance += forceClose.LimboBalance\n\t\t\tresult = append(result, forceClose)\n\t\t}\n\t}\n\n\treturn result, limboBalance, nil\n}\n\n// fetchWaitingCloseChannels queries the database for a list of channels\n// that have their closing transactions broadcast but not confirmed yet.\n// The returned result is used in the response of the PendingChannels RPC.",
      "length": 4563,
      "tokens": 579,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) fetchWaitingCloseChannels() (waitingCloseChannels,",
      "content": "func (r *rpcServer) fetchWaitingCloseChannels() (waitingCloseChannels,\n\tint64, error) {\n\n\t// We'll also fetch all channels that are open, but have had their\n\t// commitment broadcasted, meaning they are waiting for the closing\n\t// transaction to confirm.\n\tchannels, err := r.server.chanStateDB.FetchWaitingCloseChannels()\n\tif err != nil {\n\t\trpcsLog.Errorf(\"unable to fetch channels waiting close: %v\",\n\t\t\terr)\n\t\treturn nil, 0, err\n\t}\n\n\tresult := make(waitingCloseChannels, 0)\n\tlimboBalance := int64(0)\n\n\t// getClosingTx is a helper closure that tries to find the closing txid\n\t// of a given waiting close channel. Notice that if the remote closes\n\t// the channel, we may not have the closing txid.\n\tgetClosingTx := func(c *channeldb.OpenChannel) (string, error) {\n\t\tvar (\n\t\t\ttx  *wire.MsgTx\n\t\t\terr error\n\t\t)\n\n\t\t// First, we try to locate the force closing txid. If not\n\t\t// found, we will then try to find its coop closing txid.\n\t\ttx, err = c.BroadcastedCommitment()\n\t\tif err == nil {\n\t\t\treturn tx.TxHash().String(), nil\n\t\t}\n\n\t\t// If the error returned is not ErrNoCloseTx, something\n\t\t// unexpected happened and we will return the error.\n\t\tif err != channeldb.ErrNoCloseTx {\n\t\t\treturn \"\", err\n\t\t}\n\n\t\t// Otherwise, we continue to locate its coop closing txid.\n\t\ttx, err = c.BroadcastedCooperative()\n\t\tif err == nil {\n\t\t\treturn tx.TxHash().String(), nil\n\t\t}\n\n\t\t// Return the error if it's not ErrNoCloseTx.\n\t\tif err != channeldb.ErrNoCloseTx {\n\t\t\treturn \"\", err\n\t\t}\n\n\t\t// Otherwise return an empty txid. This can happen if the\n\t\t// remote broadcast the closing txid and we haven't recorded it\n\t\t// yet.\n\t\treturn \"\", nil\n\t}\n\n\tfor _, waitingClose := range channels {\n\t\tpub := waitingClose.IdentityPub.SerializeCompressed()\n\t\tchanPoint := waitingClose.FundingOutpoint\n\n\t\tvar commitments lnrpc.PendingChannelsResponse_Commitments\n\n\t\t// Report local commit. May not be present when DLP is active.\n\t\tif waitingClose.LocalCommitment.CommitTx != nil {\n\t\t\tcommitments.LocalTxid =\n\t\t\t\twaitingClose.LocalCommitment.CommitTx.TxHash().\n\t\t\t\t\tString()\n\n\t\t\tcommitments.LocalCommitFeeSat = uint64(\n\t\t\t\twaitingClose.LocalCommitment.CommitFee,\n\t\t\t)\n\t\t}\n\n\t\t// Report remote commit. May not be present when DLP is active.\n\t\tif waitingClose.RemoteCommitment.CommitTx != nil {\n\t\t\tcommitments.RemoteTxid =\n\t\t\t\twaitingClose.RemoteCommitment.CommitTx.TxHash().\n\t\t\t\t\tString()\n\n\t\t\tcommitments.RemoteCommitFeeSat = uint64(\n\t\t\t\twaitingClose.RemoteCommitment.CommitFee,\n\t\t\t)\n\t\t}\n\n\t\t// Report the remote pending commit if any.\n\t\tremoteCommitDiff, err := waitingClose.RemoteCommitChainTip()\n\n\t\tswitch {\n\t\t// Don't set hash if there is no pending remote commit.\n\t\tcase err == channeldb.ErrNoPendingCommit:\n\n\t\t// An unexpected error occurred.\n\t\tcase err != nil:\n\t\t\treturn nil, 0, err\n\n\t\t// There is a pending remote commit. Set its hash in the\n\t\t// response.\n\t\tdefault:\n\t\t\thash := remoteCommitDiff.Commitment.CommitTx.TxHash()\n\t\t\tcommitments.RemotePendingTxid = hash.String()\n\t\t\tcommitments.RemoteCommitFeeSat = uint64(\n\t\t\t\tremoteCommitDiff.Commitment.CommitFee,\n\t\t\t)\n\t\t}\n\n\t\tfwdPkgs, err := waitingClose.LoadFwdPkgs()\n\t\tif err != nil {\n\t\t\trpcsLog.Errorf(\"unable to load forwarding packages \"+\n\t\t\t\t\"for channel:%s, %v\",\n\t\t\t\twaitingClose.ShortChannelID, err)\n\t\t\treturn nil, 0, err\n\t\t}\n\n\t\t// Get the closing txid.\n\t\t// NOTE: the closing txid could be empty here if it's the\n\t\t// remote broadcasted the closing tx.\n\t\tclosingTxid, err := getClosingTx(waitingClose)\n\t\tif err != nil {\n\t\t\trpcsLog.Errorf(\"unable to find closing txid for \"+\n\t\t\t\t\"channel:%s, %v\",\n\t\t\t\twaitingClose.ShortChannelID, err)\n\t\t\treturn nil, 0, err\n\t\t}\n\n\t\tchannel := &lnrpc.PendingChannelsResponse_PendingChannel{\n\t\t\tRemoteNodePub:         hex.EncodeToString(pub),\n\t\t\tChannelPoint:          chanPoint.String(),\n\t\t\tCapacity:              int64(waitingClose.Capacity),\n\t\t\tLocalBalance:          int64(waitingClose.LocalCommitment.LocalBalance.ToSatoshis()),\n\t\t\tRemoteBalance:         int64(waitingClose.LocalCommitment.RemoteBalance.ToSatoshis()),\n\t\t\tLocalChanReserveSat:   int64(waitingClose.LocalChanCfg.ChanReserve),\n\t\t\tRemoteChanReserveSat:  int64(waitingClose.RemoteChanCfg.ChanReserve),\n\t\t\tInitiator:             rpcInitiator(waitingClose.IsInitiator),\n\t\t\tCommitmentType:        rpcCommitmentType(waitingClose.ChanType),\n\t\t\tNumForwardingPackages: int64(len(fwdPkgs)),\n\t\t\tChanStatusFlags:       waitingClose.ChanStatus().String(),\n\t\t\tPrivate:               isPrivate(waitingClose),\n\t\t}\n\n\t\twaitingCloseResp := &lnrpc.PendingChannelsResponse_WaitingCloseChannel{\n\t\t\tChannel:      channel,\n\t\t\tLimboBalance: channel.LocalBalance,\n\t\t\tCommitments:  &commitments,\n\t\t\tClosingTxid:  closingTxid,\n\t\t}\n\n\t\t// A close tx has been broadcasted, all our balance will be in\n\t\t// limbo until it confirms.\n\t\tresult = append(result, waitingCloseResp)\n\t\tlimboBalance += channel.LocalBalance\n\t}\n\n\treturn result, limboBalance, nil\n}\n\n// PendingChannels returns a list of all the channels that are currently\n// considered \"pending\". A channel is pending if it has finished the funding\n// workflow and is waiting for confirmations for the funding txn, or is in the\n// process of closure, either initiated cooperatively or non-cooperatively.",
      "length": 4919,
      "tokens": 573,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) PendingChannels(ctx context.Context,",
      "content": "func (r *rpcServer) PendingChannels(ctx context.Context,\n\tin *lnrpc.PendingChannelsRequest) (\n\t*lnrpc.PendingChannelsResponse, error) {\n\n\trpcsLog.Debugf(\"[pendingchannels]\")\n\n\tresp := &lnrpc.PendingChannelsResponse{}\n\n\t// First, we find all the channels that will soon be opened.\n\tpendingOpenChannels, err := r.fetchPendingOpenChannels()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tresp.PendingOpenChannels = pendingOpenChannels\n\n\t// Second, we fetch all channels that considered pending force closing.\n\t// This means the channels here have their closing transactions\n\t// confirmed but not considered fully resolved yet. For instance, they\n\t// may have a second level HTLCs to be resolved onchain.\n\tpendingCloseChannels, limbo, err := r.fetchPendingForceCloseChannels()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tresp.PendingForceClosingChannels = pendingCloseChannels\n\tresp.TotalLimboBalance = limbo\n\n\t// Third, we fetch all channels that are open, but have had their\n\t// commitment broadcasted, meaning they are waiting for the closing\n\t// transaction to confirm.\n\twaitingCloseChannels, limbo, err := r.fetchWaitingCloseChannels()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tresp.WaitingCloseChannels = waitingCloseChannels\n\tresp.TotalLimboBalance += limbo\n\n\treturn resp, nil\n}\n\n// arbitratorPopulateForceCloseResp populates the pending channels response\n// message with channel resolution information from the contract resolvers.",
      "length": 1322,
      "tokens": 169,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) arbitratorPopulateForceCloseResp(chanPoint *wire.OutPoint,",
      "content": "func (r *rpcServer) arbitratorPopulateForceCloseResp(chanPoint *wire.OutPoint,\n\tcurrentHeight int32,\n\tforceClose *lnrpc.PendingChannelsResponse_ForceClosedChannel) error {\n\n\t// Query for contract resolvers state.\n\tarbitrator, err := r.server.chainArb.GetChannelArbitrator(*chanPoint)\n\tif err != nil {\n\t\treturn err\n\t}\n\treports := arbitrator.Report()\n\n\tfor _, report := range reports {\n\t\tswitch report.Type {\n\t\t// For a direct output, populate/update the top level\n\t\t// response properties.\n\t\tcase contractcourt.ReportOutputUnencumbered:\n\t\t\t// Populate the maturity height fields for the direct\n\t\t\t// commitment output to us.\n\t\t\tforceClose.MaturityHeight = report.MaturityHeight\n\n\t\t\t// If the transaction has been confirmed, then we can\n\t\t\t// compute how many blocks it has left.\n\t\t\tif forceClose.MaturityHeight != 0 {\n\t\t\t\tforceClose.BlocksTilMaturity =\n\t\t\t\t\tint32(forceClose.MaturityHeight) -\n\t\t\t\t\t\tcurrentHeight\n\t\t\t}\n\n\t\t// Add htlcs to the PendingHtlcs response property.\n\t\tcase contractcourt.ReportOutputIncomingHtlc,\n\t\t\tcontractcourt.ReportOutputOutgoingHtlc:\n\n\t\t\t// Don't report details on htlcs that are no longer in\n\t\t\t// limbo.\n\t\t\tif report.LimboBalance == 0 {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tincoming := report.Type == contractcourt.ReportOutputIncomingHtlc\n\t\t\thtlc := &lnrpc.PendingHTLC{\n\t\t\t\tIncoming:       incoming,\n\t\t\t\tAmount:         int64(report.Amount),\n\t\t\t\tOutpoint:       report.Outpoint.String(),\n\t\t\t\tMaturityHeight: report.MaturityHeight,\n\t\t\t\tStage:          report.Stage,\n\t\t\t}\n\n\t\t\tif htlc.MaturityHeight != 0 {\n\t\t\t\thtlc.BlocksTilMaturity =\n\t\t\t\t\tint32(htlc.MaturityHeight) - currentHeight\n\t\t\t}\n\n\t\t\tforceClose.PendingHtlcs = append(forceClose.PendingHtlcs, htlc)\n\n\t\tcase contractcourt.ReportOutputAnchor:\n\t\t\t// There are three resolution states for the anchor:\n\t\t\t// limbo, lost and recovered. Derive the current state\n\t\t\t// from the limbo and recovered balances.\n\t\t\tswitch {\n\t\t\tcase report.RecoveredBalance != 0:\n\t\t\t\tforceClose.Anchor = lnrpc.PendingChannelsResponse_ForceClosedChannel_RECOVERED\n\n\t\t\tcase report.LimboBalance != 0:\n\t\t\t\tforceClose.Anchor = lnrpc.PendingChannelsResponse_ForceClosedChannel_LIMBO\n\n\t\t\tdefault:\n\t\t\t\tforceClose.Anchor = lnrpc.PendingChannelsResponse_ForceClosedChannel_LOST\n\t\t\t}\n\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"unknown report output type: %v\",\n\t\t\t\treport.Type)\n\t\t}\n\n\t\tforceClose.LimboBalance += int64(report.LimboBalance)\n\t\tforceClose.RecoveredBalance += int64(report.RecoveredBalance)\n\t}\n\n\treturn nil\n}\n\n// nurseryPopulateForceCloseResp populates the pending channels response\n// message with contract resolution information from utxonursery.",
      "length": 2417,
      "tokens": 244,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) nurseryPopulateForceCloseResp(chanPoint *wire.OutPoint,",
      "content": "func (r *rpcServer) nurseryPopulateForceCloseResp(chanPoint *wire.OutPoint,\n\tcurrentHeight int32,\n\tforceClose *lnrpc.PendingChannelsResponse_ForceClosedChannel) error {\n\n\t// Query for the maturity state for this force closed channel. If we\n\t// didn't have any time-locked outputs, then the nursery may not know of\n\t// the contract.\n\tnurseryInfo, err := r.server.utxoNursery.NurseryReport(chanPoint)\n\tif err == contractcourt.ErrContractNotFound {\n\t\treturn nil\n\t}\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to obtain \"+\n\t\t\t\"nursery report for ChannelPoint(%v): %v\",\n\t\t\tchanPoint, err)\n\t}\n\n\t// If the nursery knows of this channel, then we can populate\n\t// information detailing exactly how much funds are time locked and also\n\t// the height in which we can ultimately sweep the funds into the\n\t// wallet.\n\tforceClose.LimboBalance = int64(nurseryInfo.LimboBalance)\n\tforceClose.RecoveredBalance = int64(nurseryInfo.RecoveredBalance)\n\n\tfor _, htlcReport := range nurseryInfo.Htlcs {\n\t\t// TODO(conner) set incoming flag appropriately after handling\n\t\t// incoming incubation\n\t\thtlc := &lnrpc.PendingHTLC{\n\t\t\tIncoming:       false,\n\t\t\tAmount:         int64(htlcReport.Amount),\n\t\t\tOutpoint:       htlcReport.Outpoint.String(),\n\t\t\tMaturityHeight: htlcReport.MaturityHeight,\n\t\t\tStage:          htlcReport.Stage,\n\t\t}\n\n\t\tif htlc.MaturityHeight != 0 {\n\t\t\thtlc.BlocksTilMaturity =\n\t\t\t\tint32(htlc.MaturityHeight) -\n\t\t\t\t\tcurrentHeight\n\t\t}\n\n\t\tforceClose.PendingHtlcs = append(forceClose.PendingHtlcs,\n\t\t\thtlc)\n\t}\n\n\treturn nil\n}\n\n// ClosedChannels returns a list of all the channels have been closed.\n// This does not include channels that are still in the process of closing.",
      "length": 1535,
      "tokens": 187,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) ClosedChannels(ctx context.Context,",
      "content": "func (r *rpcServer) ClosedChannels(ctx context.Context,\n\tin *lnrpc.ClosedChannelsRequest) (*lnrpc.ClosedChannelsResponse,\n\terror) {\n\n\t// Show all channels when no filter flags are set.\n\tfilterResults := in.Cooperative || in.LocalForce ||\n\t\tin.RemoteForce || in.Breach || in.FundingCanceled ||\n\t\tin.Abandoned\n\n\tresp := &lnrpc.ClosedChannelsResponse{}\n\n\tdbChannels, err := r.server.chanStateDB.FetchClosedChannels(false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// In order to make the response easier to parse for clients, we'll\n\t// sort the set of closed channels by their closing height before\n\t// serializing the proto response.\n\tsort.Slice(dbChannels, func(i, j int) bool {\n\t\treturn dbChannels[i].CloseHeight < dbChannels[j].CloseHeight\n\t})\n\n\tfor _, dbChannel := range dbChannels {\n\t\tif dbChannel.IsPending {\n\t\t\tcontinue\n\t\t}\n\n\t\tswitch dbChannel.CloseType {\n\t\tcase channeldb.CooperativeClose:\n\t\t\tif filterResults && !in.Cooperative {\n\t\t\t\tcontinue\n\t\t\t}\n\t\tcase channeldb.LocalForceClose:\n\t\t\tif filterResults && !in.LocalForce {\n\t\t\t\tcontinue\n\t\t\t}\n\t\tcase channeldb.RemoteForceClose:\n\t\t\tif filterResults && !in.RemoteForce {\n\t\t\t\tcontinue\n\t\t\t}\n\t\tcase channeldb.BreachClose:\n\t\t\tif filterResults && !in.Breach {\n\t\t\t\tcontinue\n\t\t\t}\n\t\tcase channeldb.FundingCanceled:\n\t\t\tif filterResults && !in.FundingCanceled {\n\t\t\t\tcontinue\n\t\t\t}\n\t\tcase channeldb.Abandoned:\n\t\t\tif filterResults && !in.Abandoned {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tchannel, err := r.createRPCClosedChannel(dbChannel)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tresp.Channels = append(resp.Channels, channel)\n\t}\n\n\treturn resp, nil\n}\n\n// LookupHtlcResolution retrieves a final htlc resolution from the database. If\n// the htlc has no final resolution yet, a NotFound grpc status code is\n// returned.",
      "length": 1621,
      "tokens": 204,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) LookupHtlcResolution(",
      "content": "func (r *rpcServer) LookupHtlcResolution(\n\tctx context.Context, in *lnrpc.LookupHtlcResolutionRequest) (\n\t*lnrpc.LookupHtlcResolutionResponse, error) {\n\n\tif !r.cfg.StoreFinalHtlcResolutions {\n\t\treturn nil, status.Error(codes.Unavailable, \"cannot lookup \"+\n\t\t\t\"with flag --store-final-htlc-resolutions=false\")\n\t}\n\n\tchanID := lnwire.NewShortChanIDFromInt(in.ChanId)\n\n\tinfo, err := r.server.chanStateDB.LookupFinalHtlc(chanID, in.HtlcIndex)\n\tswitch {\n\tcase errors.Is(err, channeldb.ErrHtlcUnknown):\n\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\n\tcase err != nil:\n\t\treturn nil, err\n\t}\n\n\treturn &lnrpc.LookupHtlcResolutionResponse{\n\t\tSettled:  info.Settled,\n\t\tOffchain: info.Offchain,\n\t}, nil\n}\n\n// ListChannels returns a description of all the open channels that this node\n// is a participant in.",
      "length": 735,
      "tokens": 73,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) ListChannels(ctx context.Context,",
      "content": "func (r *rpcServer) ListChannels(ctx context.Context,\n\tin *lnrpc.ListChannelsRequest) (*lnrpc.ListChannelsResponse, error) {\n\n\tif in.ActiveOnly && in.InactiveOnly {\n\t\treturn nil, fmt.Errorf(\"either `active_only` or \" +\n\t\t\t\"`inactive_only` can be set, but not both\")\n\t}\n\n\tif in.PublicOnly && in.PrivateOnly {\n\t\treturn nil, fmt.Errorf(\"either `public_only` or \" +\n\t\t\t\"`private_only` can be set, but not both\")\n\t}\n\n\tif len(in.Peer) > 0 && len(in.Peer) != 33 {\n\t\t_, err := route.NewVertexFromBytes(in.Peer)\n\t\treturn nil, fmt.Errorf(\"invalid `peer` key: %v\", err)\n\t}\n\n\tresp := &lnrpc.ListChannelsResponse{}\n\n\tdbChannels, err := r.server.chanStateDB.FetchAllOpenChannels()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trpcsLog.Debugf(\"[listchannels] fetched %v channels from DB\",\n\t\tlen(dbChannels))\n\n\tfor _, dbChannel := range dbChannels {\n\t\tnodePub := dbChannel.IdentityPub\n\t\tnodePubBytes := nodePub.SerializeCompressed()\n\t\tchanPoint := dbChannel.FundingOutpoint\n\n\t\t// If the caller requested channels for a target node, skip any\n\t\t// that don't match the provided pubkey.\n\t\tif len(in.Peer) > 0 && !bytes.Equal(nodePubBytes, in.Peer) {\n\t\t\tcontinue\n\t\t}\n\n\t\tvar peerOnline bool\n\t\tif _, err := r.server.FindPeer(nodePub); err == nil {\n\t\t\tpeerOnline = true\n\t\t}\n\n\t\tchannelID := lnwire.NewChanIDFromOutPoint(&chanPoint)\n\t\tvar linkActive bool\n\t\tif link, err := r.server.htlcSwitch.GetLink(channelID); err == nil {\n\t\t\t// A channel is only considered active if it is known\n\t\t\t// by the switch *and* able to forward\n\t\t\t// incoming/outgoing payments.\n\t\t\tlinkActive = link.EligibleToForward()\n\t\t}\n\n\t\t// Next, we'll determine whether we should add this channel to\n\t\t// our list depending on the type of channels requested to us.\n\t\tisActive := peerOnline && linkActive\n\t\tchannel, err := createRPCOpenChannel(\n\t\t\tr, dbChannel, isActive, in.PeerAliasLookup,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// We'll only skip returning this channel if we were requested\n\t\t// for a specific kind and this channel doesn't satisfy it.\n\t\tswitch {\n\t\tcase in.ActiveOnly && !isActive:\n\t\t\tcontinue\n\t\tcase in.InactiveOnly && isActive:\n\t\t\tcontinue\n\t\tcase in.PublicOnly && channel.Private:\n\t\t\tcontinue\n\t\tcase in.PrivateOnly && !channel.Private:\n\t\t\tcontinue\n\t\t}\n\n\t\tresp.Channels = append(resp.Channels, channel)\n\t}\n\n\treturn resp, nil\n}\n\n// rpcCommitmentType takes the channel type and converts it to an rpc commitment\n// type value.",
      "length": 2255,
      "tokens": 307,
      "embedding": []
    },
    {
      "slug": "func rpcCommitmentType(chanType channeldb.ChannelType) lnrpc.CommitmentType {",
      "content": "func rpcCommitmentType(chanType channeldb.ChannelType) lnrpc.CommitmentType {\n\t// Extract the commitment type from the channel type flags. We must\n\t// first check whether it has anchors, since in that case it would also\n\t// be tweakless.\n\tif chanType.HasLeaseExpiration() {\n\t\treturn lnrpc.CommitmentType_SCRIPT_ENFORCED_LEASE\n\t}\n\n\tif chanType.HasAnchors() {\n\t\treturn lnrpc.CommitmentType_ANCHORS\n\t}\n\n\tif chanType.IsTweakless() {\n\t\treturn lnrpc.CommitmentType_STATIC_REMOTE_KEY\n\t}\n\n\treturn lnrpc.CommitmentType_LEGACY\n}\n\n// createChannelConstraint creates a *lnrpc.ChannelConstraints using the\n// *Channeldb.ChannelConfig.",
      "length": 524,
      "tokens": 59,
      "embedding": []
    },
    {
      "slug": "func createChannelConstraint(",
      "content": "func createChannelConstraint(\n\tchanCfg *channeldb.ChannelConfig) *lnrpc.ChannelConstraints {\n\treturn &lnrpc.ChannelConstraints{\n\t\tCsvDelay:          uint32(chanCfg.CsvDelay),\n\t\tChanReserveSat:    uint64(chanCfg.ChanReserve),\n\t\tDustLimitSat:      uint64(chanCfg.DustLimit),\n\t\tMaxPendingAmtMsat: uint64(chanCfg.MaxPendingAmount),\n\t\tMinHtlcMsat:       uint64(chanCfg.MinHTLC),\n\t\tMaxAcceptedHtlcs:  uint32(chanCfg.MaxAcceptedHtlcs),\n\t}\n}\n\n// isPrivate evaluates the ChannelFlags of the db channel to determine if the\n// channel is private or not.",
      "length": 500,
      "tokens": 39,
      "embedding": []
    },
    {
      "slug": "func isPrivate(dbChannel *channeldb.OpenChannel) bool {",
      "content": "func isPrivate(dbChannel *channeldb.OpenChannel) bool {\n\tif dbChannel == nil {\n\t\treturn false\n\t}\n\treturn dbChannel.ChannelFlags&lnwire.FFAnnounceChannel != 1\n}\n\n// createRPCOpenChannel creates an *lnrpc.Channel from the *channeldb.Channel.",
      "length": 177,
      "tokens": 21,
      "embedding": []
    },
    {
      "slug": "func createRPCOpenChannel(r *rpcServer, dbChannel *channeldb.OpenChannel,",
      "content": "func createRPCOpenChannel(r *rpcServer, dbChannel *channeldb.OpenChannel,\n\tisActive, peerAliasLookup bool) (*lnrpc.Channel, error) {\n\n\tnodePub := dbChannel.IdentityPub\n\tnodeID := hex.EncodeToString(nodePub.SerializeCompressed())\n\tchanPoint := dbChannel.FundingOutpoint\n\n\t// As this is required for display purposes, we'll calculate\n\t// the weight of the commitment transaction. We also add on the\n\t// estimated weight of the witness to calculate the weight of\n\t// the transaction if it were to be immediately unilaterally\n\t// broadcast.\n\tlocalCommit := dbChannel.LocalCommitment\n\tutx := btcutil.NewTx(localCommit.CommitTx)\n\tcommitBaseWeight := blockchain.GetTransactionWeight(utx)\n\tcommitWeight := commitBaseWeight + input.WitnessCommitmentTxWeight\n\n\tlocalBalance := localCommit.LocalBalance\n\tremoteBalance := localCommit.RemoteBalance\n\n\t// As an artifact of our usage of mSAT internally, either party\n\t// may end up in a state where they're holding a fractional\n\t// amount of satoshis which can't be expressed within the\n\t// actual commitment output. Since we round down when going\n\t// from mSAT -> SAT, we may at any point be adding an\n\t// additional SAT to miners fees. As a result, we display a\n\t// commitment fee that accounts for this externally.\n\tvar sumOutputs btcutil.Amount\n\tfor _, txOut := range localCommit.CommitTx.TxOut {\n\t\tsumOutputs += btcutil.Amount(txOut.Value)\n\t}\n\texternalCommitFee := dbChannel.Capacity - sumOutputs\n\n\t// Extract the commitment type from the channel type flags.\n\tcommitmentType := rpcCommitmentType(dbChannel.ChanType)\n\n\tdbScid := dbChannel.ShortChannelID\n\n\t// Fetch the set of aliases for the channel.\n\tchannelAliases := r.server.aliasMgr.GetAliases(dbScid)\n\n\tchannel := &lnrpc.Channel{\n\t\tActive:                isActive,\n\t\tPrivate:               isPrivate(dbChannel),\n\t\tRemotePubkey:          nodeID,\n\t\tChannelPoint:          chanPoint.String(),\n\t\tChanId:                dbScid.ToUint64(),\n\t\tCapacity:              int64(dbChannel.Capacity),\n\t\tLocalBalance:          int64(localBalance.ToSatoshis()),\n\t\tRemoteBalance:         int64(remoteBalance.ToSatoshis()),\n\t\tCommitFee:             int64(externalCommitFee),\n\t\tCommitWeight:          commitWeight,\n\t\tFeePerKw:              int64(localCommit.FeePerKw),\n\t\tTotalSatoshisSent:     int64(dbChannel.TotalMSatSent.ToSatoshis()),\n\t\tTotalSatoshisReceived: int64(dbChannel.TotalMSatReceived.ToSatoshis()),\n\t\tNumUpdates:            localCommit.CommitHeight,\n\t\tPendingHtlcs:          make([]*lnrpc.HTLC, len(localCommit.Htlcs)),\n\t\tInitiator:             dbChannel.IsInitiator,\n\t\tChanStatusFlags:       dbChannel.ChanStatus().String(),\n\t\tStaticRemoteKey:       commitmentType == lnrpc.CommitmentType_STATIC_REMOTE_KEY,\n\t\tCommitmentType:        commitmentType,\n\t\tThawHeight:            dbChannel.ThawHeight,\n\t\tLocalConstraints: createChannelConstraint(\n\t\t\t&dbChannel.LocalChanCfg,\n\t\t),\n\t\tRemoteConstraints: createChannelConstraint(\n\t\t\t&dbChannel.RemoteChanCfg,\n\t\t),\n\t\tAliasScids:            make([]uint64, 0, len(channelAliases)),\n\t\tZeroConf:              dbChannel.IsZeroConf(),\n\t\tZeroConfConfirmedScid: dbChannel.ZeroConfRealScid().ToUint64(),\n\t\t// TODO: remove the following deprecated fields\n\t\tCsvDelay:             uint32(dbChannel.LocalChanCfg.CsvDelay),\n\t\tLocalChanReserveSat:  int64(dbChannel.LocalChanCfg.ChanReserve),\n\t\tRemoteChanReserveSat: int64(dbChannel.RemoteChanCfg.ChanReserve),\n\t}\n\n\t// Look up our channel peer's node alias if the caller requests it.\n\tif peerAliasLookup {\n\t\tpeerAlias, err := r.server.graphDB.LookupAlias(nodePub)\n\t\tif err != nil {\n\t\t\tpeerAlias = fmt.Sprintf(\"unable to lookup \"+\n\t\t\t\t\"peer alias: %v\", err)\n\t\t}\n\t\tchannel.PeerAlias = peerAlias\n\t}\n\n\t// Populate the set of aliases.\n\tfor _, chanAlias := range channelAliases {\n\t\tchannel.AliasScids = append(\n\t\t\tchannel.AliasScids, chanAlias.ToUint64(),\n\t\t)\n\t}\n\n\tfor i, htlc := range localCommit.Htlcs {\n\t\tvar rHash [32]byte\n\t\tcopy(rHash[:], htlc.RHash[:])\n\n\t\tcircuitMap := r.server.htlcSwitch.CircuitLookup()\n\n\t\tvar forwardingChannel, forwardingHtlcIndex uint64\n\t\tswitch {\n\t\tcase htlc.Incoming:\n\t\t\tcircuit := circuitMap.LookupCircuit(\n\t\t\t\thtlcswitch.CircuitKey{\n\t\t\t\t\tChanID: dbChannel.ShortChannelID,\n\t\t\t\t\tHtlcID: htlc.HtlcIndex,\n\t\t\t\t},\n\t\t\t)\n\t\t\tif circuit != nil && circuit.Outgoing != nil {\n\t\t\t\tforwardingChannel = circuit.Outgoing.ChanID.\n\t\t\t\t\tToUint64()\n\n\t\t\t\tforwardingHtlcIndex = circuit.Outgoing.HtlcID\n\t\t\t}\n\n\t\tcase !htlc.Incoming:\n\t\t\tcircuit := circuitMap.LookupOpenCircuit(\n\t\t\t\thtlcswitch.CircuitKey{\n\t\t\t\t\tChanID: dbChannel.ShortChannelID,\n\t\t\t\t\tHtlcID: htlc.HtlcIndex,\n\t\t\t\t},\n\t\t\t)\n\n\t\t\t// If the incoming channel id is the special hop.Source\n\t\t\t// value, the htlc index is a local payment identifier.\n\t\t\t// In this case, report nothing.\n\t\t\tif circuit != nil &&\n\t\t\t\tcircuit.Incoming.ChanID != hop.Source {\n\n\t\t\t\tforwardingChannel = circuit.Incoming.ChanID.\n\t\t\t\t\tToUint64()\n\n\t\t\t\tforwardingHtlcIndex = circuit.Incoming.HtlcID\n\t\t\t}\n\t\t}\n\n\t\tchannel.PendingHtlcs[i] = &lnrpc.HTLC{\n\t\t\tIncoming:            htlc.Incoming,\n\t\t\tAmount:              int64(htlc.Amt.ToSatoshis()),\n\t\t\tHashLock:            rHash[:],\n\t\t\tExpirationHeight:    htlc.RefundTimeout,\n\t\t\tHtlcIndex:           htlc.HtlcIndex,\n\t\t\tForwardingChannel:   forwardingChannel,\n\t\t\tForwardingHtlcIndex: forwardingHtlcIndex,\n\t\t}\n\n\t\t// Add the Pending Htlc Amount to UnsettledBalance field.\n\t\tchannel.UnsettledBalance += channel.PendingHtlcs[i].Amount\n\t}\n\n\t// If we initiated opening the channel, the zero height remote balance\n\t// is the push amount. Otherwise, our starting balance is the push\n\t// amount. If there is no push amount, these values will simply be zero.\n\tif dbChannel.IsInitiator {\n\t\tamt := dbChannel.InitialRemoteBalance.ToSatoshis()\n\t\tchannel.PushAmountSat = uint64(amt)\n\t} else {\n\t\tamt := dbChannel.InitialLocalBalance.ToSatoshis()\n\t\tchannel.PushAmountSat = uint64(amt)\n\t}\n\n\tif len(dbChannel.LocalShutdownScript) > 0 {\n\t\t_, addresses, _, err := txscript.ExtractPkScriptAddrs(\n\t\t\tdbChannel.LocalShutdownScript, r.cfg.ActiveNetParams.Params,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// We only expect one upfront shutdown address for a channel. If\n\t\t// LocalShutdownScript is non-zero, there should be one payout\n\t\t// address set.\n\t\tif len(addresses) != 1 {\n\t\t\treturn nil, fmt.Errorf(\"expected one upfront shutdown \"+\n\t\t\t\t\"address, got: %v\", len(addresses))\n\t\t}\n\n\t\tchannel.CloseAddress = addresses[0].String()\n\t}\n\n\t// If the server hasn't fully started yet, it's possible that the\n\t// channel event store hasn't either, so it won't be able to consume any\n\t// requests until then. To prevent blocking, we'll just omit the uptime\n\t// related fields for now.\n\tif !r.server.Started() {\n\t\treturn channel, nil\n\t}\n\n\tpeer, err := route.NewVertexFromBytes(nodePub.SerializeCompressed())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Query the event store for additional information about the channel.\n\t// Do not fail if it is not available, because there is a potential\n\t// race between a channel being added to our node and the event store\n\t// being notified of it.\n\toutpoint := dbChannel.FundingOutpoint\n\tinfo, err := r.server.chanEventStore.GetChanInfo(outpoint, peer)\n\tswitch err {\n\t// If the store does not know about the channel, we just log it.\n\tcase chanfitness.ErrChannelNotFound:\n\t\trpcsLog.Infof(\"channel: %v not found by channel event store\",\n\t\t\toutpoint)\n\n\t// If we got our channel info, we further populate the channel.\n\tcase nil:\n\t\tchannel.Uptime = int64(info.Uptime.Seconds())\n\t\tchannel.Lifetime = int64(info.Lifetime.Seconds())\n\n\t// If we get an unexpected error, we return it.\n\tdefault:\n\t\treturn nil, err\n\t}\n\n\treturn channel, nil\n}\n\n// createRPCClosedChannel creates an *lnrpc.ClosedChannelSummary from a\n// *channeldb.ChannelCloseSummary.",
      "length": 7387,
      "tokens": 793,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) createRPCClosedChannel(",
      "content": "func (r *rpcServer) createRPCClosedChannel(\n\tdbChannel *channeldb.ChannelCloseSummary) (*lnrpc.ChannelCloseSummary, error) {\n\n\tnodePub := dbChannel.RemotePub\n\tnodeID := hex.EncodeToString(nodePub.SerializeCompressed())\n\n\tvar (\n\t\tcloseType      lnrpc.ChannelCloseSummary_ClosureType\n\t\topenInit       lnrpc.Initiator\n\t\tcloseInitiator lnrpc.Initiator\n\t\terr            error\n\t)\n\n\t// Lookup local and remote cooperative initiators. If these values\n\t// are not known they will just return unknown.\n\topenInit, closeInitiator, err = r.getInitiators(\n\t\t&dbChannel.ChanPoint,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Convert the close type to rpc type.\n\tswitch dbChannel.CloseType {\n\tcase channeldb.CooperativeClose:\n\t\tcloseType = lnrpc.ChannelCloseSummary_COOPERATIVE_CLOSE\n\tcase channeldb.LocalForceClose:\n\t\tcloseType = lnrpc.ChannelCloseSummary_LOCAL_FORCE_CLOSE\n\tcase channeldb.RemoteForceClose:\n\t\tcloseType = lnrpc.ChannelCloseSummary_REMOTE_FORCE_CLOSE\n\tcase channeldb.BreachClose:\n\t\tcloseType = lnrpc.ChannelCloseSummary_BREACH_CLOSE\n\tcase channeldb.FundingCanceled:\n\t\tcloseType = lnrpc.ChannelCloseSummary_FUNDING_CANCELED\n\tcase channeldb.Abandoned:\n\t\tcloseType = lnrpc.ChannelCloseSummary_ABANDONED\n\t}\n\n\tdbScid := dbChannel.ShortChanID\n\n\t// Fetch the set of aliases for this channel.\n\tchannelAliases := r.server.aliasMgr.GetAliases(dbScid)\n\n\tchannel := &lnrpc.ChannelCloseSummary{\n\t\tCapacity:          int64(dbChannel.Capacity),\n\t\tRemotePubkey:      nodeID,\n\t\tCloseHeight:       dbChannel.CloseHeight,\n\t\tCloseType:         closeType,\n\t\tChannelPoint:      dbChannel.ChanPoint.String(),\n\t\tChanId:            dbChannel.ShortChanID.ToUint64(),\n\t\tSettledBalance:    int64(dbChannel.SettledBalance),\n\t\tTimeLockedBalance: int64(dbChannel.TimeLockedBalance),\n\t\tChainHash:         dbChannel.ChainHash.String(),\n\t\tClosingTxHash:     dbChannel.ClosingTXID.String(),\n\t\tOpenInitiator:     openInit,\n\t\tCloseInitiator:    closeInitiator,\n\t\tAliasScids:        make([]uint64, 0, len(channelAliases)),\n\t}\n\n\t// Populate the set of aliases.\n\tfor _, chanAlias := range channelAliases {\n\t\tchannel.AliasScids = append(\n\t\t\tchannel.AliasScids, chanAlias.ToUint64(),\n\t\t)\n\t}\n\n\t// Populate any historical data that the summary needs.\n\thistChan, err := r.server.chanStateDB.FetchHistoricalChannel(\n\t\t&dbChannel.ChanPoint,\n\t)\n\tswitch err {\n\t// The channel was closed in a pre-historic version of lnd. Ignore the\n\t// error.\n\tcase channeldb.ErrNoHistoricalBucket:\n\tcase channeldb.ErrChannelNotFound:\n\n\tcase nil:\n\t\tif histChan.IsZeroConf() && histChan.ZeroConfConfirmed() {\n\t\t\t// If the channel was zero-conf, it may have confirmed.\n\t\t\t// Populate the confirmed SCID if so.\n\t\t\tconfirmedScid := histChan.ZeroConfRealScid().ToUint64()\n\t\t\tchannel.ZeroConfConfirmedScid = confirmedScid\n\t\t}\n\n\t// Non-nil error not due to older versions of lnd.\n\tdefault:\n\t\treturn nil, err\n\t}\n\n\treports, err := r.server.miscDB.FetchChannelReports(\n\t\t*r.cfg.ActiveNetParams.GenesisHash, &dbChannel.ChanPoint,\n\t)\n\tswitch err {\n\t// If the channel does not have its resolver outcomes stored,\n\t// ignore it.\n\tcase channeldb.ErrNoChainHashBucket:\n\t\tfallthrough\n\tcase channeldb.ErrNoChannelSummaries:\n\t\treturn channel, nil\n\n\t// If there is no error, fallthrough the switch to process reports.\n\tcase nil:\n\n\t// If another error occurred, return it.\n\tdefault:\n\t\treturn nil, err\n\t}\n\n\tfor _, report := range reports {\n\t\trpcResolution, err := rpcChannelResolution(report)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tchannel.Resolutions = append(channel.Resolutions, rpcResolution)\n\t}\n\n\treturn channel, nil\n}\n",
      "length": 3381,
      "tokens": 336,
      "embedding": []
    },
    {
      "slug": "func rpcChannelResolution(report *channeldb.ResolverReport) (*lnrpc.Resolution,",
      "content": "func rpcChannelResolution(report *channeldb.ResolverReport) (*lnrpc.Resolution,\n\terror) {\n\n\tres := &lnrpc.Resolution{\n\t\tAmountSat: uint64(report.Amount),\n\t\tOutpoint: &lnrpc.OutPoint{\n\t\t\tOutputIndex: report.OutPoint.Index,\n\t\t\tTxidStr:     report.OutPoint.Hash.String(),\n\t\t\tTxidBytes:   report.OutPoint.Hash[:],\n\t\t},\n\t}\n\n\tif report.SpendTxID != nil {\n\t\tres.SweepTxid = report.SpendTxID.String()\n\t}\n\n\tswitch report.ResolverType {\n\tcase channeldb.ResolverTypeAnchor:\n\t\tres.ResolutionType = lnrpc.ResolutionType_ANCHOR\n\n\tcase channeldb.ResolverTypeIncomingHtlc:\n\t\tres.ResolutionType = lnrpc.ResolutionType_INCOMING_HTLC\n\n\tcase channeldb.ResolverTypeOutgoingHtlc:\n\t\tres.ResolutionType = lnrpc.ResolutionType_OUTGOING_HTLC\n\n\tcase channeldb.ResolverTypeCommit:\n\t\tres.ResolutionType = lnrpc.ResolutionType_COMMIT\n\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown resolver type: %v\",\n\t\t\treport.ResolverType)\n\t}\n\n\tswitch report.ResolverOutcome {\n\tcase channeldb.ResolverOutcomeClaimed:\n\t\tres.Outcome = lnrpc.ResolutionOutcome_CLAIMED\n\n\tcase channeldb.ResolverOutcomeUnclaimed:\n\t\tres.Outcome = lnrpc.ResolutionOutcome_UNCLAIMED\n\n\tcase channeldb.ResolverOutcomeAbandoned:\n\t\tres.Outcome = lnrpc.ResolutionOutcome_ABANDONED\n\n\tcase channeldb.ResolverOutcomeFirstStage:\n\t\tres.Outcome = lnrpc.ResolutionOutcome_FIRST_STAGE\n\n\tcase channeldb.ResolverOutcomeTimeout:\n\t\tres.Outcome = lnrpc.ResolutionOutcome_TIMEOUT\n\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown outcome: %v\",\n\t\t\treport.ResolverOutcome)\n\t}\n\n\treturn res, nil\n}\n\n// getInitiators returns an initiator enum that provides information about the\n// party that initiated channel's open and close. This information is obtained\n// from the historical channel bucket, so unknown values are returned when the\n// channel is not present (which indicates that it was closed before we started\n// writing channels to the historical close bucket).",
      "length": 1727,
      "tokens": 156,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) getInitiators(chanPoint *wire.OutPoint) (",
      "content": "func (r *rpcServer) getInitiators(chanPoint *wire.OutPoint) (\n\tlnrpc.Initiator,\n\tlnrpc.Initiator, error) {\n\n\tvar (\n\t\topenInitiator  = lnrpc.Initiator_INITIATOR_UNKNOWN\n\t\tcloseInitiator = lnrpc.Initiator_INITIATOR_UNKNOWN\n\t)\n\n\t// To get the close initiator for cooperative closes, we need\n\t// to get the channel status from the historical channel bucket.\n\thistChan, err := r.server.chanStateDB.FetchHistoricalChannel(chanPoint)\n\tswitch {\n\t// The node has upgraded from a version where we did not store\n\t// historical channels, and has not closed a channel since. Do\n\t// not return an error, initiator values are unknown.\n\tcase err == channeldb.ErrNoHistoricalBucket:\n\t\treturn openInitiator, closeInitiator, nil\n\n\t// The channel was closed before we started storing historical\n\t// channels. Do  not return an error, initiator values are unknown.\n\tcase err == channeldb.ErrChannelNotFound:\n\t\treturn openInitiator, closeInitiator, nil\n\n\tcase err != nil:\n\t\treturn 0, 0, err\n\t}\n\n\t// If we successfully looked up the channel, determine initiator based\n\t// on channels status.\n\tif histChan.IsInitiator {\n\t\topenInitiator = lnrpc.Initiator_INITIATOR_LOCAL\n\t} else {\n\t\topenInitiator = lnrpc.Initiator_INITIATOR_REMOTE\n\t}\n\n\tlocalInit := histChan.HasChanStatus(\n\t\tchanneldb.ChanStatusLocalCloseInitiator,\n\t)\n\n\tremoteInit := histChan.HasChanStatus(\n\t\tchanneldb.ChanStatusRemoteCloseInitiator,\n\t)\n\n\tswitch {\n\t// There is a possible case where closes were attempted by both parties.\n\t// We return the initiator as both in this case to provide full\n\t// information about the close.\n\tcase localInit && remoteInit:\n\t\tcloseInitiator = lnrpc.Initiator_INITIATOR_BOTH\n\n\tcase localInit:\n\t\tcloseInitiator = lnrpc.Initiator_INITIATOR_LOCAL\n\n\tcase remoteInit:\n\t\tcloseInitiator = lnrpc.Initiator_INITIATOR_REMOTE\n\t}\n\n\treturn openInitiator, closeInitiator, nil\n}\n\n// SubscribeChannelEvents returns a uni-directional stream (server -> client)\n// for notifying the client of newly active, inactive or closed channels.",
      "length": 1864,
      "tokens": 235,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) SubscribeChannelEvents(req *lnrpc.ChannelEventSubscription,",
      "content": "func (r *rpcServer) SubscribeChannelEvents(req *lnrpc.ChannelEventSubscription,\n\tupdateStream lnrpc.Lightning_SubscribeChannelEventsServer) error {\n\n\tchannelEventSub, err := r.server.channelNotifier.SubscribeChannelEvents()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Ensure that the resources for the client is cleaned up once either\n\t// the server, or client exits.\n\tdefer channelEventSub.Cancel()\n\n\tfor {\n\t\tselect {\n\t\t// A new update has been sent by the channel router, we'll\n\t\t// marshal it into the form expected by the gRPC client, then\n\t\t// send it off to the client(s).\n\t\tcase e := <-channelEventSub.Updates():\n\t\t\tvar update *lnrpc.ChannelEventUpdate\n\t\t\tswitch event := e.(type) {\n\t\t\tcase channelnotifier.PendingOpenChannelEvent:\n\t\t\t\tupdate = &lnrpc.ChannelEventUpdate{\n\t\t\t\t\tType: lnrpc.ChannelEventUpdate_PENDING_OPEN_CHANNEL,\n\t\t\t\t\tChannel: &lnrpc.ChannelEventUpdate_PendingOpenChannel{\n\t\t\t\t\t\tPendingOpenChannel: &lnrpc.PendingUpdate{\n\t\t\t\t\t\t\tTxid:        event.ChannelPoint.Hash[:],\n\t\t\t\t\t\t\tOutputIndex: event.ChannelPoint.Index,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\tcase channelnotifier.OpenChannelEvent:\n\t\t\t\tchannel, err := createRPCOpenChannel(\n\t\t\t\t\tr, event.Channel, true, false,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\tupdate = &lnrpc.ChannelEventUpdate{\n\t\t\t\t\tType: lnrpc.ChannelEventUpdate_OPEN_CHANNEL,\n\t\t\t\t\tChannel: &lnrpc.ChannelEventUpdate_OpenChannel{\n\t\t\t\t\t\tOpenChannel: channel,\n\t\t\t\t\t},\n\t\t\t\t}\n\n\t\t\tcase channelnotifier.ClosedChannelEvent:\n\t\t\t\tclosedChannel, err := r.createRPCClosedChannel(\n\t\t\t\t\tevent.CloseSummary,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\tupdate = &lnrpc.ChannelEventUpdate{\n\t\t\t\t\tType: lnrpc.ChannelEventUpdate_CLOSED_CHANNEL,\n\t\t\t\t\tChannel: &lnrpc.ChannelEventUpdate_ClosedChannel{\n\t\t\t\t\t\tClosedChannel: closedChannel,\n\t\t\t\t\t},\n\t\t\t\t}\n\n\t\t\tcase channelnotifier.ActiveChannelEvent:\n\t\t\t\tupdate = &lnrpc.ChannelEventUpdate{\n\t\t\t\t\tType: lnrpc.ChannelEventUpdate_ACTIVE_CHANNEL,\n\t\t\t\t\tChannel: &lnrpc.ChannelEventUpdate_ActiveChannel{\n\t\t\t\t\t\tActiveChannel: &lnrpc.ChannelPoint{\n\t\t\t\t\t\t\tFundingTxid: &lnrpc.ChannelPoint_FundingTxidBytes{\n\t\t\t\t\t\t\t\tFundingTxidBytes: event.ChannelPoint.Hash[:],\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tOutputIndex: event.ChannelPoint.Index,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t}\n\n\t\t\tcase channelnotifier.InactiveChannelEvent:\n\t\t\t\tupdate = &lnrpc.ChannelEventUpdate{\n\t\t\t\t\tType: lnrpc.ChannelEventUpdate_INACTIVE_CHANNEL,\n\t\t\t\t\tChannel: &lnrpc.ChannelEventUpdate_InactiveChannel{\n\t\t\t\t\t\tInactiveChannel: &lnrpc.ChannelPoint{\n\t\t\t\t\t\t\tFundingTxid: &lnrpc.ChannelPoint_FundingTxidBytes{\n\t\t\t\t\t\t\t\tFundingTxidBytes: event.ChannelPoint.Hash[:],\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tOutputIndex: event.ChannelPoint.Index,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t}\n\n\t\t\t// Completely ignore ActiveLinkEvent and\n\t\t\t// InactiveLinkEvent as this is explicitly not exposed\n\t\t\t// to the RPC.\n\t\t\tcase channelnotifier.ActiveLinkEvent,\n\t\t\t\tchannelnotifier.InactiveLinkEvent:\n\n\t\t\t\tcontinue\n\n\t\t\tcase channelnotifier.FullyResolvedChannelEvent:\n\t\t\t\tupdate = &lnrpc.ChannelEventUpdate{\n\t\t\t\t\tType: lnrpc.ChannelEventUpdate_FULLY_RESOLVED_CHANNEL,\n\t\t\t\t\tChannel: &lnrpc.ChannelEventUpdate_FullyResolvedChannel{\n\t\t\t\t\t\tFullyResolvedChannel: &lnrpc.ChannelPoint{\n\t\t\t\t\t\t\tFundingTxid: &lnrpc.ChannelPoint_FundingTxidBytes{\n\t\t\t\t\t\t\t\tFundingTxidBytes: event.ChannelPoint.Hash[:],\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tOutputIndex: event.ChannelPoint.Index,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t}\n\n\t\t\tdefault:\n\t\t\t\treturn fmt.Errorf(\"unexpected channel event update: %v\", event)\n\t\t\t}\n\n\t\t\tif err := updateStream.Send(update); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t// The response stream's context for whatever reason has been\n\t\t// closed. If context is closed by an exceeded deadline we will\n\t\t// return an error.\n\t\tcase <-updateStream.Context().Done():\n\t\t\tif errors.Is(updateStream.Context().Err(), context.Canceled) {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn updateStream.Context().Err()\n\n\t\tcase <-r.quit:\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\n// paymentStream enables different types of payment streams, such as:\n// lnrpc.Lightning_SendPaymentServer and lnrpc.Lightning_SendToRouteServer to\n// execute sendPayment. We use this struct as a sort of bridge to enable code\n// re-use between SendPayment and SendToRoute.",
      "length": 3885,
      "tokens": 343,
      "embedding": []
    },
    {
      "slug": "type paymentStream struct {",
      "content": "type paymentStream struct {\n\trecv func() (*rpcPaymentRequest, error)\n\tsend func(*lnrpc.SendResponse) error\n}\n\n// rpcPaymentRequest wraps lnrpc.SendRequest so that routes from\n// lnrpc.SendToRouteRequest can be passed to sendPayment.",
      "length": 199,
      "tokens": 23,
      "embedding": []
    },
    {
      "slug": "type rpcPaymentRequest struct {",
      "content": "type rpcPaymentRequest struct {\n\t*lnrpc.SendRequest\n\troute *route.Route\n}\n\n// SendPayment dispatches a bi-directional streaming RPC for sending payments\n// through the Lightning Network. A single RPC invocation creates a persistent\n// bi-directional stream allowing clients to rapidly send payments through the\n// Lightning Network with a single persistent connection.",
      "length": 329,
      "tokens": 45,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) SendPayment(stream lnrpc.Lightning_SendPaymentServer) error {",
      "content": "func (r *rpcServer) SendPayment(stream lnrpc.Lightning_SendPaymentServer) error {\n\tvar lock sync.Mutex\n\n\treturn r.sendPayment(&paymentStream{\n\t\trecv: func() (*rpcPaymentRequest, error) {\n\t\t\treq, err := stream.Recv()\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\treturn &rpcPaymentRequest{\n\t\t\t\tSendRequest: req,\n\t\t\t}, nil\n\t\t},\n\t\tsend: func(r *lnrpc.SendResponse) error {\n\t\t\t// Calling stream.Send concurrently is not safe.\n\t\t\tlock.Lock()\n\t\t\tdefer lock.Unlock()\n\t\t\treturn stream.Send(r)\n\t\t},\n\t})\n}\n\n// SendToRoute dispatches a bi-directional streaming RPC for sending payments\n// through the Lightning Network via predefined routes passed in. A single RPC\n// invocation creates a persistent bi-directional stream allowing clients to\n// rapidly send payments through the Lightning Network with a single persistent\n// connection.",
      "length": 719,
      "tokens": 97,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) SendToRoute(stream lnrpc.Lightning_SendToRouteServer) error {",
      "content": "func (r *rpcServer) SendToRoute(stream lnrpc.Lightning_SendToRouteServer) error {\n\tvar lock sync.Mutex\n\n\treturn r.sendPayment(&paymentStream{\n\t\trecv: func() (*rpcPaymentRequest, error) {\n\t\t\treq, err := stream.Recv()\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\treturn r.unmarshallSendToRouteRequest(req)\n\t\t},\n\t\tsend: func(r *lnrpc.SendResponse) error {\n\t\t\t// Calling stream.Send concurrently is not safe.\n\t\t\tlock.Lock()\n\t\t\tdefer lock.Unlock()\n\t\t\treturn stream.Send(r)\n\t\t},\n\t})\n}\n\n// unmarshallSendToRouteRequest unmarshalls an rpc sendtoroute request",
      "length": 451,
      "tokens": 53,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) unmarshallSendToRouteRequest(",
      "content": "func (r *rpcServer) unmarshallSendToRouteRequest(\n\treq *lnrpc.SendToRouteRequest) (*rpcPaymentRequest, error) {\n\n\tif req.Route == nil {\n\t\treturn nil, fmt.Errorf(\"unable to send, no route provided\")\n\t}\n\n\troute, err := r.routerBackend.UnmarshallRoute(req.Route)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &rpcPaymentRequest{\n\t\tSendRequest: &lnrpc.SendRequest{\n\t\t\tPaymentHash:       req.PaymentHash,\n\t\t\tPaymentHashString: req.PaymentHashString,\n\t\t},\n\t\troute: route,\n\t}, nil\n}\n\n// rpcPaymentIntent is a small wrapper struct around the of values we can\n// receive from a client over RPC if they wish to send a payment. We'll either\n// extract these fields from a payment request (which may include routing\n// hints), or we'll get a fully populated route from the user that we'll pass\n// directly to the channel router for dispatching.",
      "length": 759,
      "tokens": 110,
      "embedding": []
    },
    {
      "slug": "type rpcPaymentIntent struct {",
      "content": "type rpcPaymentIntent struct {\n\tmsat               lnwire.MilliSatoshi\n\tfeeLimit           lnwire.MilliSatoshi\n\tcltvLimit          uint32\n\tdest               route.Vertex\n\trHash              [32]byte\n\tcltvDelta          uint16\n\trouteHints         [][]zpay32.HopHint\n\toutgoingChannelIDs []uint64\n\tlastHop            *route.Vertex\n\tdestFeatures       *lnwire.FeatureVector\n\tpaymentAddr        *[32]byte\n\tpayReq             []byte\n\tmetadata           []byte\n\n\tdestCustomRecords record.CustomSet\n\n\troute *route.Route\n}\n\n// extractPaymentIntent attempts to parse the complete details required to\n// dispatch a client from the information presented by an RPC client. There are\n// three ways a client can specify their payment details: a payment request,\n// via manual details, or via a complete route.",
      "length": 742,
      "tokens": 77,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) extractPaymentIntent(rpcPayReq *rpcPaymentRequest) (rpcPaymentIntent, error) {",
      "content": "func (r *rpcServer) extractPaymentIntent(rpcPayReq *rpcPaymentRequest) (rpcPaymentIntent, error) {\n\tpayIntent := rpcPaymentIntent{}\n\n\t// If a route was specified, then we can use that directly.\n\tif rpcPayReq.route != nil {\n\t\t// If the user is using the REST interface, then they'll be\n\t\t// passing the payment hash as a hex encoded string.\n\t\tif rpcPayReq.PaymentHashString != \"\" {\n\t\t\tpaymentHash, err := hex.DecodeString(\n\t\t\t\trpcPayReq.PaymentHashString,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn payIntent, err\n\t\t\t}\n\n\t\t\tcopy(payIntent.rHash[:], paymentHash)\n\t\t} else {\n\t\t\tcopy(payIntent.rHash[:], rpcPayReq.PaymentHash)\n\t\t}\n\n\t\tpayIntent.route = rpcPayReq.route\n\t\treturn payIntent, nil\n\t}\n\n\t// If there are no routes specified, pass along a outgoing channel\n\t// restriction if specified. The main server rpc does not support\n\t// multiple channel restrictions.\n\tif rpcPayReq.OutgoingChanId != 0 {\n\t\tpayIntent.outgoingChannelIDs = []uint64{\n\t\t\trpcPayReq.OutgoingChanId,\n\t\t}\n\t}\n\n\t// Pass along a last hop restriction if specified.\n\tif len(rpcPayReq.LastHopPubkey) > 0 {\n\t\tlastHop, err := route.NewVertexFromBytes(\n\t\t\trpcPayReq.LastHopPubkey,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn payIntent, err\n\t\t}\n\t\tpayIntent.lastHop = &lastHop\n\t}\n\n\t// Take the CLTV limit from the request if set, otherwise use the max.\n\tcltvLimit, err := routerrpc.ValidateCLTVLimit(\n\t\trpcPayReq.CltvLimit, r.cfg.MaxOutgoingCltvExpiry,\n\t)\n\tif err != nil {\n\t\treturn payIntent, err\n\t}\n\tpayIntent.cltvLimit = cltvLimit\n\n\tcustomRecords := record.CustomSet(rpcPayReq.DestCustomRecords)\n\tif err := customRecords.Validate(); err != nil {\n\t\treturn payIntent, err\n\t}\n\tpayIntent.destCustomRecords = customRecords\n\n\tvalidateDest := func(dest route.Vertex) error {\n\t\tif rpcPayReq.AllowSelfPayment {\n\t\t\treturn nil\n\t\t}\n\n\t\tif dest == r.selfNode {\n\t\t\treturn errors.New(\"self-payments not allowed\")\n\t\t}\n\n\t\treturn nil\n\t}\n\n\t// If the payment request field isn't blank, then the details of the\n\t// invoice are encoded entirely within the encoded payReq.  So we'll\n\t// attempt to decode it, populating the payment accordingly.\n\tif rpcPayReq.PaymentRequest != \"\" {\n\t\tpayReq, err := zpay32.Decode(\n\t\t\trpcPayReq.PaymentRequest, r.cfg.ActiveNetParams.Params,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn payIntent, err\n\t\t}\n\n\t\t// Next, we'll ensure that this payreq hasn't already expired.\n\t\terr = routerrpc.ValidatePayReqExpiry(payReq)\n\t\tif err != nil {\n\t\t\treturn payIntent, err\n\t\t}\n\n\t\t// If the amount was not included in the invoice, then we let\n\t\t// the payer specify the amount of satoshis they wish to send.\n\t\t// We override the amount to pay with the amount provided from\n\t\t// the payment request.\n\t\tif payReq.MilliSat == nil {\n\t\t\tamt, err := lnrpc.UnmarshallAmt(\n\t\t\t\trpcPayReq.Amt, rpcPayReq.AmtMsat,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn payIntent, err\n\t\t\t}\n\t\t\tif amt == 0 {\n\t\t\t\treturn payIntent, errors.New(\"amount must be \" +\n\t\t\t\t\t\"specified when paying a zero amount \" +\n\t\t\t\t\t\"invoice\")\n\t\t\t}\n\n\t\t\tpayIntent.msat = amt\n\t\t} else {\n\t\t\tpayIntent.msat = *payReq.MilliSat\n\t\t}\n\n\t\t// Calculate the fee limit that should be used for this payment.\n\t\tpayIntent.feeLimit = lnrpc.CalculateFeeLimit(\n\t\t\trpcPayReq.FeeLimit, payIntent.msat,\n\t\t)\n\n\t\tcopy(payIntent.rHash[:], payReq.PaymentHash[:])\n\t\tdestKey := payReq.Destination.SerializeCompressed()\n\t\tcopy(payIntent.dest[:], destKey)\n\t\tpayIntent.cltvDelta = uint16(payReq.MinFinalCLTVExpiry())\n\t\tpayIntent.routeHints = payReq.RouteHints\n\t\tpayIntent.payReq = []byte(rpcPayReq.PaymentRequest)\n\t\tpayIntent.destFeatures = payReq.Features\n\t\tpayIntent.paymentAddr = payReq.PaymentAddr\n\t\tpayIntent.metadata = payReq.Metadata\n\n\t\tif err := validateDest(payIntent.dest); err != nil {\n\t\t\treturn payIntent, err\n\t\t}\n\n\t\t// Do bounds checking with the block padding.\n\t\terr = routing.ValidateCLTVLimit(\n\t\t\tpayIntent.cltvLimit, payIntent.cltvDelta, true,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn payIntent, err\n\t\t}\n\n\t\treturn payIntent, nil\n\t}\n\n\t// At this point, a destination MUST be specified, so we'll convert it\n\t// into the proper representation now. The destination will either be\n\t// encoded as raw bytes, or via a hex string.\n\tvar pubBytes []byte\n\tif len(rpcPayReq.Dest) != 0 {\n\t\tpubBytes = rpcPayReq.Dest\n\t} else {\n\t\tvar err error\n\t\tpubBytes, err = hex.DecodeString(rpcPayReq.DestString)\n\t\tif err != nil {\n\t\t\treturn payIntent, err\n\t\t}\n\t}\n\tif len(pubBytes) != 33 {\n\t\treturn payIntent, errors.New(\"invalid key length\")\n\t}\n\tcopy(payIntent.dest[:], pubBytes)\n\n\tif err := validateDest(payIntent.dest); err != nil {\n\t\treturn payIntent, err\n\t}\n\n\t// Payment address may not be needed by legacy invoices.\n\tif len(rpcPayReq.PaymentAddr) != 0 && len(rpcPayReq.PaymentAddr) != 32 {\n\t\treturn payIntent, errors.New(\"invalid payment address length\")\n\t}\n\n\t// Set the payment address if it was explicitly defined with the\n\t// rpcPaymentRequest.\n\t// Note that the payment address for the payIntent should be nil if none\n\t// was provided with the rpcPaymentRequest.\n\tif len(rpcPayReq.PaymentAddr) != 0 {\n\t\tpayIntent.paymentAddr = &[32]byte{}\n\t\tcopy(payIntent.paymentAddr[:], rpcPayReq.PaymentAddr)\n\t}\n\n\t// Otherwise, If the payment request field was not specified\n\t// (and a custom route wasn't specified), construct the payment\n\t// from the other fields.\n\tpayIntent.msat, err = lnrpc.UnmarshallAmt(\n\t\trpcPayReq.Amt, rpcPayReq.AmtMsat,\n\t)\n\tif err != nil {\n\t\treturn payIntent, err\n\t}\n\n\t// Calculate the fee limit that should be used for this payment.\n\tpayIntent.feeLimit = lnrpc.CalculateFeeLimit(\n\t\trpcPayReq.FeeLimit, payIntent.msat,\n\t)\n\n\tif rpcPayReq.FinalCltvDelta != 0 {\n\t\tpayIntent.cltvDelta = uint16(rpcPayReq.FinalCltvDelta)\n\t} else {\n\t\t// If no final cltv delta is given, assume the default that we\n\t\t// use when creating an invoice. We do not assume the default of\n\t\t// 9 blocks that is defined in BOLT-11, because this is never\n\t\t// enough for other lnd nodes.\n\t\tpayIntent.cltvDelta = uint16(r.cfg.Bitcoin.TimeLockDelta)\n\t}\n\n\t// Do bounds checking with the block padding so the router isn't left\n\t// with a zombie payment in case the user messes up.\n\terr = routing.ValidateCLTVLimit(\n\t\tpayIntent.cltvLimit, payIntent.cltvDelta, true,\n\t)\n\tif err != nil {\n\t\treturn payIntent, err\n\t}\n\n\t// If the user is manually specifying payment details, then the payment\n\t// hash may be encoded as a string.\n\tswitch {\n\tcase rpcPayReq.PaymentHashString != \"\":\n\t\tpaymentHash, err := hex.DecodeString(\n\t\t\trpcPayReq.PaymentHashString,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn payIntent, err\n\t\t}\n\n\t\tcopy(payIntent.rHash[:], paymentHash)\n\n\tdefault:\n\t\tcopy(payIntent.rHash[:], rpcPayReq.PaymentHash)\n\t}\n\n\t// Unmarshal any custom destination features.\n\tpayIntent.destFeatures, err = routerrpc.UnmarshalFeatures(\n\t\trpcPayReq.DestFeatures,\n\t)\n\tif err != nil {\n\t\treturn payIntent, err\n\t}\n\n\treturn payIntent, nil\n}\n",
      "length": 6375,
      "tokens": 868,
      "embedding": []
    },
    {
      "slug": "type paymentIntentResponse struct {",
      "content": "type paymentIntentResponse struct {\n\tRoute    *route.Route\n\tPreimage [32]byte\n\tErr      error\n}\n\n// dispatchPaymentIntent attempts to fully dispatch an RPC payment intent.\n// We'll either pass the payment as a whole to the channel router, or give it a\n// pre-built route. The first error this method returns denotes if we were\n// unable to save the payment. The second error returned denotes if the payment\n// didn't succeed.",
      "length": 380,
      "tokens": 64,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) dispatchPaymentIntent(",
      "content": "func (r *rpcServer) dispatchPaymentIntent(\n\tpayIntent *rpcPaymentIntent) (*paymentIntentResponse, error) {\n\n\t// Construct a payment request to send to the channel router. If the\n\t// payment is successful, the route chosen will be returned. Otherwise,\n\t// we'll get a non-nil error.\n\tvar (\n\t\tpreImage  [32]byte\n\t\troute     *route.Route\n\t\trouterErr error\n\t)\n\n\t// If a route was specified, then we'll pass the route directly to the\n\t// router, otherwise we'll create a payment session to execute it.\n\tif payIntent.route == nil {\n\t\tpayment := &routing.LightningPayment{\n\t\t\tTarget:             payIntent.dest,\n\t\t\tAmount:             payIntent.msat,\n\t\t\tFinalCLTVDelta:     payIntent.cltvDelta,\n\t\t\tFeeLimit:           payIntent.feeLimit,\n\t\t\tCltvLimit:          payIntent.cltvLimit,\n\t\t\tRouteHints:         payIntent.routeHints,\n\t\t\tOutgoingChannelIDs: payIntent.outgoingChannelIDs,\n\t\t\tLastHop:            payIntent.lastHop,\n\t\t\tPaymentRequest:     payIntent.payReq,\n\t\t\tPayAttemptTimeout:  routing.DefaultPayAttemptTimeout,\n\t\t\tDestCustomRecords:  payIntent.destCustomRecords,\n\t\t\tDestFeatures:       payIntent.destFeatures,\n\t\t\tPaymentAddr:        payIntent.paymentAddr,\n\t\t\tMetadata:           payIntent.metadata,\n\n\t\t\t// Don't enable multi-part payments on the main rpc.\n\t\t\t// Users need to use routerrpc for that.\n\t\t\tMaxParts: 1,\n\t\t}\n\t\terr := payment.SetPaymentHash(payIntent.rHash)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tpreImage, route, routerErr = r.server.chanRouter.SendPayment(\n\t\t\tpayment,\n\t\t)\n\t} else {\n\t\tvar attempt *channeldb.HTLCAttempt\n\t\tattempt, routerErr = r.server.chanRouter.SendToRoute(\n\t\t\tpayIntent.rHash, payIntent.route,\n\t\t)\n\n\t\tif routerErr == nil {\n\t\t\tpreImage = attempt.Settle.Preimage\n\t\t}\n\n\t\troute = payIntent.route\n\t}\n\n\t// If the route failed, then we'll return a nil save err, but a non-nil\n\t// routing err.\n\tif routerErr != nil {\n\t\trpcsLog.Warnf(\"Unable to send payment: %v\", routerErr)\n\n\t\treturn &paymentIntentResponse{\n\t\t\tErr: routerErr,\n\t\t}, nil\n\t}\n\n\treturn &paymentIntentResponse{\n\t\tRoute:    route,\n\t\tPreimage: preImage,\n\t}, nil\n}\n\n// sendPayment takes a paymentStream (a source of pre-built routes or payment\n// requests) and continually attempt to dispatch payment requests written to\n// the write end of the stream. Responses will also be streamed back to the\n// client via the write end of the stream. This method is by both SendToRoute\n// and SendPayment as the logic is virtually identical.",
      "length": 2301,
      "tokens": 277,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) sendPayment(stream *paymentStream) error {",
      "content": "func (r *rpcServer) sendPayment(stream *paymentStream) error {\n\tpayChan := make(chan *rpcPaymentIntent)\n\terrChan := make(chan error, 1)\n\n\t// We don't allow payments to be sent while the daemon itself is still\n\t// syncing as we may be trying to sent a payment over a \"stale\"\n\t// channel.\n\tif !r.server.Started() {\n\t\treturn ErrServerNotActive\n\t}\n\n\t// TODO(roasbeef): check payment filter to see if already used?\n\n\t// In order to limit the level of concurrency and prevent a client from\n\t// attempting to OOM the server, we'll set up a semaphore to create an\n\t// upper ceiling on the number of outstanding payments.\n\tconst numOutstandingPayments = 2000\n\thtlcSema := make(chan struct{}, numOutstandingPayments)\n\tfor i := 0; i < numOutstandingPayments; i++ {\n\t\thtlcSema <- struct{}{}\n\t}\n\n\t// We keep track of the running goroutines and set up a quit signal we\n\t// can use to request them to exit if the method returns because of an\n\t// encountered error.\n\tvar wg sync.WaitGroup\n\treqQuit := make(chan struct{})\n\tdefer close(reqQuit)\n\n\t// Launch a new goroutine to handle reading new payment requests from\n\t// the client. This way we can handle errors independently of blocking\n\t// and waiting for the next payment request to come through.\n\t// TODO(joostjager): Callers expect result to come in in the same order\n\t// as the request were sent, but this is far from guarantueed in the\n\t// code below.\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-reqQuit:\n\t\t\t\treturn\n\n\t\t\tdefault:\n\t\t\t\t// Receive the next pending payment within the\n\t\t\t\t// stream sent by the client. If we read the\n\t\t\t\t// EOF sentinel, then the client has closed the\n\t\t\t\t// stream, and we can exit normally.\n\t\t\t\tnextPayment, err := stream.recv()\n\t\t\t\tif err == io.EOF {\n\t\t\t\t\tclose(payChan)\n\t\t\t\t\treturn\n\t\t\t\t} else if err != nil {\n\t\t\t\t\trpcsLog.Errorf(\"Failed receiving from \"+\n\t\t\t\t\t\t\"stream: %v\", err)\n\n\t\t\t\t\tselect {\n\t\t\t\t\tcase errChan <- err:\n\t\t\t\t\tdefault:\n\t\t\t\t\t}\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\t// Populate the next payment, either from the\n\t\t\t\t// payment request, or from the explicitly set\n\t\t\t\t// fields. If the payment proto wasn't well\n\t\t\t\t// formed, then we'll send an error reply and\n\t\t\t\t// wait for the next payment.\n\t\t\t\tpayIntent, err := r.extractPaymentIntent(\n\t\t\t\t\tnextPayment,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\tif err := stream.send(&lnrpc.SendResponse{\n\t\t\t\t\t\tPaymentError: err.Error(),\n\t\t\t\t\t\tPaymentHash:  payIntent.rHash[:],\n\t\t\t\t\t}); err != nil {\n\t\t\t\t\t\trpcsLog.Errorf(\"Failed \"+\n\t\t\t\t\t\t\t\"sending on \"+\n\t\t\t\t\t\t\t\"stream: %v\", err)\n\n\t\t\t\t\t\tselect {\n\t\t\t\t\t\tcase errChan <- err:\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\t// If the payment was well formed, then we'll\n\t\t\t\t// send to the dispatch goroutine, or exit,\n\t\t\t\t// which ever comes first.\n\t\t\t\tselect {\n\t\t\t\tcase payChan <- &payIntent:\n\t\t\t\tcase <-reqQuit:\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\nsendLoop:\n\tfor {\n\t\tselect {\n\n\t\t// If we encounter and error either during sending or\n\t\t// receiving, we return directly, closing the stream.\n\t\tcase err := <-errChan:\n\t\t\treturn err\n\n\t\tcase <-r.quit:\n\t\t\treturn errors.New(\"rpc server shutting down\")\n\n\t\tcase payIntent, ok := <-payChan:\n\t\t\t// If the receive loop is done, we break the send loop\n\t\t\t// and wait for the ongoing payments to finish before\n\t\t\t// exiting.\n\t\t\tif !ok {\n\t\t\t\tbreak sendLoop\n\t\t\t}\n\n\t\t\t// We launch a new goroutine to execute the current\n\t\t\t// payment so we can continue to serve requests while\n\t\t\t// this payment is being dispatched.\n\t\t\twg.Add(1)\n\t\t\tgo func(payIntent *rpcPaymentIntent) {\n\t\t\t\tdefer wg.Done()\n\n\t\t\t\t// Attempt to grab a free semaphore slot, using\n\t\t\t\t// a defer to eventually release the slot\n\t\t\t\t// regardless of payment success.\n\t\t\t\tselect {\n\t\t\t\tcase <-htlcSema:\n\t\t\t\tcase <-reqQuit:\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tdefer func() {\n\t\t\t\t\thtlcSema <- struct{}{}\n\t\t\t\t}()\n\n\t\t\t\tresp, saveErr := r.dispatchPaymentIntent(\n\t\t\t\t\tpayIntent,\n\t\t\t\t)\n\n\t\t\t\tswitch {\n\t\t\t\t// If we were unable to save the state of the\n\t\t\t\t// payment, then we'll return the error to the\n\t\t\t\t// user, and terminate.\n\t\t\t\tcase saveErr != nil:\n\t\t\t\t\trpcsLog.Errorf(\"Failed dispatching \"+\n\t\t\t\t\t\t\"payment intent: %v\", saveErr)\n\n\t\t\t\t\tselect {\n\t\t\t\t\tcase errChan <- saveErr:\n\t\t\t\t\tdefault:\n\t\t\t\t\t}\n\t\t\t\t\treturn\n\n\t\t\t\t// If we receive payment error than, instead of\n\t\t\t\t// terminating the stream, send error response\n\t\t\t\t// to the user.\n\t\t\t\tcase resp.Err != nil:\n\t\t\t\t\terr := stream.send(&lnrpc.SendResponse{\n\t\t\t\t\t\tPaymentError: resp.Err.Error(),\n\t\t\t\t\t\tPaymentHash:  payIntent.rHash[:],\n\t\t\t\t\t})\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\trpcsLog.Errorf(\"Failed \"+\n\t\t\t\t\t\t\t\"sending error \"+\n\t\t\t\t\t\t\t\"response: %v\", err)\n\n\t\t\t\t\t\tselect {\n\t\t\t\t\t\tcase errChan <- err:\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tbackend := r.routerBackend\n\t\t\t\tmarshalledRouted, err := backend.MarshallRoute(\n\t\t\t\t\tresp.Route,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\terrChan <- err\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\terr = stream.send(&lnrpc.SendResponse{\n\t\t\t\t\tPaymentHash:     payIntent.rHash[:],\n\t\t\t\t\tPaymentPreimage: resp.Preimage[:],\n\t\t\t\t\tPaymentRoute:    marshalledRouted,\n\t\t\t\t})\n\t\t\t\tif err != nil {\n\t\t\t\t\trpcsLog.Errorf(\"Failed sending \"+\n\t\t\t\t\t\t\"response: %v\", err)\n\n\t\t\t\t\tselect {\n\t\t\t\t\tcase errChan <- err:\n\t\t\t\t\tdefault:\n\t\t\t\t\t}\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}(payIntent)\n\t\t}\n\t}\n\n\t// Wait for all goroutines to finish before closing the stream.\n\twg.Wait()\n\treturn nil\n}\n\n// SendPaymentSync is the synchronous non-streaming version of SendPayment.\n// This RPC is intended to be consumed by clients of the REST proxy.\n// Additionally, this RPC expects the destination's public key and the payment\n// hash (if any) to be encoded as hex strings.",
      "length": 5268,
      "tokens": 783,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) SendPaymentSync(ctx context.Context,",
      "content": "func (r *rpcServer) SendPaymentSync(ctx context.Context,\n\tnextPayment *lnrpc.SendRequest) (*lnrpc.SendResponse, error) {\n\n\treturn r.sendPaymentSync(ctx, &rpcPaymentRequest{\n\t\tSendRequest: nextPayment,\n\t})\n}\n\n// SendToRouteSync is the synchronous non-streaming version of SendToRoute.\n// This RPC is intended to be consumed by clients of the REST proxy.\n// Additionally, this RPC expects the payment hash (if any) to be encoded as\n// hex strings.",
      "length": 378,
      "tokens": 52,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) SendToRouteSync(ctx context.Context,",
      "content": "func (r *rpcServer) SendToRouteSync(ctx context.Context,\n\treq *lnrpc.SendToRouteRequest) (*lnrpc.SendResponse, error) {\n\n\tif req.Route == nil {\n\t\treturn nil, fmt.Errorf(\"unable to send, no routes provided\")\n\t}\n\n\tpaymentRequest, err := r.unmarshallSendToRouteRequest(req)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn r.sendPaymentSync(ctx, paymentRequest)\n}\n\n// sendPaymentSync is the synchronous variant of sendPayment. It will block and\n// wait until the payment has been fully completed.",
      "length": 419,
      "tokens": 57,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) sendPaymentSync(ctx context.Context,",
      "content": "func (r *rpcServer) sendPaymentSync(ctx context.Context,\n\tnextPayment *rpcPaymentRequest) (*lnrpc.SendResponse, error) {\n\n\t// We don't allow payments to be sent while the daemon itself is still\n\t// syncing as we may be trying to sent a payment over a \"stale\"\n\t// channel.\n\tif !r.server.Started() {\n\t\treturn nil, ErrServerNotActive\n\t}\n\n\t// First we'll attempt to map the proto describing the next payment to\n\t// an intent that we can pass to local sub-systems.\n\tpayIntent, err := r.extractPaymentIntent(nextPayment)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// With the payment validated, we'll now attempt to dispatch the\n\t// payment.\n\tresp, saveErr := r.dispatchPaymentIntent(&payIntent)\n\tswitch {\n\tcase saveErr != nil:\n\t\treturn nil, saveErr\n\n\tcase resp.Err != nil:\n\t\treturn &lnrpc.SendResponse{\n\t\t\tPaymentError: resp.Err.Error(),\n\t\t\tPaymentHash:  payIntent.rHash[:],\n\t\t}, nil\n\t}\n\n\trpcRoute, err := r.routerBackend.MarshallRoute(resp.Route)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &lnrpc.SendResponse{\n\t\tPaymentHash:     payIntent.rHash[:],\n\t\tPaymentPreimage: resp.Preimage[:],\n\t\tPaymentRoute:    rpcRoute,\n\t}, nil\n}\n\n// AddInvoice attempts to add a new invoice to the invoice database. Any\n// duplicated invoices are rejected, therefore all invoices *must* have a\n// unique payment preimage.",
      "length": 1197,
      "tokens": 169,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) AddInvoice(ctx context.Context,",
      "content": "func (r *rpcServer) AddInvoice(ctx context.Context,\n\tinvoice *lnrpc.Invoice) (*lnrpc.AddInvoiceResponse, error) {\n\n\tdefaultDelta := r.cfg.Bitcoin.TimeLockDelta\n\tif r.cfg.registeredChains.PrimaryChain() == chainreg.LitecoinChain {\n\t\tdefaultDelta = r.cfg.Litecoin.TimeLockDelta\n\t}\n\n\taddInvoiceCfg := &invoicesrpc.AddInvoiceConfig{\n\t\tAddInvoice:        r.server.invoices.AddInvoice,\n\t\tIsChannelActive:   r.server.htlcSwitch.HasActiveLink,\n\t\tChainParams:       r.cfg.ActiveNetParams.Params,\n\t\tNodeSigner:        r.server.nodeSigner,\n\t\tDefaultCLTVExpiry: defaultDelta,\n\t\tChanDB:            r.server.chanStateDB,\n\t\tGraph:             r.server.graphDB,\n\t\tGenInvoiceFeatures: func() *lnwire.FeatureVector {\n\t\t\treturn r.server.featureMgr.Get(feature.SetInvoice)\n\t\t},\n\t\tGenAmpInvoiceFeatures: func() *lnwire.FeatureVector {\n\t\t\treturn r.server.featureMgr.Get(feature.SetInvoiceAmp)\n\t\t},\n\t\tGetAlias: r.server.aliasMgr.GetPeerAlias,\n\t}\n\n\tvalue, err := lnrpc.UnmarshallAmt(invoice.Value, invoice.ValueMsat)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Convert the passed routing hints to the required format.\n\trouteHints, err := invoicesrpc.CreateZpay32HopHints(invoice.RouteHints)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\taddInvoiceData := &invoicesrpc.AddInvoiceData{\n\t\tMemo:            invoice.Memo,\n\t\tValue:           value,\n\t\tDescriptionHash: invoice.DescriptionHash,\n\t\tExpiry:          invoice.Expiry,\n\t\tFallbackAddr:    invoice.FallbackAddr,\n\t\tCltvExpiry:      invoice.CltvExpiry,\n\t\tPrivate:         invoice.Private,\n\t\tRouteHints:      routeHints,\n\t\tAmp:             invoice.IsAmp,\n\t}\n\n\tif invoice.RPreimage != nil {\n\t\tpreimage, err := lntypes.MakePreimage(invoice.RPreimage)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\taddInvoiceData.Preimage = &preimage\n\t}\n\n\thash, dbInvoice, err := invoicesrpc.AddInvoice(\n\t\tctx, addInvoiceCfg, addInvoiceData,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &lnrpc.AddInvoiceResponse{\n\t\tAddIndex:       dbInvoice.AddIndex,\n\t\tPaymentRequest: string(dbInvoice.PaymentRequest),\n\t\tRHash:          hash[:],\n\t\tPaymentAddr:    dbInvoice.Terms.PaymentAddr[:],\n\t}, nil\n}\n\n// LookupInvoice attempts to look up an invoice according to its payment hash.\n// The passed payment hash *must* be exactly 32 bytes, if not an error is\n// returned.",
      "length": 2132,
      "tokens": 193,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) LookupInvoice(ctx context.Context,",
      "content": "func (r *rpcServer) LookupInvoice(ctx context.Context,\n\treq *lnrpc.PaymentHash) (*lnrpc.Invoice, error) {\n\n\tvar (\n\t\tpayHash [32]byte\n\t\trHash   []byte\n\t\terr     error\n\t)\n\n\t// If the RHash as a raw string was provided, then decode that and use\n\t// that directly. Otherwise, we use the raw bytes provided.\n\tif req.RHashStr != \"\" {\n\t\trHash, err = hex.DecodeString(req.RHashStr)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else {\n\t\trHash = req.RHash\n\t}\n\n\t// Ensure that the payment hash is *exactly* 32-bytes.\n\tif len(rHash) != 0 && len(rHash) != 32 {\n\t\treturn nil, fmt.Errorf(\"payment hash must be exactly \"+\n\t\t\t\"32 bytes, is instead %v\", len(rHash))\n\t}\n\tcopy(payHash[:], rHash)\n\n\trpcsLog.Tracef(\"[lookupinvoice] searching for invoice %x\", payHash[:])\n\n\tinvoice, err := r.server.invoices.LookupInvoice(payHash)\n\tswitch {\n\tcase errors.Is(err, invoices.ErrInvoiceNotFound):\n\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\tcase err != nil:\n\t\treturn nil, err\n\t}\n\n\trpcsLog.Tracef(\"[lookupinvoice] located invoice %v\",\n\t\tnewLogClosure(func() string {\n\t\t\treturn spew.Sdump(invoice)\n\t\t}))\n\n\trpcInvoice, err := invoicesrpc.CreateRPCInvoice(\n\t\t&invoice, r.cfg.ActiveNetParams.Params,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn rpcInvoice, nil\n}\n\n// ListInvoices returns a list of all the invoices currently stored within the\n// database. Any active debug invoices are ignored.",
      "length": 1271,
      "tokens": 177,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) ListInvoices(ctx context.Context,",
      "content": "func (r *rpcServer) ListInvoices(ctx context.Context,\n\treq *lnrpc.ListInvoiceRequest) (*lnrpc.ListInvoiceResponse, error) {\n\n\t// If the number of invoices was not specified, then we'll default to\n\t// returning the latest 100 invoices.\n\tif req.NumMaxInvoices == 0 {\n\t\treq.NumMaxInvoices = 100\n\t}\n\n\t// If both dates are set, we check that the start date is less than the\n\t// end date, otherwise we'll get an empty result.\n\tif req.CreationDateStart != 0 && req.CreationDateEnd != 0 {\n\t\tif req.CreationDateStart >= req.CreationDateEnd {\n\t\t\treturn nil, fmt.Errorf(\"start date(%v) must be before \"+\n\t\t\t\t\"end date(%v)\", req.CreationDateStart,\n\t\t\t\treq.CreationDateEnd)\n\t\t}\n\t}\n\n\t// Next, we'll map the proto request into a format that is understood by\n\t// the database.\n\tq := invoices.InvoiceQuery{\n\t\tIndexOffset:    req.IndexOffset,\n\t\tNumMaxInvoices: req.NumMaxInvoices,\n\t\tPendingOnly:    req.PendingOnly,\n\t\tReversed:       req.Reversed,\n\t}\n\n\t// Attach the start date if set.\n\tif req.CreationDateStart != 0 {\n\t\tq.CreationDateStart = time.Unix(int64(req.CreationDateStart), 0)\n\t}\n\n\t// Attach the end date if set.\n\tif req.CreationDateEnd != 0 {\n\t\tq.CreationDateEnd = time.Unix(int64(req.CreationDateEnd), 0)\n\t}\n\n\tinvoiceSlice, err := r.server.miscDB.QueryInvoices(q)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to query invoices: %v\", err)\n\t}\n\n\t// Before returning the response, we'll need to convert each invoice\n\t// into it's proto representation.\n\tresp := &lnrpc.ListInvoiceResponse{\n\t\tInvoices:         make([]*lnrpc.Invoice, len(invoiceSlice.Invoices)),\n\t\tFirstIndexOffset: invoiceSlice.FirstIndexOffset,\n\t\tLastIndexOffset:  invoiceSlice.LastIndexOffset,\n\t}\n\tfor i, invoice := range invoiceSlice.Invoices {\n\t\tinvoice := invoice\n\t\tresp.Invoices[i], err = invoicesrpc.CreateRPCInvoice(\n\t\t\t&invoice, r.cfg.ActiveNetParams.Params,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn resp, nil\n}\n\n// SubscribeInvoices returns a uni-directional stream (server -> client) for\n// notifying the client of newly added/settled invoices.",
      "length": 1914,
      "tokens": 243,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) SubscribeInvoices(req *lnrpc.InvoiceSubscription,",
      "content": "func (r *rpcServer) SubscribeInvoices(req *lnrpc.InvoiceSubscription,\n\tupdateStream lnrpc.Lightning_SubscribeInvoicesServer) error {\n\n\tinvoiceClient, err := r.server.invoices.SubscribeNotifications(\n\t\treq.AddIndex, req.SettleIndex,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer invoiceClient.Cancel()\n\n\tfor {\n\t\tselect {\n\t\tcase newInvoice := <-invoiceClient.NewInvoices:\n\t\t\trpcInvoice, err := invoicesrpc.CreateRPCInvoice(\n\t\t\t\tnewInvoice, r.cfg.ActiveNetParams.Params,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tif err := updateStream.Send(rpcInvoice); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\tcase settledInvoice := <-invoiceClient.SettledInvoices:\n\t\t\trpcInvoice, err := invoicesrpc.CreateRPCInvoice(\n\t\t\t\tsettledInvoice, r.cfg.ActiveNetParams.Params,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tif err := updateStream.Send(rpcInvoice); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t// The response stream's context for whatever reason has been\n\t\t// closed. If context is closed by an exceeded deadline we will\n\t\t// return an error.\n\t\tcase <-updateStream.Context().Done():\n\t\t\tif errors.Is(updateStream.Context().Err(), context.Canceled) {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn updateStream.Context().Err()\n\n\t\tcase <-r.quit:\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\n// SubscribeTransactions creates a uni-directional stream (server -> client) in\n// which any newly discovered transactions relevant to the wallet are sent\n// over.",
      "length": 1270,
      "tokens": 153,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) SubscribeTransactions(req *lnrpc.GetTransactionsRequest,",
      "content": "func (r *rpcServer) SubscribeTransactions(req *lnrpc.GetTransactionsRequest,\n\tupdateStream lnrpc.Lightning_SubscribeTransactionsServer) error {\n\n\ttxClient, err := r.server.cc.Wallet.SubscribeTransactions()\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer txClient.Cancel()\n\trpcsLog.Infof(\"New transaction subscription\")\n\n\tfor {\n\t\tselect {\n\t\tcase tx := <-txClient.ConfirmedTransactions():\n\t\t\tdetail := lnrpc.RPCTransaction(tx)\n\t\t\tif err := updateStream.Send(detail); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\tcase tx := <-txClient.UnconfirmedTransactions():\n\t\t\tdetail := lnrpc.RPCTransaction(tx)\n\t\t\tif err := updateStream.Send(detail); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t// The response stream's context for whatever reason has been\n\t\t// closed. If context is closed by an exceeded deadline we will\n\t\t// return an error.\n\t\tcase <-updateStream.Context().Done():\n\t\t\trpcsLog.Infof(\"Canceling transaction subscription\")\n\t\t\tif errors.Is(updateStream.Context().Err(), context.Canceled) {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn updateStream.Context().Err()\n\n\t\tcase <-r.quit:\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\n// GetTransactions returns a list of describing all the known transactions\n// relevant to the wallet.",
      "length": 1052,
      "tokens": 124,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) GetTransactions(ctx context.Context,",
      "content": "func (r *rpcServer) GetTransactions(ctx context.Context,\n\treq *lnrpc.GetTransactionsRequest) (*lnrpc.TransactionDetails, error) {\n\n\t// To remain backwards compatible with the old api, default to the\n\t// special case end height which will return transactions from the start\n\t// height until the chain tip, including unconfirmed transactions.\n\tvar endHeight = btcwallet.UnconfirmedHeight\n\n\t// If the user has provided an end height, we overwrite our default.\n\tif req.EndHeight != 0 {\n\t\tendHeight = req.EndHeight\n\t}\n\n\ttransactions, err := r.server.cc.Wallet.ListTransactionDetails(\n\t\treq.StartHeight, endHeight, req.Account,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn lnrpc.RPCTransactionDetails(transactions), nil\n}\n\n// DescribeGraph returns a description of the latest graph state from the PoV\n// of the node. The graph information is partitioned into two components: all\n// the nodes/vertexes, and all the edges that connect the vertexes themselves.\n// As this is a directed graph, the edges also contain the node directional\n// specific routing policy which includes: the time lock delta, fee\n// information, etc.",
      "length": 1037,
      "tokens": 151,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) DescribeGraph(ctx context.Context,",
      "content": "func (r *rpcServer) DescribeGraph(ctx context.Context,\n\treq *lnrpc.ChannelGraphRequest) (*lnrpc.ChannelGraph, error) {\n\n\tresp := &lnrpc.ChannelGraph{}\n\tincludeUnannounced := req.IncludeUnannounced\n\n\t// Check to see if the cache is already populated, if so then we can\n\t// just return it directly.\n\t//\n\t// TODO(roasbeef): move this to an interceptor level feature?\n\tgraphCacheActive := r.cfg.Caches.RPCGraphCacheDuration != 0\n\tif graphCacheActive {\n\t\tr.graphCache.Lock()\n\t\tdefer r.graphCache.Unlock()\n\n\t\tif r.describeGraphResp != nil {\n\t\t\treturn r.describeGraphResp, nil\n\t\t}\n\t}\n\n\t// Obtain the pointer to the global singleton channel graph, this will\n\t// provide a consistent view of the graph due to bolt db's\n\t// transactional model.\n\tgraph := r.server.graphDB\n\n\t// First iterate through all the known nodes (connected or unconnected\n\t// within the graph), collating their current state into the RPC\n\t// response.\n\terr := graph.ForEachNode(func(_ kvdb.RTx, node *channeldb.LightningNode) error {\n\t\tlnNode := marshalNode(node)\n\n\t\tresp.Nodes = append(resp.Nodes, lnNode)\n\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Next, for each active channel we know of within the graph, create a\n\t// similar response which details both the edge information as well as\n\t// the routing policies of th nodes connecting the two edges.\n\terr = graph.ForEachChannel(func(edgeInfo *channeldb.ChannelEdgeInfo,\n\t\tc1, c2 *channeldb.ChannelEdgePolicy) error {\n\n\t\t// Do not include unannounced channels unless specifically\n\t\t// requested. Unannounced channels include both private channels as\n\t\t// well as public channels whose authentication proof were not\n\t\t// confirmed yet, hence were not announced.\n\t\tif !includeUnannounced && edgeInfo.AuthProof == nil {\n\t\t\treturn nil\n\t\t}\n\n\t\tedge := marshalDbEdge(edgeInfo, c1, c2)\n\t\tresp.Edges = append(resp.Edges, edge)\n\n\t\treturn nil\n\t})\n\tif err != nil && err != channeldb.ErrGraphNoEdgesFound {\n\t\treturn nil, err\n\t}\n\n\t// We still have the mutex held, so we can safely populate the cache\n\t// now to save on GC churn for this query, but only if the cache isn't\n\t// disabled.\n\tif graphCacheActive {\n\t\tr.describeGraphResp = resp\n\t}\n\n\treturn resp, nil\n}\n\n// marshalExtraOpaqueData marshals the given tlv data. If the tlv stream is\n// malformed or empty, an empty map is returned. This makes the method safe to\n// use on unvalidated data.",
      "length": 2236,
      "tokens": 333,
      "embedding": []
    },
    {
      "slug": "func marshalExtraOpaqueData(data []byte) map[uint64][]byte {",
      "content": "func marshalExtraOpaqueData(data []byte) map[uint64][]byte {\n\tr := bytes.NewReader(data)\n\n\ttlvStream, err := tlv.NewStream()\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\t// Since ExtraOpaqueData is provided by a potentially malicious peer,\n\t// pass it into the P2P decoding variant.\n\tparsedTypes, err := tlvStream.DecodeWithParsedTypesP2P(r)\n\tif err != nil || len(parsedTypes) == 0 {\n\t\treturn nil\n\t}\n\n\trecords := make(map[uint64][]byte)\n\tfor k, v := range parsedTypes {\n\t\trecords[uint64(k)] = v\n\t}\n\n\treturn records\n}\n",
      "length": 425,
      "tokens": 66,
      "embedding": []
    },
    {
      "slug": "func marshalDbEdge(edgeInfo *channeldb.ChannelEdgeInfo,",
      "content": "func marshalDbEdge(edgeInfo *channeldb.ChannelEdgeInfo,\n\tc1, c2 *channeldb.ChannelEdgePolicy) *lnrpc.ChannelEdge {\n\n\t// Make sure the policies match the node they belong to. c1 should point\n\t// to the policy for NodeKey1, and c2 for NodeKey2.\n\tif c1 != nil && c1.ChannelFlags&lnwire.ChanUpdateDirection == 1 ||\n\t\tc2 != nil && c2.ChannelFlags&lnwire.ChanUpdateDirection == 0 {\n\n\t\tc2, c1 = c1, c2\n\t}\n\n\tvar lastUpdate int64\n\tif c1 != nil {\n\t\tlastUpdate = c1.LastUpdate.Unix()\n\t}\n\tif c2 != nil && c2.LastUpdate.Unix() > lastUpdate {\n\t\tlastUpdate = c2.LastUpdate.Unix()\n\t}\n\n\tcustomRecords := marshalExtraOpaqueData(edgeInfo.ExtraOpaqueData)\n\n\tedge := &lnrpc.ChannelEdge{\n\t\tChannelId: edgeInfo.ChannelID,\n\t\tChanPoint: edgeInfo.ChannelPoint.String(),\n\t\t// TODO(roasbeef): update should be on edge info itself\n\t\tLastUpdate:    uint32(lastUpdate),\n\t\tNode1Pub:      hex.EncodeToString(edgeInfo.NodeKey1Bytes[:]),\n\t\tNode2Pub:      hex.EncodeToString(edgeInfo.NodeKey2Bytes[:]),\n\t\tCapacity:      int64(edgeInfo.Capacity),\n\t\tCustomRecords: customRecords,\n\t}\n\n\tif c1 != nil {\n\t\tedge.Node1Policy = marshalDBRoutingPolicy(c1)\n\t}\n\n\tif c2 != nil {\n\t\tedge.Node2Policy = marshalDBRoutingPolicy(c2)\n\t}\n\n\treturn edge\n}\n",
      "length": 1100,
      "tokens": 128,
      "embedding": []
    },
    {
      "slug": "func marshalDBRoutingPolicy(",
      "content": "func marshalDBRoutingPolicy(\n\tpolicy *channeldb.ChannelEdgePolicy) *lnrpc.RoutingPolicy {\n\n\tdisabled := policy.ChannelFlags&lnwire.ChanUpdateDisabled != 0\n\n\tcustomRecords := marshalExtraOpaqueData(policy.ExtraOpaqueData)\n\n\treturn &lnrpc.RoutingPolicy{\n\t\tTimeLockDelta:    uint32(policy.TimeLockDelta),\n\t\tMinHtlc:          int64(policy.MinHTLC),\n\t\tMaxHtlcMsat:      uint64(policy.MaxHTLC),\n\t\tFeeBaseMsat:      int64(policy.FeeBaseMSat),\n\t\tFeeRateMilliMsat: int64(policy.FeeProportionalMillionths),\n\t\tDisabled:         disabled,\n\t\tLastUpdate:       uint32(policy.LastUpdate.Unix()),\n\t\tCustomRecords:    customRecords,\n\t}\n}\n\n// GetNodeMetrics returns all available node metrics calculated from the\n// current channel graph.",
      "length": 672,
      "tokens": 46,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) GetNodeMetrics(ctx context.Context,",
      "content": "func (r *rpcServer) GetNodeMetrics(ctx context.Context,\n\treq *lnrpc.NodeMetricsRequest) (*lnrpc.NodeMetricsResponse, error) {\n\n\t// Get requested metric types.\n\tgetCentrality := false\n\tfor _, t := range req.Types {\n\t\tif t == lnrpc.NodeMetricType_BETWEENNESS_CENTRALITY {\n\t\t\tgetCentrality = true\n\t\t}\n\t}\n\n\t// Only centrality can be requested for now.\n\tif !getCentrality {\n\t\treturn nil, nil\n\t}\n\n\tresp := &lnrpc.NodeMetricsResponse{\n\t\tBetweennessCentrality: make(map[string]*lnrpc.FloatMetric),\n\t}\n\n\t// Obtain the pointer to the global singleton channel graph, this will\n\t// provide a consistent view of the graph due to bolt db's\n\t// transactional model.\n\tgraph := r.server.graphDB\n\n\t// Calculate betweenness centrality if requested. Note that depending on the\n\t// graph size, this may take up to a few minutes.\n\tchannelGraph := autopilot.ChannelGraphFromDatabase(graph)\n\tcentralityMetric, err := autopilot.NewBetweennessCentralityMetric(\n\t\truntime.NumCPU(),\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := centralityMetric.Refresh(channelGraph); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Fill normalized and non normalized centrality.\n\tcentrality := centralityMetric.GetMetric(true)\n\tfor nodeID, val := range centrality {\n\t\tresp.BetweennessCentrality[hex.EncodeToString(nodeID[:])] =\n\t\t\t&lnrpc.FloatMetric{\n\t\t\t\tNormalizedValue: val,\n\t\t\t}\n\t}\n\n\tcentrality = centralityMetric.GetMetric(false)\n\tfor nodeID, val := range centrality {\n\t\tresp.BetweennessCentrality[hex.EncodeToString(nodeID[:])].Value = val\n\t}\n\n\treturn resp, nil\n}\n\n// GetChanInfo returns the latest authenticated network announcement for the\n// given channel identified by its channel ID: an 8-byte integer which uniquely\n// identifies the location of transaction's funding output within the block\n// chain.",
      "length": 1653,
      "tokens": 211,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) GetChanInfo(ctx context.Context,",
      "content": "func (r *rpcServer) GetChanInfo(ctx context.Context,\n\tin *lnrpc.ChanInfoRequest) (*lnrpc.ChannelEdge, error) {\n\n\tgraph := r.server.graphDB\n\n\tedgeInfo, edge1, edge2, err := graph.FetchChannelEdgesByID(in.ChanId)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Convert the database's edge format into the network/RPC edge format\n\t// which couples the edge itself along with the directional node\n\t// routing policies of each node involved within the channel.\n\tchannelEdge := marshalDbEdge(edgeInfo, edge1, edge2)\n\n\treturn channelEdge, nil\n}\n\n// GetNodeInfo returns the latest advertised and aggregate authenticated\n// channel information for the specified node identified by its public key.",
      "length": 611,
      "tokens": 85,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) GetNodeInfo(ctx context.Context,",
      "content": "func (r *rpcServer) GetNodeInfo(ctx context.Context,\n\tin *lnrpc.NodeInfoRequest) (*lnrpc.NodeInfo, error) {\n\n\tgraph := r.server.graphDB\n\n\t// First, parse the hex-encoded public key into a full in-memory public\n\t// key object we can work with for querying.\n\tpubKey, err := route.NewVertexFromStr(in.PubKey)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// With the public key decoded, attempt to fetch the node corresponding\n\t// to this public key. If the node cannot be found, then an error will\n\t// be returned.\n\tnode, err := graph.FetchLightningNode(pubKey)\n\tswitch {\n\tcase err == channeldb.ErrGraphNodeNotFound:\n\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\tcase err != nil:\n\t\treturn nil, err\n\t}\n\n\t// With the node obtained, we'll now iterate through all its out going\n\t// edges to gather some basic statistics about its out going channels.\n\tvar (\n\t\tnumChannels   uint32\n\t\ttotalCapacity btcutil.Amount\n\t\tchannels      []*lnrpc.ChannelEdge\n\t)\n\n\tif err := node.ForEachChannel(nil, func(_ kvdb.RTx,\n\t\tedge *channeldb.ChannelEdgeInfo,\n\t\tc1, c2 *channeldb.ChannelEdgePolicy) error {\n\n\t\tnumChannels++\n\t\ttotalCapacity += edge.Capacity\n\n\t\t// Only populate the node's channels if the user requested them.\n\t\tif in.IncludeChannels {\n\t\t\t// Do not include unannounced channels - private\n\t\t\t// channels or public channels whose authentication\n\t\t\t// proof were not confirmed yet.\n\t\t\tif edge.AuthProof == nil {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\t// Convert the database's edge format into the\n\t\t\t// network/RPC edge format.\n\t\t\tchannelEdge := marshalDbEdge(edge, c1, c2)\n\t\t\tchannels = append(channels, channelEdge)\n\t\t}\n\n\t\treturn nil\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &lnrpc.NodeInfo{\n\t\tNode:          marshalNode(node),\n\t\tNumChannels:   numChannels,\n\t\tTotalCapacity: int64(totalCapacity),\n\t\tChannels:      channels,\n\t}, nil\n}\n",
      "length": 1703,
      "tokens": 234,
      "embedding": []
    },
    {
      "slug": "func marshalNode(node *channeldb.LightningNode) *lnrpc.LightningNode {",
      "content": "func marshalNode(node *channeldb.LightningNode) *lnrpc.LightningNode {\n\tnodeAddrs := make([]*lnrpc.NodeAddress, len(node.Addresses))\n\tfor i, addr := range node.Addresses {\n\t\tnodeAddr := &lnrpc.NodeAddress{\n\t\t\tNetwork: addr.Network(),\n\t\t\tAddr:    addr.String(),\n\t\t}\n\t\tnodeAddrs[i] = nodeAddr\n\t}\n\n\tfeatures := invoicesrpc.CreateRPCFeatures(node.Features)\n\n\tcustomRecords := marshalExtraOpaqueData(node.ExtraOpaqueData)\n\n\treturn &lnrpc.LightningNode{\n\t\tLastUpdate:    uint32(node.LastUpdate.Unix()),\n\t\tPubKey:        hex.EncodeToString(node.PubKeyBytes[:]),\n\t\tAddresses:     nodeAddrs,\n\t\tAlias:         node.Alias,\n\t\tColor:         routing.EncodeHexColor(node.Color),\n\t\tFeatures:      features,\n\t\tCustomRecords: customRecords,\n\t}\n}\n\n// QueryRoutes attempts to query the daemons' Channel Router for a possible\n// route to a target destination capable of carrying a specific amount of\n// satoshis within the route's flow. The returned route contains the full\n// details required to craft and send an HTLC, also including the necessary\n// information that should be present within the Sphinx packet encapsulated\n// within the HTLC.\n//\n// TODO(roasbeef): should return a slice of routes in reality\n//   - create separate PR to send based on well formatted route",
      "length": 1151,
      "tokens": 135,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) QueryRoutes(ctx context.Context,",
      "content": "func (r *rpcServer) QueryRoutes(ctx context.Context,\n\tin *lnrpc.QueryRoutesRequest) (*lnrpc.QueryRoutesResponse, error) {\n\n\treturn r.routerBackend.QueryRoutes(ctx, in)\n}\n\n// GetNetworkInfo returns some basic stats about the known channel graph from\n// the PoV of the node.",
      "length": 213,
      "tokens": 27,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) GetNetworkInfo(ctx context.Context,",
      "content": "func (r *rpcServer) GetNetworkInfo(ctx context.Context,\n\t_ *lnrpc.NetworkInfoRequest) (*lnrpc.NetworkInfo, error) {\n\n\tgraph := r.server.graphDB\n\n\tvar (\n\t\tnumNodes             uint32\n\t\tnumChannels          uint32\n\t\tmaxChanOut           uint32\n\t\ttotalNetworkCapacity btcutil.Amount\n\t\tminChannelSize       btcutil.Amount = math.MaxInt64\n\t\tmaxChannelSize       btcutil.Amount\n\t\tmedianChanSize       btcutil.Amount\n\t)\n\n\t// We'll use this map to de-duplicate channels during our traversal.\n\t// This is needed since channels are directional, so there will be two\n\t// edges for each channel within the graph.\n\tseenChans := make(map[uint64]struct{})\n\n\t// We also keep a list of all encountered capacities, in order to\n\t// calculate the median channel size.\n\tvar allChans []btcutil.Amount\n\n\t// We'll run through all the known nodes in the within our view of the\n\t// network, tallying up the total number of nodes, and also gathering\n\t// each node so we can measure the graph diameter and degree stats\n\t// below.\n\terr := graph.ForEachNodeCached(func(node route.Vertex,\n\t\tedges map[uint64]*channeldb.DirectedChannel) error {\n\n\t\t// Increment the total number of nodes with each iteration.\n\t\tnumNodes++\n\n\t\t// For each channel we'll compute the out degree of each node,\n\t\t// and also update our running tallies of the min/max channel\n\t\t// capacity, as well as the total channel capacity. We pass\n\t\t// through the db transaction from the outer view so we can\n\t\t// re-use it within this inner view.\n\t\tvar outDegree uint32\n\t\tfor _, edge := range edges {\n\t\t\t// Bump up the out degree for this node for each\n\t\t\t// channel encountered.\n\t\t\toutDegree++\n\n\t\t\t// If we've already seen this channel, then we'll\n\t\t\t// return early to ensure that we don't double-count\n\t\t\t// stats.\n\t\t\tif _, ok := seenChans[edge.ChannelID]; ok {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\t// Compare the capacity of this channel against the\n\t\t\t// running min/max to see if we should update the\n\t\t\t// extrema.\n\t\t\tchanCapacity := edge.Capacity\n\t\t\tif chanCapacity < minChannelSize {\n\t\t\t\tminChannelSize = chanCapacity\n\t\t\t}\n\t\t\tif chanCapacity > maxChannelSize {\n\t\t\t\tmaxChannelSize = chanCapacity\n\t\t\t}\n\n\t\t\t// Accumulate the total capacity of this channel to the\n\t\t\t// network wide-capacity.\n\t\t\ttotalNetworkCapacity += chanCapacity\n\n\t\t\tnumChannels++\n\n\t\t\tseenChans[edge.ChannelID] = struct{}{}\n\t\t\tallChans = append(allChans, edge.Capacity)\n\t\t}\n\n\t\t// Finally, if the out degree of this node is greater than what\n\t\t// we've seen so far, update the maxChanOut variable.\n\t\tif outDegree > maxChanOut {\n\t\t\tmaxChanOut = outDegree\n\t\t}\n\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Query the graph for the current number of zombie channels.\n\tnumZombies, err := graph.NumZombies()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Find the median.\n\tmedianChanSize = autopilot.Median(allChans)\n\n\t// If we don't have any channels, then reset the minChannelSize to zero\n\t// to avoid outputting NaN in encoded JSON.\n\tif numChannels == 0 {\n\t\tminChannelSize = 0\n\t}\n\n\t// Graph diameter.\n\tchannelGraph := autopilot.ChannelGraphFromCachedDatabase(graph)\n\tsimpleGraph, err := autopilot.NewSimpleGraph(channelGraph)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tstart := time.Now()\n\tdiameter := simpleGraph.DiameterRadialCutoff()\n\trpcsLog.Infof(\"elapsed time for diameter (%d) calculation: %v\", diameter,\n\t\ttime.Since(start))\n\n\t// TODO(roasbeef): also add oldest channel?\n\tnetInfo := &lnrpc.NetworkInfo{\n\t\tGraphDiameter:        diameter,\n\t\tMaxOutDegree:         maxChanOut,\n\t\tAvgOutDegree:         float64(2*numChannels) / float64(numNodes),\n\t\tNumNodes:             numNodes,\n\t\tNumChannels:          numChannels,\n\t\tTotalNetworkCapacity: int64(totalNetworkCapacity),\n\t\tAvgChannelSize:       float64(totalNetworkCapacity) / float64(numChannels),\n\n\t\tMinChannelSize:       int64(minChannelSize),\n\t\tMaxChannelSize:       int64(maxChannelSize),\n\t\tMedianChannelSizeSat: int64(medianChanSize),\n\t\tNumZombieChans:       numZombies,\n\t}\n\n\t// Similarly, if we don't have any channels, then we'll also set the\n\t// average channel size to zero in order to avoid weird JSON encoding\n\t// outputs.\n\tif numChannels == 0 {\n\t\tnetInfo.AvgChannelSize = 0\n\t}\n\n\treturn netInfo, nil\n}\n\n// StopDaemon will send a shutdown request to the interrupt handler, triggering\n// a graceful shutdown of the daemon.",
      "length": 4091,
      "tokens": 554,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) StopDaemon(_ context.Context,",
      "content": "func (r *rpcServer) StopDaemon(_ context.Context,\n\t_ *lnrpc.StopRequest) (*lnrpc.StopResponse, error) {\n\n\t// Before we even consider a shutdown, are we currently in recovery\n\t// mode? We don't want to allow shutting down during recovery because\n\t// that would mean the user would have to manually continue the rescan\n\t// process next time by using `lncli unlock --recovery_window X`\n\t// otherwise some funds wouldn't be picked up.\n\tisRecoveryMode, progress, err := r.server.cc.Wallet.GetRecoveryInfo()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to get wallet recovery info: %v\",\n\t\t\terr)\n\t}\n\tif isRecoveryMode && progress < 1 {\n\t\treturn nil, fmt.Errorf(\"wallet recovery in progress, cannot \" +\n\t\t\t\"shut down, please wait until rescan finishes\")\n\t}\n\n\tr.interceptor.RequestShutdown()\n\treturn &lnrpc.StopResponse{}, nil\n}\n\n// SubscribeChannelGraph launches a streaming RPC that allows the caller to\n// receive notifications upon any changes the channel graph topology from the\n// review of the responding node. Events notified include: new nodes coming\n// online, nodes updating their authenticated attributes, new channels being\n// advertised, updates in the routing policy for a directional channel edge,\n// and finally when prior channels are closed on-chain.",
      "length": 1189,
      "tokens": 176,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) SubscribeChannelGraph(req *lnrpc.GraphTopologySubscription,",
      "content": "func (r *rpcServer) SubscribeChannelGraph(req *lnrpc.GraphTopologySubscription,\n\tupdateStream lnrpc.Lightning_SubscribeChannelGraphServer) error {\n\n\t// First, we start by subscribing to a new intent to receive\n\t// notifications from the channel router.\n\tclient, err := r.server.chanRouter.SubscribeTopology()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Ensure that the resources for the topology update client is cleaned\n\t// up once either the server, or client exists.\n\tdefer client.Cancel()\n\n\tfor {\n\t\tselect {\n\n\t\t// A new update has been sent by the channel router, we'll\n\t\t// marshal it into the form expected by the gRPC client, then\n\t\t// send it off.\n\t\tcase topChange, ok := <-client.TopologyChanges:\n\t\t\t// If the second value from the channel read is nil,\n\t\t\t// then this means that the channel router is exiting\n\t\t\t// or the notification client was canceled. So we'll\n\t\t\t// exit early.\n\t\t\tif !ok {\n\t\t\t\treturn errors.New(\"server shutting down\")\n\t\t\t}\n\n\t\t\t// Convert the struct from the channel router into the\n\t\t\t// form expected by the gRPC service then send it off\n\t\t\t// to the client.\n\t\t\tgraphUpdate := marshallTopologyChange(topChange)\n\t\t\tif err := updateStream.Send(graphUpdate); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t// The response stream's context for whatever reason has been\n\t\t// closed. If context is closed by an exceeded deadline\n\t\t// we will return an error.\n\t\tcase <-updateStream.Context().Done():\n\t\t\tif errors.Is(updateStream.Context().Err(), context.Canceled) {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn updateStream.Context().Err()\n\n\t\t// The server is quitting, so we'll exit immediately. Returning\n\t\t// nil will close the clients read end of the stream.\n\t\tcase <-r.quit:\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\n// marshallTopologyChange performs a mapping from the topology change struct\n// returned by the router to the form of notifications expected by the current\n// gRPC service.",
      "length": 1736,
      "tokens": 266,
      "embedding": []
    },
    {
      "slug": "func marshallTopologyChange(topChange *routing.TopologyChange) *lnrpc.GraphTopologyUpdate {",
      "content": "func marshallTopologyChange(topChange *routing.TopologyChange) *lnrpc.GraphTopologyUpdate {\n\n\t// encodeKey is a simple helper function that converts a live public\n\t// key into a hex-encoded version of the compressed serialization for\n\t// the public key.\n\tencodeKey := func(k *btcec.PublicKey) string {\n\t\treturn hex.EncodeToString(k.SerializeCompressed())\n\t}\n\n\tnodeUpdates := make([]*lnrpc.NodeUpdate, len(topChange.NodeUpdates))\n\tfor i, nodeUpdate := range topChange.NodeUpdates {\n\t\tnodeAddrs := make([]*lnrpc.NodeAddress, 0, len(nodeUpdate.Addresses))\n\t\tfor _, addr := range nodeUpdate.Addresses {\n\t\t\tnodeAddr := &lnrpc.NodeAddress{\n\t\t\t\tNetwork: addr.Network(),\n\t\t\t\tAddr:    addr.String(),\n\t\t\t}\n\t\t\tnodeAddrs = append(nodeAddrs, nodeAddr)\n\t\t}\n\n\t\taddrs := make([]string, len(nodeUpdate.Addresses))\n\t\tfor i, addr := range nodeUpdate.Addresses {\n\t\t\taddrs[i] = addr.String()\n\t\t}\n\n\t\tnodeUpdates[i] = &lnrpc.NodeUpdate{\n\t\t\tAddresses:     addrs,\n\t\t\tNodeAddresses: nodeAddrs,\n\t\t\tIdentityKey:   encodeKey(nodeUpdate.IdentityKey),\n\t\t\tAlias:         nodeUpdate.Alias,\n\t\t\tColor:         nodeUpdate.Color,\n\t\t\tFeatures: invoicesrpc.CreateRPCFeatures(\n\t\t\t\tnodeUpdate.Features,\n\t\t\t),\n\t\t}\n\t}\n\n\tchannelUpdates := make([]*lnrpc.ChannelEdgeUpdate, len(topChange.ChannelEdgeUpdates))\n\tfor i, channelUpdate := range topChange.ChannelEdgeUpdates {\n\t\tchannelUpdates[i] = &lnrpc.ChannelEdgeUpdate{\n\t\t\tChanId: channelUpdate.ChanID,\n\t\t\tChanPoint: &lnrpc.ChannelPoint{\n\t\t\t\tFundingTxid: &lnrpc.ChannelPoint_FundingTxidBytes{\n\t\t\t\t\tFundingTxidBytes: channelUpdate.ChanPoint.Hash[:],\n\t\t\t\t},\n\t\t\t\tOutputIndex: channelUpdate.ChanPoint.Index,\n\t\t\t},\n\t\t\tCapacity: int64(channelUpdate.Capacity),\n\t\t\tRoutingPolicy: &lnrpc.RoutingPolicy{\n\t\t\t\tTimeLockDelta:    uint32(channelUpdate.TimeLockDelta),\n\t\t\t\tMinHtlc:          int64(channelUpdate.MinHTLC),\n\t\t\t\tMaxHtlcMsat:      uint64(channelUpdate.MaxHTLC),\n\t\t\t\tFeeBaseMsat:      int64(channelUpdate.BaseFee),\n\t\t\t\tFeeRateMilliMsat: int64(channelUpdate.FeeRate),\n\t\t\t\tDisabled:         channelUpdate.Disabled,\n\t\t\t},\n\t\t\tAdvertisingNode: encodeKey(channelUpdate.AdvertisingNode),\n\t\t\tConnectingNode:  encodeKey(channelUpdate.ConnectingNode),\n\t\t}\n\t}\n\n\tclosedChans := make([]*lnrpc.ClosedChannelUpdate, len(topChange.ClosedChannels))\n\tfor i, closedChan := range topChange.ClosedChannels {\n\t\tclosedChans[i] = &lnrpc.ClosedChannelUpdate{\n\t\t\tChanId:       closedChan.ChanID,\n\t\t\tCapacity:     int64(closedChan.Capacity),\n\t\t\tClosedHeight: closedChan.ClosedHeight,\n\t\t\tChanPoint: &lnrpc.ChannelPoint{\n\t\t\t\tFundingTxid: &lnrpc.ChannelPoint_FundingTxidBytes{\n\t\t\t\t\tFundingTxidBytes: closedChan.ChanPoint.Hash[:],\n\t\t\t\t},\n\t\t\t\tOutputIndex: closedChan.ChanPoint.Index,\n\t\t\t},\n\t\t}\n\t}\n\n\treturn &lnrpc.GraphTopologyUpdate{\n\t\tNodeUpdates:    nodeUpdates,\n\t\tChannelUpdates: channelUpdates,\n\t\tClosedChans:    closedChans,\n\t}\n}\n\n// ListPayments returns a list of outgoing payments determined by a paginated\n// database query.",
      "length": 2723,
      "tokens": 212,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) ListPayments(ctx context.Context,",
      "content": "func (r *rpcServer) ListPayments(ctx context.Context,\n\treq *lnrpc.ListPaymentsRequest) (*lnrpc.ListPaymentsResponse, error) {\n\n\trpcsLog.Debugf(\"[ListPayments]\")\n\n\t// If both dates are set, we check that the start date is less than the\n\t// end date, otherwise we'll get an empty result.\n\tif req.CreationDateStart != 0 && req.CreationDateEnd != 0 {\n\t\tif req.CreationDateStart >= req.CreationDateEnd {\n\t\t\treturn nil, fmt.Errorf(\"start date(%v) must be before \"+\n\t\t\t\t\"end date(%v)\", req.CreationDateStart,\n\t\t\t\treq.CreationDateEnd)\n\t\t}\n\t}\n\n\tquery := channeldb.PaymentsQuery{\n\t\tIndexOffset:       req.IndexOffset,\n\t\tMaxPayments:       req.MaxPayments,\n\t\tReversed:          req.Reversed,\n\t\tIncludeIncomplete: req.IncludeIncomplete,\n\t\tCountTotal:        req.CountTotalPayments,\n\t}\n\n\t// Attach the start date if set.\n\tif req.CreationDateStart != 0 {\n\t\tquery.CreationDateStart = time.Unix(\n\t\t\tint64(req.CreationDateStart), 0,\n\t\t)\n\t}\n\n\t// Attach the end date if set.\n\tif req.CreationDateEnd != 0 {\n\t\tquery.CreationDateEnd = time.Unix(\n\t\t\tint64(req.CreationDateEnd), 0,\n\t\t)\n\t}\n\n\t// If the maximum number of payments wasn't specified, then we'll\n\t// default to return the maximal number of payments representable.\n\tif req.MaxPayments == 0 {\n\t\tquery.MaxPayments = math.MaxUint64\n\t}\n\n\tpaymentsQuerySlice, err := r.server.miscDB.QueryPayments(query)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tpaymentsResp := &lnrpc.ListPaymentsResponse{\n\t\tLastIndexOffset:  paymentsQuerySlice.LastIndexOffset,\n\t\tFirstIndexOffset: paymentsQuerySlice.FirstIndexOffset,\n\t\tTotalNumPayments: paymentsQuerySlice.TotalCount,\n\t}\n\n\tfor _, payment := range paymentsQuerySlice.Payments {\n\t\tpayment := payment\n\n\t\trpcPayment, err := r.routerBackend.MarshallPayment(payment)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tpaymentsResp.Payments = append(\n\t\t\tpaymentsResp.Payments, rpcPayment,\n\t\t)\n\t}\n\n\treturn paymentsResp, nil\n}\n\n// DeletePayment deletes a payment from the DB given its payment hash. If\n// failedHtlcsOnly is set, only failed HTLC attempts of the payment will be\n// deleted.",
      "length": 1913,
      "tokens": 226,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) DeletePayment(ctx context.Context,",
      "content": "func (r *rpcServer) DeletePayment(ctx context.Context,\n\treq *lnrpc.DeletePaymentRequest) (\n\t*lnrpc.DeletePaymentResponse, error) {\n\n\thash, err := lntypes.MakeHash(req.PaymentHash)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trpcsLog.Infof(\"[DeletePayment] payment_identifier=%v, \"+\n\t\t\"failed_htlcs_only=%v\", hash, req.FailedHtlcsOnly)\n\n\terr = r.server.miscDB.DeletePayment(hash, req.FailedHtlcsOnly)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &lnrpc.DeletePaymentResponse{}, nil\n}\n\n// DeleteAllPayments deletes all outgoing payments from DB.",
      "length": 466,
      "tokens": 50,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) DeleteAllPayments(ctx context.Context,",
      "content": "func (r *rpcServer) DeleteAllPayments(ctx context.Context,\n\treq *lnrpc.DeleteAllPaymentsRequest) (\n\t*lnrpc.DeleteAllPaymentsResponse, error) {\n\n\trpcsLog.Infof(\"[DeleteAllPayments] failed_payments_only=%v, \"+\n\t\t\"failed_htlcs_only=%v\", req.FailedPaymentsOnly,\n\t\treq.FailedHtlcsOnly)\n\n\terr := r.server.miscDB.DeletePayments(\n\t\treq.FailedPaymentsOnly, req.FailedHtlcsOnly,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &lnrpc.DeleteAllPaymentsResponse{}, nil\n}\n\n// DebugLevel allows a caller to programmatically set the logging verbosity of\n// lnd. The logging can be targeted according to a coarse daemon-wide logging\n// level, or in a granular fashion to specify the logging for a target\n// sub-system.",
      "length": 625,
      "tokens": 72,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) DebugLevel(ctx context.Context,",
      "content": "func (r *rpcServer) DebugLevel(ctx context.Context,\n\treq *lnrpc.DebugLevelRequest) (*lnrpc.DebugLevelResponse, error) {\n\n\t// If show is set, then we simply print out the list of available\n\t// sub-systems.\n\tif req.Show {\n\t\treturn &lnrpc.DebugLevelResponse{\n\t\t\tSubSystems: strings.Join(\n\t\t\t\tr.cfg.LogWriter.SupportedSubsystems(), \" \",\n\t\t\t),\n\t\t}, nil\n\t}\n\n\trpcsLog.Infof(\"[debuglevel] changing debug level to: %v\", req.LevelSpec)\n\n\t// Otherwise, we'll attempt to set the logging level using the\n\t// specified level spec.\n\terr := build.ParseAndSetDebugLevels(req.LevelSpec, r.cfg.LogWriter)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &lnrpc.DebugLevelResponse{}, nil\n}\n\n// DecodePayReq takes an encoded payment request string and attempts to decode\n// it, returning a full description of the conditions encoded within the\n// payment request.",
      "length": 762,
      "tokens": 101,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) DecodePayReq(ctx context.Context,",
      "content": "func (r *rpcServer) DecodePayReq(ctx context.Context,\n\treq *lnrpc.PayReqString) (*lnrpc.PayReq, error) {\n\n\trpcsLog.Tracef(\"[decodepayreq] decoding: %v\", req.PayReq)\n\n\t// Fist we'll attempt to decode the payment request string, if the\n\t// request is invalid or the checksum doesn't match, then we'll exit\n\t// here with an error.\n\tpayReq, err := zpay32.Decode(req.PayReq, r.cfg.ActiveNetParams.Params)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Let the fields default to empty strings.\n\tdesc := \"\"\n\tif payReq.Description != nil {\n\t\tdesc = *payReq.Description\n\t}\n\n\tdescHash := []byte(\"\")\n\tif payReq.DescriptionHash != nil {\n\t\tdescHash = payReq.DescriptionHash[:]\n\t}\n\n\tfallbackAddr := \"\"\n\tif payReq.FallbackAddr != nil {\n\t\tfallbackAddr = payReq.FallbackAddr.String()\n\t}\n\n\t// Expiry time will default to 3600 seconds if not specified\n\t// explicitly.\n\texpiry := int64(payReq.Expiry().Seconds())\n\n\t// Convert between the `lnrpc` and `routing` types.\n\trouteHints := invoicesrpc.CreateRPCRouteHints(payReq.RouteHints)\n\n\tvar amtSat, amtMsat int64\n\tif payReq.MilliSat != nil {\n\t\tamtSat = int64(payReq.MilliSat.ToSatoshis())\n\t\tamtMsat = int64(*payReq.MilliSat)\n\t}\n\n\t// Extract the payment address from the payment request, if present.\n\tvar paymentAddr []byte\n\tif payReq.PaymentAddr != nil {\n\t\tpaymentAddr = payReq.PaymentAddr[:]\n\t}\n\n\tdest := payReq.Destination.SerializeCompressed()\n\treturn &lnrpc.PayReq{\n\t\tDestination:     hex.EncodeToString(dest),\n\t\tPaymentHash:     hex.EncodeToString(payReq.PaymentHash[:]),\n\t\tNumSatoshis:     amtSat,\n\t\tNumMsat:         amtMsat,\n\t\tTimestamp:       payReq.Timestamp.Unix(),\n\t\tDescription:     desc,\n\t\tDescriptionHash: hex.EncodeToString(descHash[:]),\n\t\tFallbackAddr:    fallbackAddr,\n\t\tExpiry:          expiry,\n\t\tCltvExpiry:      int64(payReq.MinFinalCLTVExpiry()),\n\t\tRouteHints:      routeHints,\n\t\tPaymentAddr:     paymentAddr,\n\t\tFeatures:        invoicesrpc.CreateRPCFeatures(payReq.Features),\n\t}, nil\n}\n\n// feeBase is the fixed point that fee rate computation are performed over.\n// Nodes on the network advertise their fee rate using this point as a base.\n// This means that the minimal possible fee rate if 1e-6, or 0.000001, or\n// 0.0001%.\nconst feeBase float64 = 1000000\n\n// FeeReport allows the caller to obtain a report detailing the current fee\n// schedule enforced by the node globally for each channel.",
      "length": 2214,
      "tokens": 268,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) FeeReport(ctx context.Context,",
      "content": "func (r *rpcServer) FeeReport(ctx context.Context,\n\t_ *lnrpc.FeeReportRequest) (*lnrpc.FeeReportResponse, error) {\n\n\t// TODO(roasbeef): use UnaryInterceptor to add automated logging\n\n\trpcsLog.Debugf(\"[feereport]\")\n\n\tchannelGraph := r.server.graphDB\n\tselfNode, err := channelGraph.SourceNode()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar feeReports []*lnrpc.ChannelFeeReport\n\terr = selfNode.ForEachChannel(nil, func(_ kvdb.RTx, chanInfo *channeldb.ChannelEdgeInfo,\n\t\tedgePolicy, _ *channeldb.ChannelEdgePolicy) error {\n\n\t\t// Self node should always have policies for its channels.\n\t\tif edgePolicy == nil {\n\t\t\treturn fmt.Errorf(\"no policy for outgoing channel %v \",\n\t\t\t\tchanInfo.ChannelID)\n\t\t}\n\n\t\t// We'll compute the effective fee rate by converting from a\n\t\t// fixed point fee rate to a floating point fee rate. The fee\n\t\t// rate field in the database the amount of mSAT charged per\n\t\t// 1mil mSAT sent, so will divide by this to get the proper fee\n\t\t// rate.\n\t\tfeeRateFixedPoint := edgePolicy.FeeProportionalMillionths\n\t\tfeeRate := float64(feeRateFixedPoint) / feeBase\n\n\t\t// TODO(roasbeef): also add stats for revenue for each channel\n\t\tfeeReports = append(feeReports, &lnrpc.ChannelFeeReport{\n\t\t\tChanId:       chanInfo.ChannelID,\n\t\t\tChannelPoint: chanInfo.ChannelPoint.String(),\n\t\t\tBaseFeeMsat:  int64(edgePolicy.FeeBaseMSat),\n\t\t\tFeePerMil:    int64(feeRateFixedPoint),\n\t\t\tFeeRate:      feeRate,\n\t\t})\n\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfwdEventLog := r.server.miscDB.ForwardingLog()\n\n\t// computeFeeSum is a helper function that computes the total fees for\n\t// a particular time slice described by a forwarding event query.\n\tcomputeFeeSum := func(query channeldb.ForwardingEventQuery) (lnwire.MilliSatoshi, error) {\n\n\t\tvar totalFees lnwire.MilliSatoshi\n\n\t\t// We'll continue to fetch the next query and accumulate the\n\t\t// fees until the next query returns no events.\n\t\tfor {\n\t\t\ttimeSlice, err := fwdEventLog.Query(query)\n\t\t\tif err != nil {\n\t\t\t\treturn 0, err\n\t\t\t}\n\n\t\t\t// If the timeslice is empty, then we'll return as\n\t\t\t// we've retrieved all the entries in this range.\n\t\t\tif len(timeSlice.ForwardingEvents) == 0 {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// Otherwise, we'll tally up an accumulate the total\n\t\t\t// fees for this time slice.\n\t\t\tfor _, event := range timeSlice.ForwardingEvents {\n\t\t\t\tfee := event.AmtIn - event.AmtOut\n\t\t\t\ttotalFees += fee\n\t\t\t}\n\n\t\t\t// We'll now take the last offset index returned as\n\t\t\t// part of this response, and modify our query to start\n\t\t\t// at this index. This has a pagination effect in the\n\t\t\t// case that our query bounds has more than 100k\n\t\t\t// entries.\n\t\t\tquery.IndexOffset = timeSlice.LastIndexOffset\n\t\t}\n\n\t\treturn totalFees, nil\n\t}\n\n\tnow := time.Now()\n\n\t// Before we perform the queries below, we'll instruct the switch to\n\t// flush any pending events to disk. This ensure we get a complete\n\t// snapshot at this particular time.\n\tif err := r.server.htlcSwitch.FlushForwardingEvents(); err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to flush forwarding \"+\n\t\t\t\"events: %v\", err)\n\t}\n\n\t// In addition to returning the current fee schedule for each channel.\n\t// We'll also perform a series of queries to obtain the total fees\n\t// earned over the past day, week, and month.\n\tdayQuery := channeldb.ForwardingEventQuery{\n\t\tStartTime:    now.Add(-time.Hour * 24),\n\t\tEndTime:      now,\n\t\tNumMaxEvents: 1000,\n\t}\n\tdayFees, err := computeFeeSum(dayQuery)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to retrieve day fees: %v\", err)\n\t}\n\n\tweekQuery := channeldb.ForwardingEventQuery{\n\t\tStartTime:    now.Add(-time.Hour * 24 * 7),\n\t\tEndTime:      now,\n\t\tNumMaxEvents: 1000,\n\t}\n\tweekFees, err := computeFeeSum(weekQuery)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to retrieve day fees: %v\", err)\n\t}\n\n\tmonthQuery := channeldb.ForwardingEventQuery{\n\t\tStartTime:    now.Add(-time.Hour * 24 * 30),\n\t\tEndTime:      now,\n\t\tNumMaxEvents: 1000,\n\t}\n\tmonthFees, err := computeFeeSum(monthQuery)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to retrieve day fees: %v\", err)\n\t}\n\n\treturn &lnrpc.FeeReportResponse{\n\t\tChannelFees: feeReports,\n\t\tDayFeeSum:   uint64(dayFees.ToSatoshis()),\n\t\tWeekFeeSum:  uint64(weekFees.ToSatoshis()),\n\t\tMonthFeeSum: uint64(monthFees.ToSatoshis()),\n\t}, nil\n}\n\n// minFeeRate is the smallest permitted fee rate within the network. This is\n// derived by the fact that fee rates are computed using a fixed point of\n// 1,000,000. As a result, the smallest representable fee rate is 1e-6, or\n// 0.000001, or 0.0001%.\nconst minFeeRate = 1e-6\n\n// UpdateChannelPolicy allows the caller to update the channel forwarding policy\n// for all channels globally, or a particular channel.",
      "length": 4442,
      "tokens": 613,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) UpdateChannelPolicy(ctx context.Context,",
      "content": "func (r *rpcServer) UpdateChannelPolicy(ctx context.Context,\n\treq *lnrpc.PolicyUpdateRequest) (*lnrpc.PolicyUpdateResponse, error) {\n\n\tvar targetChans []wire.OutPoint\n\tswitch scope := req.Scope.(type) {\n\t// If the request is targeting all active channels, then we don't need\n\t// target any channels by their channel point.\n\tcase *lnrpc.PolicyUpdateRequest_Global:\n\n\t// Otherwise, we're targeting an individual channel by its channel\n\t// point.\n\tcase *lnrpc.PolicyUpdateRequest_ChanPoint:\n\t\ttxid, err := lnrpc.GetChanPointFundingTxid(scope.ChanPoint)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\ttargetChans = append(targetChans, wire.OutPoint{\n\t\t\tHash:  *txid,\n\t\t\tIndex: scope.ChanPoint.OutputIndex,\n\t\t})\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown scope: %v\", scope)\n\t}\n\n\tvar feeRateFixed uint32\n\n\tswitch {\n\t// The request should use either the fee rate in percent, or the new\n\t// ppm rate, but not both.\n\tcase req.FeeRate != 0 && req.FeeRatePpm != 0:\n\t\terrMsg := \"cannot set both FeeRate and FeeRatePpm at the \" +\n\t\t\t\"same time\"\n\n\t\treturn nil, status.Errorf(codes.InvalidArgument, errMsg)\n\n\t// If the request is using fee_rate.\n\tcase req.FeeRate != 0:\n\t\t// As a sanity check, if the fee isn't zero, we'll ensure that\n\t\t// the passed fee rate is below 1e-6, or the lowest allowed\n\t\t// non-zero fee rate expressible within the protocol.\n\t\tif req.FeeRate != 0 && req.FeeRate < minFeeRate {\n\t\t\treturn nil, fmt.Errorf(\"fee rate of %v is too \"+\n\t\t\t\t\"small, min fee rate is %v\", req.FeeRate,\n\t\t\t\tminFeeRate)\n\t\t}\n\n\t\t// We'll also need to convert the floating point fee rate we\n\t\t// accept over RPC to the fixed point rate that we use within\n\t\t// the protocol. We do this by multiplying the passed fee rate\n\t\t// by the fee base. This gives us the fixed point, scaled by 1\n\t\t// million that's used within the protocol.\n\t\t//\n\t\t// Because of the inaccurate precision of the IEEE 754\n\t\t// standard, we need to round the product of feerate and\n\t\t// feebase.\n\t\tfeeRateFixed = uint32(math.Round(req.FeeRate * feeBase))\n\n\t// Otherwise, we use the fee_rate_ppm parameter.\n\tcase req.FeeRatePpm != 0:\n\t\tfeeRateFixed = req.FeeRatePpm\n\t}\n\n\t// We'll also ensure that the user isn't setting a CLTV delta that\n\t// won't give outgoing HTLCs enough time to fully resolve if needed.\n\tif req.TimeLockDelta < minTimeLockDelta {\n\t\treturn nil, fmt.Errorf(\"time lock delta of %v is too small, \"+\n\t\t\t\"minimum supported is %v\", req.TimeLockDelta,\n\t\t\tminTimeLockDelta)\n\t}\n\n\tbaseFeeMsat := lnwire.MilliSatoshi(req.BaseFeeMsat)\n\tfeeSchema := routing.FeeSchema{\n\t\tBaseFee: baseFeeMsat,\n\t\tFeeRate: feeRateFixed,\n\t}\n\n\tmaxHtlc := lnwire.MilliSatoshi(req.MaxHtlcMsat)\n\tvar minHtlc *lnwire.MilliSatoshi\n\tif req.MinHtlcMsatSpecified {\n\t\tmin := lnwire.MilliSatoshi(req.MinHtlcMsat)\n\t\tminHtlc = &min\n\t}\n\n\tchanPolicy := routing.ChannelPolicy{\n\t\tFeeSchema:     feeSchema,\n\t\tTimeLockDelta: req.TimeLockDelta,\n\t\tMaxHTLC:       maxHtlc,\n\t\tMinHTLC:       minHtlc,\n\t}\n\n\trpcsLog.Debugf(\"[updatechanpolicy] updating channel policy base_fee=%v, \"+\n\t\t\"rate_fixed=%v, time_lock_delta: %v, \"+\n\t\t\"min_htlc=%v, max_htlc=%v, targets=%v\",\n\t\treq.BaseFeeMsat, feeRateFixed, req.TimeLockDelta,\n\t\tminHtlc, maxHtlc,\n\t\tspew.Sdump(targetChans))\n\n\t// With the scope resolved, we'll now send this to the local channel\n\t// manager so it can propagate the new policy for our target channel(s).\n\tfailedUpdates, err := r.server.localChanMgr.UpdatePolicy(chanPolicy,\n\t\ttargetChans...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &lnrpc.PolicyUpdateResponse{\n\t\tFailedUpdates: failedUpdates,\n\t}, nil\n}\n\n// ForwardingHistory allows the caller to query the htlcswitch for a record of\n// all HTLC's forwarded within the target time range, and integer offset within\n// that time range. If no time-range is specified, then the first chunk of the\n// past 24 hrs of forwarding history are returned.\n\n// A list of forwarding events are returned. The size of each forwarding event\n// is 40 bytes, and the max message size able to be returned in gRPC is 4 MiB.\n// In order to safely stay under this max limit, we'll return 50k events per\n// response.  Each response has the index offset of the last entry. The index\n// offset can be provided to the request to allow the caller to skip a series\n// of records.",
      "length": 4041,
      "tokens": 587,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) ForwardingHistory(ctx context.Context,",
      "content": "func (r *rpcServer) ForwardingHistory(ctx context.Context,\n\treq *lnrpc.ForwardingHistoryRequest) (*lnrpc.ForwardingHistoryResponse,\n\terror) {\n\n\trpcsLog.Debugf(\"[forwardinghistory]\")\n\n\t// Before we perform the queries below, we'll instruct the switch to\n\t// flush any pending events to disk. This ensure we get a complete\n\t// snapshot at this particular time.\n\tif err := r.server.htlcSwitch.FlushForwardingEvents(); err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to flush forwarding \"+\n\t\t\t\"events: %v\", err)\n\t}\n\n\tvar (\n\t\tstartTime, endTime time.Time\n\n\t\tnumEvents uint32\n\t)\n\n\t// startTime defaults to the Unix epoch (0 unixtime, or\n\t// midnight 01-01-1970).\n\tstartTime = time.Unix(int64(req.StartTime), 0)\n\n\t// If the end time wasn't specified, assume a default end time of now.\n\tif req.EndTime == 0 {\n\t\tnow := time.Now()\n\t\tendTime = now\n\t} else {\n\t\tendTime = time.Unix(int64(req.EndTime), 0)\n\t}\n\n\t// If the number of events wasn't specified, then we'll default to\n\t// returning the last 100 events.\n\tnumEvents = req.NumMaxEvents\n\tif numEvents == 0 {\n\t\tnumEvents = 100\n\t}\n\n\t// Next, we'll map the proto request into a format that is understood by\n\t// the forwarding log.\n\teventQuery := channeldb.ForwardingEventQuery{\n\t\tStartTime:    startTime,\n\t\tEndTime:      endTime,\n\t\tIndexOffset:  req.IndexOffset,\n\t\tNumMaxEvents: numEvents,\n\t}\n\ttimeSlice, err := r.server.miscDB.ForwardingLog().Query(eventQuery)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to query forwarding log: %v\",\n\t\t\terr)\n\t}\n\n\t// chanToPeerAlias caches previously looked up channel information.\n\tchanToPeerAlias := make(map[lnwire.ShortChannelID]string)\n\n\t// Helper function to extract a peer's node alias given its SCID.\n\tgetRemoteAlias := func(chanID lnwire.ShortChannelID) (string, error) {\n\t\t// If we'd previously seen this chanID then return the cached\n\t\t// peer alias.\n\t\tif peerAlias, ok := chanToPeerAlias[chanID]; ok {\n\t\t\treturn peerAlias, nil\n\t\t}\n\n\t\t// Else call the server to look up the peer alias.\n\t\tedge, err := r.GetChanInfo(ctx, &lnrpc.ChanInfoRequest{\n\t\t\tChanId: chanID.ToUint64(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\n\t\tremotePub := edge.Node1Pub\n\t\tif r.selfNode.String() == edge.Node1Pub {\n\t\t\tremotePub = edge.Node2Pub\n\t\t}\n\n\t\tvertex, err := route.NewVertexFromStr(remotePub)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\n\t\tpeer, err := r.server.graphDB.FetchLightningNode(vertex)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\n\t\t// Cache the peer alias.\n\t\tchanToPeerAlias[chanID] = peer.Alias\n\n\t\treturn peer.Alias, nil\n\t}\n\n\t// TODO(roasbeef): add settlement latency?\n\t//  * use FPE on all records?\n\n\t// With the events retrieved, we'll now map them into the proper proto\n\t// response.\n\t//\n\t// TODO(roasbeef): show in ns for the outside?\n\tfwdingEvents := make(\n\t\t[]*lnrpc.ForwardingEvent, len(timeSlice.ForwardingEvents),\n\t)\n\tresp := &lnrpc.ForwardingHistoryResponse{\n\t\tForwardingEvents: fwdingEvents,\n\t\tLastOffsetIndex:  timeSlice.LastIndexOffset,\n\t}\n\tfor i, event := range timeSlice.ForwardingEvents {\n\t\tamtInMsat := event.AmtIn\n\t\tamtOutMsat := event.AmtOut\n\t\tfeeMsat := event.AmtIn - event.AmtOut\n\n\t\tresp.ForwardingEvents[i] = &lnrpc.ForwardingEvent{\n\t\t\tTimestamp:   uint64(event.Timestamp.Unix()),\n\t\t\tTimestampNs: uint64(event.Timestamp.UnixNano()),\n\t\t\tChanIdIn:    event.IncomingChanID.ToUint64(),\n\t\t\tChanIdOut:   event.OutgoingChanID.ToUint64(),\n\t\t\tAmtIn:       uint64(amtInMsat.ToSatoshis()),\n\t\t\tAmtOut:      uint64(amtOutMsat.ToSatoshis()),\n\t\t\tFee:         uint64(feeMsat.ToSatoshis()),\n\t\t\tFeeMsat:     uint64(feeMsat),\n\t\t\tAmtInMsat:   uint64(amtInMsat),\n\t\t\tAmtOutMsat:  uint64(amtOutMsat),\n\t\t}\n\n\t\tif req.PeerAliasLookup {\n\t\t\taliasIn, err := getRemoteAlias(event.IncomingChanID)\n\t\t\tif err != nil {\n\t\t\t\taliasIn = fmt.Sprintf(\"unable to lookup peer \"+\n\t\t\t\t\t\"alias: %v\", err)\n\t\t\t}\n\t\t\taliasOut, err := getRemoteAlias(event.OutgoingChanID)\n\t\t\tif err != nil {\n\t\t\t\taliasOut = fmt.Sprintf(\"unable to lookup peer\"+\n\t\t\t\t\t\"alias: %v\", err)\n\t\t\t}\n\t\t\tresp.ForwardingEvents[i].PeerAliasIn = aliasIn\n\t\t\tresp.ForwardingEvents[i].PeerAliasOut = aliasOut\n\t\t}\n\t}\n\n\treturn resp, nil\n}\n\n// ExportChannelBackup attempts to return an encrypted static channel backup\n// for the target channel identified by it channel point. The backup is\n// encrypted with a key generated from the aezeed seed of the user. The\n// returned backup can either be restored using the RestoreChannelBackup method\n// once lnd is running, or via the InitWallet and UnlockWallet methods from the\n// WalletUnlocker service.",
      "length": 4253,
      "tokens": 537,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) ExportChannelBackup(ctx context.Context,",
      "content": "func (r *rpcServer) ExportChannelBackup(ctx context.Context,\n\tin *lnrpc.ExportChannelBackupRequest) (*lnrpc.ChannelBackup, error) {\n\n\t// First, we'll convert the lnrpc channel point into a wire.OutPoint\n\t// that we can manipulate.\n\ttxid, err := lnrpc.GetChanPointFundingTxid(in.ChanPoint)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tchanPoint := wire.OutPoint{\n\t\tHash:  *txid,\n\t\tIndex: in.ChanPoint.OutputIndex,\n\t}\n\n\t// Next, we'll attempt to fetch a channel backup for this channel from\n\t// the database. If this channel has been closed, or the outpoint is\n\t// unknown, then we'll return an error\n\tunpackedBackup, err := chanbackup.FetchBackupForChan(\n\t\tchanPoint, r.server.chanStateDB, r.server.addrSource,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// At this point, we have an unpacked backup (plaintext) so we'll now\n\t// attempt to serialize and encrypt it in order to create a packed\n\t// backup.\n\tpackedBackups, err := chanbackup.PackStaticChanBackups(\n\t\t[]chanbackup.Single{*unpackedBackup},\n\t\tr.server.cc.KeyRing,\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"packing of back ups failed: %v\", err)\n\t}\n\n\t// Before we proceed, we'll ensure that we received a backup for this\n\t// channel, otherwise, we'll bail out.\n\tpackedBackup, ok := packedBackups[chanPoint]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"expected single backup for \"+\n\t\t\t\"ChannelPoint(%v), got %v\", chanPoint,\n\t\t\tlen(packedBackup))\n\t}\n\n\treturn &lnrpc.ChannelBackup{\n\t\tChanPoint:  in.ChanPoint,\n\t\tChanBackup: packedBackup,\n\t}, nil\n}\n\n// VerifyChanBackup allows a caller to verify the integrity of a channel backup\n// snapshot. This method will accept both either a packed Single or a packed\n// Multi. Specifying both will result in an error.",
      "length": 1588,
      "tokens": 226,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) VerifyChanBackup(ctx context.Context,",
      "content": "func (r *rpcServer) VerifyChanBackup(ctx context.Context,\n\tin *lnrpc.ChanBackupSnapshot) (*lnrpc.VerifyChanBackupResponse, error) {\n\n\tswitch {\n\t// If neither a Single or Multi has been specified, then we have nothing\n\t// to verify.\n\tcase in.GetSingleChanBackups() == nil && in.GetMultiChanBackup() == nil:\n\t\treturn nil, errors.New(\"either a Single or Multi channel \" +\n\t\t\t\"backup must be specified\")\n\n\t// Either a Single or a Multi must be specified, but not both.\n\tcase in.GetSingleChanBackups() != nil && in.GetMultiChanBackup() != nil:\n\t\treturn nil, errors.New(\"either a Single or Multi channel \" +\n\t\t\t\"backup must be specified, but not both\")\n\n\t// If a Single is specified then we'll only accept one of them to allow\n\t// the caller to map the valid/invalid state for each individual Single.\n\tcase in.GetSingleChanBackups() != nil:\n\t\tchanBackupsProtos := in.GetSingleChanBackups().ChanBackups\n\t\tif len(chanBackupsProtos) != 1 {\n\t\t\treturn nil, errors.New(\"only one Single is accepted \" +\n\t\t\t\t\"at a time\")\n\t\t}\n\n\t\t// First, we'll convert the raw byte slice into a type we can\n\t\t// work with a bit better.\n\t\tchanBackup := chanbackup.PackedSingles(\n\t\t\t[][]byte{chanBackupsProtos[0].ChanBackup},\n\t\t)\n\n\t\t// With our PackedSingles created, we'll attempt to unpack the\n\t\t// backup. If this fails, then we know the backup is invalid for\n\t\t// some reason.\n\t\t_, err := chanBackup.Unpack(r.server.cc.KeyRing)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"invalid single channel \"+\n\t\t\t\t\"backup: %v\", err)\n\t\t}\n\n\tcase in.GetMultiChanBackup() != nil:\n\t\t// We'll convert the raw byte slice into a PackedMulti that we\n\t\t// can easily work with.\n\t\tpackedMultiBackup := in.GetMultiChanBackup().MultiChanBackup\n\t\tpackedMulti := chanbackup.PackedMulti(packedMultiBackup)\n\n\t\t// We'll now attempt to unpack the Multi. If this fails, then we\n\t\t// know it's invalid.\n\t\t_, err := packedMulti.Unpack(r.server.cc.KeyRing)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"invalid multi channel backup: \"+\n\t\t\t\t\"%v\", err)\n\t\t}\n\t}\n\n\treturn &lnrpc.VerifyChanBackupResponse{}, nil\n}\n\n// createBackupSnapshot converts the passed Single backup into a snapshot which\n// contains individual packed single backups, as well as a single packed multi\n// backup.",
      "length": 2098,
      "tokens": 299,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) createBackupSnapshot(backups []chanbackup.Single) (",
      "content": "func (r *rpcServer) createBackupSnapshot(backups []chanbackup.Single) (\n\t*lnrpc.ChanBackupSnapshot, error) {\n\n\t// Once we have the set of back ups, we'll attempt to pack them all\n\t// into a series of single channel backups.\n\tsingleChanPackedBackups, err := chanbackup.PackStaticChanBackups(\n\t\tbackups, r.server.cc.KeyRing,\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to pack set of chan \"+\n\t\t\t\"backups: %v\", err)\n\t}\n\n\t// Now that we have our set of single packed backups, we'll morph that\n\t// into a form that the proto response requires.\n\tnumBackups := len(singleChanPackedBackups)\n\tsingleBackupResp := &lnrpc.ChannelBackups{\n\t\tChanBackups: make([]*lnrpc.ChannelBackup, 0, numBackups),\n\t}\n\tfor chanPoint, singlePackedBackup := range singleChanPackedBackups {\n\t\ttxid := chanPoint.Hash\n\t\trpcChanPoint := &lnrpc.ChannelPoint{\n\t\t\tFundingTxid: &lnrpc.ChannelPoint_FundingTxidBytes{\n\t\t\t\tFundingTxidBytes: txid[:],\n\t\t\t},\n\t\t\tOutputIndex: chanPoint.Index,\n\t\t}\n\n\t\tsingleBackupResp.ChanBackups = append(\n\t\t\tsingleBackupResp.ChanBackups,\n\t\t\t&lnrpc.ChannelBackup{\n\t\t\t\tChanPoint:  rpcChanPoint,\n\t\t\t\tChanBackup: singlePackedBackup,\n\t\t\t},\n\t\t)\n\t}\n\n\t// In addition, to the set of single chan backups, we'll also create a\n\t// single multi-channel backup which can be serialized into a single\n\t// file for safe storage.\n\tvar b bytes.Buffer\n\tunpackedMultiBackup := chanbackup.Multi{\n\t\tStaticBackups: backups,\n\t}\n\terr = unpackedMultiBackup.PackToWriter(&b, r.server.cc.KeyRing)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to multi-pack backups: %v\", err)\n\t}\n\n\tmultiBackupResp := &lnrpc.MultiChanBackup{\n\t\tMultiChanBackup: b.Bytes(),\n\t}\n\tfor _, singleBackup := range singleBackupResp.ChanBackups {\n\t\tmultiBackupResp.ChanPoints = append(\n\t\t\tmultiBackupResp.ChanPoints, singleBackup.ChanPoint,\n\t\t)\n\t}\n\n\treturn &lnrpc.ChanBackupSnapshot{\n\t\tSingleChanBackups: singleBackupResp,\n\t\tMultiChanBackup:   multiBackupResp,\n\t}, nil\n}\n\n// ExportAllChannelBackups returns static channel backups for all existing\n// channels known to lnd. A set of regular singular static channel backups for\n// each channel are returned. Additionally, a multi-channel backup is returned\n// as well, which contains a single encrypted blob containing the backups of\n// each channel.",
      "length": 2103,
      "tokens": 254,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) ExportAllChannelBackups(ctx context.Context,",
      "content": "func (r *rpcServer) ExportAllChannelBackups(ctx context.Context,\n\tin *lnrpc.ChanBackupExportRequest) (*lnrpc.ChanBackupSnapshot, error) {\n\n\t// First, we'll attempt to read back ups for ALL currently opened\n\t// channels from disk.\n\tallUnpackedBackups, err := chanbackup.FetchStaticChanBackups(\n\t\tr.server.chanStateDB, r.server.addrSource,\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to fetch all static chan \"+\n\t\t\t\"backups: %v\", err)\n\t}\n\n\t// With the backups assembled, we'll create a full snapshot.\n\treturn r.createBackupSnapshot(allUnpackedBackups)\n}\n\n// RestoreChannelBackups accepts a set of singular channel backups, or a single\n// encrypted multi-chan backup and attempts to recover any funds remaining\n// within the channel. If we're able to unpack the backup, then the new channel\n// will be shown under listchannels, as well as pending channels.",
      "length": 778,
      "tokens": 108,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) RestoreChannelBackups(ctx context.Context,",
      "content": "func (r *rpcServer) RestoreChannelBackups(ctx context.Context,\n\tin *lnrpc.RestoreChanBackupRequest) (*lnrpc.RestoreBackupResponse, error) {\n\n\t// The server hasn't yet started, so it won't be able to service any of\n\t// our requests, so we'll bail early here.\n\tif !r.server.Started() {\n\t\treturn nil, ErrServerNotActive\n\t}\n\n\t// First, we'll make our implementation of the\n\t// chanbackup.ChannelRestorer interface which we'll use to properly\n\t// restore either a set of chanbackup.Single or chanbackup.Multi\n\t// backups.\n\tchanRestorer := &chanDBRestorer{\n\t\tdb:         r.server.chanStateDB,\n\t\tsecretKeys: r.server.cc.KeyRing,\n\t\tchainArb:   r.server.chainArb,\n\t}\n\n\t// We'll accept either a list of Single backups, or a single Multi\n\t// backup which contains several single backups.\n\tswitch {\n\tcase in.GetChanBackups() != nil:\n\t\tchanBackupsProtos := in.GetChanBackups()\n\n\t\t// Now that we know what type of backup we're working with,\n\t\t// we'll parse them all out into a more suitable format.\n\t\tpackedBackups := make([][]byte, 0, len(chanBackupsProtos.ChanBackups))\n\t\tfor _, chanBackup := range chanBackupsProtos.ChanBackups {\n\t\t\tpackedBackups = append(\n\t\t\t\tpackedBackups, chanBackup.ChanBackup,\n\t\t\t)\n\t\t}\n\n\t\t// With our backups obtained, we'll now restore them which will\n\t\t// write the new backups to disk, and then attempt to connect\n\t\t// out to any peers that we know of which were our prior\n\t\t// channel peers.\n\t\terr := chanbackup.UnpackAndRecoverSingles(\n\t\t\tchanbackup.PackedSingles(packedBackups),\n\t\t\tr.server.cc.KeyRing, chanRestorer, r.server,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to unpack single \"+\n\t\t\t\t\"backups: %v\", err)\n\t\t}\n\n\tcase in.GetMultiChanBackup() != nil:\n\t\tpackedMultiBackup := in.GetMultiChanBackup()\n\n\t\t// With our backups obtained, we'll now restore them which will\n\t\t// write the new backups to disk, and then attempt to connect\n\t\t// out to any peers that we know of which were our prior\n\t\t// channel peers.\n\t\tpackedMulti := chanbackup.PackedMulti(packedMultiBackup)\n\t\terr := chanbackup.UnpackAndRecoverMulti(\n\t\t\tpackedMulti, r.server.cc.KeyRing, chanRestorer,\n\t\t\tr.server,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to unpack chan \"+\n\t\t\t\t\"backup: %v\", err)\n\t\t}\n\t}\n\n\treturn &lnrpc.RestoreBackupResponse{}, nil\n}\n\n// SubscribeChannelBackups allows a client to sub-subscribe to the most up to\n// date information concerning the state of all channel back ups. Each time a\n// new channel is added, we return the new set of channels, along with a\n// multi-chan backup containing the backup info for all channels. Each time a\n// channel is closed, we send a new update, which contains new new chan back\n// ups, but the updated set of encrypted multi-chan backups with the closed\n// channel(s) removed.",
      "length": 2609,
      "tokens": 369,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) SubscribeChannelBackups(req *lnrpc.ChannelBackupSubscription,",
      "content": "func (r *rpcServer) SubscribeChannelBackups(req *lnrpc.ChannelBackupSubscription,\n\tupdateStream lnrpc.Lightning_SubscribeChannelBackupsServer) error {\n\n\t// First, we'll subscribe to the primary channel notifier so we can\n\t// obtain events for new pending/opened/closed channels.\n\tchanSubscription, err := r.server.channelNotifier.SubscribeChannelEvents()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdefer chanSubscription.Cancel()\n\tfor {\n\t\tselect {\n\t\t// A new event has been sent by the channel notifier, we'll\n\t\t// assemble, then sling out a new event to the client.\n\t\tcase e := <-chanSubscription.Updates():\n\t\t\t// TODO(roasbeef): batch dispatch ntnfs\n\n\t\t\tswitch e.(type) {\n\n\t\t\t// We only care about new/closed channels, so we'll\n\t\t\t// skip any events for active/inactive channels.\n\t\t\t// To make the subscription behave the same way as the\n\t\t\t// synchronous call and the file based backup, we also\n\t\t\t// include pending channels in the update.\n\t\t\tcase channelnotifier.ActiveChannelEvent:\n\t\t\t\tcontinue\n\t\t\tcase channelnotifier.InactiveChannelEvent:\n\t\t\t\tcontinue\n\t\t\tcase channelnotifier.ActiveLinkEvent:\n\t\t\t\tcontinue\n\t\t\tcase channelnotifier.InactiveLinkEvent:\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Now that we know the channel state has changed,\n\t\t\t// we'll obtains the current set of single channel\n\t\t\t// backups from disk.\n\t\t\tchanBackups, err := chanbackup.FetchStaticChanBackups(\n\t\t\t\tr.server.chanStateDB, r.server.addrSource,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"unable to fetch all \"+\n\t\t\t\t\t\"static chan backups: %v\", err)\n\t\t\t}\n\n\t\t\t// With our backups obtained, we'll pack them into a\n\t\t\t// snapshot and send them back to the client.\n\t\t\tbackupSnapshot, err := r.createBackupSnapshot(\n\t\t\t\tchanBackups,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\terr = updateStream.Send(backupSnapshot)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t// The response stream's context for whatever reason has been\n\t\t// closed. If context is closed by an exceeded deadline we will\n\t\t// return an error.\n\t\tcase <-updateStream.Context().Done():\n\t\t\tif errors.Is(updateStream.Context().Err(), context.Canceled) {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn updateStream.Context().Err()\n\n\t\tcase <-r.quit:\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\n// ChannelAcceptor dispatches a bi-directional streaming RPC in which\n// OpenChannel requests are sent to the client and the client responds with\n// a boolean that tells LND whether or not to accept the channel. This allows\n// node operators to specify their own criteria for accepting inbound channels\n// through a single persistent connection.",
      "length": 2363,
      "tokens": 323,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) ChannelAcceptor(stream lnrpc.Lightning_ChannelAcceptorServer) error {",
      "content": "func (r *rpcServer) ChannelAcceptor(stream lnrpc.Lightning_ChannelAcceptorServer) error {\n\tchainedAcceptor := r.chanPredicate\n\n\t// Create a new RPCAcceptor which will send requests into the\n\t// newRequests channel when it receives them.\n\trpcAcceptor := chanacceptor.NewRPCAcceptor(\n\t\tstream.Recv, stream.Send, r.cfg.AcceptorTimeout,\n\t\tr.cfg.ActiveNetParams.Params, r.quit,\n\t)\n\n\t// Add the RPCAcceptor to the ChainedAcceptor and defer its removal.\n\tid := chainedAcceptor.AddAcceptor(rpcAcceptor)\n\tdefer chainedAcceptor.RemoveAcceptor(id)\n\n\t// Run the rpc acceptor, which will accept requests for channel\n\t// acceptance decisions from our chained acceptor, send them to the\n\t// channel acceptor and listen for and report responses. This function\n\t// blocks, and will exit if the rpcserver receives the instruction to\n\t// shutdown, or the client cancels.\n\treturn rpcAcceptor.Run()\n}\n\n// BakeMacaroon allows the creation of a new macaroon with custom read and write\n// permissions. No first-party caveats are added since this can be done offline.\n// If the --allow-external-permissions flag is set, the RPC will allow\n// external permissions that LND is not aware of.",
      "length": 1049,
      "tokens": 147,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) BakeMacaroon(ctx context.Context,",
      "content": "func (r *rpcServer) BakeMacaroon(ctx context.Context,\n\treq *lnrpc.BakeMacaroonRequest) (*lnrpc.BakeMacaroonResponse, error) {\n\n\trpcsLog.Debugf(\"[bakemacaroon]\")\n\n\t// If the --no-macaroons flag is used to start lnd, the macaroon service\n\t// is not initialized. Therefore we can't bake new macaroons.\n\tif r.macService == nil {\n\t\treturn nil, errMacaroonDisabled\n\t}\n\n\thelpMsg := fmt.Sprintf(\"supported actions are %v, supported entities \"+\n\t\t\"are %v\", validActions, validEntities)\n\n\t// Don't allow empty permission list as it doesn't make sense to have\n\t// a macaroon that is not allowed to access any RPC.\n\tif len(req.Permissions) == 0 {\n\t\treturn nil, fmt.Errorf(\"permission list cannot be empty. \"+\n\t\t\t\"specify at least one action/entity pair. %s\", helpMsg)\n\t}\n\n\t// Validate and map permission struct used by gRPC to the one used by\n\t// the bakery. If the --allow-external-permissions flag is set, we\n\t// will not validate, but map.\n\trequestedPermissions := make([]bakery.Op, len(req.Permissions))\n\tfor idx, op := range req.Permissions {\n\t\tif req.AllowExternalPermissions {\n\t\t\trequestedPermissions[idx] = bakery.Op{\n\t\t\t\tEntity: op.Entity,\n\t\t\t\tAction: op.Action,\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\tif !stringInSlice(op.Entity, validEntities) {\n\t\t\treturn nil, fmt.Errorf(\"invalid permission entity. %s\",\n\t\t\t\thelpMsg)\n\t\t}\n\n\t\t// Either we have the special entity \"uri\" which specifies a\n\t\t// full gRPC URI or we have one of the pre-defined actions.\n\t\tif op.Entity == macaroons.PermissionEntityCustomURI {\n\t\t\tallPermissions := r.interceptorChain.Permissions()\n\t\t\t_, ok := allPermissions[op.Action]\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(\"invalid permission \" +\n\t\t\t\t\t\"action, must be an existing URI in \" +\n\t\t\t\t\t\"the format /package.Service/\" +\n\t\t\t\t\t\"MethodName\")\n\t\t\t}\n\t\t} else if !stringInSlice(op.Action, validActions) {\n\t\t\treturn nil, fmt.Errorf(\"invalid permission action. %s\",\n\t\t\t\thelpMsg)\n\t\t}\n\n\t\trequestedPermissions[idx] = bakery.Op{\n\t\t\tEntity: op.Entity,\n\t\t\tAction: op.Action,\n\t\t}\n\t}\n\n\t// Convert root key id from uint64 to bytes. Because the\n\t// DefaultRootKeyID is a digit 0 expressed in a byte slice of a string\n\t// \"0\", we will keep the IDs in the same format - all must be numeric,\n\t// and must be a byte slice of string value of the digit, e.g.,\n\t// uint64(123) to string(123).\n\trootKeyID := []byte(strconv.FormatUint(req.RootKeyId, 10))\n\n\t// Bake new macaroon with the given permissions and send it binary\n\t// serialized and hex encoded to the client.\n\tnewMac, err := r.macService.NewMacaroon(\n\t\tctx, rootKeyID, requestedPermissions...,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tnewMacBytes, err := newMac.M().MarshalBinary()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tresp := &lnrpc.BakeMacaroonResponse{}\n\tresp.Macaroon = hex.EncodeToString(newMacBytes)\n\n\treturn resp, nil\n}\n\n// ListMacaroonIDs returns a list of macaroon root key IDs in use.",
      "length": 2697,
      "tokens": 380,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) ListMacaroonIDs(ctx context.Context,",
      "content": "func (r *rpcServer) ListMacaroonIDs(ctx context.Context,\n\treq *lnrpc.ListMacaroonIDsRequest) (\n\t*lnrpc.ListMacaroonIDsResponse, error) {\n\n\trpcsLog.Debugf(\"[listmacaroonids]\")\n\n\t// If the --no-macaroons flag is used to start lnd, the macaroon service\n\t// is not initialized. Therefore we can't show any IDs.\n\tif r.macService == nil {\n\t\treturn nil, errMacaroonDisabled\n\t}\n\n\trootKeyIDByteSlice, err := r.macService.ListMacaroonIDs(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar rootKeyIDs []uint64\n\tfor _, value := range rootKeyIDByteSlice {\n\t\t// Convert bytes into uint64.\n\t\tid, err := strconv.ParseUint(string(value), 10, 64)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\trootKeyIDs = append(rootKeyIDs, id)\n\t}\n\n\treturn &lnrpc.ListMacaroonIDsResponse{RootKeyIds: rootKeyIDs}, nil\n}\n\n// DeleteMacaroonID removes a specific macaroon ID.",
      "length": 746,
      "tokens": 99,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) DeleteMacaroonID(ctx context.Context,",
      "content": "func (r *rpcServer) DeleteMacaroonID(ctx context.Context,\n\treq *lnrpc.DeleteMacaroonIDRequest) (\n\t*lnrpc.DeleteMacaroonIDResponse, error) {\n\n\trpcsLog.Debugf(\"[deletemacaroonid]\")\n\n\t// If the --no-macaroons flag is used to start lnd, the macaroon service\n\t// is not initialized. Therefore we can't delete any IDs.\n\tif r.macService == nil {\n\t\treturn nil, errMacaroonDisabled\n\t}\n\n\t// Convert root key id from uint64 to bytes. Because the\n\t// DefaultRootKeyID is a digit 0 expressed in a byte slice of a string\n\t// \"0\", we will keep the IDs in the same format - all must be digit, and\n\t// must be a byte slice of string value of the digit.\n\trootKeyID := []byte(strconv.FormatUint(req.RootKeyId, 10))\n\tdeletedIDBytes, err := r.macService.DeleteMacaroonID(ctx, rootKeyID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &lnrpc.DeleteMacaroonIDResponse{\n\t\t// If the root key ID doesn't exist, it won't be deleted. We\n\t\t// will return a response with deleted = false, otherwise true.\n\t\tDeleted: deletedIDBytes != nil,\n\t}, nil\n}\n\n// ListPermissions lists all RPC method URIs and their required macaroon\n// permissions to access them.",
      "length": 1036,
      "tokens": 160,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) ListPermissions(_ context.Context,",
      "content": "func (r *rpcServer) ListPermissions(_ context.Context,\n\t_ *lnrpc.ListPermissionsRequest) (*lnrpc.ListPermissionsResponse,\n\terror) {\n\n\trpcsLog.Debugf(\"[listpermissions]\")\n\n\tpermissionMap := make(map[string]*lnrpc.MacaroonPermissionList)\n\tfor uri, perms := range r.interceptorChain.Permissions() {\n\t\trpcPerms := make([]*lnrpc.MacaroonPermission, len(perms))\n\t\tfor idx, perm := range perms {\n\t\t\trpcPerms[idx] = &lnrpc.MacaroonPermission{\n\t\t\t\tEntity: perm.Entity,\n\t\t\t\tAction: perm.Action,\n\t\t\t}\n\t\t}\n\t\tpermissionMap[uri] = &lnrpc.MacaroonPermissionList{\n\t\t\tPermissions: rpcPerms,\n\t\t}\n\t}\n\n\treturn &lnrpc.ListPermissionsResponse{\n\t\tMethodPermissions: permissionMap,\n\t}, nil\n}\n\n// CheckMacaroonPermissions checks the caveats and permissions of a macaroon.",
      "length": 667,
      "tokens": 60,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) CheckMacaroonPermissions(ctx context.Context,",
      "content": "func (r *rpcServer) CheckMacaroonPermissions(ctx context.Context,\n\treq *lnrpc.CheckMacPermRequest) (*lnrpc.CheckMacPermResponse, error) {\n\n\t// Turn grpc macaroon permission into bakery.Op for the server to\n\t// process.\n\tpermissions := make([]bakery.Op, len(req.Permissions))\n\tfor idx, perm := range req.Permissions {\n\t\tpermissions[idx] = bakery.Op{\n\t\t\tEntity: perm.Entity,\n\t\t\tAction: perm.Action,\n\t\t}\n\t}\n\n\terr := r.macService.CheckMacAuth(\n\t\tctx, req.Macaroon, permissions, req.FullMethod,\n\t)\n\tif err != nil {\n\t\treturn nil, status.Error(codes.InvalidArgument, err.Error())\n\t}\n\n\treturn &lnrpc.CheckMacPermResponse{\n\t\tValid: true,\n\t}, nil\n}\n\n// FundingStateStep is an advanced funding related call that allows the caller\n// to either execute some preparatory steps for a funding workflow, or manually\n// progress a funding workflow. The primary way a funding flow is identified is\n// via its pending channel ID. As an example, this method can be used to\n// specify that we're expecting a funding flow for a particular pending channel\n// ID, for which we need to use specific parameters.  Alternatively, this can\n// be used to interactively drive PSBT signing for funding for partially\n// complete funding transactions.",
      "length": 1119,
      "tokens": 159,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) FundingStateStep(ctx context.Context,",
      "content": "func (r *rpcServer) FundingStateStep(ctx context.Context,\n\tin *lnrpc.FundingTransitionMsg) (*lnrpc.FundingStateStepResp, error) {\n\n\tvar pendingChanID [32]byte\n\tswitch {\n\t// If this is a message to register a new shim that is an external\n\t// channel point, then we'll contact the wallet to register this new\n\t// shim. A user will use this method to register a new channel funding\n\t// workflow which has already been partially negotiated outside of the\n\t// core protocol.\n\tcase in.GetShimRegister() != nil &&\n\t\tin.GetShimRegister().GetChanPointShim() != nil:\n\n\t\trpcShimIntent := in.GetShimRegister().GetChanPointShim()\n\n\t\t// Using the rpc shim as a template, we'll construct a new\n\t\t// chanfunding.Assembler that is able to express proper\n\t\t// formulation of this expected channel.\n\t\tshimAssembler, err := newFundingShimAssembler(\n\t\t\trpcShimIntent, false, r.server.cc.KeyRing,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treq := &chanfunding.Request{\n\t\t\tRemoteAmt: btcutil.Amount(rpcShimIntent.Amt),\n\t\t}\n\t\tshimIntent, err := shimAssembler.ProvisionChannel(req)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// Once we have the intent, we'll register it with the wallet.\n\t\t// Once we receive an incoming funding request that uses this\n\t\t// pending channel ID, then this shim will be dispatched in\n\t\t// place of our regular funding workflow.\n\t\tcopy(pendingChanID[:], rpcShimIntent.PendingChanId)\n\t\terr = r.server.cc.Wallet.RegisterFundingIntent(\n\t\t\tpendingChanID, shimIntent,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t// There is no need to register a PSBT shim before opening the channel,\n\t// even though our RPC message structure allows for it. Inform the user\n\t// by returning a proper error instead of just doing nothing.\n\tcase in.GetShimRegister() != nil &&\n\t\tin.GetShimRegister().GetPsbtShim() != nil:\n\n\t\treturn nil, fmt.Errorf(\"PSBT shim must only be sent when \" +\n\t\t\t\"opening a channel\")\n\n\t// If this is a transition to cancel an existing shim, then we'll pass\n\t// this message along to the wallet, informing it that the intent no\n\t// longer needs to be considered and should be cleaned up.\n\tcase in.GetShimCancel() != nil:\n\t\trpcsLog.Debugf(\"Canceling funding shim for pending_id=%x\",\n\t\t\tin.GetShimCancel().PendingChanId)\n\n\t\tcopy(pendingChanID[:], in.GetShimCancel().PendingChanId)\n\t\terr := r.server.cc.Wallet.CancelFundingIntent(pendingChanID)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t// If this is a transition to verify the PSBT for an existing shim,\n\t// we'll do so and then store the verified PSBT for later so we can\n\t// compare it to the final, signed one.\n\tcase in.GetPsbtVerify() != nil:\n\t\trpcsLog.Debugf(\"Verifying PSBT for pending_id=%x\",\n\t\t\tin.GetPsbtVerify().PendingChanId)\n\n\t\tcopy(pendingChanID[:], in.GetPsbtVerify().PendingChanId)\n\t\tpacket, err := psbt.NewFromRawBytes(\n\t\t\tbytes.NewReader(in.GetPsbtVerify().FundedPsbt), false,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"error parsing psbt: %v\", err)\n\t\t}\n\n\t\terr = r.server.cc.Wallet.PsbtFundingVerify(\n\t\t\tpendingChanID, packet, in.GetPsbtVerify().SkipFinalize,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t// If this is a transition to finalize the PSBT funding flow, we compare\n\t// the final PSBT to the previously verified one and if nothing\n\t// unexpected was changed, continue the channel opening process.\n\tcase in.GetPsbtFinalize() != nil:\n\t\tmsg := in.GetPsbtFinalize()\n\t\trpcsLog.Debugf(\"Finalizing PSBT for pending_id=%x\",\n\t\t\tmsg.PendingChanId)\n\n\t\tcopy(pendingChanID[:], in.GetPsbtFinalize().PendingChanId)\n\n\t\tvar (\n\t\t\tpacket *psbt.Packet\n\t\t\trawTx  *wire.MsgTx\n\t\t\terr    error\n\t\t)\n\n\t\t// Either the signed PSBT or the raw transaction need to be set\n\t\t// but not both at the same time.\n\t\tswitch {\n\t\tcase len(msg.SignedPsbt) > 0 && len(msg.FinalRawTx) > 0:\n\t\t\treturn nil, fmt.Errorf(\"cannot set both signed PSBT \" +\n\t\t\t\t\"and final raw TX at the same time\")\n\n\t\tcase len(msg.SignedPsbt) > 0:\n\t\t\tpacket, err = psbt.NewFromRawBytes(\n\t\t\t\tbytes.NewReader(in.GetPsbtFinalize().SignedPsbt),\n\t\t\t\tfalse,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"error parsing psbt: %v\",\n\t\t\t\t\terr)\n\t\t\t}\n\n\t\tcase len(msg.FinalRawTx) > 0:\n\t\t\trawTx = &wire.MsgTx{}\n\t\t\terr = rawTx.Deserialize(bytes.NewReader(msg.FinalRawTx))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"error parsing final \"+\n\t\t\t\t\t\"raw TX: %v\", err)\n\t\t\t}\n\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"PSBT or raw transaction to \" +\n\t\t\t\t\"finalize missing\")\n\t\t}\n\n\t\terr = r.server.cc.Wallet.PsbtFundingFinalize(\n\t\t\tpendingChanID, packet, rawTx,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// TODO(roasbeef): extend PendingChannels to also show shims\n\n\t// TODO(roasbeef): return resulting state? also add a method to query\n\t// current state?\n\treturn &lnrpc.FundingStateStepResp{}, nil\n}\n\n// RegisterRPCMiddleware adds a new gRPC middleware to the interceptor chain. A\n// gRPC middleware is software component external to lnd that aims to add\n// additional business logic to lnd by observing/intercepting/validating\n// incoming gRPC client requests and (if needed) replacing/overwriting outgoing\n// messages before they're sent to the client. When registering the middleware\n// must identify itself and indicate what custom macaroon caveats it wants to\n// be responsible for. Only requests that contain a macaroon with that specific\n// custom caveat are then sent to the middleware for inspection. As a security\n// measure, _no_ middleware can intercept requests made with _unencumbered_\n// macaroons!",
      "length": 5216,
      "tokens": 721,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) RegisterRPCMiddleware(",
      "content": "func (r *rpcServer) RegisterRPCMiddleware(\n\tstream lnrpc.Lightning_RegisterRPCMiddlewareServer) error {\n\n\t// This is a security critical functionality and needs to be enabled\n\t// specifically by the user.\n\tif !r.cfg.RPCMiddleware.Enable {\n\t\treturn fmt.Errorf(\"RPC middleware not enabled in config\")\n\t}\n\n\t// When registering a middleware the first message being sent from the\n\t// middleware must be a registration message containing its name and the\n\t// custom caveat it wants to register for.\n\tvar (\n\t\tregisterChan     = make(chan *lnrpc.MiddlewareRegistration, 1)\n\t\tregisterDoneChan = make(chan struct{})\n\t\terrChan          = make(chan error, 1)\n\t)\n\tctxc, cancel := context.WithTimeout(\n\t\tstream.Context(), r.cfg.RPCMiddleware.InterceptTimeout,\n\t)\n\tdefer cancel()\n\n\t// Read the first message in a goroutine because the Recv method blocks\n\t// until the message arrives.\n\tgo func() {\n\t\tmsg, err := stream.Recv()\n\t\tif err != nil {\n\t\t\terrChan <- err\n\n\t\t\treturn\n\t\t}\n\n\t\tregisterChan <- msg.GetRegister()\n\t}()\n\n\t// Wait for the initial message to arrive or time out if it takes too\n\t// long.\n\tvar registerMsg *lnrpc.MiddlewareRegistration\n\tselect {\n\tcase registerMsg = <-registerChan:\n\t\tif registerMsg == nil {\n\t\t\treturn fmt.Errorf(\"invalid initial middleware \" +\n\t\t\t\t\"registration message\")\n\t\t}\n\n\tcase err := <-errChan:\n\t\treturn fmt.Errorf(\"error receiving initial middleware \"+\n\t\t\t\"registration message: %v\", err)\n\n\tcase <-ctxc.Done():\n\t\treturn ctxc.Err()\n\n\tcase <-r.quit:\n\t\treturn ErrServerShuttingDown\n\t}\n\n\t// Make sure the registration is valid.\n\tconst nameMinLength = 5\n\tif len(registerMsg.MiddlewareName) < nameMinLength {\n\t\treturn fmt.Errorf(\"invalid middleware name, use descriptive \"+\n\t\t\t\"name of at least %d characters\", nameMinLength)\n\t}\n\n\treadOnly := registerMsg.ReadOnlyMode\n\tcaveatName := registerMsg.CustomMacaroonCaveatName\n\tswitch {\n\tcase readOnly && len(caveatName) > 0:\n\t\treturn fmt.Errorf(\"cannot set read-only and custom caveat \" +\n\t\t\t\"name at the same time\")\n\n\tcase !readOnly && len(caveatName) < nameMinLength:\n\t\treturn fmt.Errorf(\"need to set either custom caveat name \"+\n\t\t\t\"of at least %d characters or read-only mode\",\n\t\t\tnameMinLength)\n\t}\n\n\tmiddleware := rpcperms.NewMiddlewareHandler(\n\t\tregisterMsg.MiddlewareName,\n\t\tcaveatName, readOnly, stream.Recv, stream.Send,\n\t\tr.cfg.RPCMiddleware.InterceptTimeout,\n\t\tr.cfg.ActiveNetParams.Params, r.quit,\n\t)\n\n\t// Add the RPC middleware to the interceptor chain and defer its\n\t// removal.\n\tif err := r.interceptorChain.RegisterMiddleware(middleware); err != nil {\n\t\treturn fmt.Errorf(\"error registering middleware: %v\", err)\n\t}\n\tdefer r.interceptorChain.RemoveMiddleware(registerMsg.MiddlewareName)\n\n\t// Send a message to the client to indicate that the registration has\n\t// successfully completed.\n\tregCompleteMsg := &lnrpc.RPCMiddlewareRequest{\n\t\tInterceptType: &lnrpc.RPCMiddlewareRequest_RegComplete{\n\t\t\tRegComplete: true,\n\t\t},\n\t}\n\n\t// Send the message in a goroutine because the Send method blocks until\n\t// the message is read by the client.\n\tgo func() {\n\t\terr := stream.Send(regCompleteMsg)\n\t\tif err != nil {\n\t\t\terrChan <- err\n\t\t\treturn\n\t\t}\n\n\t\tclose(registerDoneChan)\n\t}()\n\n\tselect {\n\tcase err := <-errChan:\n\t\treturn fmt.Errorf(\"error sending middleware registration \"+\n\t\t\t\"complete message: %v\", err)\n\n\tcase <-ctxc.Done():\n\t\treturn ctxc.Err()\n\n\tcase <-r.quit:\n\t\treturn ErrServerShuttingDown\n\n\tcase <-registerDoneChan:\n\t}\n\n\treturn middleware.Run()\n}\n\n// SendCustomMessage sends a custom peer message.",
      "length": 3301,
      "tokens": 420,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) SendCustomMessage(ctx context.Context, req *lnrpc.SendCustomMessageRequest) (",
      "content": "func (r *rpcServer) SendCustomMessage(ctx context.Context, req *lnrpc.SendCustomMessageRequest) (\n\t*lnrpc.SendCustomMessageResponse, error) {\n\n\tpeer, err := route.NewVertexFromBytes(req.Peer)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\terr = r.server.SendCustomMessage(\n\t\tpeer, lnwire.MessageType(req.Type), req.Data,\n\t)\n\tswitch {\n\tcase err == ErrPeerNotConnected:\n\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\tcase err != nil:\n\t\treturn nil, err\n\t}\n\n\treturn &lnrpc.SendCustomMessageResponse{}, nil\n}\n\n// SubscribeCustomMessages subscribes to a stream of incoming custom peer\n// messages.",
      "length": 475,
      "tokens": 57,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) SubscribeCustomMessages(req *lnrpc.SubscribeCustomMessagesRequest,",
      "content": "func (r *rpcServer) SubscribeCustomMessages(req *lnrpc.SubscribeCustomMessagesRequest,\n\tserver lnrpc.Lightning_SubscribeCustomMessagesServer) error {\n\n\tclient, err := r.server.SubscribeCustomMessages()\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer client.Cancel()\n\n\tfor {\n\t\tselect {\n\t\tcase <-client.Quit():\n\t\t\treturn errors.New(\"shutdown\")\n\n\t\tcase <-server.Context().Done():\n\t\t\treturn server.Context().Err()\n\n\t\tcase update := <-client.Updates():\n\t\t\tcustomMsg := update.(*CustomMessage)\n\n\t\t\terr := server.Send(&lnrpc.CustomMessage{\n\t\t\t\tPeer: customMsg.Peer[:],\n\t\t\t\tData: customMsg.Msg.Data,\n\t\t\t\tType: uint32(customMsg.Msg.Type),\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n}\n\n// ListAliases returns the set of all aliases we have ever allocated along with\n// their base SCID's and possibly a separate confirmed SCID in the case of\n// zero-conf.",
      "length": 725,
      "tokens": 88,
      "embedding": []
    },
    {
      "slug": "func (r *rpcServer) ListAliases(ctx context.Context,",
      "content": "func (r *rpcServer) ListAliases(ctx context.Context,\n\tin *lnrpc.ListAliasesRequest) (*lnrpc.ListAliasesResponse, error) {\n\n\t// Fetch the map of all aliases.\n\tmapAliases := r.server.aliasMgr.ListAliases()\n\n\t// Fill out the response. This does not include the zero-conf confirmed\n\t// SCID. Doing so would require more database lookups and it can be\n\t// cross-referenced with the output of listchannels/closedchannels.\n\tresp := &lnrpc.ListAliasesResponse{\n\t\tAliasMaps: make([]*lnrpc.AliasMap, 0),\n\t}\n\n\tfor base, set := range mapAliases {\n\t\trpcMap := &lnrpc.AliasMap{\n\t\t\tBaseScid: base.ToUint64(),\n\t\t}\n\t\tfor _, alias := range set {\n\t\t\trpcMap.Aliases = append(\n\t\t\t\trpcMap.Aliases, alias.ToUint64(),\n\t\t\t)\n\t\t}\n\t\tresp.AliasMaps = append(resp.AliasMaps, rpcMap)\n\t}\n\n\treturn resp, nil\n}\n\n// rpcInitiator returns the correct lnrpc initiator for channels where we have\n// a record of the opening channel.",
      "length": 811,
      "tokens": 109,
      "embedding": []
    },
    {
      "slug": "func rpcInitiator(isInitiator bool) lnrpc.Initiator {",
      "content": "func rpcInitiator(isInitiator bool) lnrpc.Initiator {\n\tif isInitiator {\n\t\treturn lnrpc.Initiator_INITIATOR_LOCAL\n\t}\n\n\treturn lnrpc.Initiator_INITIATOR_REMOTE\n}\n",
      "length": 100,
      "tokens": 9,
      "embedding": []
    }
  ]
}