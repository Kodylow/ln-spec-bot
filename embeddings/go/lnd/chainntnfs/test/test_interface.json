{
  "filepath": "../implementations/go/lnd/chainntnfs/test/test_interface.go",
  "package": "chainntnfstest",
  "sections": [
    {
      "slug": "//go:build dev",
      "content": "//go:build dev\n// +build dev\n\npackage chainntnfstest\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"log\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/btcsuite/btcd/btcutil\"\n\t\"github.com/btcsuite/btcd/chaincfg/chainhash\"\n\t\"github.com/btcsuite/btcd/integration/rpctest\"\n\t\"github.com/btcsuite/btcd/rpcclient\"\n\t\"github.com/btcsuite/btcd/wire\"\n\t\"github.com/btcsuite/btcwallet/chain\"\n\t_ \"github.com/btcsuite/btcwallet/walletdb/bdb\" // Required to auto-register the boltdb walletdb implementation.\n\t\"github.com/lightninglabs/neutrino\"\n\t\"github.com/lightningnetwork/lnd/blockcache\"\n\t\"github.com/lightningnetwork/lnd/chainntnfs\"\n\t\"github.com/lightningnetwork/lnd/chainntnfs/bitcoindnotify\"\n\t\"github.com/lightningnetwork/lnd/chainntnfs/btcdnotify\"\n\t\"github.com/lightningnetwork/lnd/chainntnfs/neutrinonotify\"\n\t\"github.com/lightningnetwork/lnd/channeldb\"\n\t\"github.com/stretchr/testify/require\"\n)\n",
      "length": 819,
      "tokens": 38,
      "embedding": []
    },
    {
      "slug": "func testSingleConfirmationNotification(miner *rpctest.Harness,",
      "content": "func testSingleConfirmationNotification(miner *rpctest.Harness,\n\tnotifier chainntnfs.TestChainNotifier, scriptDispatch bool, t *testing.T) {\n\n\t// We'd like to test the case of being notified once a txid reaches\n\t// a *single* confirmation.\n\t//\n\t// So first, let's send some coins to \"ourself\", obtaining a txid.\n\t// We're spending from a coinbase output here, so we use the dedicated\n\t// function.\n\ttxid, pkScript, err := chainntnfs.GetTestTxidAndScript(miner)\n\trequire.NoError(t, err, \"unable to create test tx\")\n\tif err := chainntnfs.WaitForMempoolTx(miner, txid); err != nil {\n\t\tt.Fatalf(\"tx not relayed to miner: %v\", err)\n\t}\n\n\t_, currentHeight, err := miner.Client.GetBestBlock()\n\trequire.NoError(t, err, \"unable to get current height\")\n\n\t// Now that we have a txid, register a confirmation notification with\n\t// the chainntfn source.\n\tnumConfs := uint32(1)\n\tvar confIntent *chainntnfs.ConfirmationEvent\n\tif scriptDispatch {\n\t\tconfIntent, err = notifier.RegisterConfirmationsNtfn(\n\t\t\tnil, pkScript, numConfs, uint32(currentHeight),\n\t\t)\n\t} else {\n\t\tconfIntent, err = notifier.RegisterConfirmationsNtfn(\n\t\t\ttxid, pkScript, numConfs, uint32(currentHeight),\n\t\t)\n\t}\n\trequire.NoError(t, err, \"unable to register ntfn\")\n\n\t// Now generate a single block, the transaction should be included which\n\t// should trigger a notification event.\n\tblockHash, err := miner.Client.Generate(1)\n\trequire.NoError(t, err, \"unable to generate single block\")\n\n\tselect {\n\tcase confInfo := <-confIntent.Confirmed:\n\t\tif !confInfo.BlockHash.IsEqual(blockHash[0]) {\n\t\t\tt.Fatalf(\"mismatched block hashes: expected %v, got %v\",\n\t\t\t\tblockHash[0], confInfo.BlockHash)\n\t\t}\n\n\t\t// Finally, we'll verify that the tx index returned is the exact same\n\t\t// as the tx index of the transaction within the block itself.\n\t\tmsgBlock, err := miner.Client.GetBlock(blockHash[0])\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to fetch block: %v\", err)\n\t\t}\n\n\t\tblock := btcutil.NewBlock(msgBlock)\n\t\tspecifiedTxHash, err := block.TxHash(int(confInfo.TxIndex))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to index into block: %v\", err)\n\t\t}\n\n\t\tif !specifiedTxHash.IsEqual(txid) {\n\t\t\tt.Fatalf(\"mismatched tx indexes: expected %v, got %v\",\n\t\t\t\ttxid, specifiedTxHash)\n\t\t}\n\tcase <-time.After(20 * time.Second):\n\t\tt.Fatalf(\"confirmation notification never received\")\n\t}\n}\n",
      "length": 2174,
      "tokens": 279,
      "embedding": []
    },
    {
      "slug": "func testMultiConfirmationNotification(miner *rpctest.Harness,",
      "content": "func testMultiConfirmationNotification(miner *rpctest.Harness,\n\tnotifier chainntnfs.TestChainNotifier, scriptDispatch bool, t *testing.T) {\n\n\t// We'd like to test the case of being notified once a txid reaches\n\t// N confirmations, where N > 1.\n\t//\n\t// Again, we'll begin by creating a fresh transaction, so we can obtain\n\t// a fresh txid.\n\ttxid, pkScript, err := chainntnfs.GetTestTxidAndScript(miner)\n\trequire.NoError(t, err, \"unable to create test addr\")\n\tif err := chainntnfs.WaitForMempoolTx(miner, txid); err != nil {\n\t\tt.Fatalf(\"tx not relayed to miner: %v\", err)\n\t}\n\n\t_, currentHeight, err := miner.Client.GetBestBlock()\n\trequire.NoError(t, err, \"unable to get current height\")\n\n\tnumConfs := uint32(6)\n\tvar confIntent *chainntnfs.ConfirmationEvent\n\tif scriptDispatch {\n\t\tconfIntent, err = notifier.RegisterConfirmationsNtfn(\n\t\t\tnil, pkScript, numConfs, uint32(currentHeight),\n\t\t)\n\t} else {\n\t\tconfIntent, err = notifier.RegisterConfirmationsNtfn(\n\t\t\ttxid, pkScript, numConfs, uint32(currentHeight),\n\t\t)\n\t}\n\trequire.NoError(t, err, \"unable to register ntfn\")\n\n\t// Now generate a six blocks. The transaction should be included in the\n\t// first block, which will be built upon by the other 5 blocks.\n\tif _, err := miner.Client.Generate(6); err != nil {\n\t\tt.Fatalf(\"unable to generate single block: %v\", err)\n\t}\n\n\t// TODO(roasbeef): reduce all timeouts after neutrino sync tightended\n\t// up\n\n\tselect {\n\tcase <-confIntent.Confirmed:\n\t\tbreak\n\tcase <-time.After(20 * time.Second):\n\t\tt.Fatalf(\"confirmation notification never received\")\n\t}\n}\n",
      "length": 1432,
      "tokens": 193,
      "embedding": []
    },
    {
      "slug": "func testBatchConfirmationNotification(miner *rpctest.Harness,",
      "content": "func testBatchConfirmationNotification(miner *rpctest.Harness,\n\tnotifier chainntnfs.TestChainNotifier, scriptDispatch bool, t *testing.T) {\n\n\t// We'd like to test a case of serving notifications to multiple\n\t// clients, each requesting to be notified once a txid receives\n\t// various numbers of confirmations.\n\tconfSpread := [6]uint32{1, 2, 3, 6, 20, 22}\n\tconfIntents := make([]*chainntnfs.ConfirmationEvent, len(confSpread))\n\n\t_, currentHeight, err := miner.Client.GetBestBlock()\n\trequire.NoError(t, err, \"unable to get current height\")\n\n\t// Create a new txid spending miner coins for each confirmation entry\n\t// in confSpread, we collect each conf intent into a slice so we can\n\t// verify they're each notified at the proper number of confirmations\n\t// below.\n\tfor i, numConfs := range confSpread {\n\t\t// All the clients with an even index will ask for the block\n\t\t// along side the conf ntfn.\n\t\tvar opts []chainntnfs.NotifierOption\n\t\tif i%2 == 0 {\n\t\t\topts = append(opts, chainntnfs.WithIncludeBlock())\n\t\t}\n\n\t\ttxid, pkScript, err := chainntnfs.GetTestTxidAndScript(miner)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to create test addr: %v\", err)\n\t\t}\n\t\tvar confIntent *chainntnfs.ConfirmationEvent\n\t\tif scriptDispatch {\n\t\t\tconfIntent, err = notifier.RegisterConfirmationsNtfn(\n\t\t\t\tnil, pkScript, numConfs, uint32(currentHeight),\n\t\t\t\topts...,\n\t\t\t)\n\t\t} else {\n\t\t\tconfIntent, err = notifier.RegisterConfirmationsNtfn(\n\t\t\t\ttxid, pkScript, numConfs, uint32(currentHeight),\n\t\t\t\topts...,\n\t\t\t)\n\t\t}\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to register ntfn: %v\", err)\n\t\t}\n\t\tconfIntents[i] = confIntent\n\t\tif err := chainntnfs.WaitForMempoolTx(miner, txid); err != nil {\n\t\t\tt.Fatalf(\"tx not relayed to miner: %v\", err)\n\t\t}\n\n\t}\n\n\tinitialConfHeight := uint32(currentHeight + 1)\n\n\t// Now, for each confirmation intent, generate the delta number of blocks\n\t// needed to trigger the confirmation notification. A goroutine is\n\t// spawned in order to verify the proper notification is triggered.\n\tfor i, numConfs := range confSpread {\n\t\tvar blocksToGen uint32\n\n\t\t// If this is the last instance, manually index to generate the\n\t\t// proper block delta in order to avoid a panic.\n\t\tif i == len(confSpread)-1 {\n\t\t\tblocksToGen = confSpread[len(confSpread)-1] - confSpread[len(confSpread)-2]\n\t\t} else {\n\t\t\tblocksToGen = confSpread[i+1] - confSpread[i]\n\t\t}\n\n\t\t// Generate the number of blocks necessary to trigger this\n\t\t// current confirmation notification.\n\t\tif _, err := miner.Client.Generate(blocksToGen); err != nil {\n\t\t\tt.Fatalf(\"unable to generate single block: %v\", err)\n\t\t}\n\n\t\tselect {\n\t\tcase conf := <-confIntents[i].Confirmed:\n\t\t\t// All of the notifications above were originally\n\t\t\t// confirmed in the same block. The returned\n\t\t\t// notification should list the initial confirmation\n\t\t\t// height rather than the height they were _fully_\n\t\t\t// confirmed.\n\t\t\tif conf.BlockHeight != initialConfHeight {\n\t\t\t\tt.Fatalf(\"notification has incorrect initial \"+\n\t\t\t\t\t\"conf height: expected %v, got %v\",\n\t\t\t\t\tinitialConfHeight, conf.BlockHeight)\n\t\t\t}\n\n\t\t\t// If this is an even client index, then we expect the\n\t\t\t// block to be populated. Otherwise, it should be\n\t\t\t// empty.\n\t\t\texpectBlock := i%2 == 0\n\t\t\trequire.Equal(t, expectBlock, conf.Block != nil)\n\t\t\tcontinue\n\t\tcase <-time.After(20 * time.Second):\n\t\t\tt.Fatalf(\"confirmation notification never received: %v\", numConfs)\n\t\t}\n\t}\n}\n",
      "length": 3196,
      "tokens": 444,
      "embedding": []
    },
    {
      "slug": "func checkNotificationFields(ntfn *chainntnfs.SpendDetail,",
      "content": "func checkNotificationFields(ntfn *chainntnfs.SpendDetail,\n\toutpoint *wire.OutPoint, spenderSha *chainhash.Hash,\n\theight int32, t *testing.T) {\n\n\tt.Helper()\n\n\tif *ntfn.SpentOutPoint != *outpoint {\n\t\tt.Fatalf(\"ntfn includes wrong output, reports \"+\n\t\t\t\"%v instead of %v\",\n\t\t\tntfn.SpentOutPoint, outpoint)\n\t}\n\tif !bytes.Equal(ntfn.SpenderTxHash[:], spenderSha[:]) {\n\t\tt.Fatalf(\"ntfn includes wrong spender tx sha, \"+\n\t\t\t\"reports %v instead of %v\",\n\t\t\tntfn.SpenderTxHash[:], spenderSha[:])\n\t}\n\tif ntfn.SpenderInputIndex != 0 {\n\t\tt.Fatalf(\"ntfn includes wrong spending input \"+\n\t\t\t\"index, reports %v, should be %v\",\n\t\t\tntfn.SpenderInputIndex, 0)\n\t}\n\tif ntfn.SpendingHeight != height {\n\t\tt.Fatalf(\"ntfn has wrong spending height: \"+\n\t\t\t\"expected %v, got %v\", height,\n\t\t\tntfn.SpendingHeight)\n\t}\n}\n",
      "length": 706,
      "tokens": 86,
      "embedding": []
    },
    {
      "slug": "func testSpendNotification(miner *rpctest.Harness,",
      "content": "func testSpendNotification(miner *rpctest.Harness,\n\tnotifier chainntnfs.TestChainNotifier, scriptDispatch bool, t *testing.T) {\n\n\t// We'd like to test the spend notifications for all ChainNotifier\n\t// concrete implementations.\n\t//\n\t// To do so, we first create a new output to our test target address.\n\toutpoint, output, privKey := chainntnfs.CreateSpendableOutput(t, miner)\n\n\t_, currentHeight, err := miner.Client.GetBestBlock()\n\trequire.NoError(t, err, \"unable to get current height\")\n\n\t// Now that we have an output index and the pkScript, register for a\n\t// spentness notification for the newly created output with multiple\n\t// clients in order to ensure the implementation can support\n\t// multi-client spend notifications.\n\tconst numClients = 5\n\tspendClients := make([]*chainntnfs.SpendEvent, numClients)\n\tfor i := 0; i < numClients; i++ {\n\t\tvar spentIntent *chainntnfs.SpendEvent\n\t\tif scriptDispatch {\n\t\t\tspentIntent, err = notifier.RegisterSpendNtfn(\n\t\t\t\tnil, output.PkScript, uint32(currentHeight),\n\t\t\t)\n\t\t} else {\n\t\t\tspentIntent, err = notifier.RegisterSpendNtfn(\n\t\t\t\toutpoint, output.PkScript, uint32(currentHeight),\n\t\t\t)\n\t\t}\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to register for spend ntfn: %v\", err)\n\t\t}\n\n\t\tspendClients[i] = spentIntent\n\t}\n\n\t// Next, create a new transaction spending that output.\n\tspendingTx := chainntnfs.CreateSpendTx(t, outpoint, output, privKey)\n\n\t// Broadcast our spending transaction.\n\tspenderSha, err := miner.Client.SendRawTransaction(spendingTx, true)\n\trequire.NoError(t, err, \"unable to broadcast tx\")\n\n\tif err := chainntnfs.WaitForMempoolTx(miner, spenderSha); err != nil {\n\t\tt.Fatalf(\"tx not relayed to miner: %v\", err)\n\t}\n\n\t// Make sure notifications are not yet sent. We launch a go routine for\n\t// all the spend clients, such that we can wait for them all in\n\t// parallel.\n\tmempoolSpendTimeout := 2 * chainntnfs.TrickleInterval\n\tmempoolSpends := make(chan *chainntnfs.SpendDetail, numClients)\n\tfor _, c := range spendClients {\n\t\tgo func(client *chainntnfs.SpendEvent) {\n\t\t\tselect {\n\t\t\tcase s := <-client.Spend:\n\t\t\t\tmempoolSpends <- s\n\t\t\tcase <-time.After(mempoolSpendTimeout):\n\t\t\t}\n\t\t}(c)\n\t}\n\n\tselect {\n\tcase <-mempoolSpends:\n\t\tt.Fatalf(\"did not expect to get notification before \" +\n\t\t\t\"block was mined\")\n\tcase <-time.After(mempoolSpendTimeout):\n\t}\n\n\t// Make sure registering a client after the tx is in the mempool still\n\t// doesn't trigger a notification.\n\tvar spentIntent *chainntnfs.SpendEvent\n\tif scriptDispatch {\n\t\tspentIntent, err = notifier.RegisterSpendNtfn(\n\t\t\tnil, output.PkScript, uint32(currentHeight),\n\t\t)\n\t} else {\n\t\tspentIntent, err = notifier.RegisterSpendNtfn(\n\t\t\toutpoint, output.PkScript, uint32(currentHeight),\n\t\t)\n\t}\n\trequire.NoError(t, err, \"unable to register for spend ntfn\")\n\n\tselect {\n\tcase <-spentIntent.Spend:\n\t\tt.Fatalf(\"did not expect to get notification before \" +\n\t\t\t\"block was mined\")\n\tcase <-time.After(mempoolSpendTimeout):\n\t}\n\tspendClients = append(spendClients, spentIntent)\n\n\t// Now we mine a single block, which should include our spend. The\n\t// notification should also be sent off.\n\tif _, err := miner.Client.Generate(1); err != nil {\n\t\tt.Fatalf(\"unable to generate single block: %v\", err)\n\t}\n\n\t_, currentHeight, err = miner.Client.GetBestBlock()\n\trequire.NoError(t, err, \"unable to get current height\")\n\n\tfor _, c := range spendClients {\n\t\tselect {\n\t\tcase ntfn := <-c.Spend:\n\t\t\t// We've received the spend nftn. So now verify all the\n\t\t\t// fields have been set properly.\n\t\t\tcheckNotificationFields(ntfn, outpoint, spenderSha,\n\t\t\t\tcurrentHeight, t)\n\t\tcase <-time.After(30 * time.Second):\n\t\t\tt.Fatalf(\"spend ntfn never received\")\n\t\t}\n\t}\n}\n",
      "length": 3464,
      "tokens": 457,
      "embedding": []
    },
    {
      "slug": "func testBlockEpochNotification(miner *rpctest.Harness,",
      "content": "func testBlockEpochNotification(miner *rpctest.Harness,\n\tnotifier chainntnfs.TestChainNotifier, t *testing.T) {\n\n\t// We'd like to test the case of multiple registered clients receiving\n\t// block epoch notifications.\n\tconst numBlocks = 10\n\tconst numNtfns = numBlocks + 1\n\tconst numClients = 5\n\tvar wg sync.WaitGroup\n\n\t// Create numClients clients which will listen for block notifications. We\n\t// expect each client to receive 11 notifications, one for the current\n\t// tip of the chain, and one for each of the ten blocks we generate\n\t// below. So we'll use a WaitGroup to synchronize the test.\n\tclientErrors := make(chan error, numClients)\n\tfor i := 0; i < numClients; i++ {\n\t\tepochClient, err := notifier.RegisterBlockEpochNtfn(nil)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to register for epoch notification\")\n\t\t}\n\n\t\twg.Add(numNtfns)\n\t\tgo func() {\n\t\t\tfor i := 0; i < numNtfns; i++ {\n\t\t\t\t// Ensure that each block epoch has a header,\n\t\t\t\t// and that header matches the contained header\n\t\t\t\t// hash.\n\t\t\t\tblockEpoch := <-epochClient.Epochs\n\t\t\t\tif blockEpoch.BlockHeader == nil {\n\t\t\t\t\tt.Logf(\"%d\", i)\n\t\t\t\t\tclientErrors <- fmt.Errorf(\"block \" +\n\t\t\t\t\t\t\"header is nil\")\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tif blockEpoch.BlockHeader.BlockHash() !=\n\t\t\t\t\t*blockEpoch.Hash {\n\n\t\t\t\t\tclientErrors <- fmt.Errorf(\"block \" +\n\t\t\t\t\t\t\"header hash mismatch\")\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\twg.Done()\n\t\t\t}\n\t\t}()\n\t}\n\n\tepochsSent := make(chan struct{})\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(epochsSent)\n\t}()\n\n\t// Now generate 10 blocks, the clients above should each receive 10\n\t// notifications, thereby unblocking the goroutine above.\n\tif _, err := miner.Client.Generate(numBlocks); err != nil {\n\t\tt.Fatalf(\"unable to generate blocks: %v\", err)\n\t}\n\n\tselect {\n\tcase err := <-clientErrors:\n\t\tt.Fatalf(\"block epoch case failed: %v\", err)\n\tcase <-epochsSent:\n\tcase <-time.After(30 * time.Second):\n\t\tt.Fatalf(\"all notifications not sent\")\n\t}\n}\n",
      "length": 1767,
      "tokens": 257,
      "embedding": []
    },
    {
      "slug": "func testMultiClientConfirmationNotification(miner *rpctest.Harness,",
      "content": "func testMultiClientConfirmationNotification(miner *rpctest.Harness,\n\tnotifier chainntnfs.TestChainNotifier, scriptDispatch bool, t *testing.T) {\n\n\t// We'd like to test the case of a multiple clients registered to\n\t// receive a confirmation notification for the same transaction.\n\ttxid, pkScript, err := chainntnfs.GetTestTxidAndScript(miner)\n\trequire.NoError(t, err, \"unable to create test tx\")\n\tif err := chainntnfs.WaitForMempoolTx(miner, txid); err != nil {\n\t\tt.Fatalf(\"tx not relayed to miner: %v\", err)\n\t}\n\n\tvar wg sync.WaitGroup\n\tconst (\n\t\tnumConfsClients = 5\n\t\tnumConfs        = 1\n\t)\n\n\t_, currentHeight, err := miner.Client.GetBestBlock()\n\trequire.NoError(t, err, \"unable to get current height\")\n\n\t// Register for a conf notification for the above generated txid with\n\t// numConfsClients distinct clients.\n\tfor i := 0; i < numConfsClients; i++ {\n\t\tvar confClient *chainntnfs.ConfirmationEvent\n\t\tif scriptDispatch {\n\t\t\tconfClient, err = notifier.RegisterConfirmationsNtfn(\n\t\t\t\tnil, pkScript, numConfs, uint32(currentHeight),\n\t\t\t)\n\t\t} else {\n\t\t\tconfClient, err = notifier.RegisterConfirmationsNtfn(\n\t\t\t\ttxid, pkScript, numConfs, uint32(currentHeight),\n\t\t\t)\n\t\t}\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to register for confirmation: %v\", err)\n\t\t}\n\n\t\twg.Add(1)\n\t\tgo func() {\n\t\t\t<-confClient.Confirmed\n\t\t\twg.Done()\n\t\t}()\n\t}\n\n\tconfsSent := make(chan struct{})\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(confsSent)\n\t}()\n\n\t// Finally, generate a single block which should trigger the unblocking\n\t// of all numConfsClients blocked on the channel read above.\n\tif _, err := miner.Client.Generate(1); err != nil {\n\t\tt.Fatalf(\"unable to generate block: %v\", err)\n\t}\n\n\tselect {\n\tcase <-confsSent:\n\tcase <-time.After(30 * time.Second):\n\t\tt.Fatalf(\"all confirmation notifications not sent\")\n\t}\n}\n\n// Tests the case in which a confirmation notification is requested for a\n// transaction that has already been included in a block. In this case, the\n// confirmation notification should be dispatched immediately.",
      "length": 1855,
      "tokens": 252,
      "embedding": []
    },
    {
      "slug": "func testTxConfirmedBeforeNtfnRegistration(miner *rpctest.Harness,",
      "content": "func testTxConfirmedBeforeNtfnRegistration(miner *rpctest.Harness,\n\tnotifier chainntnfs.TestChainNotifier, scriptDispatch bool, t *testing.T) {\n\n\t// First, let's send some coins to \"ourself\", obtaining a txid.  We're\n\t// spending from a coinbase output here, so we use the dedicated\n\t// function.\n\ttxid3, pkScript3, err := chainntnfs.GetTestTxidAndScript(miner)\n\trequire.NoError(t, err, \"unable to create test tx\")\n\tif err := chainntnfs.WaitForMempoolTx(miner, txid3); err != nil {\n\t\tt.Fatalf(\"tx not relayed to miner: %v\", err)\n\t}\n\n\t// Generate another block containing tx 3, but we won't register conf\n\t// notifications for this tx until much later. The notifier must check\n\t// older blocks when the confirmation event is registered below to ensure\n\t// that the TXID hasn't already been included in the chain, otherwise the\n\t// notification will never be sent.\n\t_, err = miner.Client.Generate(1)\n\trequire.NoError(t, err, \"unable to generate block\")\n\n\ttxid1, pkScript1, err := chainntnfs.GetTestTxidAndScript(miner)\n\trequire.NoError(t, err, \"unable to create test tx\")\n\tif err := chainntnfs.WaitForMempoolTx(miner, txid1); err != nil {\n\t\tt.Fatalf(\"tx not relayed to miner: %v\", err)\n\t}\n\n\ttxid2, pkScript2, err := chainntnfs.GetTestTxidAndScript(miner)\n\trequire.NoError(t, err, \"unable to create test tx\")\n\tif err := chainntnfs.WaitForMempoolTx(miner, txid2); err != nil {\n\t\tt.Fatalf(\"tx not relayed to miner: %v\", err)\n\t}\n\n\t_, currentHeight, err := miner.Client.GetBestBlock()\n\trequire.NoError(t, err, \"unable to get current height\")\n\n\t// Now generate another block containing txs 1 & 2.\n\tblockHash, err := miner.Client.Generate(1)\n\trequire.NoError(t, err, \"unable to generate block\")\n\n\t// Register a confirmation notification with the chainntfn source for tx2,\n\t// which is included in the last block. The height hint is the height before\n\t// the block is included. This notification should fire immediately since\n\t// only 1 confirmation is required.\n\tvar ntfn1 *chainntnfs.ConfirmationEvent\n\tif scriptDispatch {\n\t\tntfn1, err = notifier.RegisterConfirmationsNtfn(\n\t\t\tnil, pkScript1, 1, uint32(currentHeight),\n\t\t\tchainntnfs.WithIncludeBlock(),\n\t\t)\n\t} else {\n\t\tntfn1, err = notifier.RegisterConfirmationsNtfn(\n\t\t\ttxid1, pkScript1, 1, uint32(currentHeight),\n\t\t)\n\t}\n\trequire.NoError(t, err, \"unable to register ntfn\")\n\n\tselect {\n\tcase confInfo := <-ntfn1.Confirmed:\n\t\t// Finally, we'll verify that the tx index returned is the exact same\n\t\t// as the tx index of the transaction within the block itself.\n\t\tmsgBlock, err := miner.Client.GetBlock(blockHash[0])\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to fetch block: %v\", err)\n\t\t}\n\t\tblock := btcutil.NewBlock(msgBlock)\n\t\tspecifiedTxHash, err := block.TxHash(int(confInfo.TxIndex))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to index into block: %v\", err)\n\t\t}\n\t\tif !specifiedTxHash.IsEqual(txid1) {\n\t\t\tt.Fatalf(\"mismatched tx indexes: expected %v, got %v\",\n\t\t\t\ttxid1, specifiedTxHash)\n\t\t}\n\n\t\t// We'll also ensure that the block height has been set\n\t\t// properly.\n\t\tif confInfo.BlockHeight != uint32(currentHeight+1) {\n\t\t\tt.Fatalf(\"incorrect block height: expected %v, got %v\",\n\t\t\t\tconfInfo.BlockHeight, currentHeight)\n\t\t}\n\n\t\t// Ensure that if this was a script dispatch, the block is set\n\t\t// as well.\n\t\tif scriptDispatch {\n\t\t\trequire.NotNil(t, confInfo.Block)\n\t\t}\n\n\t\tbreak\n\tcase <-time.After(20 * time.Second):\n\t\tt.Fatalf(\"confirmation notification never received\")\n\t}\n\n\t// Register a confirmation notification for tx2, requiring 3 confirmations.\n\t// This transaction is only partially confirmed, so the notification should\n\t// not fire yet.\n\tvar ntfn2 *chainntnfs.ConfirmationEvent\n\tif scriptDispatch {\n\t\tntfn2, err = notifier.RegisterConfirmationsNtfn(\n\t\t\tnil, pkScript2, 3, uint32(currentHeight),\n\t\t\tchainntnfs.WithIncludeBlock(),\n\t\t)\n\t} else {\n\t\tntfn2, err = notifier.RegisterConfirmationsNtfn(\n\t\t\ttxid2, pkScript2, 3, uint32(currentHeight),\n\t\t)\n\t}\n\trequire.NoError(t, err, \"unable to register ntfn\")\n\n\t// Fully confirm tx3.\n\t_, err = miner.Client.Generate(2)\n\trequire.NoError(t, err, \"unable to generate block\")\n\n\tselect {\n\tcase <-ntfn2.Confirmed:\n\tcase <-time.After(10 * time.Second):\n\t\tt.Fatalf(\"confirmation notification never received\")\n\t}\n\n\tselect {\n\tcase <-ntfn1.Confirmed:\n\t\tt.Fatalf(\"received multiple confirmations for tx\")\n\tcase <-time.After(1 * time.Second):\n\t}\n\n\t// Finally register a confirmation notification for tx3, requiring 1\n\t// confirmation. Ensure that conf notifications do not refire on txs\n\t// 1 or 2.\n\tvar ntfn3 *chainntnfs.ConfirmationEvent\n\tif scriptDispatch {\n\t\tntfn3, err = notifier.RegisterConfirmationsNtfn(\n\t\t\tnil, pkScript3, 1, uint32(currentHeight-1),\n\t\t\tchainntnfs.WithIncludeBlock(),\n\t\t)\n\t} else {\n\t\tntfn3, err = notifier.RegisterConfirmationsNtfn(\n\t\t\ttxid3, pkScript3, 1, uint32(currentHeight-1),\n\t\t)\n\t}\n\trequire.NoError(t, err, \"unable to register ntfn\")\n\n\t// We'll also register for a confirmation notification with the pkscript\n\t// of a different transaction. This notification shouldn't fire since we\n\t// match on both txid and pkscript.\n\tvar ntfn4 *chainntnfs.ConfirmationEvent\n\tntfn4, err = notifier.RegisterConfirmationsNtfn(\n\t\ttxid3, pkScript2, 1, uint32(currentHeight-1),\n\t)\n\trequire.NoError(t, err, \"unable to register ntfn\")\n\n\tselect {\n\tcase confInfo := <-ntfn3.Confirmed:\n\t\tif scriptDispatch {\n\t\t\trequire.NotNil(t, confInfo.Block)\n\t\t}\n\tcase <-time.After(10 * time.Second):\n\t\tt.Fatalf(\"confirmation notification never received\")\n\t}\n\n\tselect {\n\tcase <-ntfn4.Confirmed:\n\t\tt.Fatalf(\"confirmation notification received\")\n\tcase <-time.After(5 * time.Second):\n\t}\n\n\ttime.Sleep(1 * time.Second)\n\n\tselect {\n\tcase <-ntfn1.Confirmed:\n\t\tt.Fatalf(\"received multiple confirmations for tx\")\n\tdefault:\n\t}\n\n\tselect {\n\tcase <-ntfn2.Confirmed:\n\t\tt.Fatalf(\"received multiple confirmations for tx\")\n\tdefault:\n\t}\n}\n\n// Test the case of a notification consumer having forget or being delayed in\n// checking for a confirmation. This should not cause the notifier to stop\n// working",
      "length": 5701,
      "tokens": 731,
      "embedding": []
    },
    {
      "slug": "func testLazyNtfnConsumer(miner *rpctest.Harness,",
      "content": "func testLazyNtfnConsumer(miner *rpctest.Harness,\n\tnotifier chainntnfs.TestChainNotifier, scriptDispatch bool, t *testing.T) {\n\n\t// Create a transaction to be notified about. We'll register for\n\t// notifications on this transaction but won't be prompt in checking them\n\ttxid, pkScript, err := chainntnfs.GetTestTxidAndScript(miner)\n\trequire.NoError(t, err, \"unable to create test tx\")\n\tif err := chainntnfs.WaitForMempoolTx(miner, txid); err != nil {\n\t\tt.Fatalf(\"tx not relayed to miner: %v\", err)\n\t}\n\n\t_, currentHeight, err := miner.Client.GetBestBlock()\n\trequire.NoError(t, err, \"unable to get current height\")\n\n\tnumConfs := uint32(3)\n\n\t// Add a block right before registering, this makes race conditions\n\t// between the historical dispatcher and the normal dispatcher more obvious\n\tif _, err := miner.Client.Generate(1); err != nil {\n\t\tt.Fatalf(\"unable to generate blocks: %v\", err)\n\t}\n\n\tvar firstConfIntent *chainntnfs.ConfirmationEvent\n\tif scriptDispatch {\n\t\tfirstConfIntent, err = notifier.RegisterConfirmationsNtfn(\n\t\t\tnil, pkScript, numConfs, uint32(currentHeight),\n\t\t)\n\t} else {\n\t\tfirstConfIntent, err = notifier.RegisterConfirmationsNtfn(\n\t\t\ttxid, pkScript, numConfs, uint32(currentHeight),\n\t\t)\n\t}\n\trequire.NoError(t, err, \"unable to register ntfn\")\n\n\t// Generate another 2 blocks, this should dispatch the confirm notification\n\tif _, err := miner.Client.Generate(2); err != nil {\n\t\tt.Fatalf(\"unable to generate blocks: %v\", err)\n\t}\n\n\t// Now make another transaction, just because we haven't checked to see\n\t// if the first transaction has confirmed doesn't mean that we shouldn't\n\t// be able to see if this transaction confirms first\n\ttxid, pkScript, err = chainntnfs.GetTestTxidAndScript(miner)\n\trequire.NoError(t, err, \"unable to create test tx\")\n\tif err := chainntnfs.WaitForMempoolTx(miner, txid); err != nil {\n\t\tt.Fatalf(\"tx not relayed to miner: %v\", err)\n\t}\n\n\t_, currentHeight, err = miner.Client.GetBestBlock()\n\trequire.NoError(t, err, \"unable to get current height\")\n\n\tnumConfs = 1\n\tvar secondConfIntent *chainntnfs.ConfirmationEvent\n\tif scriptDispatch {\n\t\tsecondConfIntent, err = notifier.RegisterConfirmationsNtfn(\n\t\t\tnil, pkScript, numConfs, uint32(currentHeight),\n\t\t)\n\t} else {\n\t\tsecondConfIntent, err = notifier.RegisterConfirmationsNtfn(\n\t\t\ttxid, pkScript, numConfs, uint32(currentHeight),\n\t\t)\n\t}\n\trequire.NoError(t, err, \"unable to register ntfn\")\n\n\tif _, err := miner.Client.Generate(1); err != nil {\n\t\tt.Fatalf(\"unable to generate blocks: %v\", err)\n\t}\n\n\tselect {\n\tcase <-secondConfIntent.Confirmed:\n\t\t// Successfully receive the second notification\n\t\tbreak\n\tcase <-time.After(30 * time.Second):\n\t\tt.Fatalf(\"Second confirmation notification never received\")\n\t}\n\n\t// Make sure the first tx confirmed successfully\n\tselect {\n\tcase <-firstConfIntent.Confirmed:\n\t\tbreak\n\tcase <-time.After(30 * time.Second):\n\t\tt.Fatalf(\"First confirmation notification never received\")\n\t}\n}\n\n// Tests the case in which a spend notification is requested for a spend that\n// has already been included in a block. In this case, the spend notification\n// should be dispatched immediately.",
      "length": 2954,
      "tokens": 380,
      "embedding": []
    },
    {
      "slug": "func testSpendBeforeNtfnRegistration(miner *rpctest.Harness,",
      "content": "func testSpendBeforeNtfnRegistration(miner *rpctest.Harness,\n\tnotifier chainntnfs.TestChainNotifier, scriptDispatch bool, t *testing.T) {\n\n\t// We'd like to test the spend notifications for all ChainNotifier\n\t// concrete implementations.\n\t//\n\t// To do so, we first create a new output to our test target address.\n\toutpoint, output, privKey := chainntnfs.CreateSpendableOutput(t, miner)\n\n\t_, heightHint, err := miner.Client.GetBestBlock()\n\trequire.NoError(t, err, \"unable to get current height\")\n\n\t// We'll then spend this output and broadcast the spend transaction.\n\tspendingTx := chainntnfs.CreateSpendTx(t, outpoint, output, privKey)\n\tspenderSha, err := miner.Client.SendRawTransaction(spendingTx, true)\n\trequire.NoError(t, err, \"unable to broadcast tx\")\n\tif err := chainntnfs.WaitForMempoolTx(miner, spenderSha); err != nil {\n\t\tt.Fatalf(\"tx not relayed to miner: %v\", err)\n\t}\n\n\t// We create an epoch client we can use to make sure the notifier is\n\t// caught up to the mining node's chain.\n\tepochClient, err := notifier.RegisterBlockEpochNtfn(nil)\n\trequire.NoError(t, err, \"unable to register for block epoch\")\n\n\t// Now we mine an additional block, which should include our spend.\n\tif _, err := miner.Client.Generate(1); err != nil {\n\t\tt.Fatalf(\"unable to generate single block: %v\", err)\n\t}\n\t_, spendHeight, err := miner.Client.GetBestBlock()\n\trequire.NoError(t, err, \"unable to get current height\")\n\n\t// checkSpends registers two clients to be notified of a spend that has\n\t// already happened. The notifier should dispatch a spend notification\n\t// immediately.\n\tcheckSpends := func() {\n\t\tt.Helper()\n\n\t\tconst numClients = 2\n\t\tspendClients := make([]*chainntnfs.SpendEvent, numClients)\n\t\tfor i := 0; i < numClients; i++ {\n\t\t\tvar spentIntent *chainntnfs.SpendEvent\n\t\t\tif scriptDispatch {\n\t\t\t\tspentIntent, err = notifier.RegisterSpendNtfn(\n\t\t\t\t\tnil, output.PkScript, uint32(heightHint),\n\t\t\t\t)\n\t\t\t} else {\n\t\t\t\tspentIntent, err = notifier.RegisterSpendNtfn(\n\t\t\t\t\toutpoint, output.PkScript,\n\t\t\t\t\tuint32(heightHint),\n\t\t\t\t)\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"unable to register for spend ntfn: %v\",\n\t\t\t\t\terr)\n\t\t\t}\n\n\t\t\tspendClients[i] = spentIntent\n\t\t}\n\n\t\tfor _, client := range spendClients {\n\t\t\tselect {\n\t\t\tcase ntfn := <-client.Spend:\n\t\t\t\t// We've received the spend nftn. So now verify\n\t\t\t\t// all the fields have been set properly.\n\t\t\t\tcheckNotificationFields(\n\t\t\t\t\tntfn, outpoint, spenderSha, spendHeight, t,\n\t\t\t\t)\n\t\t\tcase <-time.After(30 * time.Second):\n\t\t\t\tt.Fatalf(\"spend ntfn never received\")\n\t\t\t}\n\t\t}\n\t}\n\n\t// Wait for the notifier to have caught up to the mined block.\n\tselect {\n\tcase _, ok := <-epochClient.Epochs:\n\t\tif !ok {\n\t\t\tt.Fatalf(\"epoch channel was closed\")\n\t\t}\n\tcase <-time.After(15 * time.Second):\n\t\tt.Fatalf(\"did not receive block epoch\")\n\t}\n\n\t// Check that the spend clients gets immediately notified for the spend\n\t// in the previous block.\n\tcheckSpends()\n\n\t// Bury the spend even deeper, and do the same check.\n\tconst numBlocks = 10\n\tif _, err := miner.Client.Generate(numBlocks); err != nil {\n\t\tt.Fatalf(\"unable to generate single block: %v\", err)\n\t}\n\n\t// Wait for the notifier to have caught up with the new blocks.\n\tfor i := 0; i < numBlocks; i++ {\n\t\tselect {\n\t\tcase _, ok := <-epochClient.Epochs:\n\t\t\tif !ok {\n\t\t\t\tt.Fatalf(\"epoch channel was closed\")\n\t\t\t}\n\t\tcase <-time.After(15 * time.Second):\n\t\t\tt.Fatalf(\"did not receive block epoch\")\n\t\t}\n\t}\n\n\t// The clients should still be notified immediately.\n\tcheckSpends()\n}\n",
      "length": 3273,
      "tokens": 461,
      "embedding": []
    },
    {
      "slug": "func testCancelSpendNtfn(node *rpctest.Harness,",
      "content": "func testCancelSpendNtfn(node *rpctest.Harness,\n\tnotifier chainntnfs.TestChainNotifier, scriptDispatch bool, t *testing.T) {\n\n\t// We'd like to test that once a spend notification is registered, it\n\t// can be canceled before the notification is dispatched.\n\n\t// First, we'll start by creating a new output that we can spend\n\t// ourselves.\n\toutpoint, output, privKey := chainntnfs.CreateSpendableOutput(t, node)\n\n\t_, currentHeight, err := node.Client.GetBestBlock()\n\trequire.NoError(t, err, \"unable to get current height\")\n\n\t// Create two clients that each registered to the spend notification.\n\t// We'll cancel the notification for the first client and leave the\n\t// notification for the second client enabled.\n\tconst numClients = 2\n\tspendClients := make([]*chainntnfs.SpendEvent, numClients)\n\tfor i := 0; i < numClients; i++ {\n\t\tvar spentIntent *chainntnfs.SpendEvent\n\t\tif scriptDispatch {\n\t\t\tspentIntent, err = notifier.RegisterSpendNtfn(\n\t\t\t\tnil, output.PkScript, uint32(currentHeight),\n\t\t\t)\n\t\t} else {\n\t\t\tspentIntent, err = notifier.RegisterSpendNtfn(\n\t\t\t\toutpoint, output.PkScript, uint32(currentHeight),\n\t\t\t)\n\t\t}\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to register for spend ntfn: %v\", err)\n\t\t}\n\n\t\tspendClients[i] = spentIntent\n\t}\n\n\t// Next, create a new transaction spending that output.\n\tspendingTx := chainntnfs.CreateSpendTx(t, outpoint, output, privKey)\n\n\t// Before we broadcast the spending transaction, we'll cancel the\n\t// notification of the first client.\n\tspendClients[1].Cancel()\n\n\t// Broadcast our spending transaction.\n\tspenderSha, err := node.Client.SendRawTransaction(spendingTx, true)\n\trequire.NoError(t, err, \"unable to broadcast tx\")\n\n\tif err := chainntnfs.WaitForMempoolTx(node, spenderSha); err != nil {\n\t\tt.Fatalf(\"tx not relayed to miner: %v\", err)\n\t}\n\n\t// Now we mine a single block, which should include our spend. The\n\t// notification should also be sent off.\n\tif _, err := node.Client.Generate(1); err != nil {\n\t\tt.Fatalf(\"unable to generate single block: %v\", err)\n\t}\n\n\t// The spend notification for the first client should have been\n\t// dispatched.\n\tselect {\n\tcase ntfn := <-spendClients[0].Spend:\n\t\t// We've received the spend nftn. So now verify all the\n\t\t// fields have been set properly.\n\t\tif *ntfn.SpentOutPoint != *outpoint {\n\t\t\tt.Fatalf(\"ntfn includes wrong output, reports \"+\n\t\t\t\t\"%v instead of %v\",\n\t\t\t\tntfn.SpentOutPoint, outpoint)\n\t\t}\n\t\tif !bytes.Equal(ntfn.SpenderTxHash[:], spenderSha[:]) {\n\t\t\tt.Fatalf(\"ntfn includes wrong spender tx sha, \"+\n\t\t\t\t\"reports %v instead of %v\",\n\t\t\t\tntfn.SpenderTxHash[:], spenderSha[:])\n\t\t}\n\t\tif ntfn.SpenderInputIndex != 0 {\n\t\t\tt.Fatalf(\"ntfn includes wrong spending input \"+\n\t\t\t\t\"index, reports %v, should be %v\",\n\t\t\t\tntfn.SpenderInputIndex, 0)\n\t\t}\n\tcase <-time.After(20 * time.Second):\n\t\tt.Fatalf(\"spend ntfn never received\")\n\t}\n\n\t// However, the spend notification of the second client should NOT have\n\t// been dispatched.\n\tselect {\n\tcase _, ok := <-spendClients[1].Spend:\n\t\tif ok {\n\t\t\tt.Fatalf(\"spend ntfn should have been canceled\")\n\t\t}\n\tcase <-time.After(20 * time.Second):\n\t\tt.Fatalf(\"spend ntfn never canceled\")\n\t}\n}\n",
      "length": 2960,
      "tokens": 399,
      "embedding": []
    },
    {
      "slug": "func testCancelEpochNtfn(node *rpctest.Harness,",
      "content": "func testCancelEpochNtfn(node *rpctest.Harness,\n\tnotifier chainntnfs.TestChainNotifier, t *testing.T) {\n\n\t// We'd like to ensure that once a client cancels their block epoch\n\t// notifications, no further notifications are sent over the channel\n\t// if/when new blocks come in.\n\tconst numClients = 2\n\n\tepochClients := make([]*chainntnfs.BlockEpochEvent, numClients)\n\tfor i := 0; i < numClients; i++ {\n\t\tepochClient, err := notifier.RegisterBlockEpochNtfn(nil)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to register for epoch notification\")\n\t\t}\n\t\tepochClients[i] = epochClient\n\t}\n\n\t// Now before we mine any blocks, cancel the notification for the first\n\t// epoch client.\n\tepochClients[0].Cancel()\n\n\t// Now mine a single block, this should trigger the logic to dispatch\n\t// epoch notifications.\n\tif _, err := node.Client.Generate(1); err != nil {\n\t\tt.Fatalf(\"unable to generate blocks: %v\", err)\n\t}\n\n\t// The epoch notification for the first client shouldn't have been\n\t// dispatched.\n\tselect {\n\tcase _, ok := <-epochClients[0].Epochs:\n\t\tif ok {\n\t\t\tt.Fatalf(\"epoch notification should have been canceled\")\n\t\t}\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatalf(\"epoch notification not sent\")\n\t}\n\n\t// However, the epoch notification for the second client should have\n\t// been dispatched as normal.\n\tselect {\n\tcase _, ok := <-epochClients[1].Epochs:\n\t\tif !ok {\n\t\t\tt.Fatalf(\"epoch was canceled\")\n\t\t}\n\tcase <-time.After(20 * time.Second):\n\t\tt.Fatalf(\"epoch notification not sent\")\n\t}\n}\n",
      "length": 1374,
      "tokens": 199,
      "embedding": []
    },
    {
      "slug": "func testReorgConf(miner *rpctest.Harness,",
      "content": "func testReorgConf(miner *rpctest.Harness,\n\tnotifier chainntnfs.TestChainNotifier, scriptDispatch bool, t *testing.T) {\n\n\t// Set up a new miner that we can use to cause a reorg.\n\tminer2, err := rpctest.New(\n\t\tchainntnfs.NetParams, nil, []string{\"--txindex\"}, \"\",\n\t)\n\trequire.NoError(t, err, \"unable to create mining node\")\n\tif err := miner2.SetUp(false, 0); err != nil {\n\t\tt.Fatalf(\"unable to set up mining node: %v\", err)\n\t}\n\tdefer miner2.TearDown()\n\n\t// We start by connecting the new miner to our original miner,\n\t// such that it will sync to our original chain.\n\tif err := rpctest.ConnectNode(miner, miner2); err != nil {\n\t\tt.Fatalf(\"unable to connect harnesses: %v\", err)\n\t}\n\tnodeSlice := []*rpctest.Harness{miner, miner2}\n\tif err := rpctest.JoinNodes(nodeSlice, rpctest.Blocks); err != nil {\n\t\tt.Fatalf(\"unable to join node on blocks: %v\", err)\n\t}\n\n\t// The two should be on the same blockheight.\n\t_, nodeHeight1, err := miner.Client.GetBestBlock()\n\tif err != nil {\n\t\tt.Fatalf(\"unable to get current blockheight %v\", err)\n\t}\n\n\t_, nodeHeight2, err := miner2.Client.GetBestBlock()\n\tif err != nil {\n\t\tt.Fatalf(\"unable to get current blockheight %v\", err)\n\t}\n\n\tif nodeHeight1 != nodeHeight2 {\n\t\tt.Fatalf(\"expected both miners to be on the same height: %v vs %v\",\n\t\t\tnodeHeight1, nodeHeight2)\n\t}\n\n\t// We disconnect the two nodes, such that we can start mining on them\n\t// individually without the other one learning about the new blocks.\n\terr = miner.Client.AddNode(miner2.P2PAddress(), rpcclient.ANRemove)\n\trequire.NoError(t, err, \"unable to remove node\")\n\n\ttxid, pkScript, err := chainntnfs.GetTestTxidAndScript(miner)\n\trequire.NoError(t, err, \"unable to create test tx\")\n\tif err := chainntnfs.WaitForMempoolTx(miner, txid); err != nil {\n\t\tt.Fatalf(\"tx not relayed to miner: %v\", err)\n\t}\n\n\t_, currentHeight, err := miner.Client.GetBestBlock()\n\trequire.NoError(t, err, \"unable to get current height\")\n\n\t// Now that we have a txid, register a confirmation notification with\n\t// the chainntfn source.\n\tnumConfs := uint32(2)\n\tvar confIntent *chainntnfs.ConfirmationEvent\n\tif scriptDispatch {\n\t\tconfIntent, err = notifier.RegisterConfirmationsNtfn(\n\t\t\tnil, pkScript, numConfs, uint32(currentHeight),\n\t\t)\n\t} else {\n\t\tconfIntent, err = notifier.RegisterConfirmationsNtfn(\n\t\t\ttxid, pkScript, numConfs, uint32(currentHeight),\n\t\t)\n\t}\n\trequire.NoError(t, err, \"unable to register ntfn\")\n\n\t// Now generate a single block, the transaction should be included.\n\t_, err = miner.Client.Generate(1)\n\trequire.NoError(t, err, \"unable to generate single block\")\n\n\t// Transaction only has one confirmation, and the notification is registered\n\t// with 2 confirmations, so we should not be notified yet.\n\tselect {\n\tcase <-confIntent.Confirmed:\n\t\tt.Fatal(\"tx was confirmed unexpectedly\")\n\tcase <-time.After(1 * time.Second):\n\t}\n\n\t// Reorganize transaction out of the chain by generating a longer fork\n\t// from the other miner. The transaction is not included in this fork.\n\t_, err = miner2.Client.Generate(2)\n\trequire.NoError(t, err)\n\n\t// Reconnect nodes to reach consensus on the longest chain. miner2's chain\n\t// should win and become active on miner1.\n\tif err := rpctest.ConnectNode(miner, miner2); err != nil {\n\t\tt.Fatalf(\"unable to connect harnesses: %v\", err)\n\t}\n\tnodeSlice = []*rpctest.Harness{miner, miner2}\n\tif err := rpctest.JoinNodes(nodeSlice, rpctest.Blocks); err != nil {\n\t\tt.Fatalf(\"unable to join node on blocks: %v\", err)\n\t}\n\n\t_, nodeHeight1, err = miner.Client.GetBestBlock()\n\tif err != nil {\n\t\tt.Fatalf(\"unable to get current blockheight %v\", err)\n\t}\n\n\t_, nodeHeight2, err = miner2.Client.GetBestBlock()\n\tif err != nil {\n\t\tt.Fatalf(\"unable to get current blockheight %v\", err)\n\t}\n\n\tif nodeHeight1 != nodeHeight2 {\n\t\tt.Fatalf(\"expected both miners to be on the same height: %v vs %v\",\n\t\t\tnodeHeight1, nodeHeight2)\n\t}\n\n\t// Even though there is one block above the height of the block that the\n\t// transaction was included in, it is not the active chain so the\n\t// notification should not be sent.\n\tselect {\n\tcase <-confIntent.Confirmed:\n\t\tt.Fatal(\"tx was confirmed unexpectedly\")\n\tcase <-time.After(1 * time.Second):\n\t}\n\n\t// Now confirm the transaction on the longest chain and verify that we\n\t// receive the notification.\n\ttx, err := miner.Client.GetRawTransaction(txid)\n\trequire.NoError(t, err, \"unable to get raw tx\")\n\n\ttxid, err = miner2.Client.SendRawTransaction(tx.MsgTx(), false)\n\trequire.NoError(t, err, \"unable to get send tx\")\n\tif err := chainntnfs.WaitForMempoolTx(miner, txid); err != nil {\n\t\tt.Fatalf(\"tx not relayed to miner: %v\", err)\n\t}\n\n\t_, err = miner.Client.Generate(3)\n\trequire.NoError(t, err, \"unable to generate single block\")\n\n\tselect {\n\tcase <-confIntent.Confirmed:\n\tcase <-time.After(20 * time.Second):\n\t\tt.Fatalf(\"confirmation notification never received\")\n\t}\n}\n\n// testReorgSpend ensures that the different ChainNotifier implementations\n// correctly handle outpoints whose spending transaction has been reorged out of\n// the chain.",
      "length": 4768,
      "tokens": 665,
      "embedding": []
    },
    {
      "slug": "func testReorgSpend(miner *rpctest.Harness,",
      "content": "func testReorgSpend(miner *rpctest.Harness,\n\tnotifier chainntnfs.TestChainNotifier, scriptDispatch bool, t *testing.T) {\n\n\t// We'll start by creating an output and registering a spend\n\t// notification for it.\n\toutpoint, output, privKey := chainntnfs.CreateSpendableOutput(t, miner)\n\t_, heightHint, err := miner.Client.GetBestBlock()\n\trequire.NoError(t, err, \"unable to retrieve current height\")\n\n\tvar spendIntent *chainntnfs.SpendEvent\n\tif scriptDispatch {\n\t\tspendIntent, err = notifier.RegisterSpendNtfn(\n\t\t\tnil, output.PkScript, uint32(heightHint),\n\t\t)\n\t} else {\n\t\tspendIntent, err = notifier.RegisterSpendNtfn(\n\t\t\toutpoint, output.PkScript, uint32(heightHint),\n\t\t)\n\t}\n\trequire.NoError(t, err, \"unable to register for spend\")\n\n\t// Set up a new miner that we can use to cause a reorg.\n\tminer2, err := rpctest.New(\n\t\tchainntnfs.NetParams, nil, []string{\"--txindex\"}, \"\",\n\t)\n\trequire.NoError(t, err, \"unable to create mining node\")\n\tif err := miner2.SetUp(false, 0); err != nil {\n\t\tt.Fatalf(\"unable to set up mining node: %v\", err)\n\t}\n\tdefer miner2.TearDown()\n\n\t// We start by connecting the new miner to our original miner, in order\n\t// to have a consistent view of the chain from both miners. They should\n\t// be on the same block height.\n\tif err := rpctest.ConnectNode(miner, miner2); err != nil {\n\t\tt.Fatalf(\"unable to connect miners: %v\", err)\n\t}\n\tnodeSlice := []*rpctest.Harness{miner, miner2}\n\tif err := rpctest.JoinNodes(nodeSlice, rpctest.Blocks); err != nil {\n\t\tt.Fatalf(\"unable to sync miners: %v\", err)\n\t}\n\t_, minerHeight1, err := miner.Client.GetBestBlock()\n\trequire.NoError(t, err, \"unable to get miner1's current height\")\n\t_, minerHeight2, err := miner2.Client.GetBestBlock()\n\trequire.NoError(t, err, \"unable to get miner2's current height\")\n\tif minerHeight1 != minerHeight2 {\n\t\tt.Fatalf(\"expected both miners to be on the same height: \"+\n\t\t\t\"%v vs %v\", minerHeight1, minerHeight2)\n\t}\n\n\t// We disconnect the two nodes, such that we can start mining on them\n\t// individually without the other one learning about the new blocks.\n\terr = miner.Client.AddNode(miner2.P2PAddress(), rpcclient.ANRemove)\n\trequire.NoError(t, err, \"unable to disconnect miners\")\n\n\t// Craft the spending transaction for the outpoint created above and\n\t// confirm it under the chain of the original miner.\n\tspendTx := chainntnfs.CreateSpendTx(t, outpoint, output, privKey)\n\tspendTxHash, err := miner.Client.SendRawTransaction(spendTx, true)\n\trequire.NoError(t, err, \"unable to broadcast spend tx\")\n\tif err := chainntnfs.WaitForMempoolTx(miner, spendTxHash); err != nil {\n\t\tt.Fatalf(\"spend tx not relayed to miner: %v\", err)\n\t}\n\tconst numBlocks = 1\n\tif _, err := miner.Client.Generate(numBlocks); err != nil {\n\t\tt.Fatalf(\"unable to generate blocks: %v\", err)\n\t}\n\t_, spendHeight, err := miner.Client.GetBestBlock()\n\trequire.NoError(t, err, \"unable to get spend height\")\n\n\t// We should see a spend notification dispatched with the correct spend\n\t// details.\n\tselect {\n\tcase spendDetails := <-spendIntent.Spend:\n\t\tcheckNotificationFields(\n\t\t\tspendDetails, outpoint, spendTxHash, spendHeight, t,\n\t\t)\n\tcase <-time.After(5 * time.Second):\n\t\tt.Fatal(\"expected spend notification to be dispatched\")\n\t}\n\n\t// Now, with the other miner, we'll generate one more block than the\n\t// other miner and connect them to cause a reorg.\n\tif _, err := miner2.Client.Generate(numBlocks + 1); err != nil {\n\t\tt.Fatalf(\"unable to generate blocks: %v\", err)\n\t}\n\tif err := rpctest.ConnectNode(miner, miner2); err != nil {\n\t\tt.Fatalf(\"unable to connect miners: %v\", err)\n\t}\n\tnodeSlice = []*rpctest.Harness{miner2, miner}\n\tif err := rpctest.JoinNodes(nodeSlice, rpctest.Blocks); err != nil {\n\t\tt.Fatalf(\"unable to sync miners: %v\", err)\n\t}\n\t_, minerHeight1, err = miner.Client.GetBestBlock()\n\trequire.NoError(t, err, \"unable to get miner1's current height\")\n\t_, minerHeight2, err = miner2.Client.GetBestBlock()\n\trequire.NoError(t, err, \"unable to get miner2's current height\")\n\tif minerHeight1 != minerHeight2 {\n\t\tt.Fatalf(\"expected both miners to be on the same height: \"+\n\t\t\t\"%v vs %v\", minerHeight1, minerHeight2)\n\t}\n\n\t// We should receive a reorg notification.\n\tselect {\n\tcase _, ok := <-spendIntent.Reorg:\n\t\tif !ok {\n\t\t\tt.Fatal(\"unexpected reorg channel closed\")\n\t\t}\n\tcase <-time.After(5 * time.Second):\n\t\tt.Fatal(\"expected to receive reorg notification\")\n\t}\n\n\t// Now that both miners are on the same chain, we'll confirm the\n\t// spending transaction of the outpoint and receive a notification for\n\t// it.\n\tif _, err = miner2.Client.SendRawTransaction(spendTx, true); err != nil {\n\t\tt.Fatalf(\"unable to broadcast spend tx: %v\", err)\n\t}\n\tif err := chainntnfs.WaitForMempoolTx(miner, spendTxHash); err != nil {\n\t\tt.Fatalf(\"tx not relayed to miner: %v\", err)\n\t}\n\tif _, err := miner.Client.Generate(numBlocks); err != nil {\n\t\tt.Fatalf(\"unable to generate single block: %v\", err)\n\t}\n\t_, spendHeight, err = miner.Client.GetBestBlock()\n\trequire.NoError(t, err, \"unable to retrieve current height\")\n\n\tselect {\n\tcase spendDetails := <-spendIntent.Spend:\n\t\tcheckNotificationFields(\n\t\t\tspendDetails, outpoint, spendTxHash, spendHeight, t,\n\t\t)\n\tcase <-time.After(5 * time.Second):\n\t\tt.Fatal(\"expected spend notification to be dispatched\")\n\t}\n}\n\n// testCatchUpClientOnMissedBlocks tests the case of multiple registered client\n// receiving historical block epoch notifications due to their best known block\n// being out of date.",
      "length": 5190,
      "tokens": 700,
      "embedding": []
    },
    {
      "slug": "func testCatchUpClientOnMissedBlocks(miner *rpctest.Harness,",
      "content": "func testCatchUpClientOnMissedBlocks(miner *rpctest.Harness,\n\tnotifier chainntnfs.TestChainNotifier, t *testing.T) {\n\n\tconst numBlocks = 10\n\tconst numClients = 5\n\tvar wg sync.WaitGroup\n\n\toutdatedHash, outdatedHeight, err := miner.Client.GetBestBlock()\n\trequire.NoError(t, err, \"unable to retrieve current height\")\n\n\t// This function is used by UnsafeStart to ensure all notifications\n\t// are fully drained before clients register for notifications.\n\tgenerateBlocks := func() error {\n\t\t_, err = miner.Client.Generate(numBlocks)\n\t\treturn err\n\t}\n\n\t// We want to ensure that when a client registers for block notifications,\n\t// the notifier's best block is at the tip of the chain. If it isn't, the\n\t// client may not receive all historical notifications.\n\tbestHeight := outdatedHeight + numBlocks\n\terr = notifier.UnsafeStart(bestHeight, nil, bestHeight, generateBlocks)\n\trequire.NoError(t, err, \"unable to unsafe start the notifier\")\n\tdefer notifier.Stop()\n\n\t// Create numClients clients whose best known block is 10 blocks behind\n\t// the tip of the chain. We expect each client to receive numBlocks\n\t// notifications, 1 for each block  they're behind.\n\tclients := make([]*chainntnfs.BlockEpochEvent, 0, numClients)\n\toutdatedBlock := &chainntnfs.BlockEpoch{\n\t\tHeight: outdatedHeight, Hash: outdatedHash,\n\t}\n\tfor i := 0; i < numClients; i++ {\n\t\tepochClient, err := notifier.RegisterBlockEpochNtfn(outdatedBlock)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to register for epoch notification: %v\", err)\n\t\t}\n\t\tclients = append(clients, epochClient)\n\t}\n\tfor expectedHeight := outdatedHeight + 1; expectedHeight <=\n\t\tbestHeight; expectedHeight++ {\n\n\t\tfor _, epochClient := range clients {\n\t\t\tselect {\n\t\t\tcase block := <-epochClient.Epochs:\n\t\t\t\tif block.Height != expectedHeight {\n\t\t\t\t\tt.Fatalf(\"received block of height: %d, \"+\n\t\t\t\t\t\t\"expected: %d\", block.Height,\n\t\t\t\t\t\texpectedHeight)\n\t\t\t\t}\n\t\t\tcase <-time.After(20 * time.Second):\n\t\t\t\tt.Fatalf(\"did not receive historical notification \"+\n\t\t\t\t\t\"for height %d\", expectedHeight)\n\t\t\t}\n\n\t\t}\n\t}\n\n\t// Finally, ensure that an extra block notification wasn't received.\n\tanyExtras := make(chan struct{}, len(clients))\n\tfor _, epochClient := range clients {\n\t\twg.Add(1)\n\t\tgo func(epochClient *chainntnfs.BlockEpochEvent) {\n\t\t\tdefer wg.Done()\n\t\t\tselect {\n\t\t\tcase <-epochClient.Epochs:\n\t\t\t\tanyExtras <- struct{}{}\n\t\t\tcase <-time.After(5 * time.Second):\n\t\t\t}\n\t\t}(epochClient)\n\t}\n\n\twg.Wait()\n\tclose(anyExtras)\n\n\tvar extraCount int\n\tfor range anyExtras {\n\t\textraCount++\n\t}\n\n\tif extraCount > 0 {\n\t\tt.Fatalf(\"received %d unexpected block notification\", extraCount)\n\t}\n}\n\n// testCatchUpOnMissedBlocks the case of multiple registered clients receiving\n// historical block epoch notifications due to the notifier's best known block\n// being out of date.",
      "length": 2622,
      "tokens": 346,
      "embedding": []
    },
    {
      "slug": "func testCatchUpOnMissedBlocks(miner *rpctest.Harness,",
      "content": "func testCatchUpOnMissedBlocks(miner *rpctest.Harness,\n\tnotifier chainntnfs.TestChainNotifier, t *testing.T) {\n\n\tconst numBlocks = 10\n\tconst numClients = 5\n\tvar wg sync.WaitGroup\n\n\t_, bestHeight, err := miner.Client.GetBestBlock()\n\tif err != nil {\n\t\tt.Fatalf(\"unable to get current blockheight %v\", err)\n\t}\n\n\t// This function is used by UnsafeStart to ensure all notifications\n\t// are fully drained before clients register for notifications.\n\tgenerateBlocks := func() error {\n\t\t_, err = miner.Client.Generate(numBlocks)\n\t\treturn err\n\t}\n\n\t// Next, start the notifier with outdated best block information.\n\terr = notifier.UnsafeStart(\n\t\tbestHeight, nil, bestHeight+numBlocks, generateBlocks,\n\t)\n\trequire.NoError(t, err, \"unable to unsafe start the notifier\")\n\tdefer notifier.Stop()\n\n\t// Create numClients clients who will listen for block notifications.\n\tclients := make([]*chainntnfs.BlockEpochEvent, 0, numClients)\n\tfor i := 0; i < numClients; i++ {\n\t\tepochClient, err := notifier.RegisterBlockEpochNtfn(nil)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to register for epoch notification: %v\", err)\n\t\t}\n\n\t\t// Drain the notification dispatched upon registration as we're\n\t\t// not interested in it.\n\t\tselect {\n\t\tcase <-epochClient.Epochs:\n\t\tcase <-time.After(5 * time.Second):\n\t\t\tt.Fatal(\"expected to receive epoch for current block \" +\n\t\t\t\t\"upon registration\")\n\t\t}\n\n\t\tclients = append(clients, epochClient)\n\t}\n\n\t// Generate a single block to trigger the backlog of historical\n\t// notifications for the previously mined blocks.\n\tif _, err := miner.Client.Generate(1); err != nil {\n\t\tt.Fatalf(\"unable to generate blocks: %v\", err)\n\t}\n\n\t// We expect each client to receive numBlocks + 1 notifications, 1 for\n\t// each block that the notifier has missed out on.\n\tfor expectedHeight := bestHeight + 1; expectedHeight <=\n\t\tbestHeight+numBlocks+1; expectedHeight++ {\n\n\t\tfor _, epochClient := range clients {\n\t\t\tselect {\n\t\t\tcase block := <-epochClient.Epochs:\n\t\t\t\tif block.Height != expectedHeight {\n\t\t\t\t\tt.Fatalf(\"received block of height: %d, \"+\n\t\t\t\t\t\t\"expected: %d\", block.Height,\n\t\t\t\t\t\texpectedHeight)\n\t\t\t\t}\n\t\t\tcase <-time.After(20 * time.Second):\n\t\t\t\tt.Fatalf(\"did not receive historical notification \"+\n\t\t\t\t\t\"for height %d\", expectedHeight)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Finally, ensure that an extra block notification wasn't received.\n\tanyExtras := make(chan struct{}, len(clients))\n\tfor _, epochClient := range clients {\n\t\twg.Add(1)\n\t\tgo func(epochClient *chainntnfs.BlockEpochEvent) {\n\t\t\tdefer wg.Done()\n\t\t\tselect {\n\t\t\tcase <-epochClient.Epochs:\n\t\t\t\tanyExtras <- struct{}{}\n\t\t\tcase <-time.After(5 * time.Second):\n\t\t\t}\n\t\t}(epochClient)\n\t}\n\n\twg.Wait()\n\tclose(anyExtras)\n\n\tvar extraCount int\n\tfor range anyExtras {\n\t\textraCount++\n\t}\n\n\tif extraCount > 0 {\n\t\tt.Fatalf(\"received %d unexpected block notification\", extraCount)\n\t}\n}\n\n// testCatchUpOnMissedBlocks tests that a client will still receive all valid\n// block notifications in the case where a notifier's best block has been reorged\n// out of the chain.",
      "length": 2832,
      "tokens": 386,
      "embedding": []
    },
    {
      "slug": "func testCatchUpOnMissedBlocksWithReorg(miner1 *rpctest.Harness,",
      "content": "func testCatchUpOnMissedBlocksWithReorg(miner1 *rpctest.Harness,\n\tnotifier chainntnfs.TestChainNotifier, t *testing.T) {\n\n\t// If this is the neutrino notifier, then we'll skip this test for now\n\t// as we're missing functionality required to ensure the test passes\n\t// reliably.\n\tif _, ok := notifier.(*neutrinonotify.NeutrinoNotifier); ok {\n\t\tt.Skip(\"skipping re-org test for neutrino\")\n\t}\n\n\tconst numBlocks = 10\n\tconst numClients = 5\n\tvar wg sync.WaitGroup\n\n\t// Set up a new miner that we can use to cause a reorg.\n\tminer2, err := rpctest.New(\n\t\tchainntnfs.NetParams, nil, []string{\"--txindex\"}, \"\",\n\t)\n\trequire.NoError(t, err, \"unable to create mining node\")\n\tif err := miner2.SetUp(false, 0); err != nil {\n\t\tt.Fatalf(\"unable to set up mining node: %v\", err)\n\t}\n\tdefer miner2.TearDown()\n\n\t// We start by connecting the new miner to our original miner,\n\t// such that it will sync to our original chain.\n\tif err := rpctest.ConnectNode(miner1, miner2); err != nil {\n\t\tt.Fatalf(\"unable to connect harnesses: %v\", err)\n\t}\n\tnodeSlice := []*rpctest.Harness{miner1, miner2}\n\tif err := rpctest.JoinNodes(nodeSlice, rpctest.Blocks); err != nil {\n\t\tt.Fatalf(\"unable to join node on blocks: %v\", err)\n\t}\n\n\t// The two should be on the same blockheight.\n\t_, nodeHeight1, err := miner1.Client.GetBestBlock()\n\tif err != nil {\n\t\tt.Fatalf(\"unable to get current blockheight %v\", err)\n\t}\n\n\t_, nodeHeight2, err := miner2.Client.GetBestBlock()\n\tif err != nil {\n\t\tt.Fatalf(\"unable to get current blockheight %v\", err)\n\t}\n\n\tif nodeHeight1 != nodeHeight2 {\n\t\tt.Fatalf(\"expected both miners to be on the same height: %v vs %v\",\n\t\t\tnodeHeight1, nodeHeight2)\n\t}\n\n\t// We disconnect the two nodes, such that we can start mining on them\n\t// individually without the other one learning about the new blocks.\n\terr = miner1.Client.AddNode(miner2.P2PAddress(), rpcclient.ANRemove)\n\trequire.NoError(t, err, \"unable to remove node\")\n\n\t// Now mine on each chain separately\n\tblocks, err := miner1.Client.Generate(numBlocks)\n\trequire.NoError(t, err, \"unable to generate single block\")\n\n\t// We generate an extra block on miner 2's chain to ensure it is the\n\t// longer chain.\n\t_, err = miner2.Client.Generate(numBlocks + 1)\n\trequire.NoError(t, err, \"unable to generate single block\")\n\n\t// Sync the two chains to ensure they will sync to miner2's chain.\n\tif err := rpctest.ConnectNode(miner1, miner2); err != nil {\n\t\tt.Fatalf(\"unable to connect harnesses: %v\", err)\n\t}\n\tnodeSlice = []*rpctest.Harness{miner1, miner2}\n\tif err := rpctest.JoinNodes(nodeSlice, rpctest.Blocks); err != nil {\n\t\tt.Fatalf(\"unable to join node on blocks: %v\", err)\n\t}\n\n\t// The two should be on the same block hash.\n\ttimeout := time.After(10 * time.Second)\n\tfor {\n\t\tnodeHash1, _, err := miner1.Client.GetBestBlock()\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to get current block hash: %v\", err)\n\t\t}\n\n\t\tnodeHash2, _, err := miner2.Client.GetBestBlock()\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to get current block hash: %v\", err)\n\t\t}\n\n\t\tif *nodeHash1 == *nodeHash2 {\n\t\t\tbreak\n\t\t}\n\t\tselect {\n\t\tcase <-timeout:\n\t\t\tt.Fatalf(\"Unable to sync two chains\")\n\t\tcase <-time.After(50 * time.Millisecond):\n\t\t\tcontinue\n\t\t}\n\t}\n\n\t// Next, start the notifier with outdated best block information.\n\t// We set the notifier's best block to be the last block mined on the\n\t// shorter chain, to test that the notifier correctly rewinds to\n\t// the common ancestor between the two chains.\n\tsyncHeight := nodeHeight1 + numBlocks + 1\n\terr = notifier.UnsafeStart(\n\t\tnodeHeight1+numBlocks, blocks[numBlocks-1], syncHeight, nil,\n\t)\n\trequire.NoError(t, err, \"Unable to unsafe start the notifier\")\n\tdefer notifier.Stop()\n\n\t// Create numClients clients who will listen for block notifications.\n\tclients := make([]*chainntnfs.BlockEpochEvent, 0, numClients)\n\tfor i := 0; i < numClients; i++ {\n\t\tepochClient, err := notifier.RegisterBlockEpochNtfn(nil)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to register for epoch notification: %v\", err)\n\t\t}\n\n\t\t// Drain the notification dispatched upon registration as we're\n\t\t// not interested in it.\n\t\tselect {\n\t\tcase <-epochClient.Epochs:\n\t\tcase <-time.After(5 * time.Second):\n\t\t\tt.Fatal(\"expected to receive epoch for current block \" +\n\t\t\t\t\"upon registration\")\n\t\t}\n\n\t\tclients = append(clients, epochClient)\n\t}\n\n\t// Generate a single block, which should trigger the notifier to rewind\n\t// to the common ancestor and dispatch notifications from there.\n\t_, err = miner2.Client.Generate(1)\n\trequire.NoError(t, err, \"unable to generate single block\")\n\n\t// If the chain backend to the notifier stores information about reorged\n\t// blocks, the notifier is able to rewind the chain to the common\n\t// ancestor between the chain tip and its outdated best known block.\n\t// In this case, the client is expected to receive numBlocks + 2\n\t// notifications, 1 for each block the notifier has missed out on from\n\t// the longer chain.\n\t//\n\t// If the chain backend does not store information about reorged blocks,\n\t// the notifier has no way of knowing where to rewind to and therefore\n\t// the client is only expected to receive notifications for blocks\n\t// whose height is greater than the notifier's best known height: 2\n\t// notifications, in this case.\n\tvar startingHeight int32\n\tswitch notifier.(type) {\n\tcase *neutrinonotify.NeutrinoNotifier:\n\t\tstartingHeight = nodeHeight1 + numBlocks + 1\n\tdefault:\n\t\tstartingHeight = nodeHeight1 + 1\n\t}\n\n\tfor expectedHeight := startingHeight; expectedHeight <=\n\t\tnodeHeight1+numBlocks+2; expectedHeight++ {\n\n\t\tfor _, epochClient := range clients {\n\t\t\tselect {\n\t\t\tcase block := <-epochClient.Epochs:\n\t\t\t\tif block.Height != expectedHeight {\n\t\t\t\t\tt.Fatalf(\"received block of height: %d, \"+\n\t\t\t\t\t\t\"expected: %d\", block.Height,\n\t\t\t\t\t\texpectedHeight)\n\t\t\t\t}\n\t\t\tcase <-time.After(20 * time.Second):\n\t\t\t\tt.Fatalf(\"did not receive historical notification \"+\n\t\t\t\t\t\"for height %d\", expectedHeight)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Finally, ensure that an extra block notification wasn't received.\n\tanyExtras := make(chan struct{}, len(clients))\n\tfor _, epochClient := range clients {\n\t\twg.Add(1)\n\t\tgo func(epochClient *chainntnfs.BlockEpochEvent) {\n\t\t\tdefer wg.Done()\n\t\t\tselect {\n\t\t\tcase <-epochClient.Epochs:\n\t\t\t\tanyExtras <- struct{}{}\n\t\t\tcase <-time.After(5 * time.Second):\n\t\t\t}\n\t\t}(epochClient)\n\t}\n\n\twg.Wait()\n\tclose(anyExtras)\n\n\tvar extraCount int\n\tfor range anyExtras {\n\t\textraCount++\n\t}\n\n\tif extraCount > 0 {\n\t\tt.Fatalf(\"received %d unexpected block notification\", extraCount)\n\t}\n}\n",
      "length": 6153,
      "tokens": 893,
      "embedding": []
    },
    {
      "slug": "type txNtfnTestCase struct {",
      "content": "type txNtfnTestCase struct {\n\tname string\n\ttest func(node *rpctest.Harness, notifier chainntnfs.TestChainNotifier,\n\t\tscriptDispatch bool, t *testing.T)\n}\n",
      "length": 121,
      "tokens": 12,
      "embedding": []
    },
    {
      "slug": "type blockNtfnTestCase struct {",
      "content": "type blockNtfnTestCase struct {\n\tname string\n\ttest func(node *rpctest.Harness, notifier chainntnfs.TestChainNotifier,\n\t\tt *testing.T)\n}\n",
      "length": 100,
      "tokens": 10,
      "embedding": []
    },
    {
      "slug": "type blockCatchupTestCase struct {",
      "content": "type blockCatchupTestCase struct {\n\tname string\n\ttest func(node *rpctest.Harness, notifier chainntnfs.TestChainNotifier,\n\t\tt *testing.T)\n}\n\nvar txNtfnTests = []txNtfnTestCase{\n\t{\n\t\tname: \"single conf ntfn\",\n\t\ttest: testSingleConfirmationNotification,\n\t},\n\t{\n\t\tname: \"multi conf ntfn\",\n\t\ttest: testMultiConfirmationNotification,\n\t},\n\t{\n\t\tname: \"batch conf ntfn\",\n\t\ttest: testBatchConfirmationNotification,\n\t},\n\t{\n\t\tname: \"multi client conf\",\n\t\ttest: testMultiClientConfirmationNotification,\n\t},\n\t{\n\t\tname: \"lazy ntfn consumer\",\n\t\ttest: testLazyNtfnConsumer,\n\t},\n\t{\n\t\tname: \"historical conf dispatch\",\n\t\ttest: testTxConfirmedBeforeNtfnRegistration,\n\t},\n\t{\n\t\tname: \"reorg conf\",\n\t\ttest: testReorgConf,\n\t},\n\t{\n\t\tname: \"spend ntfn\",\n\t\ttest: testSpendNotification,\n\t},\n\t{\n\t\tname: \"historical spend dispatch\",\n\t\ttest: testSpendBeforeNtfnRegistration,\n\t},\n\t{\n\t\tname: \"reorg spend\",\n\t\ttest: testReorgSpend,\n\t},\n\t{\n\t\tname: \"cancel spend ntfn\",\n\t\ttest: testCancelSpendNtfn,\n\t},\n}\n\nvar blockNtfnTests = []blockNtfnTestCase{\n\t{\n\t\tname: \"block epoch\",\n\t\ttest: testBlockEpochNotification,\n\t},\n\t{\n\t\tname: \"cancel epoch ntfn\",\n\t\ttest: testCancelEpochNtfn,\n\t},\n}\n\nvar blockCatchupTests = []blockCatchupTestCase{\n\t{\n\t\tname: \"catch up client on historical block epoch ntfns\",\n\t\ttest: testCatchUpClientOnMissedBlocks,\n\t},\n\t{\n\t\tname: \"test catch up on missed blocks\",\n\t\ttest: testCatchUpOnMissedBlocks,\n\t},\n\t{\n\t\tname: \"test catch up on missed blocks w/ reorged best block\",\n\t\ttest: testCatchUpOnMissedBlocksWithReorg,\n\t},\n}\n\n// TestInterfaces tests all registered interfaces with a unified set of tests\n// which exercise each of the required methods found within the ChainNotifier\n// interface.\n//\n// NOTE: In the future, when additional implementations of the ChainNotifier\n// interface have been implemented, in order to ensure the new concrete\n// implementation is automatically tested, two steps must be undertaken. First,\n// one needs add a \"non-captured\" (_) import from the new sub-package. This\n// import should trigger an init() method within the package which registers\n// the interface. Second, an additional case in the switch within the main loop\n// below needs to be added which properly initializes the interface.",
      "length": 2083,
      "tokens": 275,
      "embedding": []
    },
    {
      "slug": "func TestInterfaces(t *testing.T, targetBackEnd string) {",
      "content": "func TestInterfaces(t *testing.T, targetBackEnd string) {\n\t// Initialize the harness around a btcd node which will serve as our\n\t// dedicated miner to generate blocks, cause re-orgs, etc. We'll set up\n\t// this node with a chain length of 125, so we have plenty of BTC to\n\t// play around with.\n\tminer := chainntnfs.NewMiner(t, nil, true, 25)\n\n\trpcConfig := miner.RPCConfig()\n\tp2pAddr := miner.P2PAddress()\n\n\tlog.Printf(\"Running %v ChainNotifier interface tests\",\n\t\t2*len(txNtfnTests)+len(blockNtfnTests)+len(blockCatchupTests))\n\n\tfor _, notifierDriver := range chainntnfs.RegisteredNotifiers() {\n\t\tnotifierType := notifierDriver.NotifierType\n\t\tif notifierType != targetBackEnd {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Initialize a height hint cache for each notifier.\n\t\ttempDir := t.TempDir()\n\t\tdb, err := channeldb.Open(tempDir)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to create db: %v\", err)\n\t\t}\n\t\ttestCfg := channeldb.CacheConfig{\n\t\t\tQueryDisable: false,\n\t\t}\n\t\thintCache, err := channeldb.NewHeightHintCache(\n\t\t\ttestCfg, db.Backend,\n\t\t)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to create height hint cache: %v\", err)\n\t\t}\n\n\t\tblockCache := blockcache.NewBlockCache(10000)\n\n\t\tvar (\n\t\t\tnewNotifier func() (chainntnfs.TestChainNotifier, error)\n\t\t)\n\n\t\tswitch notifierType {\n\t\tcase \"bitcoind\":\n\t\t\tvar bitcoindConn *chain.BitcoindConn\n\t\t\tbitcoindConn = chainntnfs.NewBitcoindBackend(\n\t\t\t\tt, p2pAddr, true, false,\n\t\t\t)\n\t\t\tnewNotifier = func() (chainntnfs.TestChainNotifier, error) {\n\t\t\t\treturn bitcoindnotify.New(\n\t\t\t\t\tbitcoindConn, chainntnfs.NetParams,\n\t\t\t\t\thintCache, hintCache, blockCache,\n\t\t\t\t), nil\n\t\t\t}\n\n\t\tcase \"bitcoind-rpc-polling\":\n\t\t\tvar bitcoindConn *chain.BitcoindConn\n\t\t\tbitcoindConn = chainntnfs.NewBitcoindBackend(\n\t\t\t\tt, p2pAddr, true, true,\n\t\t\t)\n\t\t\tnewNotifier = func() (chainntnfs.TestChainNotifier, error) {\n\t\t\t\treturn bitcoindnotify.New(\n\t\t\t\t\tbitcoindConn, chainntnfs.NetParams,\n\t\t\t\t\thintCache, hintCache, blockCache,\n\t\t\t\t), nil\n\t\t\t}\n\n\t\tcase \"btcd\":\n\t\t\tnewNotifier = func() (chainntnfs.TestChainNotifier, error) {\n\t\t\t\treturn btcdnotify.New(\n\t\t\t\t\t&rpcConfig, chainntnfs.NetParams,\n\t\t\t\t\thintCache, hintCache, blockCache,\n\t\t\t\t)\n\t\t\t}\n\n\t\tcase \"neutrino\":\n\t\t\tvar spvNode *neutrino.ChainService\n\t\t\tspvNode = chainntnfs.NewNeutrinoBackend(t, p2pAddr)\n\t\t\tnewNotifier = func() (chainntnfs.TestChainNotifier, error) {\n\t\t\t\treturn neutrinonotify.New(\n\t\t\t\t\tspvNode, hintCache, hintCache,\n\t\t\t\t\tblockCache,\n\t\t\t\t), nil\n\t\t\t}\n\t\t}\n\n\t\tlog.Printf(\"Running ChainNotifier interface tests for: %v\",\n\t\t\tnotifierType)\n\n\t\tnotifier, err := newNotifier()\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to create %v notifier: %v\",\n\t\t\t\tnotifierType, err)\n\t\t}\n\t\tif err := notifier.Start(); err != nil {\n\t\t\tt.Fatalf(\"unable to start notifier %v: %v\",\n\t\t\t\tnotifierType, err)\n\t\t}\n\n\t\tfor _, txNtfnTest := range txNtfnTests {\n\t\t\tfor _, scriptDispatch := range []bool{false, true} {\n\t\t\t\ttestName := fmt.Sprintf(\"%v %v\", notifierType,\n\t\t\t\t\ttxNtfnTest.name)\n\t\t\t\tif scriptDispatch {\n\t\t\t\t\ttestName += \" with script dispatch\"\n\t\t\t\t}\n\t\t\t\tsuccess := t.Run(testName, func(t *testing.T) {\n\t\t\t\t\ttxNtfnTest.test(\n\t\t\t\t\t\tminer, notifier, scriptDispatch,\n\t\t\t\t\t\tt,\n\t\t\t\t\t)\n\t\t\t\t})\n\t\t\t\tif !success {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tfor _, blockNtfnTest := range blockNtfnTests {\n\t\t\ttestName := fmt.Sprintf(\"%v %v\", notifierType,\n\t\t\t\tblockNtfnTest.name)\n\t\t\tsuccess := t.Run(testName, func(t *testing.T) {\n\t\t\t\tblockNtfnTest.test(miner, notifier, t)\n\t\t\t})\n\t\t\tif !success {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\tnotifier.Stop()\n\n\t\t// Run catchup tests separately since they require restarting\n\t\t// the notifier every time.\n\t\tfor _, blockCatchupTest := range blockCatchupTests {\n\t\t\tnotifier, err = newNotifier()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"unable to create %v notifier: %v\",\n\t\t\t\t\tnotifierType, err)\n\t\t\t}\n\n\t\t\ttestName := fmt.Sprintf(\"%v %v\", notifierType,\n\t\t\t\tblockCatchupTest.name)\n\n\t\t\tsuccess := t.Run(testName, func(t *testing.T) {\n\t\t\t\tblockCatchupTest.test(miner, notifier, t)\n\t\t\t})\n\t\t\tif !success {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n}\n",
      "length": 3725,
      "tokens": 434,
      "embedding": []
    }
  ]
}