{
  "filepath": "../implementations/go/lnd/chainntnfs/neutrinonotify/neutrino.go",
  "package": "neutrinonotify",
  "sections": [
    {
      "slug": "type NeutrinoNotifier struct {",
      "content": "type NeutrinoNotifier struct {\n\tepochClientCounter uint64 // To be used atomically.\n\n\tstart   sync.Once\n\tactive  int32 // To be used atomically.\n\tstopped int32 // To be used atomically.\n\n\tbestBlockMtx sync.RWMutex\n\tbestBlock    chainntnfs.BlockEpoch\n\n\tp2pNode   *neutrino.ChainService\n\tchainView *neutrino.Rescan\n\n\tchainConn *NeutrinoChainConn\n\n\tnotificationCancels  chan interface{}\n\tnotificationRegistry chan interface{}\n\n\ttxNotifier *chainntnfs.TxNotifier\n\n\tblockEpochClients map[uint64]*blockEpochRegistration\n\n\trescanErr <-chan error\n\n\tchainUpdates chan *filteredBlock\n\n\ttxUpdates *queue.ConcurrentQueue\n\n\t// spendHintCache is a cache used to query and update the latest height\n\t// hints for an outpoint. Each height hint represents the earliest\n\t// height at which the outpoint could have been spent within the chain.\n\tspendHintCache chainntnfs.SpendHintCache\n\n\t// confirmHintCache is a cache used to query the latest height hints for\n\t// a transaction. Each height hint represents the earliest height at\n\t// which the transaction could have confirmed within the chain.\n\tconfirmHintCache chainntnfs.ConfirmHintCache\n\n\t// blockCache is an LRU block cache.\n\tblockCache *blockcache.BlockCache\n\n\twg   sync.WaitGroup\n\tquit chan struct{}\n}\n\n// Ensure NeutrinoNotifier implements the ChainNotifier interface at compile time.\nvar _ chainntnfs.ChainNotifier = (*NeutrinoNotifier)(nil)\n\n// New creates a new instance of the NeutrinoNotifier concrete implementation\n// of the ChainNotifier interface.\n//\n// NOTE: The passed neutrino node should already be running and active before\n// being passed into this function.",
      "length": 1530,
      "tokens": 192,
      "embedding": []
    },
    {
      "slug": "func New(node *neutrino.ChainService, spendHintCache chainntnfs.SpendHintCache,",
      "content": "func New(node *neutrino.ChainService, spendHintCache chainntnfs.SpendHintCache,\n\tconfirmHintCache chainntnfs.ConfirmHintCache,\n\tblockCache *blockcache.BlockCache) *NeutrinoNotifier {\n\n\treturn &NeutrinoNotifier{\n\t\tnotificationCancels:  make(chan interface{}),\n\t\tnotificationRegistry: make(chan interface{}),\n\n\t\tblockEpochClients: make(map[uint64]*blockEpochRegistration),\n\n\t\tp2pNode:   node,\n\t\tchainConn: &NeutrinoChainConn{node},\n\n\t\trescanErr: make(chan error),\n\n\t\tchainUpdates: make(chan *filteredBlock, 100),\n\n\t\ttxUpdates: queue.NewConcurrentQueue(10),\n\n\t\tspendHintCache:   spendHintCache,\n\t\tconfirmHintCache: confirmHintCache,\n\n\t\tblockCache: blockCache,\n\n\t\tquit: make(chan struct{}),\n\t}\n}\n\n// Start contacts the running neutrino light client and kicks off an initial\n// empty rescan.",
      "length": 678,
      "tokens": 56,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoNotifier) Start() error {",
      "content": "func (n *NeutrinoNotifier) Start() error {\n\tvar startErr error\n\tn.start.Do(func() {\n\t\tstartErr = n.startNotifier()\n\t})\n\treturn startErr\n}\n\n// Stop shuts down the NeutrinoNotifier.",
      "length": 129,
      "tokens": 18,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoNotifier) Stop() error {",
      "content": "func (n *NeutrinoNotifier) Stop() error {\n\t// Already shutting down?\n\tif atomic.AddInt32(&n.stopped, 1) != 1 {\n\t\treturn nil\n\t}\n\n\tchainntnfs.Log.Info(\"neutrino notifier shutting down\")\n\n\tclose(n.quit)\n\tn.wg.Wait()\n\n\tn.txUpdates.Stop()\n\n\t// Notify all pending clients of our shutdown by closing the related\n\t// notification channels.\n\tfor _, epochClient := range n.blockEpochClients {\n\t\tclose(epochClient.cancelChan)\n\t\tepochClient.wg.Wait()\n\n\t\tclose(epochClient.epochChan)\n\t}\n\tn.txNotifier.TearDown()\n\n\treturn nil\n}\n\n// Started returns true if this instance has been started, and false otherwise.",
      "length": 527,
      "tokens": 63,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoNotifier) Started() bool {",
      "content": "func (n *NeutrinoNotifier) Started() bool {\n\treturn atomic.LoadInt32(&n.active) != 0\n}\n",
      "length": 41,
      "tokens": 5,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoNotifier) startNotifier() error {",
      "content": "func (n *NeutrinoNotifier) startNotifier() error {\n\t// Start our concurrent queues before starting the rescan, to ensure\n\t// onFilteredBlockConnected and onRelavantTx callbacks won't be\n\t// blocked.\n\tn.txUpdates.Start()\n\n\t// First, we'll obtain the latest block height of the p2p node. We'll\n\t// start the auto-rescan from this point. Once a caller actually wishes\n\t// to register a chain view, the rescan state will be rewound\n\t// accordingly.\n\tstartingPoint, err := n.p2pNode.BestBlock()\n\tif err != nil {\n\t\tn.txUpdates.Stop()\n\t\treturn err\n\t}\n\tstartingHeader, err := n.p2pNode.GetBlockHeader(\n\t\t&startingPoint.Hash,\n\t)\n\tif err != nil {\n\t\tn.txUpdates.Stop()\n\t\treturn err\n\t}\n\n\tn.bestBlock.Hash = &startingPoint.Hash\n\tn.bestBlock.Height = startingPoint.Height\n\tn.bestBlock.BlockHeader = startingHeader\n\n\tn.txNotifier = chainntnfs.NewTxNotifier(\n\t\tuint32(n.bestBlock.Height), chainntnfs.ReorgSafetyLimit,\n\t\tn.confirmHintCache, n.spendHintCache,\n\t)\n\n\t// Next, we'll create our set of rescan options. Currently it's\n\t// required that a user MUST set an addr/outpoint/txid when creating a\n\t// rescan. To get around this, we'll add a \"zero\" outpoint, that won't\n\t// actually be matched.\n\tvar zeroInput neutrino.InputWithScript\n\trescanOptions := []neutrino.RescanOption{\n\t\tneutrino.StartBlock(startingPoint),\n\t\tneutrino.QuitChan(n.quit),\n\t\tneutrino.NotificationHandlers(\n\t\t\trpcclient.NotificationHandlers{\n\t\t\t\tOnFilteredBlockConnected:    n.onFilteredBlockConnected,\n\t\t\t\tOnFilteredBlockDisconnected: n.onFilteredBlockDisconnected,\n\t\t\t\tOnRedeemingTx:               n.onRelevantTx,\n\t\t\t},\n\t\t),\n\t\tneutrino.WatchInputs(zeroInput),\n\t}\n\n\t// Finally, we'll create our rescan struct, start it, and launch all\n\t// the goroutines we need to operate this ChainNotifier instance.\n\tn.chainView = neutrino.NewRescan(\n\t\t&neutrino.RescanChainSource{\n\t\t\tChainService: n.p2pNode,\n\t\t},\n\t\trescanOptions...,\n\t)\n\tn.rescanErr = n.chainView.Start()\n\n\tn.wg.Add(1)\n\tgo n.notificationDispatcher()\n\n\t// Set the active flag now that we've completed the full\n\t// startup.\n\tatomic.StoreInt32(&n.active, 1)\n\n\treturn nil\n}\n\n// filteredBlock represents a new block which has been connected to the main\n// chain. The slice of transactions will only be populated if the block\n// includes a transaction that confirmed one of our watched txids, or spends\n// one of the outputs currently being watched.",
      "length": 2231,
      "tokens": 267,
      "embedding": []
    },
    {
      "slug": "type filteredBlock struct {",
      "content": "type filteredBlock struct {\n\theader *wire.BlockHeader\n\thash   chainhash.Hash\n\theight uint32\n\ttxns   []*btcutil.Tx\n\n\t// connected is true if this update is a new block and false if it is a\n\t// disconnected block.\n\tconnect bool\n}\n\n// rescanFilterUpdate represents a request that will be sent to the\n// notificaionRegistry in order to prevent race conditions between the filter\n// update and new block notifications.",
      "length": 373,
      "tokens": 59,
      "embedding": []
    },
    {
      "slug": "type rescanFilterUpdate struct {",
      "content": "type rescanFilterUpdate struct {\n\tupdateOptions []neutrino.UpdateOption\n\terrChan       chan error\n}\n\n// onFilteredBlockConnected is a callback which is executed each a new block is\n// connected to the end of the main chain.",
      "length": 185,
      "tokens": 28,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoNotifier) onFilteredBlockConnected(height int32,",
      "content": "func (n *NeutrinoNotifier) onFilteredBlockConnected(height int32,\n\theader *wire.BlockHeader, txns []*btcutil.Tx) {\n\n\t// Append this new chain update to the end of the queue of new chain\n\t// updates.\n\tselect {\n\tcase n.chainUpdates <- &filteredBlock{\n\t\thash:    header.BlockHash(),\n\t\theight:  uint32(height),\n\t\ttxns:    txns,\n\t\theader:  header,\n\t\tconnect: true,\n\t}:\n\tcase <-n.quit:\n\t}\n}\n\n// onFilteredBlockDisconnected is a callback which is executed each time a new\n// block has been disconnected from the end of the mainchain due to a re-org.",
      "length": 459,
      "tokens": 70,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoNotifier) onFilteredBlockDisconnected(height int32,",
      "content": "func (n *NeutrinoNotifier) onFilteredBlockDisconnected(height int32,\n\theader *wire.BlockHeader) {\n\n\t// Append this new chain update to the end of the queue of new chain\n\t// disconnects.\n\tselect {\n\tcase n.chainUpdates <- &filteredBlock{\n\t\thash:    header.BlockHash(),\n\t\theight:  uint32(height),\n\t\tconnect: false,\n\t}:\n\tcase <-n.quit:\n\t}\n}\n\n// relevantTx represents a relevant transaction to the notifier that fulfills\n// any outstanding spend requests.",
      "length": 366,
      "tokens": 53,
      "embedding": []
    },
    {
      "slug": "type relevantTx struct {",
      "content": "type relevantTx struct {\n\ttx      *btcutil.Tx\n\tdetails *btcjson.BlockDetails\n}\n\n// onRelevantTx is a callback that proxies relevant transaction notifications\n// from the backend to the notifier's main event handler.",
      "length": 185,
      "tokens": 25,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoNotifier) onRelevantTx(tx *btcutil.Tx, details *btcjson.BlockDetails) {",
      "content": "func (n *NeutrinoNotifier) onRelevantTx(tx *btcutil.Tx, details *btcjson.BlockDetails) {\n\tselect {\n\tcase n.txUpdates.ChanIn() <- &relevantTx{tx, details}:\n\tcase <-n.quit:\n\t}\n}\n\n// connectFilteredBlock is called when we receive a filteredBlock from the\n// backend. If the block is ahead of what we're expecting, we'll attempt to\n// catch up and then process the block.",
      "length": 270,
      "tokens": 44,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoNotifier) connectFilteredBlock(update *filteredBlock) {",
      "content": "func (n *NeutrinoNotifier) connectFilteredBlock(update *filteredBlock) {\n\tn.bestBlockMtx.Lock()\n\tdefer n.bestBlockMtx.Unlock()\n\n\tif update.height != uint32(n.bestBlock.Height+1) {\n\t\tchainntnfs.Log.Infof(\"Missed blocks, attempting to catch up\")\n\n\t\t_, missedBlocks, err := chainntnfs.HandleMissedBlocks(\n\t\t\tn.chainConn, n.txNotifier, n.bestBlock,\n\t\t\tint32(update.height), false,\n\t\t)\n\t\tif err != nil {\n\t\t\tchainntnfs.Log.Error(err)\n\t\t\treturn\n\t\t}\n\n\t\tfor _, block := range missedBlocks {\n\t\t\tfilteredBlock, err := n.getFilteredBlock(block)\n\t\t\tif err != nil {\n\t\t\t\tchainntnfs.Log.Error(err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\terr = n.handleBlockConnected(filteredBlock)\n\t\t\tif err != nil {\n\t\t\t\tchainntnfs.Log.Error(err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\terr := n.handleBlockConnected(update)\n\tif err != nil {\n\t\tchainntnfs.Log.Error(err)\n\t}\n}\n\n// disconnectFilteredBlock is called when our disconnected filtered block\n// callback is fired. It attempts to rewind the chain to before the\n// disconnection and updates our best block.",
      "length": 884,
      "tokens": 105,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoNotifier) disconnectFilteredBlock(update *filteredBlock) {",
      "content": "func (n *NeutrinoNotifier) disconnectFilteredBlock(update *filteredBlock) {\n\tn.bestBlockMtx.Lock()\n\tdefer n.bestBlockMtx.Unlock()\n\n\tif update.height != uint32(n.bestBlock.Height) {\n\t\tchainntnfs.Log.Infof(\"Missed disconnected blocks, attempting\" +\n\t\t\t\" to catch up\")\n\t}\n\tnewBestBlock, err := chainntnfs.RewindChain(n.chainConn, n.txNotifier,\n\t\tn.bestBlock, int32(update.height-1),\n\t)\n\tif err != nil {\n\t\tchainntnfs.Log.Errorf(\"Unable to rewind chain from height %d\"+\n\t\t\t\"to height %d: %v\", n.bestBlock.Height,\n\t\t\tupdate.height-1, err,\n\t\t)\n\t}\n\n\tn.bestBlock = newBestBlock\n}\n\n// drainChainUpdates is called after updating the filter. It reads every\n// buffered item off the chan and returns when no more are available. It is\n// used to ensure that callers performing a historical scan properly update\n// their EndHeight to scan blocks that did not have the filter applied at\n// processing time. Without this, a race condition exists that could allow a\n// spend or confirmation notification to be missed. It is unlikely this would\n// occur in a real-world scenario, and instead would manifest itself in tests.",
      "length": 1002,
      "tokens": 142,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoNotifier) drainChainUpdates() {",
      "content": "func (n *NeutrinoNotifier) drainChainUpdates() {\n\tfor {\n\t\tselect {\n\t\tcase update := <-n.chainUpdates:\n\t\t\tif update.connect {\n\t\t\t\tn.connectFilteredBlock(update)\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tn.disconnectFilteredBlock(update)\n\t\tdefault:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// notificationDispatcher is the primary goroutine which handles client\n// notification registrations, as well as notification dispatches.",
      "length": 317,
      "tokens": 37,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoNotifier) notificationDispatcher() {",
      "content": "func (n *NeutrinoNotifier) notificationDispatcher() {\n\tdefer n.wg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase cancelMsg := <-n.notificationCancels:\n\t\t\tswitch msg := cancelMsg.(type) {\n\t\t\tcase *epochCancel:\n\t\t\t\tchainntnfs.Log.Infof(\"Cancelling epoch \"+\n\t\t\t\t\t\"notification, epoch_id=%v\", msg.epochID)\n\n\t\t\t\t// First, we'll lookup the original\n\t\t\t\t// registration in order to stop the active\n\t\t\t\t// queue goroutine.\n\t\t\t\treg := n.blockEpochClients[msg.epochID]\n\t\t\t\treg.epochQueue.Stop()\n\n\t\t\t\t// Next, close the cancel channel for this\n\t\t\t\t// specific client, and wait for the client to\n\t\t\t\t// exit.\n\t\t\t\tclose(n.blockEpochClients[msg.epochID].cancelChan)\n\t\t\t\tn.blockEpochClients[msg.epochID].wg.Wait()\n\n\t\t\t\t// Once the client has exited, we can then\n\t\t\t\t// safely close the channel used to send epoch\n\t\t\t\t// notifications, in order to notify any\n\t\t\t\t// listeners that the intent has been\n\t\t\t\t// canceled.\n\t\t\t\tclose(n.blockEpochClients[msg.epochID].epochChan)\n\t\t\t\tdelete(n.blockEpochClients, msg.epochID)\n\t\t\t}\n\n\t\tcase registerMsg := <-n.notificationRegistry:\n\t\t\tswitch msg := registerMsg.(type) {\n\t\t\tcase *chainntnfs.HistoricalConfDispatch:\n\t\t\t\t// We'll start a historical rescan chain of the\n\t\t\t\t// chain asynchronously to prevent blocking\n\t\t\t\t// potentially long rescans.\n\t\t\t\tn.wg.Add(1)\n\n\t\t\t\t//nolint:lll\n\t\t\t\tgo func(msg *chainntnfs.HistoricalConfDispatch) {\n\t\t\t\t\tdefer n.wg.Done()\n\n\t\t\t\t\tconfDetails, err := n.historicalConfDetails(\n\t\t\t\t\t\tmsg.ConfRequest,\n\t\t\t\t\t\tmsg.StartHeight, msg.EndHeight,\n\t\t\t\t\t)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tchainntnfs.Log.Error(err)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\n\t\t\t\t\t// If the historical dispatch finished\n\t\t\t\t\t// without error, we will invoke\n\t\t\t\t\t// UpdateConfDetails even if none were\n\t\t\t\t\t// found. This allows the notifier to\n\t\t\t\t\t// begin safely updating the height hint\n\t\t\t\t\t// cache at tip, since any pending\n\t\t\t\t\t// rescans have now completed.\n\t\t\t\t\terr = n.txNotifier.UpdateConfDetails(\n\t\t\t\t\t\tmsg.ConfRequest, confDetails,\n\t\t\t\t\t)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tchainntnfs.Log.Error(err)\n\t\t\t\t\t}\n\t\t\t\t}(msg)\n\n\t\t\tcase *blockEpochRegistration:\n\t\t\t\tchainntnfs.Log.Infof(\"New block epoch subscription\")\n\n\t\t\t\tn.blockEpochClients[msg.epochID] = msg\n\n\t\t\t\t// If the client did not provide their best\n\t\t\t\t// known block, then we'll immediately dispatch\n\t\t\t\t// a notification for the current tip.\n\t\t\t\tif msg.bestBlock == nil {\n\t\t\t\t\tn.notifyBlockEpochClient(\n\t\t\t\t\t\tmsg, n.bestBlock.Height,\n\t\t\t\t\t\tn.bestBlock.Hash,\n\t\t\t\t\t\tn.bestBlock.BlockHeader,\n\t\t\t\t\t)\n\n\t\t\t\t\tmsg.errorChan <- nil\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\t// Otherwise, we'll attempt to deliver the\n\t\t\t\t// backlog of notifications from their best\n\t\t\t\t// known block.\n\t\t\t\tn.bestBlockMtx.Lock()\n\t\t\t\tbestHeight := n.bestBlock.Height\n\t\t\t\tn.bestBlockMtx.Unlock()\n\n\t\t\t\tmissedBlocks, err := chainntnfs.GetClientMissedBlocks(\n\t\t\t\t\tn.chainConn, msg.bestBlock, bestHeight,\n\t\t\t\t\tfalse,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\tmsg.errorChan <- err\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tfor _, block := range missedBlocks {\n\t\t\t\t\tn.notifyBlockEpochClient(\n\t\t\t\t\t\tmsg, block.Height, block.Hash,\n\t\t\t\t\t\tblock.BlockHeader,\n\t\t\t\t\t)\n\t\t\t\t}\n\n\t\t\t\tmsg.errorChan <- nil\n\n\t\t\tcase *rescanFilterUpdate:\n\t\t\t\terr := n.chainView.Update(msg.updateOptions...)\n\t\t\t\tif err != nil {\n\t\t\t\t\tchainntnfs.Log.Errorf(\"Unable to \"+\n\t\t\t\t\t\t\"update rescan filter: %v\", err)\n\t\t\t\t}\n\n\t\t\t\t// Drain the chainUpdates chan so the caller\n\t\t\t\t// listening on errChan can be sure that\n\t\t\t\t// updates after receiving the error will have\n\t\t\t\t// the filter applied. This allows the caller\n\t\t\t\t// to update their EndHeight if they're\n\t\t\t\t// performing a historical scan.\n\t\t\t\tn.drainChainUpdates()\n\n\t\t\t\t// After draining, send the error to the\n\t\t\t\t// caller.\n\t\t\t\tmsg.errChan <- err\n\t\t\t}\n\n\t\tcase item := <-n.chainUpdates:\n\t\t\tupdate := item\n\t\t\tif update.connect {\n\t\t\t\tn.connectFilteredBlock(update)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tn.disconnectFilteredBlock(update)\n\n\t\tcase txUpdate := <-n.txUpdates.ChanOut():\n\t\t\t// A new relevant transaction notification has been\n\t\t\t// received from the backend. We'll attempt to process\n\t\t\t// it to determine if it fulfills any outstanding\n\t\t\t// confirmation and/or spend requests and dispatch\n\t\t\t// notifications for them.\n\t\t\tupdate := txUpdate.(*relevantTx)\n\t\t\terr := n.txNotifier.ProcessRelevantSpendTx(\n\t\t\t\tupdate.tx, uint32(update.details.Height),\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tchainntnfs.Log.Errorf(\"Unable to process \"+\n\t\t\t\t\t\"transaction %v: %v\", update.tx.Hash(),\n\t\t\t\t\terr)\n\t\t\t}\n\n\t\tcase err := <-n.rescanErr:\n\t\t\tchainntnfs.Log.Errorf(\"Error during rescan: %v\", err)\n\n\t\tcase <-n.quit:\n\t\t\treturn\n\n\t\t}\n\t}\n}\n\n// historicalConfDetails looks up whether a confirmation request (txid/output\n// script) has already been included in a block in the active chain and, if so,\n// returns details about said block.",
      "length": 4458,
      "tokens": 524,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoNotifier) historicalConfDetails(confRequest chainntnfs.ConfRequest,",
      "content": "func (n *NeutrinoNotifier) historicalConfDetails(confRequest chainntnfs.ConfRequest,\n\tstartHeight, endHeight uint32) (*chainntnfs.TxConfirmation, error) {\n\n\t// Starting from the height hint, we'll walk forwards in the chain to\n\t// see if this transaction/output script has already been confirmed.\n\tfor scanHeight := endHeight; scanHeight >= startHeight && scanHeight > 0; scanHeight-- {\n\t\t// Ensure we haven't been requested to shut down before\n\t\t// processing the next height.\n\t\tselect {\n\t\tcase <-n.quit:\n\t\t\treturn nil, chainntnfs.ErrChainNotifierShuttingDown\n\t\tdefault:\n\t\t}\n\n\t\t// First, we'll fetch the block header for this height so we\n\t\t// can compute the current block hash.\n\t\tblockHash, err := n.p2pNode.GetBlockHash(int64(scanHeight))\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to get header for height=%v: %v\",\n\t\t\t\tscanHeight, err)\n\t\t}\n\n\t\t// With the hash computed, we can now fetch the basic filter for this\n\t\t// height. Since the range of required items is known we avoid\n\t\t// roundtrips by requesting a batched response and save bandwidth by\n\t\t// limiting the max number of items per batch. Since neutrino populates\n\t\t// its underline filters cache with the batch response, the next call\n\t\t// will execute a network query only once per batch and not on every\n\t\t// iteration.\n\t\tregFilter, err := n.p2pNode.GetCFilter(\n\t\t\t*blockHash, wire.GCSFilterRegular,\n\t\t\tneutrino.NumRetries(5),\n\t\t\tneutrino.OptimisticReverseBatch(),\n\t\t\tneutrino.MaxBatchSize(int64(scanHeight-startHeight+1)),\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to retrieve regular filter for \"+\n\t\t\t\t\"height=%v: %v\", scanHeight, err)\n\t\t}\n\n\t\t// In the case that the filter exists, we'll attempt to see if\n\t\t// any element in it matches our target public key script.\n\t\tkey := builder.DeriveKey(blockHash)\n\t\tmatch, err := regFilter.Match(key, confRequest.PkScript.Script())\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to query filter: %v\", err)\n\t\t}\n\n\t\t// If there's no match, then we can continue forward to the\n\t\t// next block.\n\t\tif !match {\n\t\t\tcontinue\n\t\t}\n\n\t\t// In the case that we do have a match, we'll fetch the block\n\t\t// from the network so we can find the positional data required\n\t\t// to send the proper response.\n\t\tblock, err := n.GetBlock(*blockHash)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to get block from network: %v\", err)\n\t\t}\n\n\t\t// For every transaction in the block, check which one matches\n\t\t// our request. If we find one that does, we can dispatch its\n\t\t// confirmation details.\n\t\tfor i, tx := range block.Transactions() {\n\t\t\tif !confRequest.MatchesTx(tx.MsgTx()) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\treturn &chainntnfs.TxConfirmation{\n\t\t\t\tTx:          tx.MsgTx(),\n\t\t\t\tBlockHash:   blockHash,\n\t\t\t\tBlockHeight: scanHeight,\n\t\t\t\tTxIndex:     uint32(i),\n\t\t\t\tBlock:       block.MsgBlock(),\n\t\t\t}, nil\n\t\t}\n\t}\n\n\treturn nil, nil\n}\n\n// handleBlockConnected applies a chain update for a new block. Any watched\n// transactions included this block will processed to either send notifications\n// now or after numConfirmations confs.\n//\n// NOTE: This method must be called with the bestBlockMtx lock held.",
      "length": 2939,
      "tokens": 431,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoNotifier) handleBlockConnected(newBlock *filteredBlock) error {",
      "content": "func (n *NeutrinoNotifier) handleBlockConnected(newBlock *filteredBlock) error {\n\t// We'll extend the txNotifier's height with the information of this\n\t// new block, which will handle all of the notification logic for us.\n\t//\n\t// We actually need the _full_ block here as well in order to be able\n\t// to send the full block back up to the client. The neutrino client\n\t// itself will only dispatch a block if one of the items we're looking\n\t// for matches, so ultimately passing it the full block will still only\n\t// result in the items we care about being dispatched.\n\trawBlock, err := n.GetBlock(newBlock.hash)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to get full block: %v\", err)\n\t}\n\terr = n.txNotifier.ConnectTip(rawBlock, newBlock.height)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to connect tip: %v\", err)\n\t}\n\n\tchainntnfs.Log.Infof(\"New block: height=%v, sha=%v\", newBlock.height,\n\t\tnewBlock.hash)\n\n\t// Now that we've guaranteed the new block extends the txNotifier's\n\t// current tip, we'll proceed to dispatch notifications to all of our\n\t// registered clients whom have had notifications fulfilled. Before\n\t// doing so, we'll make sure update our in memory state in order to\n\t// satisfy any client requests based upon the new block.\n\tn.bestBlock.Hash = &newBlock.hash\n\tn.bestBlock.Height = int32(newBlock.height)\n\tn.bestBlock.BlockHeader = newBlock.header\n\n\tn.notifyBlockEpochs(\n\t\tint32(newBlock.height), &newBlock.hash, newBlock.header,\n\t)\n\treturn n.txNotifier.NotifyHeight(newBlock.height)\n}\n\n// getFilteredBlock is a utility to retrieve the full filtered block from a block epoch.",
      "length": 1481,
      "tokens": 220,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoNotifier) getFilteredBlock(epoch chainntnfs.BlockEpoch) (*filteredBlock, error) {",
      "content": "func (n *NeutrinoNotifier) getFilteredBlock(epoch chainntnfs.BlockEpoch) (*filteredBlock, error) {\n\trawBlock, err := n.GetBlock(*epoch.Hash)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to get block: %v\", err)\n\t}\n\n\ttxns := rawBlock.Transactions()\n\n\tblock := &filteredBlock{\n\t\thash:    *epoch.Hash,\n\t\theight:  uint32(epoch.Height),\n\t\theader:  &rawBlock.MsgBlock().Header,\n\t\ttxns:    txns,\n\t\tconnect: true,\n\t}\n\treturn block, nil\n}\n\n// notifyBlockEpochs notifies all registered block epoch clients of the newly\n// connected block to the main chain.",
      "length": 433,
      "tokens": 57,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoNotifier) notifyBlockEpochs(newHeight int32, newSha *chainhash.Hash,",
      "content": "func (n *NeutrinoNotifier) notifyBlockEpochs(newHeight int32, newSha *chainhash.Hash,\n\tblockHeader *wire.BlockHeader) {\n\n\tfor _, client := range n.blockEpochClients {\n\t\tn.notifyBlockEpochClient(client, newHeight, newSha, blockHeader)\n\t}\n}\n\n// notifyBlockEpochClient sends a registered block epoch client a notification\n// about a specific block.",
      "length": 251,
      "tokens": 31,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoNotifier) notifyBlockEpochClient(epochClient *blockEpochRegistration,",
      "content": "func (n *NeutrinoNotifier) notifyBlockEpochClient(epochClient *blockEpochRegistration,\n\theight int32, sha *chainhash.Hash, blockHeader *wire.BlockHeader) {\n\n\tepoch := &chainntnfs.BlockEpoch{\n\t\tHeight:      height,\n\t\tHash:        sha,\n\t\tBlockHeader: blockHeader,\n\t}\n\n\tselect {\n\tcase epochClient.epochQueue.ChanIn() <- epoch:\n\tcase <-epochClient.cancelChan:\n\tcase <-n.quit:\n\t}\n}\n\n// RegisterSpendNtfn registers an intent to be notified once the target\n// outpoint/output script has been spent by a transaction on-chain. When\n// intending to be notified of the spend of an output script, a nil outpoint\n// must be used. The heightHint should represent the earliest height in the\n// chain of the transaction that spent the outpoint/output script.\n//\n// Once a spend of has been detected, the details of the spending event will be\n// sent across the 'Spend' channel.",
      "length": 752,
      "tokens": 112,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoNotifier) RegisterSpendNtfn(outpoint *wire.OutPoint,",
      "content": "func (n *NeutrinoNotifier) RegisterSpendNtfn(outpoint *wire.OutPoint,\n\tpkScript []byte, heightHint uint32) (*chainntnfs.SpendEvent, error) {\n\n\t// Register the conf notification with the TxNotifier. A non-nil value\n\t// for `dispatch` will be returned if we are required to perform a\n\t// manual scan for the confirmation. Otherwise the notifier will begin\n\t// watching at tip for the transaction to confirm.\n\tntfn, err := n.txNotifier.RegisterSpend(outpoint, pkScript, heightHint)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// To determine whether this outpoint has been spent on-chain, we'll\n\t// update our filter to watch for the transaction at tip and we'll also\n\t// dispatch a historical rescan to determine if it has been spent in the\n\t// past.\n\t//\n\t// We'll update our filter first to ensure we can immediately detect the\n\t// spend at tip.\n\tif outpoint == nil {\n\t\toutpoint = &chainntnfs.ZeroOutPoint\n\t}\n\tinputToWatch := neutrino.InputWithScript{\n\t\tOutPoint: *outpoint,\n\t\tPkScript: pkScript,\n\t}\n\tupdateOptions := []neutrino.UpdateOption{\n\t\tneutrino.AddInputs(inputToWatch),\n\t\tneutrino.DisableDisconnectedNtfns(true),\n\t}\n\n\t// We'll use the txNotifier's tip as the starting point of our filter\n\t// update. In the case of an output script spend request, we'll check if\n\t// we should perform a historical rescan and start from there, as we\n\t// cannot do so with GetUtxo since it matches outpoints.\n\trewindHeight := ntfn.Height\n\tif ntfn.HistoricalDispatch != nil && *outpoint == chainntnfs.ZeroOutPoint {\n\t\trewindHeight = ntfn.HistoricalDispatch.StartHeight\n\t}\n\tupdateOptions = append(updateOptions, neutrino.Rewind(rewindHeight))\n\n\terrChan := make(chan error, 1)\n\tselect {\n\tcase n.notificationRegistry <- &rescanFilterUpdate{\n\t\tupdateOptions: updateOptions,\n\t\terrChan:       errChan,\n\t}:\n\tcase <-n.quit:\n\t\treturn nil, chainntnfs.ErrChainNotifierShuttingDown\n\t}\n\n\tselect {\n\tcase err = <-errChan:\n\tcase <-n.quit:\n\t\treturn nil, chainntnfs.ErrChainNotifierShuttingDown\n\t}\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to update filter: %v\", err)\n\t}\n\n\t// If the txNotifier didn't return any details to perform a historical\n\t// scan of the chain, or if we already performed one like in the case of\n\t// output script spend requests, then we can return early as there's\n\t// nothing left for us to do.\n\tif ntfn.HistoricalDispatch == nil || *outpoint == chainntnfs.ZeroOutPoint {\n\t\treturn ntfn.Event, nil\n\t}\n\n\t// Grab the current best height as the height may have been updated\n\t// while we were draining the chainUpdates queue.\n\tn.bestBlockMtx.RLock()\n\tcurrentHeight := uint32(n.bestBlock.Height)\n\tn.bestBlockMtx.RUnlock()\n\n\tntfn.HistoricalDispatch.EndHeight = currentHeight\n\n\t// With the filter updated, we'll dispatch our historical rescan to\n\t// ensure we detect the spend if it happened in the past.\n\tn.wg.Add(1)\n\tgo func() {\n\t\tdefer n.wg.Done()\n\n\t\t// We'll ensure that neutrino is caught up to the starting\n\t\t// height before we attempt to fetch the UTXO from the chain.\n\t\t// If we're behind, then we may miss a notification dispatch.\n\t\tfor {\n\t\t\tn.bestBlockMtx.RLock()\n\t\t\tcurrentHeight := uint32(n.bestBlock.Height)\n\t\t\tn.bestBlockMtx.RUnlock()\n\n\t\t\tif currentHeight >= ntfn.HistoricalDispatch.StartHeight {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tselect {\n\t\t\tcase <-time.After(time.Millisecond * 200):\n\t\t\tcase <-n.quit:\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\tspendReport, err := n.p2pNode.GetUtxo(\n\t\t\tneutrino.WatchInputs(inputToWatch),\n\t\t\tneutrino.StartBlock(&headerfs.BlockStamp{\n\t\t\t\tHeight: int32(ntfn.HistoricalDispatch.StartHeight),\n\t\t\t}),\n\t\t\tneutrino.EndBlock(&headerfs.BlockStamp{\n\t\t\t\tHeight: int32(ntfn.HistoricalDispatch.EndHeight),\n\t\t\t}),\n\t\t\tneutrino.ProgressHandler(func(processedHeight uint32) {\n\t\t\t\t// We persist the rescan progress to achieve incremental\n\t\t\t\t// behavior across restarts, otherwise long rescans may\n\t\t\t\t// start from the beginning with every restart.\n\t\t\t\terr := n.spendHintCache.CommitSpendHint(\n\t\t\t\t\tprocessedHeight,\n\t\t\t\t\tntfn.HistoricalDispatch.SpendRequest)\n\t\t\t\tif err != nil {\n\t\t\t\t\tchainntnfs.Log.Errorf(\"Failed to update rescan \"+\n\t\t\t\t\t\t\"progress: %v\", err)\n\t\t\t\t}\n\t\t\t}),\n\t\t\tneutrino.QuitChan(n.quit),\n\t\t)\n\t\tif err != nil && !strings.Contains(err.Error(), \"not found\") {\n\t\t\tchainntnfs.Log.Errorf(\"Failed getting UTXO: %v\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// If a spend report was returned, and the transaction is present, then\n\t\t// this means that the output is already spent.\n\t\tvar spendDetails *chainntnfs.SpendDetail\n\t\tif spendReport != nil && spendReport.SpendingTx != nil {\n\t\t\tspendingTxHash := spendReport.SpendingTx.TxHash()\n\t\t\tspendDetails = &chainntnfs.SpendDetail{\n\t\t\t\tSpentOutPoint:     outpoint,\n\t\t\t\tSpenderTxHash:     &spendingTxHash,\n\t\t\t\tSpendingTx:        spendReport.SpendingTx,\n\t\t\t\tSpenderInputIndex: spendReport.SpendingInputIndex,\n\t\t\t\tSpendingHeight:    int32(spendReport.SpendingTxHeight),\n\t\t\t}\n\t\t}\n\n\t\t// Finally, no matter whether the rescan found a spend in the past or\n\t\t// not, we'll mark our historical rescan as complete to ensure the\n\t\t// outpoint's spend hint gets updated upon connected/disconnected\n\t\t// blocks.\n\t\terr = n.txNotifier.UpdateSpendDetails(\n\t\t\tntfn.HistoricalDispatch.SpendRequest, spendDetails,\n\t\t)\n\t\tif err != nil {\n\t\t\tchainntnfs.Log.Errorf(\"Failed to update spend details: %v\", err)\n\t\t\treturn\n\t\t}\n\t}()\n\n\treturn ntfn.Event, nil\n}\n\n// RegisterConfirmationsNtfn registers an intent to be notified once the target\n// txid/output script has reached numConfs confirmations on-chain. When\n// intending to be notified of the confirmation of an output script, a nil txid\n// must be used. The heightHint should represent the earliest height at which\n// the txid/output script could have been included in the chain.\n//\n// Progress on the number of confirmations left can be read from the 'Updates'\n// channel. Once it has reached all of its confirmations, a notification will be\n// sent across the 'Confirmed' channel.",
      "length": 5599,
      "tokens": 729,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoNotifier) RegisterConfirmationsNtfn(txid *chainhash.Hash,",
      "content": "func (n *NeutrinoNotifier) RegisterConfirmationsNtfn(txid *chainhash.Hash,\n\tpkScript []byte, numConfs, heightHint uint32,\n\topts ...chainntnfs.NotifierOption) (*chainntnfs.ConfirmationEvent, error) {\n\n\t// Register the conf notification with the TxNotifier. A non-nil value\n\t// for `dispatch` will be returned if we are required to perform a\n\t// manual scan for the confirmation. Otherwise the notifier will begin\n\t// watching at tip for the transaction to confirm.\n\tntfn, err := n.txNotifier.RegisterConf(\n\t\ttxid, pkScript, numConfs, heightHint, opts...,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// To determine whether this transaction has confirmed on-chain, we'll\n\t// update our filter to watch for the transaction at tip and we'll also\n\t// dispatch a historical rescan to determine if it has confirmed in the\n\t// past.\n\t//\n\t// We'll update our filter first to ensure we can immediately detect the\n\t// confirmation at tip. To do so, we'll map the script into an address\n\t// type so we can instruct neutrino to match if the transaction\n\t// containing the script is found in a block.\n\tparams := n.p2pNode.ChainParams()\n\t_, addrs, _, err := txscript.ExtractPkScriptAddrs(pkScript, &params)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to extract script: %v\", err)\n\t}\n\n\t// We'll send the filter update request to the notifier's main event\n\t// handler and wait for its response.\n\terrChan := make(chan error, 1)\n\tselect {\n\tcase n.notificationRegistry <- &rescanFilterUpdate{\n\t\tupdateOptions: []neutrino.UpdateOption{\n\t\t\tneutrino.AddAddrs(addrs...),\n\t\t\tneutrino.Rewind(ntfn.Height),\n\t\t\tneutrino.DisableDisconnectedNtfns(true),\n\t\t},\n\t\terrChan: errChan,\n\t}:\n\tcase <-n.quit:\n\t\treturn nil, chainntnfs.ErrChainNotifierShuttingDown\n\t}\n\n\tselect {\n\tcase err = <-errChan:\n\tcase <-n.quit:\n\t\treturn nil, chainntnfs.ErrChainNotifierShuttingDown\n\t}\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to update filter: %v\", err)\n\t}\n\n\t// If a historical rescan was not requested by the txNotifier, then we\n\t// can return to the caller.\n\tif ntfn.HistoricalDispatch == nil {\n\t\treturn ntfn.Event, nil\n\t}\n\n\t// Grab the current best height as the height may have been updated\n\t// while we were draining the chainUpdates queue.\n\tn.bestBlockMtx.RLock()\n\tcurrentHeight := uint32(n.bestBlock.Height)\n\tn.bestBlockMtx.RUnlock()\n\n\tntfn.HistoricalDispatch.EndHeight = currentHeight\n\n\t// Finally, with the filter updated, we can dispatch the historical\n\t// rescan to ensure we can detect if the event happened in the past.\n\tselect {\n\tcase n.notificationRegistry <- ntfn.HistoricalDispatch:\n\tcase <-n.quit:\n\t\treturn nil, chainntnfs.ErrChainNotifierShuttingDown\n\t}\n\n\treturn ntfn.Event, nil\n}\n\n// GetBlock is used to retrieve the block with the given hash. Since the block\n// cache used by neutrino will be the same as that used by LND (since it is\n// passed to neutrino on initialisation), the neutrino GetBlock method can be\n// called directly since it already uses the block cache. However, neutrino\n// does not lock the block cache mutex for the given block hash and so that is\n// done here.",
      "length": 2905,
      "tokens": 430,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoNotifier) GetBlock(hash chainhash.Hash) (",
      "content": "func (n *NeutrinoNotifier) GetBlock(hash chainhash.Hash) (\n\t*btcutil.Block, error) {\n\n\tn.blockCache.HashMutex.Lock(lntypes.Hash(hash))\n\tdefer n.blockCache.HashMutex.Unlock(lntypes.Hash(hash))\n\n\treturn n.p2pNode.GetBlock(hash)\n}\n\n// blockEpochRegistration represents a client's intent to receive a\n// notification with each newly connected block.",
      "length": 277,
      "tokens": 25,
      "embedding": []
    },
    {
      "slug": "type blockEpochRegistration struct {",
      "content": "type blockEpochRegistration struct {\n\tepochID uint64\n\n\tepochChan chan *chainntnfs.BlockEpoch\n\n\tepochQueue *queue.ConcurrentQueue\n\n\tcancelChan chan struct{}\n\n\tbestBlock *chainntnfs.BlockEpoch\n\n\terrorChan chan error\n\n\twg sync.WaitGroup\n}\n\n// epochCancel is a message sent to the NeutrinoNotifier when a client wishes\n// to cancel an outstanding epoch notification that has yet to be dispatched.",
      "length": 339,
      "tokens": 44,
      "embedding": []
    },
    {
      "slug": "type epochCancel struct {",
      "content": "type epochCancel struct {\n\tepochID uint64\n}\n\n// RegisterBlockEpochNtfn returns a BlockEpochEvent which subscribes the\n// caller to receive notifications, of each new block connected to the main\n// chain. Clients have the option of passing in their best known block, which\n// the notifier uses to check if they are behind on blocks and catch them up. If\n// they do not provide one, then a notification will be dispatched immediately\n// for the current tip of the chain upon a successful registration.",
      "length": 465,
      "tokens": 80,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoNotifier) RegisterBlockEpochNtfn(",
      "content": "func (n *NeutrinoNotifier) RegisterBlockEpochNtfn(\n\tbestBlock *chainntnfs.BlockEpoch) (*chainntnfs.BlockEpochEvent, error) {\n\n\treg := &blockEpochRegistration{\n\t\tepochQueue: queue.NewConcurrentQueue(20),\n\t\tepochChan:  make(chan *chainntnfs.BlockEpoch, 20),\n\t\tcancelChan: make(chan struct{}),\n\t\tepochID:    atomic.AddUint64(&n.epochClientCounter, 1),\n\t\tbestBlock:  bestBlock,\n\t\terrorChan:  make(chan error, 1),\n\t}\n\treg.epochQueue.Start()\n\n\t// Before we send the request to the main goroutine, we'll launch a new\n\t// goroutine to proxy items added to our queue to the client itself.\n\t// This ensures that all notifications are received *in order*.\n\treg.wg.Add(1)\n\tgo func() {\n\t\tdefer reg.wg.Done()\n\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase ntfn := <-reg.epochQueue.ChanOut():\n\t\t\t\tblockNtfn := ntfn.(*chainntnfs.BlockEpoch)\n\t\t\t\tselect {\n\t\t\t\tcase reg.epochChan <- blockNtfn:\n\n\t\t\t\tcase <-reg.cancelChan:\n\t\t\t\t\treturn\n\n\t\t\t\tcase <-n.quit:\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\tcase <-reg.cancelChan:\n\t\t\t\treturn\n\n\t\t\tcase <-n.quit:\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\tselect {\n\tcase <-n.quit:\n\t\t// As we're exiting before the registration could be sent,\n\t\t// we'll stop the queue now ourselves.\n\t\treg.epochQueue.Stop()\n\n\t\treturn nil, errors.New(\"chainntnfs: system interrupt while \" +\n\t\t\t\"attempting to register for block epoch notification.\")\n\tcase n.notificationRegistry <- reg:\n\t\treturn &chainntnfs.BlockEpochEvent{\n\t\t\tEpochs: reg.epochChan,\n\t\t\tCancel: func() {\n\t\t\t\tcancel := &epochCancel{\n\t\t\t\t\tepochID: reg.epochID,\n\t\t\t\t}\n\n\t\t\t\t// Submit epoch cancellation to notification dispatcher.\n\t\t\t\tselect {\n\t\t\t\tcase n.notificationCancels <- cancel:\n\t\t\t\t\t// Cancellation is being handled, drain the epoch channel until it is\n\t\t\t\t\t// closed before yielding to caller.\n\t\t\t\t\tfor {\n\t\t\t\t\t\tselect {\n\t\t\t\t\t\tcase _, ok := <-reg.epochChan:\n\t\t\t\t\t\t\tif !ok {\n\t\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\tcase <-n.quit:\n\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\tcase <-n.quit:\n\t\t\t\t}\n\t\t\t},\n\t\t}, nil\n\t}\n}\n\n// NeutrinoChainConn is a wrapper around neutrino's chain backend in order\n// to satisfy the chainntnfs.ChainConn interface.",
      "length": 1902,
      "tokens": 233,
      "embedding": []
    },
    {
      "slug": "type NeutrinoChainConn struct {",
      "content": "type NeutrinoChainConn struct {\n\tp2pNode *neutrino.ChainService\n}\n\n// GetBlockHeader returns the block header for a hash.",
      "length": 86,
      "tokens": 12,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoChainConn) GetBlockHeader(blockHash *chainhash.Hash) (*wire.BlockHeader, error) {",
      "content": "func (n *NeutrinoChainConn) GetBlockHeader(blockHash *chainhash.Hash) (*wire.BlockHeader, error) {\n\treturn n.p2pNode.GetBlockHeader(blockHash)\n}\n\n// GetBlockHeaderVerbose returns a verbose block header result for a hash. This\n// result only contains the height with a nil hash.",
      "length": 174,
      "tokens": 25,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoChainConn) GetBlockHeaderVerbose(blockHash *chainhash.Hash) (",
      "content": "func (n *NeutrinoChainConn) GetBlockHeaderVerbose(blockHash *chainhash.Hash) (\n\t*btcjson.GetBlockHeaderVerboseResult, error) {\n\n\theight, err := n.p2pNode.GetBlockHeight(blockHash)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// Since only the height is used from the result, leave the hash nil.\n\treturn &btcjson.GetBlockHeaderVerboseResult{Height: int32(height)}, nil\n}\n\n// GetBlockHash returns the hash from a block height.",
      "length": 329,
      "tokens": 44,
      "embedding": []
    },
    {
      "slug": "func (n *NeutrinoChainConn) GetBlockHash(blockHeight int64) (*chainhash.Hash, error) {",
      "content": "func (n *NeutrinoChainConn) GetBlockHash(blockHeight int64) (*chainhash.Hash, error) {\n\treturn n.p2pNode.GetBlockHash(blockHeight)\n}\n",
      "length": 44,
      "tokens": 3,
      "embedding": []
    }
  ]
}