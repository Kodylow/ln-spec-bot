{
  "filepath": "../implementations/go/lnd/watchtower/wtclient/client.go",
  "package": "wtclient",
  "sections": [
    {
      "slug": "func genActiveSessionFilter(anchor bool) func(*ClientSession) bool {",
      "content": "func genActiveSessionFilter(anchor bool) func(*ClientSession) bool {\n\treturn func(s *ClientSession) bool {\n\t\treturn s.Status == wtdb.CSessionActive &&\n\t\t\tanchor == s.Policy.IsAnchorChannel()\n\t}\n}\n\n// RegisteredTower encompasses information about a registered watchtower with\n// the client.",
      "length": 213,
      "tokens": 27,
      "embedding": []
    },
    {
      "slug": "type RegisteredTower struct {",
      "content": "type RegisteredTower struct {\n\t*wtdb.Tower\n\n\t// Sessions is the set of sessions corresponding to the watchtower.\n\tSessions map[wtdb.SessionID]*wtdb.ClientSession\n\n\t// ActiveSessionCandidate determines whether the watchtower is currently\n\t// being considered for new sessions.\n\tActiveSessionCandidate bool\n}\n\n// Client is the primary interface used by the daemon to control a client's\n// lifecycle and backup revoked states.",
      "length": 382,
      "tokens": 51,
      "embedding": []
    },
    {
      "slug": "type Client interface {",
      "content": "type Client interface {\n\t// AddTower adds a new watchtower reachable at the given address and\n\t// considers it for new sessions. If the watchtower already exists, then\n\t// any new addresses included will be considered when dialing it for\n\t// session negotiations and backups.\n\tAddTower(*lnwire.NetAddress) error\n\n\t// RemoveTower removes a watchtower from being considered for future\n\t// session negotiations and from being used for any subsequent backups\n\t// until it's added again. If an address is provided, then this call\n\t// only serves as a way of removing the address from the watchtower\n\t// instead.\n\tRemoveTower(*btcec.PublicKey, net.Addr) error\n\n\t// RegisteredTowers retrieves the list of watchtowers registered with\n\t// the client.\n\tRegisteredTowers(...wtdb.ClientSessionListOption) ([]*RegisteredTower,\n\t\terror)\n\n\t// LookupTower retrieves a registered watchtower through its public key.\n\tLookupTower(*btcec.PublicKey,\n\t\t...wtdb.ClientSessionListOption) (*RegisteredTower, error)\n\n\t// Stats returns the in-memory statistics of the client since startup.\n\tStats() ClientStats\n\n\t// Policy returns the active client policy configuration.\n\tPolicy() wtpolicy.Policy\n\n\t// RegisterChannel persistently initializes any channel-dependent\n\t// parameters within the client. This should be called during link\n\t// startup to ensure that the client is able to support the link during\n\t// operation.\n\tRegisterChannel(lnwire.ChannelID) error\n\n\t// BackupState initiates a request to back up a particular revoked\n\t// state. If the method returns nil, the backup is guaranteed to be\n\t// successful unless the client is force quit, or the justice\n\t// transaction would create dust outputs when trying to abide by the\n\t// negotiated policy. If the channel we're trying to back up doesn't\n\t// have a tweak for the remote party's output, then isTweakless should\n\t// be true.\n\tBackupState(*lnwire.ChannelID, *lnwallet.BreachRetribution,\n\t\tchanneldb.ChannelType) error\n\n\t// Start initializes the watchtower client, allowing it process requests\n\t// to backup revoked channel states.\n\tStart() error\n\n\t// Stop attempts a graceful shutdown of the watchtower client. In doing\n\t// so, it will attempt to flush the pipeline and deliver any queued\n\t// states to the tower before exiting.\n\tStop() error\n\n\t// ForceQuit will forcibly shutdown the watchtower client. Calling this\n\t// may lead to queued states being dropped.\n\tForceQuit()\n}\n\n// Config provides the TowerClient with access to the resources it requires to\n// perform its duty. All nillable fields must be non-nil for the tower to be\n// initialized properly.",
      "length": 2509,
      "tokens": 363,
      "embedding": []
    },
    {
      "slug": "type Config struct {",
      "content": "type Config struct {\n\t// Signer provides access to the wallet so that the client can sign\n\t// justice transactions that spend from a remote party's commitment\n\t// transaction.\n\tSigner input.Signer\n\n\t// NewAddress generates a new on-chain sweep pkscript.\n\tNewAddress func() ([]byte, error)\n\n\t// SecretKeyRing is used to derive the session keys used to communicate\n\t// with the tower. The client only stores the KeyLocators internally so\n\t// that we never store private keys on disk.\n\tSecretKeyRing ECDHKeyRing\n\n\t// Dial connects to an addr using the specified net and returns the\n\t// connection object.\n\tDial tor.DialFunc\n\n\t// AuthDialer establishes a brontide connection over an onion or clear\n\t// network.\n\tAuthDial AuthDialer\n\n\t// DB provides access to the client's stable storage medium.\n\tDB DB\n\n\t// Policy is the session policy the client will propose when creating\n\t// new sessions with the tower. If the policy differs from any active\n\t// sessions recorded in the database, those sessions will be ignored and\n\t// new sessions will be requested immediately.\n\tPolicy wtpolicy.Policy\n\n\t// ChainHash identifies the chain that the client is on and for which\n\t// the tower must be watching to monitor for breaches.\n\tChainHash chainhash.Hash\n\n\t// ForceQuitDelay is the duration after attempting to shutdown that the\n\t// client will automatically abort any pending backups if an unclean\n\t// shutdown is detected. If the value is less than or equal to zero, a\n\t// call to Stop may block indefinitely. The client can always be\n\t// ForceQuit externally irrespective of the chosen parameter.\n\tForceQuitDelay time.Duration\n\n\t// ReadTimeout is the duration we will wait during a read before\n\t// breaking out of a blocking read. If the value is less than or equal\n\t// to zero, the default will be used instead.\n\tReadTimeout time.Duration\n\n\t// WriteTimeout is the duration we will wait during a write before\n\t// breaking out of a blocking write. If the value is less than or equal\n\t// to zero, the default will be used instead.\n\tWriteTimeout time.Duration\n\n\t// MinBackoff defines the initial backoff applied to connections with\n\t// watchtowers. Subsequent backoff durations will grow exponentially up\n\t// until MaxBackoff.\n\tMinBackoff time.Duration\n\n\t// MaxBackoff defines the maximum backoff applied to connections with\n\t// watchtowers. If the exponential backoff produces a timeout greater\n\t// than this value, the backoff will be clamped to MaxBackoff.\n\tMaxBackoff time.Duration\n}\n\n// newTowerMsg is an internal message we'll use within the TowerClient to signal\n// that a new tower can be considered.",
      "length": 2510,
      "tokens": 404,
      "embedding": []
    },
    {
      "slug": "type newTowerMsg struct {",
      "content": "type newTowerMsg struct {\n\t// addr is the tower's reachable address that we'll use to establish a\n\t// connection with.\n\taddr *lnwire.NetAddress\n\n\t// errChan is the channel through which we'll send a response back to\n\t// the caller when handling their request.\n\t//\n\t// NOTE: This channel must be buffered.\n\terrChan chan error\n}\n\n// staleTowerMsg is an internal message we'll use within the TowerClient to\n// signal that a tower should no longer be considered.",
      "length": 420,
      "tokens": 72,
      "embedding": []
    },
    {
      "slug": "type staleTowerMsg struct {",
      "content": "type staleTowerMsg struct {\n\t// pubKey is the identifying public key of the watchtower.\n\tpubKey *btcec.PublicKey\n\n\t// addr is an optional field that when set signals that the address\n\t// should be removed from the watchtower's set of addresses, indicating\n\t// that it is stale. If it's not set, then the watchtower should be\n\t// no longer be considered for new sessions.\n\taddr net.Addr\n\n\t// errChan is the channel through which we'll send a response back to\n\t// the caller when handling their request.\n\t//\n\t// NOTE: This channel must be buffered.\n\terrChan chan error\n}\n\n// TowerClient is a concrete implementation of the Client interface, offering a\n// non-blocking, reliable subsystem for backing up revoked states to a specified\n// private tower.",
      "length": 702,
      "tokens": 119,
      "embedding": []
    },
    {
      "slug": "type TowerClient struct {",
      "content": "type TowerClient struct {\n\tstarted sync.Once\n\tstopped sync.Once\n\tforced  sync.Once\n\n\tcfg *Config\n\n\tlog btclog.Logger\n\n\tpipeline *taskPipeline\n\n\tnegotiator        SessionNegotiator\n\tcandidateTowers   TowerCandidateIterator\n\tcandidateSessions map[wtdb.SessionID]*ClientSession\n\tactiveSessions    sessionQueueSet\n\n\tsessionQueue *sessionQueue\n\tprevTask     *backupTask\n\n\tbackupMu          sync.Mutex\n\tsummaries         wtdb.ChannelSummaries\n\tchanCommitHeights map[lnwire.ChannelID]uint64\n\n\tstatTicker *time.Ticker\n\tstats      *ClientStats\n\n\tnewTowers   chan *newTowerMsg\n\tstaleTowers chan *staleTowerMsg\n\n\twg        sync.WaitGroup\n\tforceQuit chan struct{}\n}\n\n// Compile-time constraint to ensure *TowerClient implements the Client\n// interface.\nvar _ Client = (*TowerClient)(nil)\n\n// New initializes a new TowerClient from the provide Config. An error is\n// returned if the client could not be initialized.",
      "length": 839,
      "tokens": 84,
      "embedding": []
    },
    {
      "slug": "func New(config *Config) (*TowerClient, error) {",
      "content": "func New(config *Config) (*TowerClient, error) {\n\t// Copy the config to prevent side effects from modifying both the\n\t// internal and external version of the Config.\n\tcfg := new(Config)\n\t*cfg = *config\n\n\t// Set the read timeout to the default if none was provided.\n\tif cfg.ReadTimeout <= 0 {\n\t\tcfg.ReadTimeout = DefaultReadTimeout\n\t}\n\n\t// Set the write timeout to the default if none was provided.\n\tif cfg.WriteTimeout <= 0 {\n\t\tcfg.WriteTimeout = DefaultWriteTimeout\n\t}\n\n\tprefix := \"(legacy)\"\n\tif cfg.Policy.IsAnchorChannel() {\n\t\tprefix = \"(anchor)\"\n\t}\n\tplog := build.NewPrefixLog(prefix, log)\n\n\t// Load the sweep pkscripts that have been generated for all previously\n\t// registered channels.\n\tchanSummaries, err := cfg.DB.FetchChanSummaries()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tc := &TowerClient{\n\t\tcfg:               cfg,\n\t\tlog:               plog,\n\t\tpipeline:          newTaskPipeline(plog),\n\t\tchanCommitHeights: make(map[lnwire.ChannelID]uint64),\n\t\tactiveSessions:    make(sessionQueueSet),\n\t\tsummaries:         chanSummaries,\n\t\tstatTicker:        time.NewTicker(DefaultStatInterval),\n\t\tstats:             new(ClientStats),\n\t\tnewTowers:         make(chan *newTowerMsg),\n\t\tstaleTowers:       make(chan *staleTowerMsg),\n\t\tforceQuit:         make(chan struct{}),\n\t}\n\n\t// perUpdate is a callback function that will be used to inspect the\n\t// full set of candidate client sessions loaded from disk, and to\n\t// determine the highest known commit height for each channel. This\n\t// allows the client to reject backups that it has already processed for\n\t// its active policy.\n\tperUpdate := func(policy wtpolicy.Policy, chanID lnwire.ChannelID,\n\t\tcommitHeight uint64) {\n\n\t\t// We only want to consider accepted updates that have been\n\t\t// accepted under an identical policy to the client's current\n\t\t// policy.\n\t\tif policy != c.cfg.Policy {\n\t\t\treturn\n\t\t}\n\n\t\t// Take the highest commit height found in the session's acked\n\t\t// updates.\n\t\theight, ok := c.chanCommitHeights[chanID]\n\t\tif !ok || commitHeight > height {\n\t\t\tc.chanCommitHeights[chanID] = commitHeight\n\t\t}\n\t}\n\n\tperMaxHeight := func(s *wtdb.ClientSession, chanID lnwire.ChannelID,\n\t\theight uint64) {\n\n\t\tperUpdate(s.Policy, chanID, height)\n\t}\n\n\tperCommittedUpdate := func(s *wtdb.ClientSession,\n\t\tu *wtdb.CommittedUpdate) {\n\n\t\tperUpdate(s.Policy, u.BackupID.ChanID, u.BackupID.CommitHeight)\n\t}\n\n\t// Load all candidate sessions and towers from the database into the\n\t// client. We will use any of these sessions if their policies match the\n\t// current policy of the client, otherwise they will be ignored and new\n\t// sessions will be requested.\n\tisAnchorClient := cfg.Policy.IsAnchorChannel()\n\tactiveSessionFilter := genActiveSessionFilter(isAnchorClient)\n\n\tcandidateTowers := newTowerListIterator()\n\tperActiveTower := func(tower *Tower) {\n\t\t// If the tower has already been marked as active, then there is\n\t\t// no need to add it to the iterator again.\n\t\tif candidateTowers.IsActive(tower.ID) {\n\t\t\treturn\n\t\t}\n\n\t\tlog.Infof(\"Using private watchtower %s, offering policy %s\",\n\t\t\ttower, cfg.Policy)\n\n\t\t// Add the tower to the set of candidate towers.\n\t\tcandidateTowers.AddCandidate(tower)\n\t}\n\n\tcandidateSessions, err := getTowerAndSessionCandidates(\n\t\tcfg.DB, cfg.SecretKeyRing, activeSessionFilter, perActiveTower,\n\t\twtdb.WithPerMaxHeight(perMaxHeight),\n\t\twtdb.WithPerCommittedUpdate(perCommittedUpdate),\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tc.candidateTowers = candidateTowers\n\tc.candidateSessions = candidateSessions\n\n\tc.negotiator = newSessionNegotiator(&NegotiatorConfig{\n\t\tDB:            cfg.DB,\n\t\tSecretKeyRing: cfg.SecretKeyRing,\n\t\tPolicy:        cfg.Policy,\n\t\tChainHash:     cfg.ChainHash,\n\t\tSendMessage:   c.sendMessage,\n\t\tReadMessage:   c.readMessage,\n\t\tDial:          c.dial,\n\t\tCandidates:    c.candidateTowers,\n\t\tMinBackoff:    cfg.MinBackoff,\n\t\tMaxBackoff:    cfg.MaxBackoff,\n\t\tLog:           plog,\n\t})\n\n\treturn c, nil\n}\n\n// getTowerAndSessionCandidates loads all the towers from the DB and then\n// fetches the sessions for each of tower. Sessions are only collected if they\n// pass the sessionFilter check. If a tower has a session that does pass the\n// sessionFilter check then the perActiveTower call-back will be called on that\n// tower.",
      "length": 4030,
      "tokens": 501,
      "embedding": []
    },
    {
      "slug": "func getTowerAndSessionCandidates(db DB, keyRing ECDHKeyRing,",
      "content": "func getTowerAndSessionCandidates(db DB, keyRing ECDHKeyRing,\n\tsessionFilter func(*ClientSession) bool,\n\tperActiveTower func(tower *Tower),\n\topts ...wtdb.ClientSessionListOption) (\n\tmap[wtdb.SessionID]*ClientSession, error) {\n\n\ttowers, err := db.ListTowers()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcandidateSessions := make(map[wtdb.SessionID]*ClientSession)\n\tfor _, dbTower := range towers {\n\t\ttower, err := NewTowerFromDBTower(dbTower)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tsessions, err := db.ListClientSessions(&tower.ID, opts...)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tfor _, s := range sessions {\n\t\t\ttowerKeyDesc, err := keyRing.DeriveKey(\n\t\t\t\tkeychain.KeyLocator{\n\t\t\t\t\tFamily: keychain.KeyFamilyTowerSession,\n\t\t\t\t\tIndex:  s.KeyIndex,\n\t\t\t\t},\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tsessionKeyECDH := keychain.NewPubKeyECDH(\n\t\t\t\ttowerKeyDesc, keyRing,\n\t\t\t)\n\n\t\t\tcs := &ClientSession{\n\t\t\t\tID:                s.ID,\n\t\t\t\tClientSessionBody: s.ClientSessionBody,\n\t\t\t\tTower:             tower,\n\t\t\t\tSessionKeyECDH:    sessionKeyECDH,\n\t\t\t}\n\n\t\t\tif !sessionFilter(cs) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Add the session to the set of candidate sessions.\n\t\t\tcandidateSessions[s.ID] = cs\n\t\t\tperActiveTower(tower)\n\t\t}\n\t}\n\n\treturn candidateSessions, nil\n}\n\n// getClientSessions retrieves the client sessions for a particular tower if\n// specified, otherwise all client sessions for all towers are retrieved. An\n// optional filter can be provided to filter out any undesired client sessions.\n//\n// NOTE: This method should only be used when deserialization of a\n// ClientSession's SessionPrivKey field is desired, otherwise, the existing\n// ListClientSessions method should be used.",
      "length": 1561,
      "tokens": 196,
      "embedding": []
    },
    {
      "slug": "func getClientSessions(db DB, keyRing ECDHKeyRing, forTower *wtdb.TowerID,",
      "content": "func getClientSessions(db DB, keyRing ECDHKeyRing, forTower *wtdb.TowerID,\n\tpassesFilter func(*ClientSession) bool,\n\topts ...wtdb.ClientSessionListOption) (\n\tmap[wtdb.SessionID]*ClientSession, error) {\n\n\tdbSessions, err := db.ListClientSessions(forTower, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Reload the tower from disk using the tower ID contained in each\n\t// candidate session. We will also rederive any session keys needed to\n\t// be able to communicate with the towers and authenticate session\n\t// requests. This prevents us from having to store the private keys on\n\t// disk.\n\tsessions := make(map[wtdb.SessionID]*ClientSession)\n\tfor _, s := range dbSessions {\n\t\tdbTower, err := db.LoadTowerByID(s.TowerID)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\ttowerKeyDesc, err := keyRing.DeriveKey(keychain.KeyLocator{\n\t\t\tFamily: keychain.KeyFamilyTowerSession,\n\t\t\tIndex:  s.KeyIndex,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tsessionKeyECDH := keychain.NewPubKeyECDH(towerKeyDesc, keyRing)\n\n\t\ttower, err := NewTowerFromDBTower(dbTower)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tcs := &ClientSession{\n\t\t\tID:                s.ID,\n\t\t\tClientSessionBody: s.ClientSessionBody,\n\t\t\tTower:             tower,\n\t\t\tSessionKeyECDH:    sessionKeyECDH,\n\t\t}\n\n\t\t// If an optional filter was provided, use it to filter out any\n\t\t// undesired sessions.\n\t\tif passesFilter != nil && !passesFilter(cs) {\n\t\t\tcontinue\n\t\t}\n\n\t\tsessions[s.ID] = cs\n\t}\n\n\treturn sessions, nil\n}\n\n// Start initializes the watchtower client by loading or negotiating an active\n// session and then begins processing backup tasks from the request pipeline.",
      "length": 1494,
      "tokens": 201,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) Start() error {",
      "content": "func (c *TowerClient) Start() error {\n\tvar returnErr error\n\tc.started.Do(func() {\n\t\tc.log.Infof(\"Watchtower client starting\")\n\n\t\t// First, restart a session queue for any sessions that have\n\t\t// committed but unacked state updates. This ensures that these\n\t\t// sessions will be able to flush the committed updates after a\n\t\t// restart.\n\t\tfor _, session := range c.candidateSessions {\n\t\t\tcommittedUpdates, err := c.cfg.DB.FetchSessionCommittedUpdates(&session.ID)\n\t\t\tif err != nil {\n\t\t\t\treturnErr = err\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif len(committedUpdates) > 0 {\n\t\t\t\tc.log.Infof(\"Starting session=%s to process \"+\n\t\t\t\t\t\"%d committed backups\", session.ID,\n\t\t\t\t\tlen(committedUpdates))\n\n\t\t\t\tc.initActiveQueue(session, committedUpdates)\n\t\t\t}\n\t\t}\n\n\t\t// Now start the session negotiator, which will allow us to\n\t\t// request new session as soon as the backupDispatcher starts\n\t\t// up.\n\t\terr := c.negotiator.Start()\n\t\tif err != nil {\n\t\t\treturnErr = err\n\t\t\treturn\n\t\t}\n\n\t\t// Start the task pipeline to which new backup tasks will be\n\t\t// submitted from active links.\n\t\tc.pipeline.Start()\n\n\t\tc.wg.Add(1)\n\t\tgo c.backupDispatcher()\n\n\t\tc.log.Infof(\"Watchtower client started successfully\")\n\t})\n\treturn returnErr\n}\n\n// Stop idempotently initiates a graceful shutdown of the watchtower client.",
      "length": 1184,
      "tokens": 159,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) Stop() error {",
      "content": "func (c *TowerClient) Stop() error {\n\tc.stopped.Do(func() {\n\t\tc.log.Debugf(\"Stopping watchtower client\")\n\n\t\t// 1. To ensure we don't hang forever on shutdown due to\n\t\t// unintended failures, we'll delay a call to force quit the\n\t\t// pipeline if a ForceQuitDelay is specified. This will have no\n\t\t// effect if the pipeline shuts down cleanly before the delay\n\t\t// fires.\n\t\t//\n\t\t// For full safety, this can be set to 0 and wait out\n\t\t// indefinitely.  However for mobile clients which may have a\n\t\t// limited amount of time to exit before the background process\n\t\t// is killed, this offers a way to ensure the process\n\t\t// terminates.\n\t\tif c.cfg.ForceQuitDelay > 0 {\n\t\t\ttime.AfterFunc(c.cfg.ForceQuitDelay, c.ForceQuit)\n\t\t}\n\n\t\t// 2. Shutdown the backup queue, which will prevent any further\n\t\t// updates from being accepted. In practice, the links should be\n\t\t// shutdown before the client has been stopped, so all updates\n\t\t// would have been added prior.\n\t\tc.pipeline.Stop()\n\n\t\t// 3. Once the backup queue has shutdown, wait for the main\n\t\t// dispatcher to exit. The backup queue will signal it's\n\t\t// completion to the dispatcher, which releases the wait group\n\t\t// after all tasks have been assigned to session queues.\n\t\tc.wg.Wait()\n\n\t\t// 4. Since all valid tasks have been assigned to session\n\t\t// queues, we no longer need to negotiate sessions.\n\t\tc.negotiator.Stop()\n\n\t\tc.log.Debugf(\"Waiting for active session queues to finish \"+\n\t\t\t\"draining, stats: %s\", c.stats)\n\n\t\t// 5. Shutdown all active session queues in parallel. These will\n\t\t// exit once all updates have been acked by the watchtower.\n\t\tc.activeSessions.ApplyAndWait(func(s *sessionQueue) func() {\n\t\t\treturn s.Stop\n\t\t})\n\n\t\t// Skip log if force quitting.\n\t\tselect {\n\t\tcase <-c.forceQuit:\n\t\t\treturn\n\t\tdefault:\n\t\t}\n\n\t\tc.log.Debugf(\"Client successfully stopped, stats: %s\", c.stats)\n\t})\n\treturn nil\n}\n\n// ForceQuit idempotently initiates an unclean shutdown of the watchtower\n// client. This should only be executed if Stop is unable to exit cleanly.",
      "length": 1920,
      "tokens": 300,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) ForceQuit() {",
      "content": "func (c *TowerClient) ForceQuit() {\n\tc.forced.Do(func() {\n\t\tc.log.Infof(\"Force quitting watchtower client\")\n\n\t\t// 1. Shutdown the backup queue, which will prevent any further\n\t\t// updates from being accepted. In practice, the links should be\n\t\t// shutdown before the client has been stopped, so all updates\n\t\t// would have been added prior.\n\t\tc.pipeline.ForceQuit()\n\n\t\t// 2. Once the backup queue has shutdown, wait for the main\n\t\t// dispatcher to exit. The backup queue will signal it's\n\t\t// completion to the dispatcher, which releases the wait group\n\t\t// after all tasks have been assigned to session queues.\n\t\tclose(c.forceQuit)\n\t\tc.wg.Wait()\n\n\t\t// 3. Since all valid tasks have been assigned to session\n\t\t// queues, we no longer need to negotiate sessions.\n\t\tc.negotiator.Stop()\n\n\t\t// 4. Force quit all active session queues in parallel. These\n\t\t// will exit once all updates have been acked by the watchtower.\n\t\tc.activeSessions.ApplyAndWait(func(s *sessionQueue) func() {\n\t\t\treturn s.ForceQuit\n\t\t})\n\n\t\tc.log.Infof(\"Watchtower client unclean shutdown complete, \"+\n\t\t\t\"stats: %s\", c.stats)\n\t})\n}\n\n// RegisterChannel persistently initializes any channel-dependent parameters\n// within the client. This should be called during link startup to ensure that\n// the client is able to support the link during operation.",
      "length": 1248,
      "tokens": 184,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) RegisterChannel(chanID lnwire.ChannelID) error {",
      "content": "func (c *TowerClient) RegisterChannel(chanID lnwire.ChannelID) error {\n\tc.backupMu.Lock()\n\tdefer c.backupMu.Unlock()\n\n\t// If a pkscript for this channel already exists, the channel has been\n\t// previously registered.\n\tif _, ok := c.summaries[chanID]; ok {\n\t\treturn nil\n\t}\n\n\t// Otherwise, generate a new sweep pkscript used to sweep funds for this\n\t// channel.\n\tpkScript, err := c.cfg.NewAddress()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Persist the sweep pkscript so that restarts will not introduce\n\t// address inflation when the channel is reregistered after a restart.\n\terr = c.cfg.DB.RegisterChannel(chanID, pkScript)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Finally, cache the pkscript in our in-memory cache to avoid db\n\t// lookups for the remainder of the daemon's execution.\n\tc.summaries[chanID] = wtdb.ClientChanSummary{\n\t\tSweepPkScript: pkScript,\n\t}\n\n\treturn nil\n}\n\n// BackupState initiates a request to back up a particular revoked state. If the\n// method returns nil, the backup is guaranteed to be successful unless the:\n//   - client is force quit,\n//   - justice transaction would create dust outputs when trying to abide by the\n//     negotiated policy, or\n//   - breached outputs contain too little value to sweep at the target sweep fee\n//     rate.",
      "length": 1153,
      "tokens": 188,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) BackupState(chanID *lnwire.ChannelID,",
      "content": "func (c *TowerClient) BackupState(chanID *lnwire.ChannelID,\n\tbreachInfo *lnwallet.BreachRetribution,\n\tchanType channeldb.ChannelType) error {\n\n\t// Retrieve the cached sweep pkscript used for this channel.\n\tc.backupMu.Lock()\n\tsummary, ok := c.summaries[*chanID]\n\tif !ok {\n\t\tc.backupMu.Unlock()\n\t\treturn ErrUnregisteredChannel\n\t}\n\n\t// Ignore backups that have already been presented to the client.\n\theight, ok := c.chanCommitHeights[*chanID]\n\tif ok && breachInfo.RevokedStateNum <= height {\n\t\tc.backupMu.Unlock()\n\t\tc.log.Debugf(\"Ignoring duplicate backup for chanid=%v at height=%d\",\n\t\t\tchanID, breachInfo.RevokedStateNum)\n\t\treturn nil\n\t}\n\n\t// This backup has a higher commit height than any known backup for this\n\t// channel. We'll update our tip so that we won't accept it again if the\n\t// link flaps.\n\tc.chanCommitHeights[*chanID] = breachInfo.RevokedStateNum\n\tc.backupMu.Unlock()\n\n\ttask := newBackupTask(\n\t\tchanID, breachInfo, summary.SweepPkScript, chanType,\n\t)\n\n\treturn c.pipeline.QueueBackupTask(task)\n}\n\n// nextSessionQueue attempts to fetch an active session from our set of\n// candidate sessions. Candidate sessions with a differing policy from the\n// active client's advertised policy will be ignored, but may be resumed if the\n// client is restarted with a matching policy. If no candidates were found, nil\n// is returned to signal that we need to request a new policy.",
      "length": 1282,
      "tokens": 174,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) nextSessionQueue() (*sessionQueue, error) {",
      "content": "func (c *TowerClient) nextSessionQueue() (*sessionQueue, error) {\n\t// Select any candidate session at random, and remove it from the set of\n\t// candidate sessions.\n\tvar candidateSession *ClientSession\n\tfor id, sessionInfo := range c.candidateSessions {\n\t\tdelete(c.candidateSessions, id)\n\n\t\t// Skip any sessions with policies that don't match the current\n\t\t// TxPolicy, as they would result in different justice\n\t\t// transactions from what is requested. These can be used again\n\t\t// if the client changes their configuration and restarting.\n\t\tif sessionInfo.Policy.TxPolicy != c.cfg.Policy.TxPolicy {\n\t\t\tcontinue\n\t\t}\n\n\t\tcandidateSession = sessionInfo\n\t\tbreak\n\t}\n\n\t// If none of the sessions could be used or none were found, we'll\n\t// return nil to signal that we need another session to be negotiated.\n\tif candidateSession == nil {\n\t\treturn nil, nil\n\t}\n\n\tupdates, err := c.cfg.DB.FetchSessionCommittedUpdates(\n\t\t&candidateSession.ID,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Initialize the session queue and spin it up, so it can begin handling\n\t// updates. If the queue was already made active on startup, this will\n\t// simply return the existing session queue from the set.\n\treturn c.getOrInitActiveQueue(candidateSession, updates), nil\n}\n\n// backupDispatcher processes events coming from the taskPipeline and is\n// responsible for detecting when the client needs to renegotiate a session to\n// fulfill continuing demand. The event loop exits after all tasks have been\n// received from the upstream taskPipeline, or the taskPipeline is force quit.\n//\n// NOTE: This method MUST be run as a goroutine.",
      "length": 1498,
      "tokens": 233,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) backupDispatcher() {",
      "content": "func (c *TowerClient) backupDispatcher() {\n\tdefer c.wg.Done()\n\n\tc.log.Tracef(\"Starting backup dispatcher\")\n\tdefer c.log.Tracef(\"Stopping backup dispatcher\")\n\n\tfor {\n\t\tswitch {\n\n\t\t// No active session queue and no additional sessions.\n\t\tcase c.sessionQueue == nil && len(c.candidateSessions) == 0:\n\t\t\tc.log.Infof(\"Requesting new session.\")\n\n\t\t\t// Immediately request a new session.\n\t\t\tc.negotiator.RequestSession()\n\n\t\t\t// Wait until we receive the newly negotiated session.\n\t\t\t// All backups sent in the meantime are queued in the\n\t\t\t// revoke queue, as we cannot process them.\n\t\tawaitSession:\n\t\t\tselect {\n\t\t\tcase session := <-c.negotiator.NewSessions():\n\t\t\t\tc.log.Infof(\"Acquired new session with id=%s\",\n\t\t\t\t\tsession.ID)\n\t\t\t\tc.candidateSessions[session.ID] = session\n\t\t\t\tc.stats.sessionAcquired()\n\n\t\t\t\t// We'll continue to choose the newly negotiated\n\t\t\t\t// session as our active session queue.\n\t\t\t\tcontinue\n\n\t\t\tcase <-c.statTicker.C:\n\t\t\t\tc.log.Infof(\"Client stats: %s\", c.stats)\n\n\t\t\t// A new tower has been requested to be added. We'll\n\t\t\t// update our persisted and in-memory state and consider\n\t\t\t// its corresponding sessions, if any, as new\n\t\t\t// candidates.\n\t\t\tcase msg := <-c.newTowers:\n\t\t\t\tmsg.errChan <- c.handleNewTower(msg)\n\n\t\t\t// A tower has been requested to be removed. We'll\n\t\t\t// only allow removal of it if the address in question\n\t\t\t// is not currently being used for session negotiation.\n\t\t\tcase msg := <-c.staleTowers:\n\t\t\t\tmsg.errChan <- c.handleStaleTower(msg)\n\n\t\t\tcase <-c.forceQuit:\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Instead of looping, we'll jump back into the select\n\t\t\t// case and await the delivery of the session to prevent\n\t\t\t// us from re-requesting additional sessions.\n\t\t\tgoto awaitSession\n\n\t\t// No active session queue but have additional sessions.\n\t\tcase c.sessionQueue == nil && len(c.candidateSessions) > 0:\n\t\t\t// We've exhausted the prior session, we'll pop another\n\t\t\t// from the remaining sessions and continue processing\n\t\t\t// backup tasks.\n\t\t\tvar err error\n\t\t\tc.sessionQueue, err = c.nextSessionQueue()\n\t\t\tif err != nil {\n\t\t\t\tc.log.Errorf(\"error fetching next session \"+\n\t\t\t\t\t\"queue: %v\", err)\n\t\t\t}\n\n\t\t\tif c.sessionQueue != nil {\n\t\t\t\tc.log.Debugf(\"Loaded next candidate session \"+\n\t\t\t\t\t\"queue id=%s\", c.sessionQueue.ID())\n\t\t\t}\n\n\t\t// Have active session queue, process backups.\n\t\tcase c.sessionQueue != nil:\n\t\t\tif c.prevTask != nil {\n\t\t\t\tc.processTask(c.prevTask)\n\n\t\t\t\t// Continue to ensure the sessionQueue is\n\t\t\t\t// properly initialized before attempting to\n\t\t\t\t// process more tasks from the pipeline.\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Normal operation where new tasks are read from the\n\t\t\t// pipeline.\n\t\t\tselect {\n\n\t\t\t// If any sessions are negotiated while we have an\n\t\t\t// active session queue, queue them for future use.\n\t\t\t// This shouldn't happen with the current design, so\n\t\t\t// it doesn't hurt to select here just in case. In the\n\t\t\t// future, we will likely allow more asynchrony so that\n\t\t\t// we can request new sessions before the session is\n\t\t\t// fully empty, which this case would handle.\n\t\t\tcase session := <-c.negotiator.NewSessions():\n\t\t\t\tc.log.Warnf(\"Acquired new session with id=%s \"+\n\t\t\t\t\t\"while processing tasks\", session.ID)\n\t\t\t\tc.candidateSessions[session.ID] = session\n\t\t\t\tc.stats.sessionAcquired()\n\n\t\t\tcase <-c.statTicker.C:\n\t\t\t\tc.log.Infof(\"Client stats: %s\", c.stats)\n\n\t\t\t// Process each backup task serially from the queue of\n\t\t\t// revoked states.\n\t\t\tcase task, ok := <-c.pipeline.NewBackupTasks():\n\t\t\t\t// All backups in the pipeline have been\n\t\t\t\t// processed, it is now safe to exit.\n\t\t\t\tif !ok {\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tc.log.Debugf(\"Processing %v\", task.id)\n\n\t\t\t\tc.stats.taskReceived()\n\t\t\t\tc.processTask(task)\n\n\t\t\t// A new tower has been requested to be added. We'll\n\t\t\t// update our persisted and in-memory state and consider\n\t\t\t// its corresponding sessions, if any, as new\n\t\t\t// candidates.\n\t\t\tcase msg := <-c.newTowers:\n\t\t\t\tmsg.errChan <- c.handleNewTower(msg)\n\n\t\t\t// A tower has been removed, so we'll remove certain\n\t\t\t// information that's persisted and also in our\n\t\t\t// in-memory state depending on the request, and set any\n\t\t\t// of its corresponding candidate sessions as inactive.\n\t\t\tcase msg := <-c.staleTowers:\n\t\t\t\tmsg.errChan <- c.handleStaleTower(msg)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// processTask attempts to schedule the given backupTask on the active\n// sessionQueue. The task will either be accepted or rejected, after which the\n// appropriate modifications to the client's state machine will be made. After\n// every invocation of processTask, the caller should ensure that the\n// sessionQueue hasn't been exhausted before proceeding to the next task. Tasks\n// that are rejected because the active sessionQueue is full will be cached as\n// the prevTask, and should be reprocessed after obtaining a new sessionQueue.",
      "length": 4589,
      "tokens": 644,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) processTask(task *backupTask) {",
      "content": "func (c *TowerClient) processTask(task *backupTask) {\n\tstatus, accepted := c.sessionQueue.AcceptTask(task)\n\tif accepted {\n\t\tc.taskAccepted(task, status)\n\t} else {\n\t\tc.taskRejected(task, status)\n\t}\n}\n\n// taskAccepted processes the acceptance of a task by a sessionQueue depending\n// on the state the sessionQueue is in *after* the task is added. The client's\n// prevTask is always removed as a result of this call. The client's\n// sessionQueue will be removed if accepting the task left the sessionQueue in\n// an exhausted state.",
      "length": 462,
      "tokens": 73,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) taskAccepted(task *backupTask, newStatus reserveStatus) {",
      "content": "func (c *TowerClient) taskAccepted(task *backupTask, newStatus reserveStatus) {\n\tc.log.Infof(\"Queued %v successfully for session %v\",\n\t\ttask.id, c.sessionQueue.ID())\n\n\tc.stats.taskAccepted()\n\n\t// If this task was accepted, we discard anything held in the prevTask.\n\t// Either it was nil before, or is the task which was just accepted.\n\tc.prevTask = nil\n\n\tswitch newStatus {\n\n\t// The sessionQueue still has capacity after accepting this task.\n\tcase reserveAvailable:\n\n\t// The sessionQueue is full after accepting this task, so we will need\n\t// to request a new one before proceeding.\n\tcase reserveExhausted:\n\t\tc.stats.sessionExhausted()\n\n\t\tc.log.Debugf(\"Session %s exhausted\", c.sessionQueue.ID())\n\n\t\t// This task left the session exhausted, set it to nil and\n\t\t// proceed to the next loop, so we can consume another\n\t\t// pre-negotiated session or request another.\n\t\tc.sessionQueue = nil\n\t}\n}\n\n// taskRejected process the rejection of a task by a sessionQueue depending on\n// the state the was in *before* the task was rejected. The client's prevTask\n// will cache the task if the sessionQueue was exhausted beforehand, and nil\n// the sessionQueue to find a new session. If the sessionQueue was not\n// exhausted, the client marks the task as ineligible, as this implies we\n// couldn't construct a valid justice transaction given the session's policy.",
      "length": 1236,
      "tokens": 193,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) taskRejected(task *backupTask, curStatus reserveStatus) {",
      "content": "func (c *TowerClient) taskRejected(task *backupTask, curStatus reserveStatus) {\n\tswitch curStatus {\n\n\t// The sessionQueue has available capacity but the task was rejected,\n\t// this indicates that the task was ineligible for backup.\n\tcase reserveAvailable:\n\t\tc.stats.taskIneligible()\n\n\t\tc.log.Infof(\"Ignoring ineligible %v\", task.id)\n\n\t\terr := c.cfg.DB.MarkBackupIneligible(\n\t\t\ttask.id.ChanID, task.id.CommitHeight,\n\t\t)\n\t\tif err != nil {\n\t\t\tc.log.Errorf(\"Unable to mark %v ineligible: %v\",\n\t\t\t\ttask.id, err)\n\n\t\t\t// It is safe to not handle this error, even if we could\n\t\t\t// not persist the result. At worst, this task may be\n\t\t\t// reprocessed on a subsequent start up, and will either\n\t\t\t// succeed do a change in session parameters or fail in\n\t\t\t// the same manner.\n\t\t}\n\n\t\t// If this task was rejected *and* the session had available\n\t\t// capacity, we discard anything held in the prevTask. Either it\n\t\t// was nil before, or is the task which was just rejected.\n\t\tc.prevTask = nil\n\n\t// The sessionQueue rejected the task because it is full, we will stash\n\t// this task and try to add it to the next available sessionQueue.\n\tcase reserveExhausted:\n\t\tc.stats.sessionExhausted()\n\n\t\tc.log.Debugf(\"Session %v exhausted, %v queued for next session\",\n\t\t\tc.sessionQueue.ID(), task.id)\n\n\t\t// Cache the task that we pulled off, so that we can process it\n\t\t// once a new session queue is available.\n\t\tc.sessionQueue = nil\n\t\tc.prevTask = task\n\t}\n}\n\n// dial connects the peer at addr using privKey as our secret key for the\n// connection. The connection will use the configured Net's resolver to resolve\n// the address for either Tor or clear net connections.",
      "length": 1522,
      "tokens": 243,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) dial(localKey keychain.SingleKeyECDH,",
      "content": "func (c *TowerClient) dial(localKey keychain.SingleKeyECDH,\n\taddr *lnwire.NetAddress) (wtserver.Peer, error) {\n\n\treturn c.cfg.AuthDial(localKey, addr, c.cfg.Dial)\n}\n\n// readMessage receives and parses the next message from the given Peer. An\n// error is returned if a message is not received before the server's read\n// timeout, the read off the wire failed, or the message could not be\n// deserialized.",
      "length": 335,
      "tokens": 53,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) readMessage(peer wtserver.Peer) (wtwire.Message, error) {",
      "content": "func (c *TowerClient) readMessage(peer wtserver.Peer) (wtwire.Message, error) {\n\t// Set a read timeout to ensure we drop the connection if nothing is\n\t// received in a timely manner.\n\terr := peer.SetReadDeadline(time.Now().Add(c.cfg.ReadTimeout))\n\tif err != nil {\n\t\terr = fmt.Errorf(\"unable to set read deadline: %v\", err)\n\t\tc.log.Errorf(\"Unable to read msg: %v\", err)\n\t\treturn nil, err\n\t}\n\n\t// Pull the next message off the wire,\n\trawMsg, err := peer.ReadNextMessage()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"unable to read message: %v\", err)\n\t\tc.log.Errorf(\"Unable to read msg: %v\", err)\n\t\treturn nil, err\n\t}\n\n\t// Parse the received message according to the watchtower wire\n\t// specification.\n\tmsgReader := bytes.NewReader(rawMsg)\n\tmsg, err := wtwire.ReadMessage(msgReader, 0)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"unable to parse message: %v\", err)\n\t\tc.log.Errorf(\"Unable to read msg: %v\", err)\n\t\treturn nil, err\n\t}\n\n\tc.logMessage(peer, msg, true)\n\n\treturn msg, nil\n}\n\n// sendMessage sends a watchtower wire message to the target peer.",
      "length": 922,
      "tokens": 143,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) sendMessage(peer wtserver.Peer, msg wtwire.Message) error {",
      "content": "func (c *TowerClient) sendMessage(peer wtserver.Peer, msg wtwire.Message) error {\n\t// Encode the next wire message into the buffer.\n\t// TODO(conner): use buffer pool\n\tvar b bytes.Buffer\n\t_, err := wtwire.WriteMessage(&b, msg, 0)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"Unable to encode msg: %v\", err)\n\t\tc.log.Errorf(\"Unable to send msg: %v\", err)\n\t\treturn err\n\t}\n\n\t// Set the write deadline for the connection, ensuring we drop the\n\t// connection if nothing is sent in a timely manner.\n\terr = peer.SetWriteDeadline(time.Now().Add(c.cfg.WriteTimeout))\n\tif err != nil {\n\t\terr = fmt.Errorf(\"unable to set write deadline: %v\", err)\n\t\tc.log.Errorf(\"Unable to send msg: %v\", err)\n\t\treturn err\n\t}\n\n\tc.logMessage(peer, msg, false)\n\n\t// Write out the full message to the remote peer.\n\t_, err = peer.Write(b.Bytes())\n\tif err != nil {\n\t\tc.log.Errorf(\"Unable to send msg: %v\", err)\n\t}\n\treturn err\n}\n\n// newSessionQueue creates a sessionQueue from a ClientSession loaded from the\n// database and supplying it with the resources needed by the client.",
      "length": 921,
      "tokens": 148,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) newSessionQueue(s *ClientSession,",
      "content": "func (c *TowerClient) newSessionQueue(s *ClientSession,\n\tupdates []wtdb.CommittedUpdate) *sessionQueue {\n\n\treturn newSessionQueue(&sessionQueueConfig{\n\t\tClientSession: s,\n\t\tChainHash:     c.cfg.ChainHash,\n\t\tDial:          c.dial,\n\t\tReadMessage:   c.readMessage,\n\t\tSendMessage:   c.sendMessage,\n\t\tSigner:        c.cfg.Signer,\n\t\tDB:            c.cfg.DB,\n\t\tMinBackoff:    c.cfg.MinBackoff,\n\t\tMaxBackoff:    c.cfg.MaxBackoff,\n\t\tLog:           c.log,\n\t}, updates)\n}\n\n// getOrInitActiveQueue checks the activeSessions set for a sessionQueue for the\n// passed ClientSession. If it exists, the active sessionQueue is returned.\n// Otherwise, a new sessionQueue is initialized and added to the set.",
      "length": 614,
      "tokens": 63,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) getOrInitActiveQueue(s *ClientSession,",
      "content": "func (c *TowerClient) getOrInitActiveQueue(s *ClientSession,\n\tupdates []wtdb.CommittedUpdate) *sessionQueue {\n\n\tif sq, ok := c.activeSessions[s.ID]; ok {\n\t\treturn sq\n\t}\n\n\treturn c.initActiveQueue(s, updates)\n}\n\n// initActiveQueue creates a new sessionQueue from the passed ClientSession,\n// adds the sessionQueue to the activeSessions set, and starts the sessionQueue\n// so that it can deliver any committed updates or begin accepting newly\n// assigned tasks.",
      "length": 386,
      "tokens": 56,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) initActiveQueue(s *ClientSession,",
      "content": "func (c *TowerClient) initActiveQueue(s *ClientSession,\n\tupdates []wtdb.CommittedUpdate) *sessionQueue {\n\n\t// Initialize the session queue, providing it with all the resources it\n\t// requires from the client instance.\n\tsq := c.newSessionQueue(s, updates)\n\n\t// Add the session queue as an active session so that we remember to\n\t// stop it on shutdown.\n\tc.activeSessions.Add(sq)\n\n\t// Start the queue so that it can be active in processing newly assigned\n\t// tasks or to upload previously committed updates.\n\tsq.Start()\n\n\treturn sq\n}\n\n// AddTower adds a new watchtower reachable at the given address and considers\n// it for new sessions. If the watchtower already exists, then any new addresses\n// included will be considered when dialing it for session negotiations and\n// backups.",
      "length": 703,
      "tokens": 113,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) AddTower(addr *lnwire.NetAddress) error {",
      "content": "func (c *TowerClient) AddTower(addr *lnwire.NetAddress) error {\n\terrChan := make(chan error, 1)\n\n\tselect {\n\tcase c.newTowers <- &newTowerMsg{\n\t\taddr:    addr,\n\t\terrChan: errChan,\n\t}:\n\tcase <-c.pipeline.quit:\n\t\treturn ErrClientExiting\n\tcase <-c.pipeline.forceQuit:\n\t\treturn ErrClientExiting\n\t}\n\n\tselect {\n\tcase err := <-errChan:\n\t\treturn err\n\tcase <-c.pipeline.quit:\n\t\treturn ErrClientExiting\n\tcase <-c.pipeline.forceQuit:\n\t\treturn ErrClientExiting\n\t}\n}\n\n// handleNewTower handles a request for a new tower to be added. If the tower\n// already exists, then its corresponding sessions, if any, will be set\n// considered as candidates.",
      "length": 543,
      "tokens": 74,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) handleNewTower(msg *newTowerMsg) error {",
      "content": "func (c *TowerClient) handleNewTower(msg *newTowerMsg) error {\n\t// We'll start by updating our persisted state, followed by our\n\t// in-memory state, with the new tower. This might not actually be a new\n\t// tower, but it might include a new address at which it can be reached.\n\tdbTower, err := c.cfg.DB.CreateTower(msg.addr)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\ttower, err := NewTowerFromDBTower(dbTower)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tc.candidateTowers.AddCandidate(tower)\n\n\t// Include all of its corresponding sessions to our set of candidates.\n\tisAnchorClient := c.cfg.Policy.IsAnchorChannel()\n\tactiveSessionFilter := genActiveSessionFilter(isAnchorClient)\n\tsessions, err := getClientSessions(\n\t\tc.cfg.DB, c.cfg.SecretKeyRing, &tower.ID, activeSessionFilter,\n\t)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to determine sessions for tower %x: \"+\n\t\t\t\"%v\", tower.IdentityKey.SerializeCompressed(), err)\n\t}\n\tfor id, session := range sessions {\n\t\tc.candidateSessions[id] = session\n\t}\n\n\treturn nil\n}\n\n// RemoveTower removes a watchtower from being considered for future session\n// negotiations and from being used for any subsequent backups until it's added\n// again. If an address is provided, then this call only serves as a way of\n// removing the address from the watchtower instead.",
      "length": 1187,
      "tokens": 172,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) RemoveTower(pubKey *btcec.PublicKey, addr net.Addr) error {",
      "content": "func (c *TowerClient) RemoveTower(pubKey *btcec.PublicKey, addr net.Addr) error {\n\terrChan := make(chan error, 1)\n\n\tselect {\n\tcase c.staleTowers <- &staleTowerMsg{\n\t\tpubKey:  pubKey,\n\t\taddr:    addr,\n\t\terrChan: errChan,\n\t}:\n\tcase <-c.pipeline.quit:\n\t\treturn ErrClientExiting\n\tcase <-c.pipeline.forceQuit:\n\t\treturn ErrClientExiting\n\t}\n\n\tselect {\n\tcase err := <-errChan:\n\t\treturn err\n\tcase <-c.pipeline.quit:\n\t\treturn ErrClientExiting\n\tcase <-c.pipeline.forceQuit:\n\t\treturn ErrClientExiting\n\t}\n}\n\n// handleNewTower handles a request for an existing tower to be removed. If none\n// of the tower's sessions have pending updates, then they will become inactive\n// and removed as candidates. If the active session queue corresponds to any of\n// these sessions, a new one will be negotiated.",
      "length": 675,
      "tokens": 95,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) handleStaleTower(msg *staleTowerMsg) error {",
      "content": "func (c *TowerClient) handleStaleTower(msg *staleTowerMsg) error {\n\t// We'll load the tower before potentially removing it in order to\n\t// retrieve its ID within the database.\n\tdbTower, err := c.cfg.DB.LoadTower(msg.pubKey)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// We'll first update our in-memory state followed by our persisted\n\t// state, with the stale tower. The removal of the tower address from\n\t// the in-memory state will fail if the address is currently being used\n\t// for a session negotiation.\n\terr = c.candidateTowers.RemoveCandidate(dbTower.ID, msg.addr)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif err := c.cfg.DB.RemoveTower(msg.pubKey, msg.addr); err != nil {\n\t\t// If the persisted state update fails, re-add the address to\n\t\t// our in-memory state.\n\t\ttower, newTowerErr := NewTowerFromDBTower(dbTower)\n\t\tif newTowerErr != nil {\n\t\t\tlog.Errorf(\"could not create new in-memory tower: %v\",\n\t\t\t\tnewTowerErr)\n\t\t} else {\n\t\t\tc.candidateTowers.AddCandidate(tower)\n\t\t}\n\n\t\treturn err\n\t}\n\n\t// If an address was provided, then we're only meant to remove the\n\t// address from the tower, so there's nothing left for us to do.\n\tif msg.addr != nil {\n\t\treturn nil\n\t}\n\n\t// Otherwise, the tower should no longer be used for future session\n\t// negotiations and backups.\n\tpubKey := msg.pubKey.SerializeCompressed()\n\tsessions, err := c.cfg.DB.ListClientSessions(&dbTower.ID)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to retrieve sessions for tower %x: \"+\n\t\t\t\"%v\", pubKey, err)\n\t}\n\tfor sessionID := range sessions {\n\t\tdelete(c.candidateSessions, sessionID)\n\t}\n\n\t// If our active session queue corresponds to the stale tower, we'll\n\t// proceed to negotiate a new one.\n\tif c.sessionQueue != nil {\n\t\tactiveTower := c.sessionQueue.tower.IdentityKey.SerializeCompressed()\n\t\tif bytes.Equal(pubKey, activeTower) {\n\t\t\tc.sessionQueue = nil\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// RegisteredTowers retrieves the list of watchtowers registered with the\n// client.",
      "length": 1797,
      "tokens": 269,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) RegisteredTowers(opts ...wtdb.ClientSessionListOption) (",
      "content": "func (c *TowerClient) RegisteredTowers(opts ...wtdb.ClientSessionListOption) (\n\t[]*RegisteredTower, error) {\n\n\t// Retrieve all of our towers along with all of our sessions.\n\ttowers, err := c.cfg.DB.ListTowers()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tclientSessions, err := c.cfg.DB.ListClientSessions(nil, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Construct a lookup map that coalesces all of the sessions for a\n\t// specific watchtower.\n\ttowerSessions := make(\n\t\tmap[wtdb.TowerID]map[wtdb.SessionID]*wtdb.ClientSession,\n\t)\n\tfor id, s := range clientSessions {\n\t\tsessions, ok := towerSessions[s.TowerID]\n\t\tif !ok {\n\t\t\tsessions = make(map[wtdb.SessionID]*wtdb.ClientSession)\n\t\t\ttowerSessions[s.TowerID] = sessions\n\t\t}\n\t\tsessions[id] = s\n\t}\n\n\tregisteredTowers := make([]*RegisteredTower, 0, len(towerSessions))\n\tfor _, tower := range towers {\n\t\tisActive := c.candidateTowers.IsActive(tower.ID)\n\t\tregisteredTowers = append(registeredTowers, &RegisteredTower{\n\t\t\tTower:                  tower,\n\t\t\tSessions:               towerSessions[tower.ID],\n\t\t\tActiveSessionCandidate: isActive,\n\t\t})\n\t}\n\n\treturn registeredTowers, nil\n}\n\n// LookupTower retrieves a registered watchtower through its public key.",
      "length": 1084,
      "tokens": 129,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) LookupTower(pubKey *btcec.PublicKey,",
      "content": "func (c *TowerClient) LookupTower(pubKey *btcec.PublicKey,\n\topts ...wtdb.ClientSessionListOption) (*RegisteredTower, error) {\n\n\ttower, err := c.cfg.DB.LoadTower(pubKey)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ttowerSessions, err := c.cfg.DB.ListClientSessions(&tower.ID, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &RegisteredTower{\n\t\tTower:                  tower,\n\t\tSessions:               towerSessions,\n\t\tActiveSessionCandidate: c.candidateTowers.IsActive(tower.ID),\n\t}, nil\n}\n\n// Stats returns the in-memory statistics of the client since startup.",
      "length": 485,
      "tokens": 54,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) Stats() ClientStats {",
      "content": "func (c *TowerClient) Stats() ClientStats {\n\treturn c.stats.Copy()\n}\n\n// Policy returns the active client policy configuration.",
      "length": 80,
      "tokens": 11,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) Policy() wtpolicy.Policy {",
      "content": "func (c *TowerClient) Policy() wtpolicy.Policy {\n\treturn c.cfg.Policy\n}\n\n// logMessage writes information about a message received from a remote peer,\n// using directional prepositions to signal whether the message was sent or\n// received.",
      "length": 185,
      "tokens": 29,
      "embedding": []
    },
    {
      "slug": "func (c *TowerClient) logMessage(",
      "content": "func (c *TowerClient) logMessage(\n\tpeer wtserver.Peer, msg wtwire.Message, read bool) {\n\n\tvar action = \"Received\"\n\tvar preposition = \"from\"\n\tif !read {\n\t\taction = \"Sending\"\n\t\tpreposition = \"to\"\n\t}\n\n\tsummary := wtwire.MessageSummary(msg)\n\tif len(summary) > 0 {\n\t\tsummary = \"(\" + summary + \")\"\n\t}\n\n\tc.log.Debugf(\"%s %s%v %s %x@%s\", action, msg.MsgType(), summary,\n\t\tpreposition, peer.RemotePub().SerializeCompressed(),\n\t\tpeer.RemoteAddr())\n}\n",
      "length": 388,
      "tokens": 52,
      "embedding": []
    }
  ]
}