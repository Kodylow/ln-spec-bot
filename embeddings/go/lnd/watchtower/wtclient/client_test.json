{
  "filepath": "../implementations/go/lnd/watchtower/wtclient/client_test.go",
  "package": "wtclient_test",
  "sections": [
    {
      "slug": "func randPrivKey(t *testing.T) *btcec.PrivateKey {",
      "content": "func randPrivKey(t *testing.T) *btcec.PrivateKey {\n\tt.Helper()\n\n\tsk, err := btcec.NewPrivateKey()\n\trequire.NoError(t, err, \"unable to generate pubkey\")\n\n\treturn sk\n}\n",
      "length": 108,
      "tokens": 14,
      "embedding": []
    },
    {
      "slug": "type mockNet struct {",
      "content": "type mockNet struct {\n\tmu            sync.RWMutex\n\tconnCallbacks map[string]func(wtserver.Peer)\n}\n",
      "length": 73,
      "tokens": 5,
      "embedding": []
    },
    {
      "slug": "func newMockNet() *mockNet {",
      "content": "func newMockNet() *mockNet {\n\treturn &mockNet{\n\t\tconnCallbacks: make(map[string]func(peer wtserver.Peer)),\n\t}\n}\n",
      "length": 79,
      "tokens": 7,
      "embedding": []
    },
    {
      "slug": "func (m *mockNet) Dial(_, _ string, _ time.Duration) (net.Conn, error) {",
      "content": "func (m *mockNet) Dial(_, _ string, _ time.Duration) (net.Conn, error) {\n\treturn nil, nil\n}\n",
      "length": 17,
      "tokens": 4,
      "embedding": []
    },
    {
      "slug": "func (m *mockNet) LookupHost(_ string) ([]string, error) {",
      "content": "func (m *mockNet) LookupHost(_ string) ([]string, error) {\n\tpanic(\"not implemented\")\n}\n",
      "length": 26,
      "tokens": 3,
      "embedding": []
    },
    {
      "slug": "func (m *mockNet) LookupSRV(_, _, _ string) (string, []*net.SRV, error) {",
      "content": "func (m *mockNet) LookupSRV(_, _, _ string) (string, []*net.SRV, error) {\n\tpanic(\"not implemented\")\n}\n",
      "length": 26,
      "tokens": 3,
      "embedding": []
    },
    {
      "slug": "func (m *mockNet) ResolveTCPAddr(_, _ string) (*net.TCPAddr, error) {",
      "content": "func (m *mockNet) ResolveTCPAddr(_, _ string) (*net.TCPAddr, error) {\n\tpanic(\"not implemented\")\n}\n",
      "length": 26,
      "tokens": 3,
      "embedding": []
    },
    {
      "slug": "func (m *mockNet) AuthDial(local keychain.SingleKeyECDH,",
      "content": "func (m *mockNet) AuthDial(local keychain.SingleKeyECDH,\n\tnetAddr *lnwire.NetAddress, _ tor.DialFunc) (wtserver.Peer, error) {\n\n\tlocalPk := local.PubKey()\n\tlocalAddr := &net.TCPAddr{\n\t\tIP:   net.IP{0x32, 0x31, 0x30, 0x29},\n\t\tPort: 36723,\n\t}\n\n\tlocalPeer, remotePeer := wtmock.NewMockConn(\n\t\tlocalPk, netAddr.IdentityKey, localAddr, netAddr.Address, 0,\n\t)\n\n\tm.mu.RLock()\n\tdefer m.mu.RUnlock()\n\tcb, ok := m.connCallbacks[netAddr.String()]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"no callback registered for this peer\")\n\t}\n\n\tcb(remotePeer)\n\n\treturn localPeer, nil\n}\n",
      "length": 477,
      "tokens": 55,
      "embedding": []
    },
    {
      "slug": "func (m *mockNet) registerConnCallback(netAddr *lnwire.NetAddress,",
      "content": "func (m *mockNet) registerConnCallback(netAddr *lnwire.NetAddress,\n\tcb func(wtserver.Peer)) {\n\n\tm.mu.Lock()\n\tdefer m.mu.Unlock()\n\n\tm.connCallbacks[netAddr.String()] = cb\n}\n",
      "length": 98,
      "tokens": 10,
      "embedding": []
    },
    {
      "slug": "func (m *mockNet) removeConnCallback(netAddr *lnwire.NetAddress) {",
      "content": "func (m *mockNet) removeConnCallback(netAddr *lnwire.NetAddress) {\n\tm.mu.Lock()\n\tdefer m.mu.Unlock()\n\n\tdelete(m.connCallbacks, netAddr.String())\n}\n",
      "length": 75,
      "tokens": 6,
      "embedding": []
    },
    {
      "slug": "type mockChannel struct {",
      "content": "type mockChannel struct {\n\tmu            sync.Mutex\n\tcommitHeight  uint64\n\tretributions  map[uint64]*lnwallet.BreachRetribution\n\tlocalBalance  lnwire.MilliSatoshi\n\tremoteBalance lnwire.MilliSatoshi\n\n\trevSK     *btcec.PrivateKey\n\trevPK     *btcec.PublicKey\n\trevKeyLoc keychain.KeyLocator\n\n\ttoRemoteSK     *btcec.PrivateKey\n\ttoRemotePK     *btcec.PublicKey\n\ttoRemoteKeyLoc keychain.KeyLocator\n\n\ttoLocalPK *btcec.PublicKey // only need to generate to-local script\n\n\tdustLimit lnwire.MilliSatoshi\n\tcsvDelay  uint32\n}\n",
      "length": 468,
      "tokens": 36,
      "embedding": []
    },
    {
      "slug": "func newMockChannel(t *testing.T, signer *wtmock.MockSigner,",
      "content": "func newMockChannel(t *testing.T, signer *wtmock.MockSigner,\n\tlocalAmt, remoteAmt lnwire.MilliSatoshi) *mockChannel {\n\n\t// Generate the revocation, to-local, and to-remote keypairs.\n\trevSK := randPrivKey(t)\n\trevPK := revSK.PubKey()\n\n\ttoLocalSK := randPrivKey(t)\n\ttoLocalPK := toLocalSK.PubKey()\n\n\ttoRemoteSK := randPrivKey(t)\n\ttoRemotePK := toRemoteSK.PubKey()\n\n\t// Register the revocation secret key and the to-remote secret key with\n\t// the signer. We will not need to sign with the to-local key, as this\n\t// is to be known only by the counterparty.\n\trevKeyLoc := signer.AddPrivKey(revSK)\n\ttoRemoteKeyLoc := signer.AddPrivKey(toRemoteSK)\n\n\tc := &mockChannel{\n\t\tretributions:   make(map[uint64]*lnwallet.BreachRetribution),\n\t\tlocalBalance:   localAmt,\n\t\tremoteBalance:  remoteAmt,\n\t\trevSK:          revSK,\n\t\trevPK:          revPK,\n\t\trevKeyLoc:      revKeyLoc,\n\t\ttoLocalPK:      toLocalPK,\n\t\ttoRemoteSK:     toRemoteSK,\n\t\ttoRemotePK:     toRemotePK,\n\t\ttoRemoteKeyLoc: toRemoteKeyLoc,\n\t\tdustLimit:      546000,\n\t\tcsvDelay:       144,\n\t}\n\n\t// Create the initial remote commitment with the initial balances.\n\tc.createRemoteCommitTx(t)\n\n\treturn c\n}\n",
      "length": 1046,
      "tokens": 115,
      "embedding": []
    },
    {
      "slug": "func (c *mockChannel) createRemoteCommitTx(t *testing.T) {",
      "content": "func (c *mockChannel) createRemoteCommitTx(t *testing.T) {\n\tt.Helper()\n\n\t// Construct the to-local witness script.\n\ttoLocalScript, err := input.CommitScriptToSelf(\n\t\tc.csvDelay, c.toLocalPK, c.revPK,\n\t)\n\trequire.NoError(t, err, \"unable to create to-local script\")\n\n\t// Compute the to-local witness script hash.\n\ttoLocalScriptHash, err := input.WitnessScriptHash(toLocalScript)\n\trequire.NoError(t, err, \"unable to create to-local witness script hash\")\n\n\t// Compute the to-remote witness script hash.\n\ttoRemoteScriptHash, err := input.CommitScriptUnencumbered(c.toRemotePK)\n\trequire.NoError(t, err, \"unable to create to-remote script\")\n\n\t// Construct the remote commitment txn, containing the to-local and\n\t// to-remote outputs. The balances are flipped since the transaction is\n\t// from the PoV of the remote party. We don't need any inputs for this\n\t// test. We increment the version with the commit height to ensure that\n\t// all commitment transactions are unique even if the same distribution\n\t// of funds is used more than once.\n\tcommitTxn := &wire.MsgTx{\n\t\tVersion: int32(c.commitHeight + 1),\n\t}\n\n\tvar (\n\t\ttoLocalSignDesc  *input.SignDescriptor\n\t\ttoRemoteSignDesc *input.SignDescriptor\n\t)\n\n\tvar outputIndex int\n\tif c.remoteBalance >= c.dustLimit {\n\t\tcommitTxn.TxOut = append(commitTxn.TxOut, &wire.TxOut{\n\t\t\tValue:    int64(c.remoteBalance.ToSatoshis()),\n\t\t\tPkScript: toLocalScriptHash,\n\t\t})\n\n\t\t// Create the sign descriptor used to sign for the to-local\n\t\t// input.\n\t\ttoLocalSignDesc = &input.SignDescriptor{\n\t\t\tKeyDesc: keychain.KeyDescriptor{\n\t\t\t\tKeyLocator: c.revKeyLoc,\n\t\t\t\tPubKey:     c.revPK,\n\t\t\t},\n\t\t\tWitnessScript: toLocalScript,\n\t\t\tOutput:        commitTxn.TxOut[outputIndex],\n\t\t\tHashType:      txscript.SigHashAll,\n\t\t}\n\t\toutputIndex++\n\t}\n\tif c.localBalance >= c.dustLimit {\n\t\tcommitTxn.TxOut = append(commitTxn.TxOut, &wire.TxOut{\n\t\t\tValue:    int64(c.localBalance.ToSatoshis()),\n\t\t\tPkScript: toRemoteScriptHash,\n\t\t})\n\n\t\t// Create the sign descriptor used to sign for the to-remote\n\t\t// input.\n\t\ttoRemoteSignDesc = &input.SignDescriptor{\n\t\t\tKeyDesc: keychain.KeyDescriptor{\n\t\t\t\tKeyLocator: c.toRemoteKeyLoc,\n\t\t\t\tPubKey:     c.toRemotePK,\n\t\t\t},\n\t\t\tWitnessScript: toRemoteScriptHash,\n\t\t\tOutput:        commitTxn.TxOut[outputIndex],\n\t\t\tHashType:      txscript.SigHashAll,\n\t\t}\n\t\toutputIndex++\n\t}\n\n\ttxid := commitTxn.TxHash()\n\n\tvar (\n\t\ttoLocalOutPoint  wire.OutPoint\n\t\ttoRemoteOutPoint wire.OutPoint\n\t)\n\n\toutputIndex = 0\n\tif toLocalSignDesc != nil {\n\t\ttoLocalOutPoint = wire.OutPoint{\n\t\t\tHash:  txid,\n\t\t\tIndex: uint32(outputIndex),\n\t\t}\n\t\toutputIndex++\n\t}\n\tif toRemoteSignDesc != nil {\n\t\ttoRemoteOutPoint = wire.OutPoint{\n\t\t\tHash:  txid,\n\t\t\tIndex: uint32(outputIndex),\n\t\t}\n\t\toutputIndex++\n\t}\n\n\tcommitKeyRing := &lnwallet.CommitmentKeyRing{\n\t\tRevocationKey: c.revPK,\n\t\tToRemoteKey:   c.toLocalPK,\n\t\tToLocalKey:    c.toRemotePK,\n\t}\n\n\tretribution := &lnwallet.BreachRetribution{\n\t\tBreachTxHash:         commitTxn.TxHash(),\n\t\tRevokedStateNum:      c.commitHeight,\n\t\tKeyRing:              commitKeyRing,\n\t\tRemoteDelay:          c.csvDelay,\n\t\tLocalOutpoint:        toRemoteOutPoint,\n\t\tLocalOutputSignDesc:  toRemoteSignDesc,\n\t\tRemoteOutpoint:       toLocalOutPoint,\n\t\tRemoteOutputSignDesc: toLocalSignDesc,\n\t}\n\n\tc.retributions[c.commitHeight] = retribution\n\tc.commitHeight++\n}\n\n// advanceState creates the next channel state and retribution without altering\n// channel balances.",
      "length": 3206,
      "tokens": 330,
      "embedding": []
    },
    {
      "slug": "func (c *mockChannel) advanceState(t *testing.T) {",
      "content": "func (c *mockChannel) advanceState(t *testing.T) {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tc.createRemoteCommitTx(t)\n}\n\n// sendPayment creates the next channel state and retribution after transferring\n// amt to the remote party.",
      "length": 166,
      "tokens": 22,
      "embedding": []
    },
    {
      "slug": "func (c *mockChannel) sendPayment(t *testing.T, amt lnwire.MilliSatoshi) {",
      "content": "func (c *mockChannel) sendPayment(t *testing.T, amt lnwire.MilliSatoshi) {\n\tt.Helper()\n\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\trequire.GreaterOrEqualf(t, c.localBalance, amt, \"insufficient funds \"+\n\t\t\"to send, need: %v, have: %v\", amt, c.localBalance)\n\n\tc.localBalance -= amt\n\tc.remoteBalance += amt\n\tc.createRemoteCommitTx(t)\n}\n\n// receivePayment creates the next channel state and retribution after\n// transferring amt to the local party.",
      "length": 348,
      "tokens": 43,
      "embedding": []
    },
    {
      "slug": "func (c *mockChannel) receivePayment(t *testing.T, amt lnwire.MilliSatoshi) {",
      "content": "func (c *mockChannel) receivePayment(t *testing.T, amt lnwire.MilliSatoshi) {\n\tt.Helper()\n\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\trequire.GreaterOrEqualf(t, c.remoteBalance, amt, \"insufficient funds \"+\n\t\t\"to recv, need: %v, have: %v\", amt, c.remoteBalance)\n\n\tc.localBalance += amt\n\tc.remoteBalance -= amt\n\tc.createRemoteCommitTx(t)\n}\n\n// getState retrieves the channel's commitment and retribution at state i.",
      "length": 315,
      "tokens": 37,
      "embedding": []
    },
    {
      "slug": "func (c *mockChannel) getState(",
      "content": "func (c *mockChannel) getState(\n\ti uint64) (chainhash.Hash, *lnwallet.BreachRetribution) {\n\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tretribution := c.retributions[i]\n\n\treturn retribution.BreachTxHash, retribution\n}\n",
      "length": 169,
      "tokens": 15,
      "embedding": []
    },
    {
      "slug": "type testHarness struct {",
      "content": "type testHarness struct {\n\tt          *testing.T\n\tcfg        harnessCfg\n\tsigner     *wtmock.MockSigner\n\tcapacity   lnwire.MilliSatoshi\n\tclientDB   *wtmock.ClientDB\n\tclientCfg  *wtclient.Config\n\tclient     wtclient.Client\n\tserverAddr *lnwire.NetAddress\n\tserverDB   *wtmock.TowerDB\n\tserverCfg  *wtserver.Config\n\tserver     *wtserver.Server\n\tnet        *mockNet\n\n\tmu       sync.Mutex\n\tchannels map[lnwire.ChannelID]*mockChannel\n\n\tquit chan struct{}\n}\n",
      "length": 404,
      "tokens": 32,
      "embedding": []
    },
    {
      "slug": "type harnessCfg struct {",
      "content": "type harnessCfg struct {\n\tlocalBalance       lnwire.MilliSatoshi\n\tremoteBalance      lnwire.MilliSatoshi\n\tpolicy             wtpolicy.Policy\n\tnoRegisterChan0    bool\n\tnoAckCreateSession bool\n\tnoServerStart      bool\n}\n",
      "length": 186,
      "tokens": 13,
      "embedding": []
    },
    {
      "slug": "func newHarness(t *testing.T, cfg harnessCfg) *testHarness {",
      "content": "func newHarness(t *testing.T, cfg harnessCfg) *testHarness {\n\ttowerTCPAddr, err := net.ResolveTCPAddr(\"tcp\", towerAddrStr)\n\trequire.NoError(t, err, \"Unable to resolve tower TCP addr\")\n\n\tprivKey, err := btcec.NewPrivateKey()\n\trequire.NoError(t, err, \"Unable to generate tower private key\")\n\tprivKeyECDH := &keychain.PrivKeyECDH{PrivKey: privKey}\n\n\ttowerPubKey := privKey.PubKey()\n\n\ttowerAddr := &lnwire.NetAddress{\n\t\tIdentityKey: towerPubKey,\n\t\tAddress:     towerTCPAddr,\n\t}\n\n\tconst timeout = 200 * time.Millisecond\n\tserverDB := wtmock.NewTowerDB()\n\n\tserverCfg := &wtserver.Config{\n\t\tDB:           serverDB,\n\t\tReadTimeout:  timeout,\n\t\tWriteTimeout: timeout,\n\t\tNodeKeyECDH:  privKeyECDH,\n\t\tNewAddress: func() (btcutil.Address, error) {\n\t\t\treturn addr, nil\n\t\t},\n\t\tNoAckCreateSession: cfg.noAckCreateSession,\n\t}\n\n\tsigner := wtmock.NewMockSigner()\n\tmockNet := newMockNet()\n\tclientDB := wtmock.NewClientDB()\n\n\tclientCfg := &wtclient.Config{\n\t\tSigner:        signer,\n\t\tDial:          mockNet.Dial,\n\t\tDB:            clientDB,\n\t\tAuthDial:      mockNet.AuthDial,\n\t\tSecretKeyRing: wtmock.NewSecretKeyRing(),\n\t\tPolicy:        cfg.policy,\n\t\tNewAddress: func() ([]byte, error) {\n\t\t\treturn addrScript, nil\n\t\t},\n\t\tReadTimeout:    timeout,\n\t\tWriteTimeout:   timeout,\n\t\tMinBackoff:     time.Millisecond,\n\t\tMaxBackoff:     time.Second,\n\t\tForceQuitDelay: 10 * time.Second,\n\t}\n\n\th := &testHarness{\n\t\tt:          t,\n\t\tcfg:        cfg,\n\t\tsigner:     signer,\n\t\tcapacity:   cfg.localBalance + cfg.remoteBalance,\n\t\tclientDB:   clientDB,\n\t\tclientCfg:  clientCfg,\n\t\tserverAddr: towerAddr,\n\t\tserverDB:   serverDB,\n\t\tserverCfg:  serverCfg,\n\t\tnet:        mockNet,\n\t\tchannels:   make(map[lnwire.ChannelID]*mockChannel),\n\t\tquit:       make(chan struct{}),\n\t}\n\tt.Cleanup(func() {\n\t\tclose(h.quit)\n\t})\n\n\tif !cfg.noServerStart {\n\t\th.startServer()\n\t\tt.Cleanup(h.stopServer)\n\t}\n\n\th.startClient()\n\tt.Cleanup(h.client.ForceQuit)\n\n\th.makeChannel(0, h.cfg.localBalance, h.cfg.remoteBalance)\n\tif !cfg.noRegisterChan0 {\n\t\th.registerChannel(0)\n\t}\n\n\treturn h\n}\n\n// startServer creates a new server using the harness's current serverCfg and\n// starts it after pointing the mockNet's callback to the new server.",
      "length": 2017,
      "tokens": 196,
      "embedding": []
    },
    {
      "slug": "func (h *testHarness) startServer() {",
      "content": "func (h *testHarness) startServer() {\n\th.t.Helper()\n\n\tvar err error\n\th.server, err = wtserver.New(h.serverCfg)\n\trequire.NoError(h.t, err)\n\n\th.net.registerConnCallback(h.serverAddr, h.server.InboundPeerConnected)\n\n\trequire.NoError(h.t, h.server.Start())\n}\n\n// stopServer stops the main harness server.",
      "length": 251,
      "tokens": 22,
      "embedding": []
    },
    {
      "slug": "func (h *testHarness) stopServer() {",
      "content": "func (h *testHarness) stopServer() {\n\th.t.Helper()\n\n\th.net.removeConnCallback(h.serverAddr)\n\n\trequire.NoError(h.t, h.server.Stop())\n}\n\n// startClient creates a new server using the harness's current clientCf and\n// starts it.",
      "length": 180,
      "tokens": 20,
      "embedding": []
    },
    {
      "slug": "func (h *testHarness) startClient() {",
      "content": "func (h *testHarness) startClient() {\n\th.t.Helper()\n\n\ttowerTCPAddr, err := net.ResolveTCPAddr(\"tcp\", towerAddrStr)\n\trequire.NoError(h.t, err)\n\ttowerAddr := &lnwire.NetAddress{\n\t\tIdentityKey: h.serverCfg.NodeKeyECDH.PubKey(),\n\t\tAddress:     towerTCPAddr,\n\t}\n\n\th.client, err = wtclient.New(h.clientCfg)\n\trequire.NoError(h.t, err)\n\trequire.NoError(h.t, h.client.Start())\n\trequire.NoError(h.t, h.client.AddTower(towerAddr))\n}\n\n// chanIDFromInt creates a unique channel id given a unique integral id.",
      "length": 442,
      "tokens": 39,
      "embedding": []
    },
    {
      "slug": "func chanIDFromInt(id uint64) lnwire.ChannelID {",
      "content": "func chanIDFromInt(id uint64) lnwire.ChannelID {\n\tvar chanID lnwire.ChannelID\n\tbinary.BigEndian.PutUint64(chanID[:8], id)\n\treturn chanID\n}\n\n// makeChannel creates new channel with id, using the localAmt and remoteAmt as\n// the starting balances. The channel will be available by using h.channel(id).\n//\n// NOTE: The method fails if channel for id already exists.",
      "length": 305,
      "tokens": 45,
      "embedding": []
    },
    {
      "slug": "func (h *testHarness) makeChannel(id uint64,",
      "content": "func (h *testHarness) makeChannel(id uint64,\n\tlocalAmt, remoteAmt lnwire.MilliSatoshi) {\n\n\th.t.Helper()\n\n\tchanID := chanIDFromInt(id)\n\tc := newMockChannel(h.t, h.signer, localAmt, remoteAmt)\n\n\tc.mu.Lock()\n\t_, ok := h.channels[chanID]\n\tif !ok {\n\t\th.channels[chanID] = c\n\t}\n\tc.mu.Unlock()\n\n\trequire.Falsef(h.t, ok, \"channel %d already created\", id)\n}\n\n// channel retrieves the channel corresponding to id.\n//\n// NOTE: The method fails if a channel for id does not exist.",
      "length": 404,
      "tokens": 57,
      "embedding": []
    },
    {
      "slug": "func (h *testHarness) channel(id uint64) *mockChannel {",
      "content": "func (h *testHarness) channel(id uint64) *mockChannel {\n\th.t.Helper()\n\n\th.mu.Lock()\n\tc, ok := h.channels[chanIDFromInt(id)]\n\th.mu.Unlock()\n\trequire.Truef(h.t, ok, \"unable to fetch channel %d\", id)\n\n\treturn c\n}\n\n// registerChannel registers the channel identified by id with the client.",
      "length": 219,
      "tokens": 29,
      "embedding": []
    },
    {
      "slug": "func (h *testHarness) registerChannel(id uint64) {",
      "content": "func (h *testHarness) registerChannel(id uint64) {\n\th.t.Helper()\n\n\tchanID := chanIDFromInt(id)\n\terr := h.client.RegisterChannel(chanID)\n\trequire.NoError(h.t, err)\n}\n\n// advanceChannelN calls advanceState on the channel identified by id the number\n// of provided times and returns the breach hints corresponding to the new\n// states.",
      "length": 272,
      "tokens": 37,
      "embedding": []
    },
    {
      "slug": "func (h *testHarness) advanceChannelN(id uint64, n int) []blob.BreachHint {",
      "content": "func (h *testHarness) advanceChannelN(id uint64, n int) []blob.BreachHint {\n\th.t.Helper()\n\n\tchannel := h.channel(id)\n\n\tvar hints []blob.BreachHint\n\tfor i := uint64(0); i < uint64(n); i++ {\n\t\tchannel.advanceState(h.t)\n\t\tbreachTxID, _ := h.channel(id).getState(i)\n\t\thints = append(hints, blob.NewBreachHintFromHash(&breachTxID))\n\t}\n\n\treturn hints\n}\n\n// backupStates instructs the channel identified by id to send backups to the\n// client for states in the range [to, from).",
      "length": 380,
      "tokens": 51,
      "embedding": []
    },
    {
      "slug": "func (h *testHarness) backupStates(id, from, to uint64, expErr error) {",
      "content": "func (h *testHarness) backupStates(id, from, to uint64, expErr error) {\n\th.t.Helper()\n\n\tfor i := from; i < to; i++ {\n\t\th.backupState(id, i, expErr)\n\t}\n}\n\n// backupStates instructs the channel identified by id to send a backup for\n// state i.",
      "length": 161,
      "tokens": 31,
      "embedding": []
    },
    {
      "slug": "func (h *testHarness) backupState(id, i uint64, expErr error) {",
      "content": "func (h *testHarness) backupState(id, i uint64, expErr error) {\n\th.t.Helper()\n\n\t_, retribution := h.channel(id).getState(i)\n\n\tchanID := chanIDFromInt(id)\n\terr := h.client.BackupState(\n\t\t&chanID, retribution, channeldb.SingleFunderBit,\n\t)\n\trequire.ErrorIs(h.t, expErr, err)\n}\n\n// sendPayments instructs the channel identified by id to send amt to the remote\n// party for each state in from-to times and returns the breach hints for states\n// [from, to).",
      "length": 375,
      "tokens": 51,
      "embedding": []
    },
    {
      "slug": "func (h *testHarness) sendPayments(id, from, to uint64,",
      "content": "func (h *testHarness) sendPayments(id, from, to uint64,\n\tamt lnwire.MilliSatoshi) []blob.BreachHint {\n\n\th.t.Helper()\n\n\tchannel := h.channel(id)\n\n\tvar hints []blob.BreachHint\n\tfor i := from; i < to; i++ {\n\t\th.channel(id).sendPayment(h.t, amt)\n\t\tbreachTxID, _ := channel.getState(i)\n\t\thints = append(hints, blob.NewBreachHintFromHash(&breachTxID))\n\t}\n\n\treturn hints\n}\n\n// receivePayment instructs the channel identified by id to recv amt from the\n// remote party for each state in from-to times and returns the breach hints for\n// states [from, to).",
      "length": 473,
      "tokens": 66,
      "embedding": []
    },
    {
      "slug": "func (h *testHarness) recvPayments(id, from, to uint64,",
      "content": "func (h *testHarness) recvPayments(id, from, to uint64,\n\tamt lnwire.MilliSatoshi) []blob.BreachHint {\n\n\th.t.Helper()\n\n\tchannel := h.channel(id)\n\n\tvar hints []blob.BreachHint\n\tfor i := from; i < to; i++ {\n\t\tchannel.receivePayment(h.t, amt)\n\t\tbreachTxID, _ := channel.getState(i)\n\t\thints = append(hints, blob.NewBreachHintFromHash(&breachTxID))\n\t}\n\n\treturn hints\n}\n\n// waitServerUpdates blocks until the breach hints provided all appear in the\n// watchtower's database or the timeout expires. This is used to test that the\n// client in fact sends the updates to the server, even if it is offline.",
      "length": 520,
      "tokens": 75,
      "embedding": []
    },
    {
      "slug": "func (h *testHarness) waitServerUpdates(hints []blob.BreachHint,",
      "content": "func (h *testHarness) waitServerUpdates(hints []blob.BreachHint,\n\ttimeout time.Duration) {\n\n\th.t.Helper()\n\n\t// If no breach hints are provided, we will wait out the full timeout to\n\t// assert that no updates appear.\n\twantUpdates := len(hints) > 0\n\n\thintSet := make(map[blob.BreachHint]struct{})\n\tfor _, hint := range hints {\n\t\thintSet[hint] = struct{}{}\n\t}\n\n\trequire.Lenf(h.t, hints, len(hintSet), \"breach hints are not unique, \"+\n\t\t\"list-len: %d set-len: %d\", len(hints), len(hintSet))\n\n\t// Closure to assert the server's matches are consistent with the hint\n\t// set.\n\tserverHasHints := func(matches []wtdb.Match) bool {\n\t\tif len(hintSet) != len(matches) {\n\t\t\treturn false\n\t\t}\n\n\t\tfor _, match := range matches {\n\t\t\t_, ok := hintSet[match.Hint]\n\t\t\trequire.Truef(h.t, ok, \"match %v in db is not in \"+\n\t\t\t\t\"hint set\", match.Hint)\n\t\t}\n\n\t\treturn true\n\t}\n\n\tfailTimeout := time.After(timeout)\n\tfor {\n\t\tselect {\n\t\tcase <-time.After(time.Second):\n\t\t\tmatches, err := h.serverDB.QueryMatches(hints)\n\t\t\trequire.NoError(h.t, err, \"unable to query for hints\")\n\n\t\t\tif wantUpdates && serverHasHints(matches) {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif wantUpdates {\n\t\t\t\th.t.Logf(\"Received %d/%d\\n\", len(matches),\n\t\t\t\t\tlen(hints))\n\t\t\t}\n\n\t\tcase <-failTimeout:\n\t\t\tmatches, err := h.serverDB.QueryMatches(hints)\n\t\t\trequire.NoError(h.t, err, \"unable to query for hints\")\n\t\t\trequire.Truef(h.t, serverHasHints(matches), \"breach \"+\n\t\t\t\t\"hints not received, only got %d/%d\",\n\t\t\t\tlen(matches), len(hints))\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// assertUpdatesForPolicy queries the server db for matches using the provided\n// breach hints, then asserts that each match has a session with the expected\n// policy.",
      "length": 1523,
      "tokens": 206,
      "embedding": []
    },
    {
      "slug": "func (h *testHarness) assertUpdatesForPolicy(hints []blob.BreachHint,",
      "content": "func (h *testHarness) assertUpdatesForPolicy(hints []blob.BreachHint,\n\texpPolicy wtpolicy.Policy) {\n\n\t// Query for matches on the provided hints.\n\tmatches, err := h.serverDB.QueryMatches(hints)\n\trequire.NoError(h.t, err)\n\n\t// Assert that the number of matches is exactly the number of provided\n\t// hints.\n\trequire.Lenf(h.t, matches, len(hints), \"expected: %d matches, got: %d\",\n\t\tlen(hints), len(matches))\n\n\t// Assert that all of the matches correspond to a session with the\n\t// expected policy.\n\tfor _, match := range matches {\n\t\tmatchPolicy := match.SessionInfo.Policy\n\t\trequire.Equal(h.t, expPolicy, matchPolicy)\n\t}\n}\n\n// addTower adds a tower found at `addr` to the client.",
      "length": 588,
      "tokens": 84,
      "embedding": []
    },
    {
      "slug": "func (h *testHarness) addTower(addr *lnwire.NetAddress) {",
      "content": "func (h *testHarness) addTower(addr *lnwire.NetAddress) {\n\th.t.Helper()\n\n\terr := h.client.AddTower(addr)\n\trequire.NoError(h.t, err)\n}\n\n// removeTower removes a tower from the client. If `addr` is specified, then the\n// only said address is removed from the tower.",
      "length": 198,
      "tokens": 30,
      "embedding": []
    },
    {
      "slug": "func (h *testHarness) removeTower(pubKey *btcec.PublicKey, addr net.Addr) {",
      "content": "func (h *testHarness) removeTower(pubKey *btcec.PublicKey, addr net.Addr) {\n\th.t.Helper()\n\n\terr := h.client.RemoveTower(pubKey, addr)\n\trequire.NoError(h.t, err)\n}\n\nconst (\n\tlocalBalance  = lnwire.MilliSatoshi(100000000)\n\tremoteBalance = lnwire.MilliSatoshi(200000000)\n)\n",
      "length": 184,
      "tokens": 17,
      "embedding": []
    },
    {
      "slug": "type clientTest struct {",
      "content": "type clientTest struct {\n\tname string\n\tcfg  harnessCfg\n\tfn   func(*testHarness)\n}\n\nvar clientTests = []clientTest{\n\t{\n\t\t// Asserts that client will return the ErrUnregisteredChannel\n\t\t// error when trying to backup states for a channel that has not\n\t\t// been registered (and received it's pkscript).\n\t\tname: \"backup unregistered channel\",\n\t\tcfg: harnessCfg{\n\t\t\tlocalBalance:  localBalance,\n\t\t\tremoteBalance: remoteBalance,\n\t\t\tpolicy: wtpolicy.Policy{\n\t\t\t\tTxPolicy: wtpolicy.TxPolicy{\n\t\t\t\t\tBlobType:     blob.TypeAltruistCommit,\n\t\t\t\t\tSweepFeeRate: wtpolicy.DefaultSweepFeeRate,\n\t\t\t\t},\n\t\t\t\tMaxUpdates: 20000,\n\t\t\t},\n\t\t\tnoRegisterChan0: true,\n\t\t},\n\t\tfn: func(h *testHarness) {\n\t\t\tconst (\n\t\t\t\tnumUpdates = 5\n\t\t\t\tchanID     = 0\n\t\t\t)\n\n\t\t\t// Advance the channel and backup the retributions. We\n\t\t\t// expect ErrUnregisteredChannel to be returned since\n\t\t\t// the channel was not registered during harness\n\t\t\t// creation.\n\t\t\th.advanceChannelN(chanID, numUpdates)\n\t\t\th.backupStates(\n\t\t\t\tchanID, 0, numUpdates,\n\t\t\t\twtclient.ErrUnregisteredChannel,\n\t\t\t)\n\t\t},\n\t},\n\t{\n\t\t// Asserts that the client returns an ErrClientExiting when\n\t\t// trying to backup channels after the Stop method has been\n\t\t// called.\n\t\tname: \"backup after stop\",\n\t\tcfg: harnessCfg{\n\t\t\tlocalBalance:  localBalance,\n\t\t\tremoteBalance: remoteBalance,\n\t\t\tpolicy: wtpolicy.Policy{\n\t\t\t\tTxPolicy: wtpolicy.TxPolicy{\n\t\t\t\t\tBlobType:     blob.TypeAltruistCommit,\n\t\t\t\t\tSweepFeeRate: wtpolicy.DefaultSweepFeeRate,\n\t\t\t\t},\n\t\t\t\tMaxUpdates: 20000,\n\t\t\t},\n\t\t},\n\t\tfn: func(h *testHarness) {\n\t\t\tconst (\n\t\t\t\tnumUpdates = 5\n\t\t\t\tchanID     = 0\n\t\t\t)\n\n\t\t\t// Stop the client, subsequent backups should fail.\n\t\t\th.client.Stop()\n\n\t\t\t// Advance the channel and try to back up the states. We\n\t\t\t// expect ErrClientExiting to be returned from\n\t\t\t// BackupState.\n\t\t\th.advanceChannelN(chanID, numUpdates)\n\t\t\th.backupStates(\n\t\t\t\tchanID, 0, numUpdates,\n\t\t\t\twtclient.ErrClientExiting,\n\t\t\t)\n\t\t},\n\t},\n\t{\n\t\t// Asserts that the client will continue to back up all states\n\t\t// that have previously been enqueued before it finishes\n\t\t// exiting.\n\t\tname: \"backup reliable flush\",\n\t\tcfg: harnessCfg{\n\t\t\tlocalBalance:  localBalance,\n\t\t\tremoteBalance: remoteBalance,\n\t\t\tpolicy: wtpolicy.Policy{\n\t\t\t\tTxPolicy: wtpolicy.TxPolicy{\n\t\t\t\t\tBlobType:     blob.TypeAltruistCommit,\n\t\t\t\t\tSweepFeeRate: wtpolicy.DefaultSweepFeeRate,\n\t\t\t\t},\n\t\t\t\tMaxUpdates: 5,\n\t\t\t},\n\t\t},\n\t\tfn: func(h *testHarness) {\n\t\t\tconst (\n\t\t\t\tnumUpdates = 5\n\t\t\t\tchanID     = 0\n\t\t\t)\n\n\t\t\t// Generate numUpdates retributions and back them up to\n\t\t\t// the tower.\n\t\t\thints := h.advanceChannelN(chanID, numUpdates)\n\t\t\th.backupStates(chanID, 0, numUpdates, nil)\n\n\t\t\t// Stop the client in the background, to assert the\n\t\t\t// pipeline is always flushed before it exits.\n\t\t\tgo h.client.Stop()\n\n\t\t\t// Wait for all of the updates to be populated in the\n\t\t\t// server's database.\n\t\t\th.waitServerUpdates(hints, time.Second)\n\t\t},\n\t},\n\t{\n\t\t// Assert that the client will not send out backups for states\n\t\t// whose justice transactions are ineligible for backup, e.g.\n\t\t// creating dust outputs.\n\t\tname: \"backup dust ineligible\",\n\t\tcfg: harnessCfg{\n\t\t\tlocalBalance:  localBalance,\n\t\t\tremoteBalance: remoteBalance,\n\t\t\tpolicy: wtpolicy.Policy{\n\t\t\t\tTxPolicy: wtpolicy.TxPolicy{\n\t\t\t\t\tBlobType:     blob.TypeAltruistCommit,\n\t\t\t\t\tSweepFeeRate: 1000000, // high sweep fee creates dust\n\t\t\t\t},\n\t\t\t\tMaxUpdates: 20000,\n\t\t\t},\n\t\t},\n\t\tfn: func(h *testHarness) {\n\t\t\tconst (\n\t\t\t\tnumUpdates = 5\n\t\t\t\tchanID     = 0\n\t\t\t)\n\n\t\t\t// Create the retributions and queue them for backup.\n\t\t\th.advanceChannelN(chanID, numUpdates)\n\t\t\th.backupStates(chanID, 0, numUpdates, nil)\n\n\t\t\t// Ensure that no updates are received by the server,\n\t\t\t// since they should all be marked as ineligible.\n\t\t\th.waitServerUpdates(nil, time.Second)\n\t\t},\n\t},\n\t{\n\t\t// Verifies that the client will properly retransmit a committed\n\t\t// state update to the watchtower after a restart if the update\n\t\t// was not acked while the client was active last.\n\t\tname: \"committed update restart\",\n\t\tcfg: harnessCfg{\n\t\t\tlocalBalance:  localBalance,\n\t\t\tremoteBalance: remoteBalance,\n\t\t\tpolicy: wtpolicy.Policy{\n\t\t\t\tTxPolicy: wtpolicy.TxPolicy{\n\t\t\t\t\tBlobType:     blob.TypeAltruistCommit,\n\t\t\t\t\tSweepFeeRate: wtpolicy.DefaultSweepFeeRate,\n\t\t\t\t},\n\t\t\t\tMaxUpdates: 20000,\n\t\t\t},\n\t\t},\n\t\tfn: func(h *testHarness) {\n\t\t\tconst (\n\t\t\t\tnumUpdates = 5\n\t\t\t\tchanID     = 0\n\t\t\t)\n\n\t\t\thints := h.advanceChannelN(0, numUpdates)\n\n\t\t\tvar numSent uint64\n\n\t\t\t// Add the first two states to the client's pipeline.\n\t\t\th.backupStates(chanID, 0, 2, nil)\n\t\t\tnumSent = 2\n\n\t\t\t// Wait for both to be reflected in the server's\n\t\t\t// database.\n\t\t\th.waitServerUpdates(hints[:numSent], time.Second)\n\n\t\t\t// Now, restart the server and prevent it from acking\n\t\t\t// state updates.\n\t\t\th.stopServer()\n\t\t\th.serverCfg.NoAckUpdates = true\n\t\t\th.startServer()\n\n\t\t\t// Send the next state update to the tower. Since the\n\t\t\t// tower isn't acking state updates, we expect this\n\t\t\t// update to be committed and sent by the session queue,\n\t\t\t// but it will never receive an ack.\n\t\t\th.backupState(chanID, numSent, nil)\n\t\t\tnumSent++\n\n\t\t\t// Force quit the client to abort the state updates it\n\t\t\t// has queued. The sleep ensures that the session queues\n\t\t\t// have enough time to commit the state updates before\n\t\t\t// the client is killed.\n\t\t\ttime.Sleep(time.Second)\n\t\t\th.client.ForceQuit()\n\n\t\t\t// Restart the server and allow it to ack the updates\n\t\t\t// after the client retransmits the unacked update.\n\t\t\th.stopServer()\n\t\t\th.serverCfg.NoAckUpdates = false\n\t\t\th.startServer()\n\n\t\t\t// Restart the client and allow it to process the\n\t\t\t// committed update.\n\t\t\th.startClient()\n\n\t\t\t// Wait for the committed update to be accepted by the\n\t\t\t// tower.\n\t\t\th.waitServerUpdates(hints[:numSent], time.Second)\n\n\t\t\t// Finally, send the rest of the updates and wait for\n\t\t\t// the tower to receive the remaining states.\n\t\t\th.backupStates(chanID, numSent, numUpdates, nil)\n\n\t\t\t// Wait for all of the updates to be populated in the\n\t\t\t// server's database.\n\t\t\th.waitServerUpdates(hints, time.Second)\n\n\t\t},\n\t},\n\t{\n\t\t// Asserts that the client will continue to retry sending state\n\t\t// updates if it doesn't receive an ack from the server. The\n\t\t// client is expected to flush everything in its in-memory\n\t\t// pipeline once the server begins sending acks again.\n\t\tname: \"no ack from server\",\n\t\tcfg: harnessCfg{\n\t\t\tlocalBalance:  localBalance,\n\t\t\tremoteBalance: remoteBalance,\n\t\t\tpolicy: wtpolicy.Policy{\n\t\t\t\tTxPolicy: wtpolicy.TxPolicy{\n\t\t\t\t\tBlobType:     blob.TypeAltruistCommit,\n\t\t\t\t\tSweepFeeRate: wtpolicy.DefaultSweepFeeRate,\n\t\t\t\t},\n\t\t\t\tMaxUpdates: 5,\n\t\t\t},\n\t\t},\n\t\tfn: func(h *testHarness) {\n\t\t\tconst (\n\t\t\t\tnumUpdates = 100\n\t\t\t\tchanID     = 0\n\t\t\t)\n\n\t\t\t// Generate the retributions that will be backed up.\n\t\t\thints := h.advanceChannelN(chanID, numUpdates)\n\n\t\t\t// Restart the server and prevent it from acking state\n\t\t\t// updates.\n\t\t\th.stopServer()\n\t\t\th.serverCfg.NoAckUpdates = true\n\t\t\th.startServer()\n\n\t\t\t// Now, queue the retributions for backup.\n\t\t\th.backupStates(chanID, 0, numUpdates, nil)\n\n\t\t\t// Stop the client in the background, to assert the\n\t\t\t// pipeline is always flushed before it exits.\n\t\t\tgo h.client.Stop()\n\n\t\t\t// Give the client time to saturate a large number of\n\t\t\t// session queues for which the server has not acked the\n\t\t\t// state updates that it has received.\n\t\t\ttime.Sleep(time.Second)\n\n\t\t\t// Restart the server and allow it to ack the updates\n\t\t\t// after the client retransmits the unacked updates.\n\t\t\th.stopServer()\n\t\t\th.serverCfg.NoAckUpdates = false\n\t\t\th.startServer()\n\n\t\t\t// Wait for all of the updates to be populated in the\n\t\t\t// server's database.\n\t\t\th.waitServerUpdates(hints, waitTime)\n\t\t},\n\t},\n\t{\n\t\t// Asserts that the client is able to send state updates to the\n\t\t// tower for a full range of channel values, assuming the sweep\n\t\t// fee rates permit it. We expect all of these to be successful\n\t\t// since a sweep transactions spending only from one output is\n\t\t// less expensive than one that sweeps both.\n\t\tname: \"send and recv\",\n\t\tcfg: harnessCfg{\n\t\t\tlocalBalance:  100000001, // ensure (% amt != 0)\n\t\t\tremoteBalance: 200000001, // ensure (% amt != 0)\n\t\t\tpolicy: wtpolicy.Policy{\n\t\t\t\tTxPolicy: wtpolicy.TxPolicy{\n\t\t\t\t\tBlobType:     blob.TypeAltruistCommit,\n\t\t\t\t\tSweepFeeRate: wtpolicy.DefaultSweepFeeRate,\n\t\t\t\t},\n\t\t\t\tMaxUpdates: 1000,\n\t\t\t},\n\t\t},\n\t\tfn: func(h *testHarness) {\n\t\t\tvar (\n\t\t\t\tcapacity   = h.cfg.localBalance + h.cfg.remoteBalance\n\t\t\t\tpaymentAmt = lnwire.MilliSatoshi(2000000)\n\t\t\t\tnumSends   = uint64(h.cfg.localBalance / paymentAmt)\n\t\t\t\tnumRecvs   = uint64(capacity / paymentAmt)\n\t\t\t\tnumUpdates = numSends + numRecvs // 200 updates\n\t\t\t\tchanID     = uint64(0)\n\t\t\t)\n\n\t\t\t// Send money to the remote party until all funds are\n\t\t\t// depleted.\n\t\t\tsendHints := h.sendPayments(chanID, 0, numSends, paymentAmt)\n\n\t\t\t// Now, sequentially receive the entire channel balance\n\t\t\t// from the remote party.\n\t\t\trecvHints := h.recvPayments(chanID, numSends, numUpdates, paymentAmt)\n\n\t\t\t// Collect the hints generated by both sending and\n\t\t\t// receiving.\n\t\t\thints := append(sendHints, recvHints...)\n\n\t\t\t// Backup the channel's states the client.\n\t\t\th.backupStates(chanID, 0, numUpdates, nil)\n\n\t\t\t// Wait for all of the updates to be populated in the\n\t\t\t// server's database.\n\t\t\th.waitServerUpdates(hints, 3*time.Second)\n\t\t},\n\t},\n\t{\n\t\t// Asserts that the client is able to support multiple links.\n\t\tname: \"multiple link backup\",\n\t\tcfg: harnessCfg{\n\t\t\tlocalBalance:  localBalance,\n\t\t\tremoteBalance: remoteBalance,\n\t\t\tpolicy: wtpolicy.Policy{\n\t\t\t\tTxPolicy: wtpolicy.TxPolicy{\n\t\t\t\t\tBlobType:     blob.TypeAltruistCommit,\n\t\t\t\t\tSweepFeeRate: wtpolicy.DefaultSweepFeeRate,\n\t\t\t\t},\n\t\t\t\tMaxUpdates: 5,\n\t\t\t},\n\t\t},\n\t\tfn: func(h *testHarness) {\n\t\t\tconst (\n\t\t\t\tnumUpdates = 5\n\t\t\t\tnumChans   = 10\n\t\t\t)\n\n\t\t\t// Initialize and register an additional 9 channels.\n\t\t\tfor id := uint64(1); id < 10; id++ {\n\t\t\t\th.makeChannel(\n\t\t\t\t\tid, h.cfg.localBalance,\n\t\t\t\t\th.cfg.remoteBalance,\n\t\t\t\t)\n\t\t\t\th.registerChannel(id)\n\t\t\t}\n\n\t\t\t// Generate the retributions for all 10 channels and\n\t\t\t// collect the breach hints.\n\t\t\tvar hints []blob.BreachHint\n\t\t\tfor id := uint64(0); id < 10; id++ {\n\t\t\t\tchanHints := h.advanceChannelN(id, numUpdates)\n\t\t\t\thints = append(hints, chanHints...)\n\t\t\t}\n\n\t\t\t// Provided all retributions to the client from all\n\t\t\t// channels.\n\t\t\tfor id := uint64(0); id < 10; id++ {\n\t\t\t\th.backupStates(id, 0, numUpdates, nil)\n\t\t\t}\n\n\t\t\t// Test reliable flush under multi-client scenario.\n\t\t\tgo h.client.Stop()\n\n\t\t\t// Wait for all of the updates to be populated in the\n\t\t\t// server's database.\n\t\t\th.waitServerUpdates(hints, 10*time.Second)\n\t\t},\n\t},\n\t{\n\t\tname: \"create session no ack\",\n\t\tcfg: harnessCfg{\n\t\t\tlocalBalance:  localBalance,\n\t\t\tremoteBalance: remoteBalance,\n\t\t\tpolicy: wtpolicy.Policy{\n\t\t\t\tTxPolicy: wtpolicy.TxPolicy{\n\t\t\t\t\tBlobType:     blob.TypeAltruistCommit,\n\t\t\t\t\tSweepFeeRate: wtpolicy.DefaultSweepFeeRate,\n\t\t\t\t},\n\t\t\t\tMaxUpdates: 5,\n\t\t\t},\n\t\t\tnoAckCreateSession: true,\n\t\t},\n\t\tfn: func(h *testHarness) {\n\t\t\tconst (\n\t\t\t\tchanID     = 0\n\t\t\t\tnumUpdates = 3\n\t\t\t)\n\n\t\t\t// Generate the retributions that will be backed up.\n\t\t\thints := h.advanceChannelN(chanID, numUpdates)\n\n\t\t\t// Now, queue the retributions for backup.\n\t\t\th.backupStates(chanID, 0, numUpdates, nil)\n\n\t\t\t// Since the client is unable to create a session, the\n\t\t\t// server should have no updates.\n\t\t\th.waitServerUpdates(nil, time.Second)\n\n\t\t\t// Force quit the client since it has queued backups.\n\t\t\th.client.ForceQuit()\n\n\t\t\t// Restart the server and allow it to ack session\n\t\t\t// creation.\n\t\t\th.stopServer()\n\t\t\th.serverCfg.NoAckCreateSession = false\n\t\t\th.startServer()\n\n\t\t\t// Restart the client with the same policy, which will\n\t\t\t// immediately try to overwrite the old session with an\n\t\t\t// identical one.\n\t\t\th.startClient()\n\n\t\t\t// Now, queue the retributions for backup.\n\t\t\th.backupStates(chanID, 0, numUpdates, nil)\n\n\t\t\t// Wait for all of the updates to be populated in the\n\t\t\t// server's database.\n\t\t\th.waitServerUpdates(hints, waitTime)\n\n\t\t\t// Assert that the server has updates for the clients\n\t\t\t// most recent policy.\n\t\t\th.assertUpdatesForPolicy(hints, h.clientCfg.Policy)\n\t\t},\n\t},\n\t{\n\t\tname: \"create session no ack change policy\",\n\t\tcfg: harnessCfg{\n\t\t\tlocalBalance:  localBalance,\n\t\t\tremoteBalance: remoteBalance,\n\t\t\tpolicy: wtpolicy.Policy{\n\t\t\t\tTxPolicy: wtpolicy.TxPolicy{\n\t\t\t\t\tBlobType:     blob.TypeAltruistCommit,\n\t\t\t\t\tSweepFeeRate: wtpolicy.DefaultSweepFeeRate,\n\t\t\t\t},\n\t\t\t\tMaxUpdates: 5,\n\t\t\t},\n\t\t\tnoAckCreateSession: true,\n\t\t},\n\t\tfn: func(h *testHarness) {\n\t\t\tconst (\n\t\t\t\tchanID     = 0\n\t\t\t\tnumUpdates = 3\n\t\t\t)\n\n\t\t\t// Generate the retributions that will be backed up.\n\t\t\thints := h.advanceChannelN(chanID, numUpdates)\n\n\t\t\t// Now, queue the retributions for backup.\n\t\t\th.backupStates(chanID, 0, numUpdates, nil)\n\n\t\t\t// Since the client is unable to create a session, the\n\t\t\t// server should have no updates.\n\t\t\th.waitServerUpdates(nil, time.Second)\n\n\t\t\t// Force quit the client since it has queued backups.\n\t\t\th.client.ForceQuit()\n\n\t\t\t// Restart the server and allow it to ack session\n\t\t\t// creation.\n\t\t\th.stopServer()\n\t\t\th.serverCfg.NoAckCreateSession = false\n\t\t\th.startServer()\n\n\t\t\t// Restart the client with a new policy, which will\n\t\t\t// immediately try to overwrite the prior session with\n\t\t\t// the old policy.\n\t\t\th.clientCfg.Policy.SweepFeeRate *= 2\n\t\t\th.startClient()\n\n\t\t\t// Now, queue the retributions for backup.\n\t\t\th.backupStates(chanID, 0, numUpdates, nil)\n\n\t\t\t// Wait for all of the updates to be populated in the\n\t\t\t// server's database.\n\t\t\th.waitServerUpdates(hints, waitTime)\n\n\t\t\t// Assert that the server has updates for the clients\n\t\t\t// most recent policy.\n\t\t\th.assertUpdatesForPolicy(hints, h.clientCfg.Policy)\n\t\t},\n\t},\n\t{\n\t\t// Asserts that the client will not request a new session if\n\t\t// already has an existing session with the same TxPolicy. This\n\t\t// permits the client to continue using policies that differ in\n\t\t// operational parameters, but don't manifest in different\n\t\t// justice transactions.\n\t\tname: \"create session change policy same txpolicy\",\n\t\tcfg: harnessCfg{\n\t\t\tlocalBalance:  localBalance,\n\t\t\tremoteBalance: remoteBalance,\n\t\t\tpolicy: wtpolicy.Policy{\n\t\t\t\tTxPolicy: wtpolicy.TxPolicy{\n\t\t\t\t\tBlobType:     blob.TypeAltruistCommit,\n\t\t\t\t\tSweepFeeRate: wtpolicy.DefaultSweepFeeRate,\n\t\t\t\t},\n\t\t\t\tMaxUpdates: 10,\n\t\t\t},\n\t\t},\n\t\tfn: func(h *testHarness) {\n\t\t\tconst (\n\t\t\t\tchanID     = 0\n\t\t\t\tnumUpdates = 6\n\t\t\t)\n\n\t\t\t// Generate the retributions that will be backed up.\n\t\t\thints := h.advanceChannelN(chanID, numUpdates)\n\n\t\t\t// Now, queue the first half of the retributions.\n\t\t\th.backupStates(chanID, 0, numUpdates/2, nil)\n\n\t\t\t// Wait for the server to collect the first half.\n\t\t\th.waitServerUpdates(hints[:numUpdates/2], time.Second)\n\n\t\t\t// Stop the client, which should have no more backups.\n\t\t\th.client.Stop()\n\n\t\t\t// Record the policy that the first half was stored\n\t\t\t// under. We'll expect the second half to also be stored\n\t\t\t// under the original policy, since we are only adjusting\n\t\t\t// the MaxUpdates. The client should detect that the\n\t\t\t// two policies have equivalent TxPolicies and continue\n\t\t\t// using the first.\n\t\t\texpPolicy := h.clientCfg.Policy\n\n\t\t\t// Restart the client with a new policy.\n\t\t\th.clientCfg.Policy.MaxUpdates = 20\n\t\t\th.startClient()\n\n\t\t\t// Now, queue the second half of the retributions.\n\t\t\th.backupStates(chanID, numUpdates/2, numUpdates, nil)\n\n\t\t\t// Wait for all of the updates to be populated in the\n\t\t\t// server's database.\n\t\t\th.waitServerUpdates(hints, waitTime)\n\n\t\t\t// Assert that the server has updates for the client's\n\t\t\t// original policy.\n\t\t\th.assertUpdatesForPolicy(hints, expPolicy)\n\t\t},\n\t},\n\t{\n\t\t// Asserts that the client will deduplicate backups presented by\n\t\t// a channel both in memory and after a restart. The client\n\t\t// should only accept backups with a commit height greater than\n\t\t// any processed already processed for a given policy.\n\t\tname: \"dedup backups\",\n\t\tcfg: harnessCfg{\n\t\t\tlocalBalance:  localBalance,\n\t\t\tremoteBalance: remoteBalance,\n\t\t\tpolicy: wtpolicy.Policy{\n\t\t\t\tTxPolicy: wtpolicy.TxPolicy{\n\t\t\t\t\tBlobType:     blob.TypeAltruistCommit,\n\t\t\t\t\tSweepFeeRate: wtpolicy.DefaultSweepFeeRate,\n\t\t\t\t},\n\t\t\t\tMaxUpdates: 5,\n\t\t\t},\n\t\t},\n\t\tfn: func(h *testHarness) {\n\t\t\tconst (\n\t\t\t\tnumUpdates = 10\n\t\t\t\tchanID     = 0\n\t\t\t)\n\n\t\t\t// Generate the retributions that will be backed up.\n\t\t\thints := h.advanceChannelN(chanID, numUpdates)\n\n\t\t\t// Queue the first half of the retributions twice, the\n\t\t\t// second batch should be entirely deduped by the\n\t\t\t// client's in-memory tracking.\n\t\t\th.backupStates(chanID, 0, numUpdates/2, nil)\n\t\t\th.backupStates(chanID, 0, numUpdates/2, nil)\n\n\t\t\t// Wait for the first half of the updates to be\n\t\t\t// populated in the server's database.\n\t\t\th.waitServerUpdates(hints[:len(hints)/2], waitTime)\n\n\t\t\t// Restart the client, so we can ensure the deduping is\n\t\t\t// maintained across restarts.\n\t\t\th.client.Stop()\n\t\t\th.startClient()\n\n\t\t\t// Try to back up the full range of retributions. Only\n\t\t\t// the second half should actually be sent.\n\t\t\th.backupStates(chanID, 0, numUpdates, nil)\n\n\t\t\t// Wait for all of the updates to be populated in the\n\t\t\t// server's database.\n\t\t\th.waitServerUpdates(hints, waitTime)\n\t\t},\n\t},\n\t{\n\t\t// Asserts that the client can continue making backups to a\n\t\t// tower that's been re-added after it's been removed.\n\t\tname: \"re-add removed tower\",\n\t\tcfg: harnessCfg{\n\t\t\tlocalBalance:  localBalance,\n\t\t\tremoteBalance: remoteBalance,\n\t\t\tpolicy: wtpolicy.Policy{\n\t\t\t\tTxPolicy: wtpolicy.TxPolicy{\n\t\t\t\t\tBlobType:     blob.TypeAltruistCommit,\n\t\t\t\t\tSweepFeeRate: wtpolicy.DefaultSweepFeeRate,\n\t\t\t\t},\n\t\t\t\tMaxUpdates: 5,\n\t\t\t},\n\t\t},\n\t\tfn: func(h *testHarness) {\n\t\t\tconst (\n\t\t\t\tchanID     = 0\n\t\t\t\tnumUpdates = 4\n\t\t\t)\n\n\t\t\t// Create four channel updates and only back up the\n\t\t\t// first two.\n\t\t\thints := h.advanceChannelN(chanID, numUpdates)\n\t\t\th.backupStates(chanID, 0, numUpdates/2, nil)\n\t\t\th.waitServerUpdates(hints[:numUpdates/2], waitTime)\n\n\t\t\t// Fully remove the tower, causing its existing sessions\n\t\t\t// to be marked inactive.\n\t\t\th.removeTower(h.serverAddr.IdentityKey, nil)\n\n\t\t\t// Back up the remaining states. Since the tower has\n\t\t\t// been removed, it shouldn't receive any updates.\n\t\t\th.backupStates(chanID, numUpdates/2, numUpdates, nil)\n\t\t\th.waitServerUpdates(nil, time.Second)\n\n\t\t\t// Re-add the tower. We prevent the tower from acking\n\t\t\t// session creation to ensure the inactive sessions are\n\t\t\t// not used.\n\t\t\th.stopServer()\n\t\t\th.serverCfg.NoAckCreateSession = true\n\t\t\th.startServer()\n\t\t\th.addTower(h.serverAddr)\n\t\t\th.waitServerUpdates(nil, time.Second)\n\n\t\t\t// Finally, allow the tower to ack session creation,\n\t\t\t// allowing the state updates to be sent through the new\n\t\t\t// session.\n\t\t\th.stopServer()\n\t\t\th.serverCfg.NoAckCreateSession = false\n\t\t\th.startServer()\n\t\t\th.waitServerUpdates(hints[numUpdates/2:], waitTime)\n\t\t},\n\t},\n\t{\n\t\t// Asserts that the client's force quite delay will properly\n\t\t// shutdown the client if it is unable to completely drain the\n\t\t// task pipeline.\n\t\tname: \"force unclean shutdown\",\n\t\tcfg: harnessCfg{\n\t\t\tlocalBalance:  localBalance,\n\t\t\tremoteBalance: remoteBalance,\n\t\t\tpolicy: wtpolicy.Policy{\n\t\t\t\tTxPolicy: wtpolicy.TxPolicy{\n\t\t\t\t\tBlobType:     blob.TypeAltruistCommit,\n\t\t\t\t\tSweepFeeRate: wtpolicy.DefaultSweepFeeRate,\n\t\t\t\t},\n\t\t\t\tMaxUpdates: 5,\n\t\t\t},\n\t\t},\n\t\tfn: func(h *testHarness) {\n\t\t\tconst (\n\t\t\t\tchanID     = 0\n\t\t\t\tnumUpdates = 6\n\t\t\t\tmaxUpdates = 5\n\t\t\t)\n\n\t\t\t// Advance the channel to create all states.\n\t\t\thints := h.advanceChannelN(chanID, numUpdates)\n\n\t\t\t// Back up 4 of the 5 states for the negotiated session.\n\t\t\th.backupStates(chanID, 0, maxUpdates-1, nil)\n\t\t\th.waitServerUpdates(hints[:maxUpdates-1], waitTime)\n\n\t\t\t// Now, restart the tower and prevent it from acking any\n\t\t\t// new sessions. We do this here as once the last slot\n\t\t\t// is exhausted the client will attempt to renegotiate.\n\t\t\th.stopServer()\n\t\t\th.serverCfg.NoAckCreateSession = true\n\t\t\th.startServer()\n\n\t\t\t// Back up the remaining two states. Once the first is\n\t\t\t// processed, the session will be exhausted but the\n\t\t\t// client won't be able to regnegotiate a session for\n\t\t\t// the final state. We'll only wait for the first five\n\t\t\t// states to arrive at the tower.\n\t\t\th.backupStates(chanID, maxUpdates-1, numUpdates, nil)\n\t\t\th.waitServerUpdates(hints[:maxUpdates], waitTime)\n\n\t\t\t// Finally, stop the client which will continue to\n\t\t\t// attempt session negotiation since it has one more\n\t\t\t// state to process. After the force quite delay\n\t\t\t// expires, the client should force quite itself and\n\t\t\t// allow the test to complete.\n\t\t\th.stopServer()\n\t\t},\n\t},\n\t{\n\t\t// Assert that if a client changes the address for a server and\n\t\t// then tries to back up updates then the client will switch to\n\t\t// the new address.\n\t\tname: \"change address of existing session\",\n\t\tcfg: harnessCfg{\n\t\t\tlocalBalance:  localBalance,\n\t\t\tremoteBalance: remoteBalance,\n\t\t\tpolicy: wtpolicy.Policy{\n\t\t\t\tTxPolicy: wtpolicy.TxPolicy{\n\t\t\t\t\tBlobType:     blob.TypeAltruistCommit,\n\t\t\t\t\tSweepFeeRate: wtpolicy.DefaultSweepFeeRate,\n\t\t\t\t},\n\t\t\t\tMaxUpdates: 5,\n\t\t\t},\n\t\t},\n\t\tfn: func(h *testHarness) {\n\t\t\tconst (\n\t\t\t\tchanID     = 0\n\t\t\t\tnumUpdates = 6\n\t\t\t\tmaxUpdates = 5\n\t\t\t)\n\n\t\t\t// Advance the channel to create all states.\n\t\t\thints := h.advanceChannelN(chanID, numUpdates)\n\n\t\t\th.backupStates(chanID, 0, numUpdates/2, nil)\n\n\t\t\t// Wait for the first half of the updates to be\n\t\t\t// populated in the server's database.\n\t\t\th.waitServerUpdates(hints[:len(hints)/2], waitTime)\n\n\t\t\t// Stop the server.\n\t\t\th.stopServer()\n\n\t\t\t// Change the address of the server.\n\t\t\ttowerTCPAddr, err := net.ResolveTCPAddr(\n\t\t\t\t\"tcp\", towerAddr2Str,\n\t\t\t)\n\t\t\trequire.NoError(h.t, err)\n\n\t\t\toldAddr := h.serverAddr.Address\n\t\t\ttowerAddr := &lnwire.NetAddress{\n\t\t\t\tIdentityKey: h.serverAddr.IdentityKey,\n\t\t\t\tAddress:     towerTCPAddr,\n\t\t\t}\n\t\t\th.serverAddr = towerAddr\n\n\t\t\t// Add the new tower address to the client.\n\t\t\terr = h.client.AddTower(towerAddr)\n\t\t\trequire.NoError(h.t, err)\n\n\t\t\t// Remove the old tower address from the client.\n\t\t\terr = h.client.RemoveTower(\n\t\t\t\ttowerAddr.IdentityKey, oldAddr,\n\t\t\t)\n\t\t\trequire.NoError(h.t, err)\n\n\t\t\t// Restart the server.\n\t\t\th.startServer()\n\n\t\t\t// Now attempt to back up the rest of the updates.\n\t\t\th.backupStates(chanID, numUpdates/2, maxUpdates, nil)\n\n\t\t\t// Assert that the server does receive the updates.\n\t\t\th.waitServerUpdates(hints[:maxUpdates], waitTime)\n\t\t},\n\t},\n\t{\n\t\t// Assert that a user is able to remove a tower address during\n\t\t// session negotiation as long as the address in question is not\n\t\t// currently being used.\n\t\tname: \"removing a tower during session negotiation\",\n\t\tcfg: harnessCfg{\n\t\t\tlocalBalance:  localBalance,\n\t\t\tremoteBalance: remoteBalance,\n\t\t\tpolicy: wtpolicy.Policy{\n\t\t\t\tTxPolicy: wtpolicy.TxPolicy{\n\t\t\t\t\tBlobType:     blob.TypeAltruistCommit,\n\t\t\t\t\tSweepFeeRate: wtpolicy.DefaultSweepFeeRate,\n\t\t\t\t},\n\t\t\t\tMaxUpdates: 5,\n\t\t\t},\n\t\t\tnoServerStart: true,\n\t\t},\n\t\tfn: func(h *testHarness) {\n\t\t\t// The server has not started yet and so no session\n\t\t\t// negotiation with the server will be in progress, so\n\t\t\t// the client should be able to remove the server.\n\t\t\terr := wait.NoError(func() error {\n\t\t\t\treturn h.client.RemoveTower(\n\t\t\t\t\th.serverAddr.IdentityKey, nil,\n\t\t\t\t)\n\t\t\t}, waitTime)\n\t\t\trequire.NoError(h.t, err)\n\n\t\t\t// Set the server up so that its Dial function hangs\n\t\t\t// when the client calls it. This will force the client\n\t\t\t// to remain in the state where it has locked the\n\t\t\t// address of the server.\n\t\t\th.server, err = wtserver.New(h.serverCfg)\n\t\t\trequire.NoError(h.t, err)\n\n\t\t\tcancel := make(chan struct{})\n\t\t\th.net.registerConnCallback(\n\t\t\t\th.serverAddr, func(peer wtserver.Peer) {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase <-h.quit:\n\t\t\t\t\tcase <-cancel:\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t)\n\n\t\t\t// Also add a new tower address.\n\t\t\ttowerTCPAddr, err := net.ResolveTCPAddr(\n\t\t\t\t\"tcp\", towerAddr2Str,\n\t\t\t)\n\t\t\trequire.NoError(h.t, err)\n\n\t\t\ttowerAddr := &lnwire.NetAddress{\n\t\t\t\tIdentityKey: h.serverAddr.IdentityKey,\n\t\t\t\tAddress:     towerTCPAddr,\n\t\t\t}\n\n\t\t\t// Register the new address in the mock-net.\n\t\t\th.net.registerConnCallback(\n\t\t\t\ttowerAddr, h.server.InboundPeerConnected,\n\t\t\t)\n\n\t\t\t// Now start the server.\n\t\t\trequire.NoError(h.t, h.server.Start())\n\n\t\t\t// Re-add the server to the client\n\t\t\terr = h.client.AddTower(h.serverAddr)\n\t\t\trequire.NoError(h.t, err)\n\n\t\t\t// Also add the new tower address.\n\t\t\terr = h.client.AddTower(towerAddr)\n\t\t\trequire.NoError(h.t, err)\n\n\t\t\t// Assert that if the client attempts to remove the\n\t\t\t// tower's first address, then it will error due to\n\t\t\t// address currently being locked for session\n\t\t\t// negotiation.\n\t\t\terr = wait.Predicate(func() bool {\n\t\t\t\terr = h.client.RemoveTower(\n\t\t\t\t\th.serverAddr.IdentityKey,\n\t\t\t\t\th.serverAddr.Address,\n\t\t\t\t)\n\t\t\t\treturn errors.Is(err, wtclient.ErrAddrInUse)\n\t\t\t}, waitTime)\n\t\t\trequire.NoError(h.t, err)\n\n\t\t\t// Assert that the second address can be removed since\n\t\t\t// it is not being used for session negotiation.\n\t\t\terr = wait.NoError(func() error {\n\t\t\t\treturn h.client.RemoveTower(\n\t\t\t\t\th.serverAddr.IdentityKey, towerTCPAddr,\n\t\t\t\t)\n\t\t\t}, waitTime)\n\t\t\trequire.NoError(h.t, err)\n\n\t\t\t// Allow the dial to the first address to stop hanging.\n\t\t\tclose(cancel)\n\n\t\t\t// Assert that the client can now remove the first\n\t\t\t// address.\n\t\t\terr = wait.NoError(func() error {\n\t\t\t\treturn h.client.RemoveTower(\n\t\t\t\t\th.serverAddr.IdentityKey, nil,\n\t\t\t\t)\n\t\t\t}, waitTime)\n\t\t\trequire.NoError(h.t, err)\n\t\t},\n\t},\n}\n\n// TestClient executes the client test suite, asserting the ability to backup\n// states in a number of failure cases and it's reliability during shutdown.",
      "length": 24533,
      "tokens": 3140,
      "embedding": []
    },
    {
      "slug": "func TestClient(t *testing.T) {",
      "content": "func TestClient(t *testing.T) {\n\tfor _, test := range clientTests {\n\t\ttc := test\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tt.Parallel()\n\n\t\t\th := newHarness(t, tc.cfg)\n\n\t\t\ttc.fn(h)\n\t\t})\n\t}\n}\n",
      "length": 146,
      "tokens": 23,
      "embedding": []
    }
  ]
}