{
  "filepath": "../implementations/go/lnd/lntest/harness.go",
  "package": "lntest",
  "sections": [
    {
      "slug": "type TestCase struct {",
      "content": "type TestCase struct {\n\t// Name specifies the test name.\n\tName string\n\n\t// TestFunc is the test case wrapped in a function.\n\tTestFunc func(t *HarnessTest)\n}\n\n// standbyNodes are a list of nodes which are created during the initialization\n// of the test and used across all test cases.",
      "length": 253,
      "tokens": 45,
      "embedding": []
    },
    {
      "slug": "type standbyNodes struct {",
      "content": "type standbyNodes struct {\n\t// Alice and Bob are the initial seeder nodes that are automatically\n\t// created to be the initial participants of the test network.\n\tAlice *node.HarnessNode\n\tBob   *node.HarnessNode\n}\n\n// HarnessTest builds on top of a testing.T with enhanced error detection. It\n// is responsible for managing the interactions among different nodes, and\n// providing easy-to-use assertions.",
      "length": 368,
      "tokens": 56,
      "embedding": []
    },
    {
      "slug": "type HarnessTest struct {",
      "content": "type HarnessTest struct {\n\t*testing.T\n\n\t// Embed the standbyNodes so we can easily access them via `ht.Alice`.\n\tstandbyNodes\n\n\t// Miner is a reference to a running full node that can be used to\n\t// create new blocks on the network.\n\tMiner *HarnessMiner\n\n\t// manager handles the start and stop of a given node.\n\tmanager *nodeManager\n\n\t// feeService is a web service that provides external fee estimates to\n\t// lnd.\n\tfeeService WebFeeService\n\n\t// Channel for transmitting stderr output from failed lightning node\n\t// to main process.\n\tlndErrorChan chan error\n\n\t// runCtx is a context with cancel method. It's used to signal when the\n\t// node needs to quit, and used as the parent context when spawning\n\t// children contexts for RPC requests.\n\trunCtx context.Context //nolint:containedctx\n\tcancel context.CancelFunc\n\n\t// stopChainBackend points to the cleanup function returned by the\n\t// chainBackend.\n\tstopChainBackend func()\n\n\t// cleaned specifies whether the cleanup has been applied for the\n\t// current HarnessTest.\n\tcleaned bool\n}\n\n// NewHarnessTest creates a new instance of a harnessTest from a regular\n// testing.T instance.",
      "length": 1068,
      "tokens": 168,
      "embedding": []
    },
    {
      "slug": "func NewHarnessTest(t *testing.T, lndBinary string, feeService WebFeeService,",
      "content": "func NewHarnessTest(t *testing.T, lndBinary string, feeService WebFeeService,\n\tdbBackend node.DatabaseBackend) *HarnessTest {\n\n\tt.Helper()\n\n\t// Create the run context.\n\tctxt, cancel := context.WithCancel(context.Background())\n\n\tmanager := newNodeManager(lndBinary, dbBackend)\n\n\treturn &HarnessTest{\n\t\tT:          t,\n\t\tmanager:    manager,\n\t\tfeeService: feeService,\n\t\trunCtx:     ctxt,\n\t\tcancel:     cancel,\n\t\t// We need to use buffered channel here as we don't want to\n\t\t// block sending errors.\n\t\tlndErrorChan: make(chan error, lndErrorChanSize),\n\t}\n}\n\n// Start will assemble the chain backend and the miner for the HarnessTest. It\n// also starts the fee service and watches lnd process error.",
      "length": 594,
      "tokens": 78,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) Start(chain node.BackendConfig, miner *HarnessMiner) {",
      "content": "func (h *HarnessTest) Start(chain node.BackendConfig, miner *HarnessMiner) {\n\t// Spawn a new goroutine to watch for any fatal errors that any of the\n\t// running lnd processes encounter. If an error occurs, then the test\n\t// case should naturally as a result and we log the server error here\n\t// to help debug.\n\tgo func() {\n\t\tselect {\n\t\tcase err, more := <-h.lndErrorChan:\n\t\t\tif !more {\n\t\t\t\treturn\n\t\t\t}\n\t\t\th.Logf(\"lnd finished with error (stderr):\\n%v\", err)\n\n\t\tcase <-h.runCtx.Done():\n\t\t\treturn\n\t\t}\n\t}()\n\n\t// Start the fee service.\n\terr := h.feeService.Start()\n\trequire.NoError(h, err, \"failed to start fee service\")\n\n\t// Assemble the node manager with chainBackend and feeServiceURL.\n\th.manager.chainBackend = chain\n\th.manager.feeServiceURL = h.feeService.URL()\n\n\t// Assemble the miner.\n\th.Miner = miner\n}\n\n// ChainBackendName returns the chain backend name used in the test.",
      "length": 770,
      "tokens": 120,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) ChainBackendName() string {",
      "content": "func (h *HarnessTest) ChainBackendName() string {\n\treturn h.manager.chainBackend.Name()\n}\n\n// Context returns the run context used in this test. Usaually it should be\n// managed by the test itself otherwise undefined behaviors will occur. It can\n// be used, however, when a test needs to have its own context being managed\n// differently. In that case, instead of using a background context, the run\n// context should be used such that the test context scope can be fully\n// controlled.",
      "length": 428,
      "tokens": 74,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) Context() context.Context {",
      "content": "func (h *HarnessTest) Context() context.Context {\n\treturn h.runCtx\n}\n\n// SetUp starts the initial seeder nodes within the test harness. The initial\n// node's wallets will be funded wallets with 10x10 BTC outputs each.",
      "length": 163,
      "tokens": 28,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) SetupStandbyNodes() {",
      "content": "func (h *HarnessTest) SetupStandbyNodes() {\n\th.Log(\"Setting up standby nodes Alice and Bob...\")\n\tdefer h.Log(\"Finshed the setup, now running tests...\")\n\n\tlndArgs := []string{\n\t\t\"--default-remote-max-htlcs=483\",\n\t\t\"--dust-threshold=5000000\",\n\t}\n\t// Start the initial seeder nodes within the test network, then connect\n\t// their respective RPC clients.\n\th.Alice = h.NewNode(\"Alice\", lndArgs)\n\th.Bob = h.NewNode(\"Bob\", lndArgs)\n\n\taddrReq := &lnrpc.NewAddressRequest{\n\t\tType: lnrpc.AddressType_WITNESS_PUBKEY_HASH,\n\t}\n\n\tconst initialFund = 1 * btcutil.SatoshiPerBitcoin\n\n\t// Load up the wallets of the seeder nodes with 100 outputs of 1 BTC\n\t// each.\n\tnodes := []*node.HarnessNode{h.Alice, h.Bob}\n\tfor _, hn := range nodes {\n\t\th.manager.standbyNodes[hn.Cfg.NodeID] = hn\n\t\tfor i := 0; i < 100; i++ {\n\t\t\tresp := hn.RPC.NewAddress(addrReq)\n\n\t\t\taddr, err := btcutil.DecodeAddress(\n\t\t\t\tresp.Address, h.Miner.ActiveNet,\n\t\t\t)\n\t\t\trequire.NoError(h, err)\n\n\t\t\taddrScript, err := txscript.PayToAddrScript(addr)\n\t\t\trequire.NoError(h, err)\n\n\t\t\toutput := &wire.TxOut{\n\t\t\t\tPkScript: addrScript,\n\t\t\t\tValue:    initialFund,\n\t\t\t}\n\t\t\th.Miner.SendOutput(output, defaultMinerFeeRate)\n\t\t}\n\t}\n\n\t// We generate several blocks in order to give the outputs created\n\t// above a good number of confirmations.\n\tconst totalTxes = 200\n\th.MineBlocksAndAssertNumTxes(numBlocksSendOutput, totalTxes)\n\n\t// Now we want to wait for the nodes to catch up.\n\th.WaitForBlockchainSync(h.Alice)\n\th.WaitForBlockchainSync(h.Bob)\n\n\t// Now block until both wallets have fully synced up.\n\tconst expectedBalance = 100 * initialFund\n\terr := wait.NoError(func() error {\n\t\taliceResp := h.Alice.RPC.WalletBalance()\n\t\tbobResp := h.Bob.RPC.WalletBalance()\n\n\t\tif aliceResp.ConfirmedBalance != expectedBalance {\n\t\t\treturn fmt.Errorf(\"expected 10 BTC, instead \"+\n\t\t\t\t\"alice has %d\", aliceResp.ConfirmedBalance)\n\t\t}\n\n\t\tif bobResp.ConfirmedBalance != expectedBalance {\n\t\t\treturn fmt.Errorf(\"expected 10 BTC, instead \"+\n\t\t\t\t\"bob has %d\", bobResp.ConfirmedBalance)\n\t\t}\n\n\t\treturn nil\n\t}, DefaultTimeout)\n\trequire.NoError(h, err, \"timeout checking balance for node\")\n}\n\n// Stop stops the test harness.",
      "length": 2017,
      "tokens": 243,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) Stop() {",
      "content": "func (h *HarnessTest) Stop() {\n\t// Do nothing if it's not started.\n\tif h.runCtx == nil {\n\t\th.Log(\"HarnessTest is not started\")\n\t\treturn\n\t}\n\n\t// Stop all running nodes.\n\tfor _, node := range h.manager.activeNodes {\n\t\th.Shutdown(node)\n\t}\n\n\tclose(h.lndErrorChan)\n\n\t// Stop the fee service.\n\terr := h.feeService.Stop()\n\trequire.NoError(h, err, \"failed to stop fee service\")\n\n\t// Stop the chainBackend.\n\th.stopChainBackend()\n\n\t// Stop the miner.\n\th.Miner.Stop()\n}\n\n// RunTestCase executes a harness test case. Any errors or panics will be\n// represented as fatal.",
      "length": 502,
      "tokens": 76,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) RunTestCase(testCase *TestCase) {",
      "content": "func (h *HarnessTest) RunTestCase(testCase *TestCase) {\n\tdefer func() {\n\t\tif err := recover(); err != nil {\n\t\t\tdescription := errors.Wrap(err, 2).ErrorStack()\n\t\t\th.Fatalf(\"Failed: (%v) panic with: \\n%v\",\n\t\t\t\ttestCase.Name, description)\n\t\t}\n\t}()\n\n\ttestCase.TestFunc(h)\n}\n\n// resetStandbyNodes resets all standby nodes by attaching the new testing.T\n// and restarting them with the original config.",
      "length": 328,
      "tokens": 45,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) resetStandbyNodes(t *testing.T) {",
      "content": "func (h *HarnessTest) resetStandbyNodes(t *testing.T) {\n\tt.Helper()\n\n\tfor _, hn := range h.manager.standbyNodes {\n\t\t// Inherit the testing.T.\n\t\th.T = t\n\n\t\t// Reset the config so the node will be using the default\n\t\t// config for the coming test. This will also inherit the\n\t\t// test's running context.\n\t\th.RestartNodeWithExtraArgs(hn, hn.Cfg.OriginalExtraArgs)\n\n\t\t// Update the node's internal state.\n\t\thn.UpdateState()\n\t}\n}\n\n// Subtest creates a child HarnessTest, which inherits the harness net and\n// stand by nodes created by the parent test. It will return a cleanup function\n// which resets  all the standby nodes' configs back to its original state and\n// create snapshots of each nodes' internal state.",
      "length": 635,
      "tokens": 102,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) Subtest(t *testing.T) *HarnessTest {",
      "content": "func (h *HarnessTest) Subtest(t *testing.T) *HarnessTest {\n\tt.Helper()\n\n\tst := &HarnessTest{\n\t\tT:            t,\n\t\tmanager:      h.manager,\n\t\tMiner:        h.Miner,\n\t\tstandbyNodes: h.standbyNodes,\n\t\tfeeService:   h.feeService,\n\t\tlndErrorChan: make(chan error, lndErrorChanSize),\n\t}\n\n\t// Inherit context from the main test.\n\tst.runCtx, st.cancel = context.WithCancel(h.runCtx)\n\n\t// Inherit the subtest for the miner.\n\tst.Miner.T = st.T\n\n\t// Reset the standby nodes.\n\tst.resetStandbyNodes(t)\n\n\t// Reset fee estimator.\n\tst.SetFeeEstimate(DefaultFeeRateSatPerKw)\n\n\t// Record block height.\n\t_, startHeight := h.Miner.GetBestBlock()\n\n\tst.Cleanup(func() {\n\t\t_, endHeight := h.Miner.GetBestBlock()\n\n\t\tst.Logf(\"finished test: %s, start height=%d, end height=%d, \"+\n\t\t\t\"mined blocks=%d\", st.manager.currentTestCase,\n\t\t\tstartHeight, endHeight, endHeight-startHeight)\n\n\t\t// Don't bother run the cleanups if the test is failed.\n\t\tif st.Failed() {\n\t\t\tst.Log(\"test failed, skipped cleanup\")\n\t\t\tst.shutdownAllNodes()\n\t\t\treturn\n\t\t}\n\n\t\t// Don't run cleanup if it's already done. This can happen if\n\t\t// we have multiple level inheritance of the parent harness\n\t\t// test. For instance, a `Subtest(st)`.\n\t\tif st.cleaned {\n\t\t\tst.Log(\"test already cleaned, skipped cleanup\")\n\t\t\treturn\n\t\t}\n\n\t\t// When we finish the test, reset the nodes' configs and take a\n\t\t// snapshot of each of the nodes' internal states.\n\t\tfor _, node := range st.manager.standbyNodes {\n\t\t\tst.cleanupStandbyNode(node)\n\t\t}\n\n\t\t// If found running nodes, shut them down.\n\t\tst.shutdownNonStandbyNodes()\n\n\t\t// We require the mempool to be cleaned from the test.\n\t\trequire.Empty(st, st.Miner.GetRawMempool(), \"mempool not \"+\n\t\t\t\"cleaned, please mine blocks to clean them all.\")\n\n\t\t// Finally, cancel the run context. We have to do it here\n\t\t// because we need to keep the context alive for the above\n\t\t// assertions used in cleanup.\n\t\tst.cancel()\n\n\t\t// We now want to mark the parent harness as cleaned to avoid\n\t\t// running cleanup again since its internal state has been\n\t\t// cleaned up by its child harness tests.\n\t\th.cleaned = true\n\t})\n\n\treturn st\n}\n\n// shutdownNonStandbyNodes will shutdown any non-standby nodes.",
      "length": 2026,
      "tokens": 277,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) shutdownNonStandbyNodes() {",
      "content": "func (h *HarnessTest) shutdownNonStandbyNodes() {\n\th.shutdownNodes(true)\n}\n\n// shutdownAllNodes will shutdown all running nodes.",
      "length": 75,
      "tokens": 9,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) shutdownAllNodes() {",
      "content": "func (h *HarnessTest) shutdownAllNodes() {\n\th.shutdownNodes(false)\n}\n\n// shutdownNodes will shutdown any non-standby nodes. If skipStandby is false,\n// all the standby nodes will be shutdown too.",
      "length": 148,
      "tokens": 22,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) shutdownNodes(skipStandby bool) {",
      "content": "func (h *HarnessTest) shutdownNodes(skipStandby bool) {\n\tfor nid, node := range h.manager.activeNodes {\n\t\t// If it's a standby node, skip.\n\t\t_, ok := h.manager.standbyNodes[nid]\n\t\tif ok && skipStandby {\n\t\t\tcontinue\n\t\t}\n\n\t\t// The process may not be in a state to always shutdown\n\t\t// immediately, so we'll retry up to a hard limit to ensure we\n\t\t// eventually shutdown.\n\t\terr := wait.NoError(func() error {\n\t\t\treturn h.manager.shutdownNode(node)\n\t\t}, DefaultTimeout)\n\n\t\tif err == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Instead of returning the error, we will log it instead. This\n\t\t// is needed so other nodes can continue their shutdown\n\t\t// processes.\n\t\th.Logf(\"unable to shutdown %s, got err: %v\", node.Name(), err)\n\t}\n}\n\n// cleanupStandbyNode is a function should be called with defer whenever a\n// subtest is created. It will reset the standby nodes configs, snapshot the\n// states, and validate the node has a clean state.",
      "length": 833,
      "tokens": 139,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) cleanupStandbyNode(hn *node.HarnessNode) {",
      "content": "func (h *HarnessTest) cleanupStandbyNode(hn *node.HarnessNode) {\n\t// Remove connections made from this test.\n\th.removeConnectionns(hn)\n\n\t// Delete all payments made from this test.\n\thn.RPC.DeleteAllPayments()\n\n\t// Finally, check the node is in a clean state for the following tests.\n\th.validateNodeState(hn)\n}\n\n// removeConnectionns will remove all connections made on the standby nodes\n// expect the connections between Alice and Bob.",
      "length": 359,
      "tokens": 52,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) removeConnectionns(hn *node.HarnessNode) {",
      "content": "func (h *HarnessTest) removeConnectionns(hn *node.HarnessNode) {\n\tresp := hn.RPC.ListPeers()\n\tfor _, peer := range resp.Peers {\n\t\t// Skip disconnecting Alice and Bob.\n\t\tswitch peer.PubKey {\n\t\tcase h.Alice.PubKeyStr:\n\t\t\tcontinue\n\t\tcase h.Bob.PubKeyStr:\n\t\t\tcontinue\n\t\t}\n\n\t\thn.RPC.DisconnectPeer(peer.PubKey)\n\t}\n}\n\n// SetTestName set the test case name.",
      "length": 271,
      "tokens": 36,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) SetTestName(name string) {",
      "content": "func (h *HarnessTest) SetTestName(name string) {\n\th.manager.currentTestCase = name\n\n\t// Overwrite the old log filename so we can create new log files.\n\tfor _, node := range h.manager.standbyNodes {\n\t\tnode.Cfg.LogFilenamePrefix = name\n\t}\n}\n\n// NewNode creates a new node and asserts its creation. The node is guaranteed\n// to have finished its initialization and all its subservers are started.",
      "length": 335,
      "tokens": 54,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) NewNode(name string,",
      "content": "func (h *HarnessTest) NewNode(name string,\n\textraArgs []string) *node.HarnessNode {\n\n\tnode, err := h.manager.newNode(h.T, name, extraArgs, nil, false)\n\trequire.NoErrorf(h, err, \"unable to create new node for %s\", name)\n\n\t// Start the node.\n\terr = node.Start(h.runCtx)\n\trequire.NoError(h, err, \"failed to start node %s\", node.Name())\n\n\treturn node\n}\n\n// Shutdown shuts down the given node and asserts that no errors occur.",
      "length": 366,
      "tokens": 53,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) Shutdown(node *node.HarnessNode) {",
      "content": "func (h *HarnessTest) Shutdown(node *node.HarnessNode) {\n\t// The process may not be in a state to always shutdown immediately, so\n\t// we'll retry up to a hard limit to ensure we eventually shutdown.\n\terr := wait.NoError(func() error {\n\t\treturn h.manager.shutdownNode(node)\n\t}, DefaultTimeout)\n\n\trequire.NoErrorf(h, err, \"unable to shutdown %v\", node.Name())\n}\n\n// SuspendNode stops the given node and returns a callback that can be used to\n// start it again.",
      "length": 391,
      "tokens": 63,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) SuspendNode(node *node.HarnessNode) func() error {",
      "content": "func (h *HarnessTest) SuspendNode(node *node.HarnessNode) func() error {\n\terr := node.Stop()\n\trequire.NoErrorf(h, err, \"failed to stop %s\", node.Name())\n\n\t// Remove the node from active nodes.\n\tdelete(h.manager.activeNodes, node.Cfg.NodeID)\n\n\treturn func() error {\n\t\th.manager.registerNode(node)\n\n\t\tif err := node.Start(h.runCtx); err != nil {\n\t\t\treturn err\n\t\t}\n\t\th.WaitForBlockchainSync(node)\n\n\t\treturn nil\n\t}\n}\n\n// RestartNode restarts a given node, unlocks it and asserts it's successfully\n// started.",
      "length": 412,
      "tokens": 54,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) RestartNode(hn *node.HarnessNode) {",
      "content": "func (h *HarnessTest) RestartNode(hn *node.HarnessNode) {\n\terr := h.manager.restartNode(h.runCtx, hn, nil)\n\trequire.NoErrorf(h, err, \"failed to restart node %s\", hn.Name())\n\n\terr = h.manager.unlockNode(hn)\n\trequire.NoErrorf(h, err, \"failed to unlock node %s\", hn.Name())\n\n\tif !hn.Cfg.SkipUnlock {\n\t\t// Give the node some time to catch up with the chain before we\n\t\t// continue with the tests.\n\t\th.WaitForBlockchainSync(hn)\n\t}\n}\n\n// RestartNodeNoUnlock restarts a given node without unlocking its wallet.",
      "length": 432,
      "tokens": 59,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) RestartNodeNoUnlock(hn *node.HarnessNode) {",
      "content": "func (h *HarnessTest) RestartNodeNoUnlock(hn *node.HarnessNode) {\n\terr := h.manager.restartNode(h.runCtx, hn, nil)\n\trequire.NoErrorf(h, err, \"failed to restart node %s\", hn.Name())\n}\n\n// RestartNodeWithChanBackups restarts a given node with the specified channel\n// backups.",
      "length": 203,
      "tokens": 26,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) RestartNodeWithChanBackups(hn *node.HarnessNode,",
      "content": "func (h *HarnessTest) RestartNodeWithChanBackups(hn *node.HarnessNode,\n\tchanBackups ...*lnrpc.ChanBackupSnapshot) {\n\n\terr := h.manager.restartNode(h.runCtx, hn, nil)\n\trequire.NoErrorf(h, err, \"failed to restart node %s\", hn.Name())\n\n\terr = h.manager.unlockNode(hn, chanBackups...)\n\trequire.NoErrorf(h, err, \"failed to unlock node %s\", hn.Name())\n\n\t// Give the node some time to catch up with the chain before we\n\t// continue with the tests.\n\th.WaitForBlockchainSync(hn)\n}\n\n// RestartNodeWithExtraArgs updates the node's config and restarts it.",
      "length": 459,
      "tokens": 58,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) RestartNodeWithExtraArgs(hn *node.HarnessNode,",
      "content": "func (h *HarnessTest) RestartNodeWithExtraArgs(hn *node.HarnessNode,\n\textraArgs []string) {\n\n\thn.SetExtraArgs(extraArgs)\n\th.RestartNode(hn)\n}\n\n// NewNodeWithSeed fully initializes a new HarnessNode after creating a fresh\n// aezeed. The provided password is used as both the aezeed password and the\n// wallet password. The generated mnemonic is returned along with the\n// initialized harness node.",
      "length": 318,
      "tokens": 46,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) NewNodeWithSeed(name string,",
      "content": "func (h *HarnessTest) NewNodeWithSeed(name string,\n\textraArgs []string, password []byte,\n\tstatelessInit bool) (*node.HarnessNode, []string, []byte) {\n\n\t// Create a request to generate a new aezeed. The new seed will have\n\t// the same password as the internal wallet.\n\treq := &lnrpc.GenSeedRequest{\n\t\tAezeedPassphrase: password,\n\t\tSeedEntropy:      nil,\n\t}\n\n\treturn h.newNodeWithSeed(name, extraArgs, req, statelessInit)\n}\n\n// newNodeWithSeed creates and initializes a new HarnessNode such that it'll be\n// ready to accept RPC calls. A `GenSeedRequest` is needed to generate the\n// seed.",
      "length": 520,
      "tokens": 73,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) newNodeWithSeed(name string,",
      "content": "func (h *HarnessTest) newNodeWithSeed(name string,\n\textraArgs []string, req *lnrpc.GenSeedRequest,\n\tstatelessInit bool) (*node.HarnessNode, []string, []byte) {\n\n\tnode, err := h.manager.newNode(\n\t\th.T, name, extraArgs, req.AezeedPassphrase, true,\n\t)\n\trequire.NoErrorf(h, err, \"unable to create new node for %s\", name)\n\n\t// Start the node with seed only, which will only create the `State`\n\t// and `WalletUnlocker` clients.\n\terr = node.StartWithNoAuth(h.runCtx)\n\trequire.NoErrorf(h, err, \"failed to start node %s\", node.Name())\n\n\t// Generate a new seed.\n\tgenSeedResp := node.RPC.GenSeed(req)\n\n\t// With the seed created, construct the init request to the node,\n\t// including the newly generated seed.\n\tinitReq := &lnrpc.InitWalletRequest{\n\t\tWalletPassword:     req.AezeedPassphrase,\n\t\tCipherSeedMnemonic: genSeedResp.CipherSeedMnemonic,\n\t\tAezeedPassphrase:   req.AezeedPassphrase,\n\t\tStatelessInit:      statelessInit,\n\t}\n\n\t// Pass the init request via rpc to finish unlocking the node. This\n\t// will also initialize the macaroon-authenticated LightningClient.\n\tadminMac, err := h.manager.initWalletAndNode(node, initReq)\n\trequire.NoErrorf(h, err, \"failed to unlock and init node %s\",\n\t\tnode.Name())\n\n\t// In stateless initialization mode we get a macaroon back that we have\n\t// to return to the test, otherwise gRPC calls won't be possible since\n\t// there are no macaroon files created in that mode.\n\t// In stateful init the admin macaroon will just be nil.\n\treturn node, genSeedResp.CipherSeedMnemonic, adminMac\n}\n\n// RestoreNodeWithSeed fully initializes a HarnessNode using a chosen mnemonic,\n// password, recovery window, and optionally a set of static channel backups.\n// After providing the initialization request to unlock the node, this method\n// will finish initializing the LightningClient such that the HarnessNode can\n// be used for regular rpc operations.",
      "length": 1771,
      "tokens": 235,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) RestoreNodeWithSeed(name string, extraArgs []string,",
      "content": "func (h *HarnessTest) RestoreNodeWithSeed(name string, extraArgs []string,\n\tpassword []byte, mnemonic []string, rootKey string,\n\trecoveryWindow int32, chanBackups *lnrpc.ChanBackupSnapshot,\n\topts ...node.Option) *node.HarnessNode {\n\n\tnode, err := h.manager.newNode(h.T, name, extraArgs, password, true)\n\trequire.NoErrorf(h, err, \"unable to create new node for %s\", name)\n\n\t// Start the node with seed only, which will only create the `State`\n\t// and `WalletUnlocker` clients.\n\terr = node.StartWithNoAuth(h.runCtx)\n\trequire.NoErrorf(h, err, \"failed to start node %s\", node.Name())\n\n\t// Create the wallet.\n\tinitReq := &lnrpc.InitWalletRequest{\n\t\tWalletPassword:     password,\n\t\tCipherSeedMnemonic: mnemonic,\n\t\tAezeedPassphrase:   password,\n\t\tExtendedMasterKey:  rootKey,\n\t\tRecoveryWindow:     recoveryWindow,\n\t\tChannelBackups:     chanBackups,\n\t}\n\t_, err = h.manager.initWalletAndNode(node, initReq)\n\trequire.NoErrorf(h, err, \"failed to unlock and init node %s\",\n\t\tnode.Name())\n\n\treturn node\n}\n\n// NewNodeEtcd starts a new node with seed that'll use an external etcd\n// database as its storage. The passed cluster flag indicates that we'd like\n// the node to join the cluster leader election. We won't wait until RPC is\n// available (this is useful when the node is not expected to become the leader\n// right away).",
      "length": 1206,
      "tokens": 157,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) NewNodeEtcd(name string, etcdCfg *etcd.Config,",
      "content": "func (h *HarnessTest) NewNodeEtcd(name string, etcdCfg *etcd.Config,\n\tpassword []byte, cluster bool,\n\tleaderSessionTTL int) *node.HarnessNode {\n\n\t// We don't want to use the embedded etcd instance.\n\th.manager.dbBackend = node.BackendBbolt\n\n\textraArgs := node.ExtraArgsEtcd(\n\t\tetcdCfg, name, cluster, leaderSessionTTL,\n\t)\n\tnode, err := h.manager.newNode(h.T, name, extraArgs, password, true)\n\trequire.NoError(h, err, \"failed to create new node with etcd\")\n\n\t// Start the node daemon only.\n\terr = node.StartLndCmd(h.runCtx)\n\trequire.NoError(h, err, \"failed to start node %s\", node.Name())\n\n\treturn node\n}\n\n// NewNodeWithSeedEtcd starts a new node with seed that'll use an external etcd\n// database as its storage. The passed cluster flag indicates that we'd like\n// the node to join the cluster leader election.",
      "length": 719,
      "tokens": 101,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) NewNodeWithSeedEtcd(name string, etcdCfg *etcd.Config,",
      "content": "func (h *HarnessTest) NewNodeWithSeedEtcd(name string, etcdCfg *etcd.Config,\n\tpassword []byte, entropy []byte, statelessInit, cluster bool,\n\tleaderSessionTTL int) (*node.HarnessNode, []string, []byte) {\n\n\t// We don't want to use the embedded etcd instance.\n\th.manager.dbBackend = node.BackendBbolt\n\n\t// Create a request to generate a new aezeed. The new seed will have\n\t// the same password as the internal wallet.\n\treq := &lnrpc.GenSeedRequest{\n\t\tAezeedPassphrase: password,\n\t\tSeedEntropy:      nil,\n\t}\n\n\textraArgs := node.ExtraArgsEtcd(\n\t\tetcdCfg, name, cluster, leaderSessionTTL,\n\t)\n\n\treturn h.newNodeWithSeed(name, extraArgs, req, statelessInit)\n}\n\n// NewNodeRemoteSigner creates a new remote signer node and asserts its\n// creation.",
      "length": 639,
      "tokens": 83,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) NewNodeRemoteSigner(name string, extraArgs []string,",
      "content": "func (h *HarnessTest) NewNodeRemoteSigner(name string, extraArgs []string,\n\tpassword []byte, watchOnly *lnrpc.WatchOnly) *node.HarnessNode {\n\n\thn, err := h.manager.newNode(h.T, name, extraArgs, password, true)\n\trequire.NoErrorf(h, err, \"unable to create new node for %s\", name)\n\n\terr = hn.StartWithNoAuth(h.runCtx)\n\trequire.NoError(h, err, \"failed to start node %s\", name)\n\n\t// With the seed created, construct the init request to the node,\n\t// including the newly generated seed.\n\tinitReq := &lnrpc.InitWalletRequest{\n\t\tWalletPassword: password,\n\t\tWatchOnly:      watchOnly,\n\t}\n\n\t// Pass the init request via rpc to finish unlocking the node. This\n\t// will also initialize the macaroon-authenticated LightningClient.\n\t_, err = h.manager.initWalletAndNode(hn, initReq)\n\trequire.NoErrorf(h, err, \"failed to init node %s\", name)\n\n\treturn hn\n}\n\n// KillNode kills the node (but won't wait for the node process to stop).",
      "length": 817,
      "tokens": 111,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) KillNode(hn *node.HarnessNode) {",
      "content": "func (h *HarnessTest) KillNode(hn *node.HarnessNode) {\n\trequire.NoErrorf(h, hn.Kill(), \"%s: kill got error\", hn.Name())\n\tdelete(h.manager.activeNodes, hn.Cfg.NodeID)\n}\n\n// SetFeeEstimate sets a fee rate to be returned from fee estimator.\n//\n// NOTE: this method will set the fee rate for a conf target of 1, which is the\n// fallback fee rate for a `WebAPIEstimator` if a higher conf target's fee rate\n// is not set. This means if the fee rate for conf target 6 is set, the fee\n// estimator will use that value instead.",
      "length": 454,
      "tokens": 80,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) SetFeeEstimate(fee chainfee.SatPerKWeight) {",
      "content": "func (h *HarnessTest) SetFeeEstimate(fee chainfee.SatPerKWeight) {\n\th.feeService.SetFeeRate(fee, 1)\n}\n\n// SetFeeEstimateWithConf sets a fee rate of a specified conf target to be\n// returned from fee estimator.",
      "length": 138,
      "tokens": 21,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) SetFeeEstimateWithConf(",
      "content": "func (h *HarnessTest) SetFeeEstimateWithConf(\n\tfee chainfee.SatPerKWeight, conf uint32) {\n\n\th.feeService.SetFeeRate(fee, conf)\n}\n\n// validateNodeState checks that the node doesn't have any uncleaned states\n// which will affect its following tests.",
      "length": 195,
      "tokens": 26,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) validateNodeState(hn *node.HarnessNode) {",
      "content": "func (h *HarnessTest) validateNodeState(hn *node.HarnessNode) {\n\terrStr := func(subject string) string {\n\t\treturn fmt.Sprintf(\"%s: found %s channels, please close \"+\n\t\t\t\"them properly\", hn.Name(), subject)\n\t}\n\t// If the node still has open channels, it's most likely that the\n\t// current test didn't close it properly.\n\trequire.Zerof(h, hn.State.OpenChannel.Active, errStr(\"active\"))\n\trequire.Zerof(h, hn.State.OpenChannel.Public, errStr(\"public\"))\n\trequire.Zerof(h, hn.State.OpenChannel.Private, errStr(\"private\"))\n\trequire.Zerof(h, hn.State.OpenChannel.Pending, errStr(\"pending open\"))\n\n\t// The number of pending force close channels should be zero.\n\trequire.Zerof(h, hn.State.CloseChannel.PendingForceClose,\n\t\terrStr(\"pending force\"))\n\n\t// The number of waiting close channels should be zero.\n\trequire.Zerof(h, hn.State.CloseChannel.WaitingClose,\n\t\terrStr(\"waiting close\"))\n\n\t// Ths number of payments should be zero.\n\t// TODO(yy): no need to check since it's deleted in the cleanup? Or\n\t// check it in a wait?\n\trequire.Zerof(h, hn.State.Payment.Total, \"%s: found \"+\n\t\t\"uncleaned payments, please delete all of them properly\",\n\t\thn.Name())\n}\n\n// GetChanPointFundingTxid takes a channel point and converts it into a chain\n// hash.",
      "length": 1140,
      "tokens": 137,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) GetChanPointFundingTxid(",
      "content": "func (h *HarnessTest) GetChanPointFundingTxid(\n\tcp *lnrpc.ChannelPoint) *chainhash.Hash {\n\n\ttxid, err := lnrpc.GetChanPointFundingTxid(cp)\n\trequire.NoError(h, err, \"unable to get txid\")\n\n\treturn txid\n}\n\n// OutPointFromChannelPoint creates an outpoint from a given channel point.",
      "length": 223,
      "tokens": 27,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) OutPointFromChannelPoint(",
      "content": "func (h *HarnessTest) OutPointFromChannelPoint(\n\tcp *lnrpc.ChannelPoint) wire.OutPoint {\n\n\ttxid := h.GetChanPointFundingTxid(cp)\n\treturn wire.OutPoint{\n\t\tHash:  *txid,\n\t\tIndex: cp.OutputIndex,\n\t}\n}\n\n// OpenChannelParams houses the params to specify when opening a new channel.",
      "length": 219,
      "tokens": 27,
      "embedding": []
    },
    {
      "slug": "type OpenChannelParams struct {",
      "content": "type OpenChannelParams struct {\n\t// Amt is the local amount being put into the channel.\n\tAmt btcutil.Amount\n\n\t// PushAmt is the amount that should be pushed to the remote when the\n\t// channel is opened.\n\tPushAmt btcutil.Amount\n\n\t// Private is a boolan indicating whether the opened channel should be\n\t// private.\n\tPrivate bool\n\n\t// SpendUnconfirmed is a boolean indicating whether we can utilize\n\t// unconfirmed outputs to fund the channel.\n\tSpendUnconfirmed bool\n\n\t// MinHtlc is the htlc_minimum_msat value set when opening the channel.\n\tMinHtlc lnwire.MilliSatoshi\n\n\t// RemoteMaxHtlcs is the remote_max_htlcs value set when opening the\n\t// channel, restricting the number of concurrent HTLCs the remote party\n\t// can add to a commitment.\n\tRemoteMaxHtlcs uint16\n\n\t// FundingShim is an optional funding shim that the caller can specify\n\t// in order to modify the channel funding workflow.\n\tFundingShim *lnrpc.FundingShim\n\n\t// SatPerVByte is the amount of satoshis to spend in chain fees per\n\t// virtual byte of the transaction.\n\tSatPerVByte btcutil.Amount\n\n\t// CommitmentType is the commitment type that should be used for the\n\t// channel to be opened.\n\tCommitmentType lnrpc.CommitmentType\n\n\t// ZeroConf is used to determine if the channel will be a zero-conf\n\t// channel. This only works if the explicit negotiation is used with\n\t// anchors or script enforced leases.\n\tZeroConf bool\n\n\t// ScidAlias denotes whether the channel will be an option-scid-alias\n\t// channel type negotiation.\n\tScidAlias bool\n\n\t// BaseFee is the channel base fee applied during the channel\n\t// announcement phase.\n\tBaseFee uint64\n\n\t// FeeRate is the channel fee rate in ppm applied during the channel\n\t// announcement phase.\n\tFeeRate uint64\n\n\t// UseBaseFee, if set, instructs the downstream logic to apply the\n\t// user-specified channel base fee to the channel update announcement.\n\t// If set to false it avoids applying a base fee of 0 and instead\n\t// activates the default configured base fee.\n\tUseBaseFee bool\n\n\t// UseFeeRate, if set, instructs the downstream logic to apply the\n\t// user-specified channel fee rate to the channel update announcement.\n\t// If set to false it avoids applying a fee rate of 0 and instead\n\t// activates the default configured fee rate.\n\tUseFeeRate bool\n}\n\n// prepareOpenChannel waits for both nodes to be synced to chain and returns an\n// OpenChannelRequest.",
      "length": 2268,
      "tokens": 363,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) prepareOpenChannel(srcNode, destNode *node.HarnessNode,",
      "content": "func (h *HarnessTest) prepareOpenChannel(srcNode, destNode *node.HarnessNode,\n\tp OpenChannelParams) *lnrpc.OpenChannelRequest {\n\n\t// Wait until srcNode and destNode have the latest chain synced.\n\t// Otherwise, we may run into a check within the funding manager that\n\t// prevents any funding workflows from being kicked off if the chain\n\t// isn't yet synced.\n\th.WaitForBlockchainSync(srcNode)\n\th.WaitForBlockchainSync(destNode)\n\n\t// Specify the minimal confirmations of the UTXOs used for channel\n\t// funding.\n\tminConfs := int32(1)\n\tif p.SpendUnconfirmed {\n\t\tminConfs = 0\n\t}\n\n\t// Prepare the request.\n\treturn &lnrpc.OpenChannelRequest{\n\t\tNodePubkey:         destNode.PubKey[:],\n\t\tLocalFundingAmount: int64(p.Amt),\n\t\tPushSat:            int64(p.PushAmt),\n\t\tPrivate:            p.Private,\n\t\tMinConfs:           minConfs,\n\t\tSpendUnconfirmed:   p.SpendUnconfirmed,\n\t\tMinHtlcMsat:        int64(p.MinHtlc),\n\t\tRemoteMaxHtlcs:     uint32(p.RemoteMaxHtlcs),\n\t\tFundingShim:        p.FundingShim,\n\t\tSatPerByte:         int64(p.SatPerVByte),\n\t\tCommitmentType:     p.CommitmentType,\n\t\tZeroConf:           p.ZeroConf,\n\t\tScidAlias:          p.ScidAlias,\n\t\tBaseFee:            p.BaseFee,\n\t\tFeeRate:            p.FeeRate,\n\t\tUseBaseFee:         p.UseBaseFee,\n\t\tUseFeeRate:         p.UseFeeRate,\n\t}\n}\n\n// OpenChannelAssertPending attempts to open a channel between srcNode and\n// destNode with the passed channel funding parameters. Once the `OpenChannel`\n// is called, it will consume the first event it receives from the open channel\n// client and asserts it's a channel pending event.",
      "length": 1448,
      "tokens": 156,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) openChannelAssertPending(srcNode,",
      "content": "func (h *HarnessTest) openChannelAssertPending(srcNode,\n\tdestNode *node.HarnessNode,\n\tp OpenChannelParams) (*lnrpc.PendingUpdate, rpc.OpenChanClient) {\n\n\t// Prepare the request and open the channel.\n\topenReq := h.prepareOpenChannel(srcNode, destNode, p)\n\trespStream := srcNode.RPC.OpenChannel(openReq)\n\n\t// Consume the \"channel pending\" update. This waits until the node\n\t// notifies us that the final message in the channel funding workflow\n\t// has been sent to the remote node.\n\tresp := h.ReceiveOpenChannelUpdate(respStream)\n\n\t// Check that the update is channel pending.\n\tupdate, ok := resp.Update.(*lnrpc.OpenStatusUpdate_ChanPending)\n\trequire.Truef(h, ok, \"expected channel pending: update, instead got %v\",\n\t\tresp)\n\n\treturn update.ChanPending, respStream\n}\n\n// OpenChannelAssertPending attempts to open a channel between srcNode and\n// destNode with the passed channel funding parameters. Once the `OpenChannel`\n// is called, it will consume the first event it receives from the open channel\n// client and asserts it's a channel pending event. It returns the\n// `PendingUpdate`.",
      "length": 1005,
      "tokens": 133,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) OpenChannelAssertPending(srcNode,",
      "content": "func (h *HarnessTest) OpenChannelAssertPending(srcNode,\n\tdestNode *node.HarnessNode, p OpenChannelParams) *lnrpc.PendingUpdate {\n\n\tresp, _ := h.openChannelAssertPending(srcNode, destNode, p)\n\treturn resp\n}\n\n// OpenChannelAssertStream attempts to open a channel between srcNode and\n// destNode with the passed channel funding parameters. Once the `OpenChannel`\n// is called, it will consume the first event it receives from the open channel\n// client and asserts it's a channel pending event. It returns the open channel\n// stream.",
      "length": 464,
      "tokens": 67,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) OpenChannelAssertStream(srcNode,",
      "content": "func (h *HarnessTest) OpenChannelAssertStream(srcNode,\n\tdestNode *node.HarnessNode, p OpenChannelParams) rpc.OpenChanClient {\n\n\t_, stream := h.openChannelAssertPending(srcNode, destNode, p)\n\treturn stream\n}\n\n// OpenChannel attempts to open a channel with the specified parameters\n// extended from Alice to Bob. Additionally, for public channels, it will mine\n// extra blocks so they are announced to the network. In specific, the\n// following items are asserted,\n//   - for non-zero conf channel, 1 blocks will be mined to confirm the funding\n//     tx.\n//   - both nodes should see the channel edge update in their network graph.\n//   - both nodes can report the status of the new channel from ListChannels.\n//   - extra blocks are mined if it's a public channel.",
      "length": 695,
      "tokens": 113,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) OpenChannel(alice, bob *node.HarnessNode,",
      "content": "func (h *HarnessTest) OpenChannel(alice, bob *node.HarnessNode,\n\tp OpenChannelParams) *lnrpc.ChannelPoint {\n\n\t// First, open the channel without announcing it.\n\tcp := h.OpenChannelNoAnnounce(alice, bob, p)\n\n\t// If this is a private channel, there's no need to mine extra blocks\n\t// since it will never be announced to the network.\n\tif p.Private {\n\t\treturn cp\n\t}\n\n\t// Mine extra blocks to announce the channel.\n\tif p.ZeroConf {\n\t\t// For a zero-conf channel, no blocks have been mined so we\n\t\t// need to mine 6 blocks.\n\t\t//\n\t\t// Mine 1 block to confirm the funding transaction.\n\t\th.MineBlocksAndAssertNumTxes(numBlocksOpenChannel, 1)\n\t} else {\n\t\t// For a regular channel, 1 block has already been mined to\n\t\t// confirm the funding transaction, so we mine 5 blocks.\n\t\th.MineBlocks(numBlocksOpenChannel - 1)\n\t}\n\n\treturn cp\n}\n\n// OpenChannelNoAnnounce attempts to open a channel with the specified\n// parameters extended from Alice to Bob without mining the necessary blocks to\n// announce the channel. Additionally, the following items are asserted,\n//   - for non-zero conf channel, 1 blocks will be mined to confirm the funding\n//     tx.\n//   - both nodes should see the channel edge update in their network graph.\n//   - both nodes can report the status of the new channel from ListChannels.",
      "length": 1194,
      "tokens": 198,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) OpenChannelNoAnnounce(alice, bob *node.HarnessNode,",
      "content": "func (h *HarnessTest) OpenChannelNoAnnounce(alice, bob *node.HarnessNode,\n\tp OpenChannelParams) *lnrpc.ChannelPoint {\n\n\tchanOpenUpdate := h.OpenChannelAssertStream(alice, bob, p)\n\n\t// Open a zero conf channel.\n\tif p.ZeroConf {\n\t\treturn h.openChannelZeroConf(alice, bob, chanOpenUpdate)\n\t}\n\n\t// Open a non-zero conf channel.\n\treturn h.openChannel(alice, bob, chanOpenUpdate)\n}\n\n// openChannel attempts to open a channel with the specified parameters\n// extended from Alice to Bob. Additionally, the following items are asserted,\n//   - 1 block is mined and the funding transaction should be found in it.\n//   - both nodes should see the channel edge update in their network graph.\n//   - both nodes can report the status of the new channel from ListChannels.",
      "length": 666,
      "tokens": 100,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) openChannel(alice, bob *node.HarnessNode,",
      "content": "func (h *HarnessTest) openChannel(alice, bob *node.HarnessNode,\n\tstream rpc.OpenChanClient) *lnrpc.ChannelPoint {\n\n\t// Mine 1 block to confirm the funding transaction.\n\tblock := h.MineBlocksAndAssertNumTxes(1, 1)[0]\n\n\t// Wait for the channel open event.\n\tfundingChanPoint := h.WaitForChannelOpenEvent(stream)\n\n\t// Check that the funding tx is found in the first block.\n\tfundingTxID := h.GetChanPointFundingTxid(fundingChanPoint)\n\th.Miner.AssertTxInBlock(block, fundingTxID)\n\n\t// Check that both alice and bob have seen the channel from their\n\t// network topology.\n\th.AssertTopologyChannelOpen(alice, fundingChanPoint)\n\th.AssertTopologyChannelOpen(bob, fundingChanPoint)\n\n\t// Check that the channel can be seen in their ListChannels.\n\th.AssertChannelExists(alice, fundingChanPoint)\n\th.AssertChannelExists(bob, fundingChanPoint)\n\n\treturn fundingChanPoint\n}\n\n// openChannelZeroConf attempts to open a channel with the specified parameters\n// extended from Alice to Bob. Additionally, the following items are asserted,\n//   - both nodes should see the channel edge update in their network graph.\n//   - both nodes can report the status of the new channel from ListChannels.",
      "length": 1078,
      "tokens": 133,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) openChannelZeroConf(alice, bob *node.HarnessNode,",
      "content": "func (h *HarnessTest) openChannelZeroConf(alice, bob *node.HarnessNode,\n\tstream rpc.OpenChanClient) *lnrpc.ChannelPoint {\n\n\t// Wait for the channel open event.\n\tfundingChanPoint := h.WaitForChannelOpenEvent(stream)\n\n\t// Check that both alice and bob have seen the channel from their\n\t// network topology.\n\th.AssertTopologyChannelOpen(alice, fundingChanPoint)\n\th.AssertTopologyChannelOpen(bob, fundingChanPoint)\n\n\t// Finally, check that the channel can be seen in their ListChannels.\n\th.AssertChannelExists(alice, fundingChanPoint)\n\th.AssertChannelExists(bob, fundingChanPoint)\n\n\treturn fundingChanPoint\n}\n\n// OpenChannelAssertErr opens a channel between node srcNode and destNode,\n// asserts that the expected error is returned from the channel opening.",
      "length": 663,
      "tokens": 75,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) OpenChannelAssertErr(srcNode, destNode *node.HarnessNode,",
      "content": "func (h *HarnessTest) OpenChannelAssertErr(srcNode, destNode *node.HarnessNode,\n\tp OpenChannelParams, expectedErr error) {\n\n\t// Prepare the request and open the channel.\n\topenReq := h.prepareOpenChannel(srcNode, destNode, p)\n\trespStream := srcNode.RPC.OpenChannel(openReq)\n\n\t// Receive an error to be sent from the stream.\n\t_, err := h.receiveOpenChannelUpdate(respStream)\n\n\t// Use string comparison here as we haven't codified all the RPC errors\n\t// yet.\n\trequire.Containsf(h, err.Error(), expectedErr.Error(), \"unexpected \"+\n\t\t\"error returned, want %v, got %v\", expectedErr, err)\n}\n\n// CloseChannelAssertPending attempts to close the channel indicated by the\n// passed channel point, initiated by the passed node. Once the CloseChannel\n// rpc is called, it will consume one event and assert it's a close pending\n// event. In addition, it will check that the closing tx can be found in the\n// mempool.",
      "length": 803,
      "tokens": 119,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) CloseChannelAssertPending(hn *node.HarnessNode,",
      "content": "func (h *HarnessTest) CloseChannelAssertPending(hn *node.HarnessNode,\n\tcp *lnrpc.ChannelPoint,\n\tforce bool) (rpc.CloseChanClient, *chainhash.Hash) {\n\n\t// Calls the rpc to close the channel.\n\tcloseReq := &lnrpc.CloseChannelRequest{\n\t\tChannelPoint: cp,\n\t\tForce:        force,\n\t}\n\tstream := hn.RPC.CloseChannel(closeReq)\n\n\t// Consume the \"channel close\" update in order to wait for the closing\n\t// transaction to be broadcast, then wait for the closing tx to be seen\n\t// within the network.\n\tevent, err := h.ReceiveCloseChannelUpdate(stream)\n\tif err != nil {\n\t\t// TODO(yy): remove the sleep once the following bug is fixed.\n\t\t// We may receive the error `cannot co-op close channel with\n\t\t// active htlcs` or `link failed to shutdown` if we close the\n\t\t// channel. We need to investigate the order of settling the\n\t\t// payments and updating commitments to properly fix it.\n\t\ttime.Sleep(2 * time.Second)\n\n\t\t// Give it another chance.\n\t\tstream = hn.RPC.CloseChannel(closeReq)\n\t\tevent, err = h.ReceiveCloseChannelUpdate(stream)\n\t\trequire.NoError(h, err)\n\t}\n\n\tpendingClose, ok := event.Update.(*lnrpc.CloseStatusUpdate_ClosePending)\n\trequire.Truef(h, ok, \"expected channel close update, instead got %v\",\n\t\tpendingClose)\n\n\tcloseTxid, err := chainhash.NewHash(pendingClose.ClosePending.Txid)\n\trequire.NoErrorf(h, err, \"unable to decode closeTxid: %v\",\n\t\tpendingClose.ClosePending.Txid)\n\n\t// Assert the closing tx is in the mempool.\n\th.Miner.AssertTxInMempool(closeTxid)\n\n\treturn stream, closeTxid\n}\n\n// CloseChannel attempts to coop close a non-anchored channel identified by the\n// passed channel point owned by the passed harness node. The following items\n// are asserted,\n//  1. a close pending event is sent from the close channel client.\n//  2. the closing tx is found in the mempool.\n//  3. the node reports the channel being waiting to close.\n//  4. a block is mined and the closing tx should be found in it.\n//  5. the node reports zero waiting close channels.\n//  6. the node receives a topology update regarding the channel close.",
      "length": 1911,
      "tokens": 276,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) CloseChannel(hn *node.HarnessNode,",
      "content": "func (h *HarnessTest) CloseChannel(hn *node.HarnessNode,\n\tcp *lnrpc.ChannelPoint) *chainhash.Hash {\n\n\tstream, _ := h.CloseChannelAssertPending(hn, cp, false)\n\n\treturn h.AssertStreamChannelCoopClosed(hn, cp, false, stream)\n}\n\n// ForceCloseChannel attempts to force close a non-anchored channel identified\n// by the passed channel point owned by the passed harness node. The following\n// items are asserted,\n//  1. a close pending event is sent from the close channel client.\n//  2. the closing tx is found in the mempool.\n//  3. the node reports the channel being waiting to close.\n//  4. a block is mined and the closing tx should be found in it.\n//  5. the node reports zero waiting close channels.\n//  6. the node receives a topology update regarding the channel close.\n//  7. mine DefaultCSV-1 blocks.\n//  8. the node reports zero pending force close channels.",
      "length": 789,
      "tokens": 129,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) ForceCloseChannel(hn *node.HarnessNode,",
      "content": "func (h *HarnessTest) ForceCloseChannel(hn *node.HarnessNode,\n\tcp *lnrpc.ChannelPoint) *chainhash.Hash {\n\n\tstream, _ := h.CloseChannelAssertPending(hn, cp, true)\n\n\tclosingTxid := h.AssertStreamChannelForceClosed(hn, cp, false, stream)\n\n\t// Cleanup the force close.\n\th.CleanupForceClose(hn, cp)\n\n\treturn closingTxid\n}\n\n// CloseChannelAssertErr closes the given channel and asserts an error\n// returned.",
      "length": 326,
      "tokens": 38,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) CloseChannelAssertErr(hn *node.HarnessNode,",
      "content": "func (h *HarnessTest) CloseChannelAssertErr(hn *node.HarnessNode,\n\tcp *lnrpc.ChannelPoint, force bool) error {\n\n\t// Calls the rpc to close the channel.\n\tcloseReq := &lnrpc.CloseChannelRequest{\n\t\tChannelPoint: cp,\n\t\tForce:        force,\n\t}\n\tstream := hn.RPC.CloseChannel(closeReq)\n\n\t// Consume the \"channel close\" update in order to wait for the closing\n\t// transaction to be broadcast, then wait for the closing tx to be seen\n\t// within the network.\n\t_, err := h.ReceiveCloseChannelUpdate(stream)\n\trequire.Errorf(h, err, \"%s: expect close channel to return an error\",\n\t\thn.Name())\n\n\treturn err\n}\n\n// IsNeutrinoBackend returns a bool indicating whether the node is using a\n// neutrino as its backend. This is useful when we want to skip certain tests\n// which cannot be done with a neutrino backend.",
      "length": 711,
      "tokens": 110,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) IsNeutrinoBackend() bool {",
      "content": "func (h *HarnessTest) IsNeutrinoBackend() bool {\n\treturn h.manager.chainBackend.Name() == NeutrinoBackendName\n}\n\n// fundCoins attempts to send amt satoshis from the internal mining node to the\n// targeted lightning node. The confirmed boolean indicates whether the\n// transaction that pays to the target should confirm. For neutrino backend,\n// the `confirmed` param is ignored.",
      "length": 323,
      "tokens": 47,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) fundCoins(amt btcutil.Amount, target *node.HarnessNode,",
      "content": "func (h *HarnessTest) fundCoins(amt btcutil.Amount, target *node.HarnessNode,\n\taddrType lnrpc.AddressType, confirmed bool) {\n\n\tinitialBalance := target.RPC.WalletBalance()\n\n\t// First, obtain an address from the target lightning node, preferring\n\t// to receive a p2wkh address s.t the output can immediately be used as\n\t// an input to a funding transaction.\n\treq := &lnrpc.NewAddressRequest{Type: addrType}\n\tresp := target.RPC.NewAddress(req)\n\taddr := h.DecodeAddress(resp.Address)\n\taddrScript := h.PayToAddrScript(addr)\n\n\t// Generate a transaction which creates an output to the target\n\t// pkScript of the desired amount.\n\toutput := &wire.TxOut{\n\t\tPkScript: addrScript,\n\t\tValue:    int64(amt),\n\t}\n\th.Miner.SendOutput(output, defaultMinerFeeRate)\n\n\t// Encode the pkScript in hex as this the format that it will be\n\t// returned via rpc.\n\texpPkScriptStr := hex.EncodeToString(addrScript)\n\n\t// Now, wait for ListUnspent to show the unconfirmed transaction\n\t// containing the correct pkscript.\n\t//\n\t// Since neutrino doesn't support unconfirmed outputs, skip this check.\n\tif !h.IsNeutrinoBackend() {\n\t\tutxos := h.AssertNumUTXOsUnconfirmed(target, 1)\n\n\t\t// Assert that the lone unconfirmed utxo contains the same\n\t\t// pkscript as the output generated above.\n\t\tpkScriptStr := utxos[0].PkScript\n\t\trequire.Equal(h, pkScriptStr, expPkScriptStr,\n\t\t\t\"pkscript mismatch\")\n\t}\n\n\t// If the transaction should remain unconfirmed, then we'll wait until\n\t// the target node's unconfirmed balance reflects the expected balance\n\t// and exit.\n\tif !confirmed && !h.IsNeutrinoBackend() {\n\t\texpectedBalance := btcutil.Amount(\n\t\t\tinitialBalance.UnconfirmedBalance,\n\t\t) + amt\n\t\th.WaitForBalanceUnconfirmed(target, expectedBalance)\n\n\t\treturn\n\t}\n\n\t// Otherwise, we'll generate 1 new blocks to ensure the output gains a\n\t// sufficient number of confirmations and wait for the balance to\n\t// reflect what's expected.\n\th.MineBlocks(1)\n\n\texpectedBalance := btcutil.Amount(initialBalance.ConfirmedBalance) + amt\n\th.WaitForBalanceConfirmed(target, expectedBalance)\n}\n\n// FundCoins attempts to send amt satoshis from the internal mining node to the\n// targeted lightning node using a P2WKH address. 2 blocks are mined after in\n// order to confirm the transaction.",
      "length": 2088,
      "tokens": 271,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) FundCoins(amt btcutil.Amount, hn *node.HarnessNode) {",
      "content": "func (h *HarnessTest) FundCoins(amt btcutil.Amount, hn *node.HarnessNode) {\n\th.fundCoins(amt, hn, lnrpc.AddressType_WITNESS_PUBKEY_HASH, true)\n}\n\n// FundCoinsUnconfirmed attempts to send amt satoshis from the internal mining\n// node to the targeted lightning node using a P2WKH address. No blocks are\n// mined after and the UTXOs are unconfirmed.",
      "length": 265,
      "tokens": 38,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) FundCoinsUnconfirmed(amt btcutil.Amount,",
      "content": "func (h *HarnessTest) FundCoinsUnconfirmed(amt btcutil.Amount,\n\thn *node.HarnessNode) {\n\n\th.fundCoins(amt, hn, lnrpc.AddressType_WITNESS_PUBKEY_HASH, false)\n}\n\n// FundCoinsNP2WKH attempts to send amt satoshis from the internal mining node\n// to the targeted lightning node using a NP2WKH address.",
      "length": 227,
      "tokens": 30,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) FundCoinsNP2WKH(amt btcutil.Amount,",
      "content": "func (h *HarnessTest) FundCoinsNP2WKH(amt btcutil.Amount,\n\ttarget *node.HarnessNode) {\n\n\th.fundCoins(amt, target, lnrpc.AddressType_NESTED_PUBKEY_HASH, true)\n}\n\n// FundCoinsP2TR attempts to send amt satoshis from the internal mining node to\n// the targeted lightning node using a P2TR address.",
      "length": 229,
      "tokens": 30,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) FundCoinsP2TR(amt btcutil.Amount,",
      "content": "func (h *HarnessTest) FundCoinsP2TR(amt btcutil.Amount,\n\ttarget *node.HarnessNode) {\n\n\th.fundCoins(amt, target, lnrpc.AddressType_TAPROOT_PUBKEY, true)\n}\n\n// completePaymentRequestsAssertStatus sends payments from a node to complete\n// all payment requests. This function does not return until all payments\n// have reached the specified status.",
      "length": 281,
      "tokens": 35,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) completePaymentRequestsAssertStatus(hn *node.HarnessNode,",
      "content": "func (h *HarnessTest) completePaymentRequestsAssertStatus(hn *node.HarnessNode,\n\tpaymentRequests []string, status lnrpc.Payment_PaymentStatus) {\n\n\t// Create a buffered chan to signal the results.\n\tresults := make(chan rpc.PaymentClient, len(paymentRequests))\n\n\t// send sends a payment and asserts if it doesn't succeeded.\n\tsend := func(payReq string) {\n\t\treq := &routerrpc.SendPaymentRequest{\n\t\t\tPaymentRequest: payReq,\n\t\t\tTimeoutSeconds: int32(wait.PaymentTimeout.Seconds()),\n\t\t\tFeeLimitMsat:   noFeeLimitMsat,\n\t\t}\n\t\tstream := hn.RPC.SendPayment(req)\n\n\t\t// Signal sent succeeded.\n\t\tresults <- stream\n\t}\n\n\t// Launch all payments simultaneously.\n\tfor _, payReq := range paymentRequests {\n\t\tpayReqCopy := payReq\n\t\tgo send(payReqCopy)\n\t}\n\n\t// Wait for all payments to report the expected status.\n\ttimer := time.After(wait.PaymentTimeout)\n\tselect {\n\tcase stream := <-results:\n\t\th.AssertPaymentStatusFromStream(stream, status)\n\n\tcase <-timer:\n\t\trequire.Fail(h, \"timeout\", \"waiting payment results timeout\")\n\t}\n}\n\n// CompletePaymentRequests sends payments from a node to complete all payment\n// requests. This function does not return until all payments successfully\n// complete without errors.",
      "length": 1071,
      "tokens": 131,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) CompletePaymentRequests(hn *node.HarnessNode,",
      "content": "func (h *HarnessTest) CompletePaymentRequests(hn *node.HarnessNode,\n\tpaymentRequests []string) {\n\n\th.completePaymentRequestsAssertStatus(\n\t\thn, paymentRequests, lnrpc.Payment_SUCCEEDED,\n\t)\n}\n\n// CompletePaymentRequestsNoWait sends payments from a node to complete all\n// payment requests without waiting for the results. Instead, it checks the\n// number of updates in the specified channel has increased.",
      "length": 327,
      "tokens": 41,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) CompletePaymentRequestsNoWait(hn *node.HarnessNode,",
      "content": "func (h *HarnessTest) CompletePaymentRequestsNoWait(hn *node.HarnessNode,\n\tpaymentRequests []string, chanPoint *lnrpc.ChannelPoint) {\n\n\t// We start by getting the current state of the client's channels. This\n\t// is needed to ensure the payments actually have been committed before\n\t// we return.\n\toldResp := h.GetChannelByChanPoint(hn, chanPoint)\n\n\t// Send payments and assert they are in-flight.\n\th.completePaymentRequestsAssertStatus(\n\t\thn, paymentRequests, lnrpc.Payment_IN_FLIGHT,\n\t)\n\n\t// We are not waiting for feedback in the form of a response, but we\n\t// should still wait long enough for the server to receive and handle\n\t// the send before cancelling the request. We wait for the number of\n\t// updates to one of our channels has increased before we return.\n\terr := wait.NoError(func() error {\n\t\tnewResp := h.GetChannelByChanPoint(hn, chanPoint)\n\n\t\t// If this channel has an increased number of updates, we\n\t\t// assume the payments are committed, and we can return.\n\t\tif newResp.NumUpdates > oldResp.NumUpdates {\n\t\t\treturn nil\n\t\t}\n\n\t\t// Otherwise return an error as the NumUpdates are not\n\t\t// increased.\n\t\treturn fmt.Errorf(\"%s: channel:%v not updated after sending \"+\n\t\t\t\"payments, old updates: %v, new updates: %v\", hn.Name(),\n\t\t\tchanPoint, oldResp.NumUpdates, newResp.NumUpdates)\n\t}, DefaultTimeout)\n\trequire.NoError(h, err, \"timeout while checking for channel updates\")\n}\n\n// OpenChannelPsbt attempts to open a channel between srcNode and destNode with\n// the passed channel funding parameters. It will assert if the expected step\n// of funding the PSBT is not received from the source node.",
      "length": 1495,
      "tokens": 220,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) OpenChannelPsbt(srcNode, destNode *node.HarnessNode,",
      "content": "func (h *HarnessTest) OpenChannelPsbt(srcNode, destNode *node.HarnessNode,\n\tp OpenChannelParams) (rpc.OpenChanClient, []byte) {\n\n\t// Wait until srcNode and destNode have the latest chain synced.\n\t// Otherwise, we may run into a check within the funding manager that\n\t// prevents any funding workflows from being kicked off if the chain\n\t// isn't yet synced.\n\th.WaitForBlockchainSync(srcNode)\n\th.WaitForBlockchainSync(destNode)\n\n\t// Send the request to open a channel to the source node now. This will\n\t// open a long-lived stream where we'll receive status updates about\n\t// the progress of the channel.\n\t// respStream := h.OpenChannelStreamAndAssert(srcNode, destNode, p)\n\treq := &lnrpc.OpenChannelRequest{\n\t\tNodePubkey:         destNode.PubKey[:],\n\t\tLocalFundingAmount: int64(p.Amt),\n\t\tPushSat:            int64(p.PushAmt),\n\t\tPrivate:            p.Private,\n\t\tSpendUnconfirmed:   p.SpendUnconfirmed,\n\t\tMinHtlcMsat:        int64(p.MinHtlc),\n\t\tFundingShim:        p.FundingShim,\n\t}\n\trespStream := srcNode.RPC.OpenChannel(req)\n\n\t// Consume the \"PSBT funding ready\" update. This waits until the node\n\t// notifies us that the PSBT can now be funded.\n\tresp := h.ReceiveOpenChannelUpdate(respStream)\n\tupd, ok := resp.Update.(*lnrpc.OpenStatusUpdate_PsbtFund)\n\trequire.Truef(h, ok, \"expected PSBT funding update, got %v\", resp)\n\n\treturn respStream, upd.PsbtFund.Psbt\n}\n\n// CleanupForceClose mines a force close commitment found in the mempool and\n// the following sweep transaction from the force closing node.",
      "length": 1394,
      "tokens": 170,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) CleanupForceClose(hn *node.HarnessNode,",
      "content": "func (h *HarnessTest) CleanupForceClose(hn *node.HarnessNode,\n\tchanPoint *lnrpc.ChannelPoint) {\n\n\t// Wait for the channel to be marked pending force close.\n\th.AssertNumPendingForceClose(hn, 1)\n\n\t// Mine enough blocks for the node to sweep its funds from the force\n\t// closed channel.\n\t//\n\t// The commit sweep resolver is able to broadcast the sweep tx up to\n\t// one block before the CSV elapses, so wait until defaulCSV-1.\n\th.MineBlocks(node.DefaultCSV - 1)\n\n\t// The node should now sweep the funds, clean up by mining the sweeping\n\t// tx.\n\th.MineBlocksAndAssertNumTxes(1, 1)\n\n\t// Mine blocks to get any second level HTLC resolved. If there are no\n\t// HTLCs, this will behave like h.AssertNumPendingCloseChannels.\n\th.mineTillForceCloseResolved(hn)\n}\n\n// mineTillForceCloseResolved asserts that the number of pending close channels\n// are zero. Each time it checks, a new block is mined using MineBlocksSlow to\n// give the node some time to catch up the chain.\n//\n// NOTE: this method is a workaround to make sure we have a clean mempool at\n// the end of a channel force closure. We cannot directly mine blocks and\n// assert channels being fully closed because the subsystems in lnd don't share\n// the same block height. This is especially the case when blocks are produced\n// too fast.\n// TODO(yy): remove this workaround when syncing blocks are unified in all the\n// subsystems.",
      "length": 1286,
      "tokens": 215,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) mineTillForceCloseResolved(hn *node.HarnessNode) {",
      "content": "func (h *HarnessTest) mineTillForceCloseResolved(hn *node.HarnessNode) {\n\t_, startHeight := h.Miner.GetBestBlock()\n\n\terr := wait.NoError(func() error {\n\t\tresp := hn.RPC.PendingChannels()\n\t\ttotal := len(resp.PendingForceClosingChannels)\n\t\tif total != 0 {\n\t\t\th.MineBlocks(1)\n\n\t\t\treturn fmt.Errorf(\"expected num of pending force \" +\n\t\t\t\t\"close channel to be zero\")\n\t\t}\n\n\t\t_, height := h.Miner.GetBestBlock()\n\t\th.Logf(\"Mined %d blocks while waiting for force closed \"+\n\t\t\t\"channel to be resolved\", height-startHeight)\n\n\t\treturn nil\n\t}, DefaultTimeout)\n\n\trequire.NoErrorf(h, err, \"assert force close resolved timeout\")\n}\n\n// CreatePayReqs is a helper method that will create a slice of payment\n// requests for the given node.",
      "length": 624,
      "tokens": 84,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) CreatePayReqs(hn *node.HarnessNode,",
      "content": "func (h *HarnessTest) CreatePayReqs(hn *node.HarnessNode,\n\tpaymentAmt btcutil.Amount, numInvoices int) ([]string,\n\t[][]byte, []*lnrpc.Invoice) {\n\n\tpayReqs := make([]string, numInvoices)\n\trHashes := make([][]byte, numInvoices)\n\tinvoices := make([]*lnrpc.Invoice, numInvoices)\n\tfor i := 0; i < numInvoices; i++ {\n\t\tpreimage := h.Random32Bytes()\n\n\t\tinvoice := &lnrpc.Invoice{\n\t\t\tMemo:      \"testing\",\n\t\t\tRPreimage: preimage,\n\t\t\tValue:     int64(paymentAmt),\n\t\t}\n\t\tresp := hn.RPC.AddInvoice(invoice)\n\n\t\t// Set the payment address in the invoice so the caller can\n\t\t// properly use it.\n\t\tinvoice.PaymentAddr = resp.PaymentAddr\n\n\t\tpayReqs[i] = resp.PaymentRequest\n\t\trHashes[i] = resp.RHash\n\t\tinvoices[i] = invoice\n\t}\n\n\treturn payReqs, rHashes, invoices\n}\n\n// BackupDB creates a backup of the current database. It will stop the node\n// first, copy the database files, and restart the node.",
      "length": 795,
      "tokens": 103,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) BackupDB(hn *node.HarnessNode) {",
      "content": "func (h *HarnessTest) BackupDB(hn *node.HarnessNode) {\n\trestart := h.SuspendNode(hn)\n\n\terr := hn.BackupDB()\n\trequire.NoErrorf(h, err, \"%s: failed to backup db\", hn.Name())\n\n\terr = restart()\n\trequire.NoErrorf(h, err, \"%s: failed to restart\", hn.Name())\n}\n\n// RestartNodeAndRestoreDB restarts a given node with a callback to restore the\n// db.",
      "length": 276,
      "tokens": 39,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) RestartNodeAndRestoreDB(hn *node.HarnessNode) {",
      "content": "func (h *HarnessTest) RestartNodeAndRestoreDB(hn *node.HarnessNode) {\n\tcb := func() error { return hn.RestoreDB() }\n\terr := h.manager.restartNode(h.runCtx, hn, cb)\n\trequire.NoErrorf(h, err, \"failed to restart node %s\", hn.Name())\n\n\terr = h.manager.unlockNode(hn)\n\trequire.NoErrorf(h, err, \"failed to unlock node %s\", hn.Name())\n\n\t// Give the node some time to catch up with the chain before we\n\t// continue with the tests.\n\th.WaitForBlockchainSync(hn)\n}\n\n// MineBlocks mines blocks and asserts all active nodes have synced to the\n// chain.\n//\n// NOTE: this differs from miner's `MineBlocks` as it requires the nodes to be\n// synced.",
      "length": 546,
      "tokens": 85,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) MineBlocks(num uint32) []*wire.MsgBlock {",
      "content": "func (h *HarnessTest) MineBlocks(num uint32) []*wire.MsgBlock {\n\t// Mining the blocks slow to give `lnd` more time to sync.\n\tblocks := h.Miner.MineBlocksSlow(num)\n\n\t// Make sure all the active nodes are synced.\n\tbestBlock := blocks[len(blocks)-1]\n\th.AssertActiveNodesSyncedTo(bestBlock)\n\n\treturn blocks\n}\n\n// MineBlocksAndAssertNumTxes mines blocks and asserts the number of\n// transactions are found in the first block. It also asserts all active nodes\n// have synced to the chain.\n//\n// NOTE: this differs from miner's `MineBlocks` as it requires the nodes to be\n// synced.",
      "length": 496,
      "tokens": 77,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) MineBlocksAndAssertNumTxes(num uint32,",
      "content": "func (h *HarnessTest) MineBlocksAndAssertNumTxes(num uint32,\n\tnumTxs int) []*wire.MsgBlock {\n\n\t// If we expect transactions to be included in the blocks we'll mine,\n\t// we wait here until they are seen in the miner's mempool.\n\ttxids := h.Miner.AssertNumTxsInMempool(numTxs)\n\n\t// Mine blocks.\n\tblocks := h.Miner.MineBlocksSlow(num)\n\n\t// Assert that all the transactions were included in the first block.\n\tfor _, txid := range txids {\n\t\th.Miner.AssertTxInBlock(blocks[0], txid)\n\t}\n\n\t// Finally, make sure all the active nodes are synced.\n\tbestBlock := blocks[len(blocks)-1]\n\th.AssertActiveNodesSyncedTo(bestBlock)\n\n\treturn blocks\n}\n\n// MineEmptyBlocks mines a given number of empty blocks.\n//\n// NOTE: this differs from miner's `MineEmptyBlocks` as it requires the nodes\n// to be synced.",
      "length": 700,
      "tokens": 103,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) MineEmptyBlocks(num int) []*wire.MsgBlock {",
      "content": "func (h *HarnessTest) MineEmptyBlocks(num int) []*wire.MsgBlock {\n\tblocks := h.Miner.MineEmptyBlocks(num)\n\n\t// Finally, make sure all the active nodes are synced.\n\th.AssertActiveNodesSynced()\n\n\treturn blocks\n}\n\n// QueryChannelByChanPoint tries to find a channel matching the channel point\n// and asserts. It returns the channel found.",
      "length": 259,
      "tokens": 36,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) QueryChannelByChanPoint(hn *node.HarnessNode,",
      "content": "func (h *HarnessTest) QueryChannelByChanPoint(hn *node.HarnessNode,\n\tchanPoint *lnrpc.ChannelPoint,\n\topts ...ListChannelOption) *lnrpc.Channel {\n\n\tchannel, err := h.findChannel(hn, chanPoint, opts...)\n\trequire.NoError(h, err, \"failed to query channel\")\n\n\treturn channel\n}\n\n// SendPaymentAndAssertStatus sends a payment from the passed node and asserts\n// the desired status is reached.",
      "length": 307,
      "tokens": 38,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) SendPaymentAndAssertStatus(hn *node.HarnessNode,",
      "content": "func (h *HarnessTest) SendPaymentAndAssertStatus(hn *node.HarnessNode,\n\treq *routerrpc.SendPaymentRequest,\n\tstatus lnrpc.Payment_PaymentStatus) *lnrpc.Payment {\n\n\tstream := hn.RPC.SendPayment(req)\n\treturn h.AssertPaymentStatusFromStream(stream, status)\n}\n\n// SendPaymentAssertFail sends a payment from the passed node and asserts the\n// payment is failed with the specified failure reason .",
      "length": 311,
      "tokens": 35,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) SendPaymentAssertFail(hn *node.HarnessNode,",
      "content": "func (h *HarnessTest) SendPaymentAssertFail(hn *node.HarnessNode,\n\treq *routerrpc.SendPaymentRequest,\n\treason lnrpc.PaymentFailureReason) *lnrpc.Payment {\n\n\tpayment := h.SendPaymentAndAssertStatus(hn, req, lnrpc.Payment_FAILED)\n\trequire.Equal(h, reason, payment.FailureReason,\n\t\t\"payment failureReason not matched\")\n\n\treturn payment\n}\n\n// SendPaymentAssertSettled sends a payment from the passed node and asserts the\n// payment is settled.",
      "length": 362,
      "tokens": 37,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) SendPaymentAssertSettled(hn *node.HarnessNode,",
      "content": "func (h *HarnessTest) SendPaymentAssertSettled(hn *node.HarnessNode,\n\treq *routerrpc.SendPaymentRequest) *lnrpc.Payment {\n\n\treturn h.SendPaymentAndAssertStatus(hn, req, lnrpc.Payment_SUCCEEDED)\n}\n\n// OpenChannelRequest is used to open a channel using the method\n// OpenMultiChannelsAsync.",
      "length": 213,
      "tokens": 22,
      "embedding": []
    },
    {
      "slug": "type OpenChannelRequest struct {",
      "content": "type OpenChannelRequest struct {\n\t// Local is the funding node.\n\tLocal *node.HarnessNode\n\n\t// Remote is the receiving node.\n\tRemote *node.HarnessNode\n\n\t// Param is the open channel params.\n\tParam OpenChannelParams\n\n\t// stream is the client created after calling OpenChannel RPC.\n\tstream rpc.OpenChanClient\n\n\t// result is a channel used to send the channel point once the funding\n\t// has succeeded.\n\tresult chan *lnrpc.ChannelPoint\n}\n\n// OpenMultiChannelsAsync takes a list of OpenChannelRequest and opens them in\n// batch. The channel points are returned in same the order of the requests\n// once all of the channel open succeeded.\n//\n// NOTE: compared to open multiple channel sequentially, this method will be\n// faster as it doesn't need to mine 6 blocks for each channel open. However,\n// it does make debugging the logs more difficult as messages are intertwined.",
      "length": 812,
      "tokens": 132,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) OpenMultiChannelsAsync(",
      "content": "func (h *HarnessTest) OpenMultiChannelsAsync(\n\treqs []*OpenChannelRequest) []*lnrpc.ChannelPoint {\n\n\t// openChannel opens a channel based on the request.\n\topenChannel := func(req *OpenChannelRequest) {\n\t\tstream := h.OpenChannelAssertStream(\n\t\t\treq.Local, req.Remote, req.Param,\n\t\t)\n\t\treq.stream = stream\n\t}\n\n\t// assertChannelOpen is a helper closure that asserts a channel is\n\t// open.\n\tassertChannelOpen := func(req *OpenChannelRequest) {\n\t\t// Wait for the channel open event from the stream.\n\t\tcp := h.WaitForChannelOpenEvent(req.stream)\n\n\t\t// Check that both alice and bob have seen the channel\n\t\t// from their channel watch request.\n\t\th.AssertTopologyChannelOpen(req.Local, cp)\n\t\th.AssertTopologyChannelOpen(req.Remote, cp)\n\n\t\t// Finally, check that the channel can be seen in their\n\t\t// ListChannels.\n\t\th.AssertChannelExists(req.Local, cp)\n\t\th.AssertChannelExists(req.Remote, cp)\n\n\t\treq.result <- cp\n\t}\n\n\t// Go through the requests and make the OpenChannel RPC call.\n\tfor _, r := range reqs {\n\t\topenChannel(r)\n\t}\n\n\t// Mine one block to confirm all the funding transactions.\n\th.MineBlocksAndAssertNumTxes(1, len(reqs))\n\n\t// Mine 5 more blocks so all the public channels are announced to the\n\t// network.\n\th.MineBlocks(numBlocksOpenChannel - 1)\n\n\t// Once the blocks are mined, we fire goroutines for each of the\n\t// request to watch for the channel openning.\n\tfor _, r := range reqs {\n\t\tr.result = make(chan *lnrpc.ChannelPoint, 1)\n\t\tgo assertChannelOpen(r)\n\t}\n\n\t// Finally, collect the results.\n\tchannelPoints := make([]*lnrpc.ChannelPoint, 0)\n\tfor _, r := range reqs {\n\t\tselect {\n\t\tcase cp := <-r.result:\n\t\t\tchannelPoints = append(channelPoints, cp)\n\n\t\tcase <-time.After(wait.ChannelOpenTimeout):\n\t\t\trequire.Failf(h, \"timeout\", \"wait channel point \"+\n\t\t\t\t\"timeout for channel %s=>%s\", r.Local.Name(),\n\t\t\t\tr.Remote.Name())\n\t\t}\n\t}\n\n\t// Assert that we have the expected num of channel points.\n\trequire.Len(h, channelPoints, len(reqs),\n\t\t\"returned channel points not match\")\n\n\treturn channelPoints\n}\n\n// ReceiveInvoiceUpdate waits until a message is received on the subscribe\n// invoice stream or the timeout is reached.",
      "length": 2005,
      "tokens": 272,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) ReceiveInvoiceUpdate(",
      "content": "func (h *HarnessTest) ReceiveInvoiceUpdate(\n\tstream rpc.InvoiceUpdateClient) *lnrpc.Invoice {\n\n\tchanMsg := make(chan *lnrpc.Invoice)\n\terrChan := make(chan error)\n\tgo func() {\n\t\t// Consume one message. This will block until the message is\n\t\t// received.\n\t\tresp, err := stream.Recv()\n\t\tif err != nil {\n\t\t\terrChan <- err\n\t\t\treturn\n\t\t}\n\t\tchanMsg <- resp\n\t}()\n\n\tselect {\n\tcase <-time.After(DefaultTimeout):\n\t\trequire.Fail(h, \"timeout\", \"timeout receiving invoice update\")\n\n\tcase err := <-errChan:\n\t\trequire.Failf(h, \"err from stream\",\n\t\t\t\"received err from stream: %v\", err)\n\n\tcase updateMsg := <-chanMsg:\n\t\treturn updateMsg\n\t}\n\n\treturn nil\n}\n\n// CalculateTxFee retrieves parent transactions and reconstructs the fee paid.",
      "length": 643,
      "tokens": 90,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) CalculateTxFee(tx *wire.MsgTx) btcutil.Amount {",
      "content": "func (h *HarnessTest) CalculateTxFee(tx *wire.MsgTx) btcutil.Amount {\n\tvar balance btcutil.Amount\n\tfor _, in := range tx.TxIn {\n\t\tparentHash := in.PreviousOutPoint.Hash\n\t\trawTx := h.Miner.GetRawTransaction(&parentHash)\n\t\tparent := rawTx.MsgTx()\n\t\tbalance += btcutil.Amount(\n\t\t\tparent.TxOut[in.PreviousOutPoint.Index].Value,\n\t\t)\n\t}\n\n\tfor _, out := range tx.TxOut {\n\t\tbalance -= btcutil.Amount(out.Value)\n\t}\n\n\treturn balance\n}\n\n// CalculateTxesFeeRate takes a list of transactions and estimates the fee rate\n// used to sweep them.\n//\n// NOTE: only used in current test file.",
      "length": 482,
      "tokens": 65,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) CalculateTxesFeeRate(txns []*wire.MsgTx) int64 {",
      "content": "func (h *HarnessTest) CalculateTxesFeeRate(txns []*wire.MsgTx) int64 {\n\tconst scale = 1000\n\n\tvar totalWeight, totalFee int64\n\tfor _, tx := range txns {\n\t\tutx := btcutil.NewTx(tx)\n\t\ttotalWeight += blockchain.GetTransactionWeight(utx)\n\n\t\tfee := h.CalculateTxFee(tx)\n\t\ttotalFee += int64(fee)\n\t}\n\tfeeRate := totalFee * scale / totalWeight\n\n\treturn feeRate\n}\n",
      "length": 269,
      "tokens": 38,
      "embedding": []
    },
    {
      "slug": "type SweptOutput struct {",
      "content": "type SweptOutput struct {\n\tOutPoint wire.OutPoint\n\tSweepTx  *wire.MsgTx\n}\n\n// FindCommitAndAnchor looks for a commitment sweep and anchor sweep in the\n// mempool. Our anchor output is identified by having multiple inputs in its\n// sweep transition, because we have to bring another input to add fees to the\n// anchor. Note that the anchor swept output may be nil if the channel did not\n// have anchors.",
      "length": 368,
      "tokens": 64,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) FindCommitAndAnchor(sweepTxns []*wire.MsgTx,",
      "content": "func (h *HarnessTest) FindCommitAndAnchor(sweepTxns []*wire.MsgTx,\n\tcloseTx string) (*SweptOutput, *SweptOutput) {\n\n\tvar commitSweep, anchorSweep *SweptOutput\n\n\tfor _, tx := range sweepTxns {\n\t\ttxHash := tx.TxHash()\n\t\tsweepTx := h.Miner.GetRawTransaction(&txHash)\n\n\t\t// We expect our commitment sweep to have a single input, and,\n\t\t// our anchor sweep to have more inputs (because the wallet\n\t\t// needs to add balance to the anchor amount). We find their\n\t\t// sweep txids here to setup appropriate resolutions. We also\n\t\t// need to find the outpoint for our resolution, which we do by\n\t\t// matching the inputs to the sweep to the close transaction.\n\t\tinputs := sweepTx.MsgTx().TxIn\n\t\tif len(inputs) == 1 {\n\t\t\tcommitSweep = &SweptOutput{\n\t\t\t\tOutPoint: inputs[0].PreviousOutPoint,\n\t\t\t\tSweepTx:  tx,\n\t\t\t}\n\t\t} else {\n\t\t\t// Since we have more than one input, we run through\n\t\t\t// them to find the one whose previous outpoint matches\n\t\t\t// the closing txid, which means this input is spending\n\t\t\t// the close tx. This will be our anchor output.\n\t\t\tfor _, txin := range inputs {\n\t\t\t\top := txin.PreviousOutPoint.Hash.String()\n\t\t\t\tif op == closeTx {\n\t\t\t\t\tanchorSweep = &SweptOutput{\n\t\t\t\t\t\tOutPoint: txin.PreviousOutPoint,\n\t\t\t\t\t\tSweepTx:  tx,\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn commitSweep, anchorSweep\n}\n\n// AssertSweepFound looks up a sweep in a nodes list of broadcast sweeps and\n// asserts it's found.\n//\n// NOTE: Does not account for node's internal state.",
      "length": 1340,
      "tokens": 210,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) AssertSweepFound(hn *node.HarnessNode,",
      "content": "func (h *HarnessTest) AssertSweepFound(hn *node.HarnessNode,\n\tsweep string, verbose bool) {\n\n\t// List all sweeps that alice's node had broadcast.\n\tsweepResp := hn.RPC.ListSweeps(verbose)\n\n\tvar found bool\n\tif verbose {\n\t\tfound = findSweepInDetails(h, sweep, sweepResp)\n\t} else {\n\t\tfound = findSweepInTxids(h, sweep, sweepResp)\n\t}\n\n\trequire.Truef(h, found, \"%s: sweep: %v not found\", sweep, hn.Name())\n}\n",
      "length": 327,
      "tokens": 47,
      "embedding": []
    },
    {
      "slug": "func findSweepInTxids(ht *HarnessTest, sweepTxid string,",
      "content": "func findSweepInTxids(ht *HarnessTest, sweepTxid string,\n\tsweepResp *walletrpc.ListSweepsResponse) bool {\n\n\tsweepTxIDs := sweepResp.GetTransactionIds()\n\trequire.NotNil(ht, sweepTxIDs, \"expected transaction ids\")\n\trequire.Nil(ht, sweepResp.GetTransactionDetails())\n\n\t// Check that the sweep tx we have just produced is present.\n\tfor _, tx := range sweepTxIDs.TransactionIds {\n\t\tif tx == sweepTxid {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n",
      "length": 365,
      "tokens": 45,
      "embedding": []
    },
    {
      "slug": "func findSweepInDetails(ht *HarnessTest, sweepTxid string,",
      "content": "func findSweepInDetails(ht *HarnessTest, sweepTxid string,\n\tsweepResp *walletrpc.ListSweepsResponse) bool {\n\n\tsweepDetails := sweepResp.GetTransactionDetails()\n\trequire.NotNil(ht, sweepDetails, \"expected transaction details\")\n\trequire.Nil(ht, sweepResp.GetTransactionIds())\n\n\tfor _, tx := range sweepDetails.Transactions {\n\t\tif tx.TxHash == sweepTxid {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\n// ConnectMiner connects the miner with the chain backend in the network.",
      "length": 392,
      "tokens": 45,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) ConnectMiner() {",
      "content": "func (h *HarnessTest) ConnectMiner() {\n\terr := h.manager.chainBackend.ConnectMiner()\n\trequire.NoError(h, err, \"failed to connect miner\")\n}\n\n// DisconnectMiner removes the connection between the miner and the chain\n// backend in the network.",
      "length": 196,
      "tokens": 26,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) DisconnectMiner() {",
      "content": "func (h *HarnessTest) DisconnectMiner() {\n\terr := h.manager.chainBackend.DisconnectMiner()\n\trequire.NoError(h, err, \"failed to disconnect miner\")\n}\n\n// QueryRoutesAndRetry attempts to keep querying a route until timeout is\n// reached.\n//\n// NOTE: when a channel is opened, we may need to query multiple times to get\n// it in our QueryRoutes RPC. This happens even after we check the channel is\n// heard by the node using ht.AssertChannelOpen. Deep down, this is because our\n// GraphTopologySubscription and QueryRoutes give different results regarding a\n// specific channel, with the formal reporting it being open while the latter\n// not, resulting GraphTopologySubscription acting \"faster\" than QueryRoutes.\n// TODO(yy): make sure related subsystems share the same view on a given\n// channel.",
      "length": 738,
      "tokens": 113,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) QueryRoutesAndRetry(hn *node.HarnessNode,",
      "content": "func (h *HarnessTest) QueryRoutesAndRetry(hn *node.HarnessNode,\n\treq *lnrpc.QueryRoutesRequest) *lnrpc.QueryRoutesResponse {\n\n\tvar routes *lnrpc.QueryRoutesResponse\n\terr := wait.NoError(func() error {\n\t\tctxt, cancel := context.WithCancel(h.runCtx)\n\t\tdefer cancel()\n\n\t\tresp, err := hn.RPC.LN.QueryRoutes(ctxt, req)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"%s: failed to query route: %w\",\n\t\t\t\thn.Name(), err)\n\t\t}\n\n\t\troutes = resp\n\n\t\treturn nil\n\t}, DefaultTimeout)\n\n\trequire.NoError(h, err, \"timeout querying routes\")\n\n\treturn routes\n}\n\n// ReceiveHtlcInterceptor waits until a message is received on the htlc\n// interceptor stream or the timeout is reached.",
      "length": 565,
      "tokens": 72,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) ReceiveHtlcInterceptor(",
      "content": "func (h *HarnessTest) ReceiveHtlcInterceptor(\n\tstream rpc.InterceptorClient) *routerrpc.ForwardHtlcInterceptRequest {\n\n\tchanMsg := make(chan *routerrpc.ForwardHtlcInterceptRequest)\n\terrChan := make(chan error)\n\tgo func() {\n\t\t// Consume one message. This will block until the message is\n\t\t// received.\n\t\tresp, err := stream.Recv()\n\t\tif err != nil {\n\t\t\terrChan <- err\n\t\t\treturn\n\t\t}\n\t\tchanMsg <- resp\n\t}()\n\n\tselect {\n\tcase <-time.After(DefaultTimeout):\n\t\trequire.Fail(h, \"timeout\", \"timeout intercepting htlc\")\n\n\tcase err := <-errChan:\n\t\trequire.Failf(h, \"err from stream\",\n\t\t\t\"received err from stream: %v\", err)\n\n\tcase updateMsg := <-chanMsg:\n\t\treturn updateMsg\n\t}\n\n\treturn nil\n}\n\n// ReceiveChannelEvent waits until a message is received from the\n// ChannelEventsClient stream or the timeout is reached.",
      "length": 725,
      "tokens": 97,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) ReceiveChannelEvent(",
      "content": "func (h *HarnessTest) ReceiveChannelEvent(\n\tstream rpc.ChannelEventsClient) *lnrpc.ChannelEventUpdate {\n\n\tchanMsg := make(chan *lnrpc.ChannelEventUpdate)\n\terrChan := make(chan error)\n\tgo func() {\n\t\t// Consume one message. This will block until the message is\n\t\t// received.\n\t\tresp, err := stream.Recv()\n\t\tif err != nil {\n\t\t\terrChan <- err\n\t\t\treturn\n\t\t}\n\t\tchanMsg <- resp\n\t}()\n\n\tselect {\n\tcase <-time.After(DefaultTimeout):\n\t\trequire.Fail(h, \"timeout\", \"timeout intercepting htlc\")\n\n\tcase err := <-errChan:\n\t\trequire.Failf(h, \"err from stream\",\n\t\t\t\"received err from stream: %v\", err)\n\n\tcase updateMsg := <-chanMsg:\n\t\treturn updateMsg\n\t}\n\n\treturn nil\n}\n\n// GetOutputIndex returns the output index of the given address in the given\n// transaction.",
      "length": 671,
      "tokens": 94,
      "embedding": []
    },
    {
      "slug": "func (h *HarnessTest) GetOutputIndex(txid *chainhash.Hash, addr string) int {",
      "content": "func (h *HarnessTest) GetOutputIndex(txid *chainhash.Hash, addr string) int {\n\t// We'll then extract the raw transaction from the mempool in order to\n\t// determine the index of the p2tr output.\n\ttx := h.Miner.GetRawTransaction(txid)\n\n\tp2trOutputIndex := -1\n\tfor i, txOut := range tx.MsgTx().TxOut {\n\t\t_, addrs, _, err := txscript.ExtractPkScriptAddrs(\n\t\t\ttxOut.PkScript, h.Miner.ActiveNet,\n\t\t)\n\t\trequire.NoError(h, err)\n\n\t\tif addrs[0].String() == addr {\n\t\t\tp2trOutputIndex = i\n\t\t}\n\t}\n\trequire.Greater(h, p2trOutputIndex, -1)\n\n\treturn p2trOutputIndex\n}\n",
      "length": 455,
      "tokens": 61,
      "embedding": []
    }
  ]
}