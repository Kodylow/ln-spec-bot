{
  "filepath": "../implementations/go/lnd/routing/router.go",
  "package": "routing",
  "sections": [
    {
      "slug": "type ChannelGraphSource interface {",
      "content": "type ChannelGraphSource interface {\n\t// AddNode is used to add information about a node to the router\n\t// database. If the node with this pubkey is not present in an existing\n\t// channel, it will be ignored.\n\tAddNode(node *channeldb.LightningNode, op ...batch.SchedulerOption) error\n\n\t// AddEdge is used to add edge/channel to the topology of the router,\n\t// after all information about channel will be gathered this\n\t// edge/channel might be used in construction of payment path.\n\tAddEdge(edge *channeldb.ChannelEdgeInfo, op ...batch.SchedulerOption) error\n\n\t// AddProof updates the channel edge info with proof which is needed to\n\t// properly announce the edge to the rest of the network.\n\tAddProof(chanID lnwire.ShortChannelID, proof *channeldb.ChannelAuthProof) error\n\n\t// UpdateEdge is used to update edge information, without this message\n\t// edge considered as not fully constructed.\n\tUpdateEdge(policy *channeldb.ChannelEdgePolicy, op ...batch.SchedulerOption) error\n\n\t// IsStaleNode returns true if the graph source has a node announcement\n\t// for the target node with a more recent timestamp. This method will\n\t// also return true if we don't have an active channel announcement for\n\t// the target node.\n\tIsStaleNode(node route.Vertex, timestamp time.Time) bool\n\n\t// IsPublicNode determines whether the given vertex is seen as a public\n\t// node in the graph from the graph's source node's point of view.\n\tIsPublicNode(node route.Vertex) (bool, error)\n\n\t// IsKnownEdge returns true if the graph source already knows of the\n\t// passed channel ID either as a live or zombie edge.\n\tIsKnownEdge(chanID lnwire.ShortChannelID) bool\n\n\t// IsStaleEdgePolicy returns true if the graph source has a channel\n\t// edge for the passed channel ID (and flags) that have a more recent\n\t// timestamp.\n\tIsStaleEdgePolicy(chanID lnwire.ShortChannelID, timestamp time.Time,\n\t\tflags lnwire.ChanUpdateChanFlags) bool\n\n\t// MarkEdgeLive clears an edge from our zombie index, deeming it as\n\t// live.\n\tMarkEdgeLive(chanID lnwire.ShortChannelID) error\n\n\t// ForAllOutgoingChannels is used to iterate over all channels\n\t// emanating from the \"source\" node which is the center of the\n\t// star-graph.\n\tForAllOutgoingChannels(cb func(tx kvdb.RTx,\n\t\tc *channeldb.ChannelEdgeInfo,\n\t\te *channeldb.ChannelEdgePolicy) error) error\n\n\t// CurrentBlockHeight returns the block height from POV of the router\n\t// subsystem.\n\tCurrentBlockHeight() (uint32, error)\n\n\t// GetChannelByID return the channel by the channel id.\n\tGetChannelByID(chanID lnwire.ShortChannelID) (*channeldb.ChannelEdgeInfo,\n\t\t*channeldb.ChannelEdgePolicy, *channeldb.ChannelEdgePolicy, error)\n\n\t// FetchLightningNode attempts to look up a target node by its identity\n\t// public key. channeldb.ErrGraphNodeNotFound is returned if the node\n\t// doesn't exist within the graph.\n\tFetchLightningNode(route.Vertex) (*channeldb.LightningNode, error)\n\n\t// ForEachNode is used to iterate over every node in the known graph.\n\tForEachNode(func(node *channeldb.LightningNode) error) error\n}\n\n// PaymentAttemptDispatcher is used by the router to send payment attempts onto\n// the network, and receive their results.",
      "length": 3033,
      "tokens": 411,
      "embedding": []
    },
    {
      "slug": "type PaymentAttemptDispatcher interface {",
      "content": "type PaymentAttemptDispatcher interface {\n\t// SendHTLC is a function that directs a link-layer switch to\n\t// forward a fully encoded payment to the first hop in the route\n\t// denoted by its public key. A non-nil error is to be returned if the\n\t// payment was unsuccessful.\n\tSendHTLC(firstHop lnwire.ShortChannelID,\n\t\tattemptID uint64,\n\t\thtlcAdd *lnwire.UpdateAddHTLC) error\n\n\t// GetAttemptResult returns the result of the payment attempt with\n\t// the given attemptID. The paymentHash should be set to the payment's\n\t// overall hash, or in case of AMP payments the payment's unique\n\t// identifier.\n\t//\n\t// The method returns a channel where the payment result will be sent\n\t// when available, or an error is encountered during forwarding. When a\n\t// result is received on the channel, the HTLC is guaranteed to no\n\t// longer be in flight.  The switch shutting down is signaled by\n\t// closing the channel. If the attemptID is unknown,\n\t// ErrPaymentIDNotFound will be returned.\n\tGetAttemptResult(attemptID uint64, paymentHash lntypes.Hash,\n\t\tdeobfuscator htlcswitch.ErrorDecrypter) (\n\t\t<-chan *htlcswitch.PaymentResult, error)\n\n\t// CleanStore calls the underlying result store, telling it is safe to\n\t// delete all entries except the ones in the keepPids map. This should\n\t// be called periodically to let the switch clean up payment results\n\t// that we have handled.\n\t// NOTE: New payment attempts MUST NOT be made after the keepPids map\n\t// has been created and this method has returned.\n\tCleanStore(keepPids map[uint64]struct{}) error\n}\n\n// PaymentSessionSource is an interface that defines a source for the router to\n// retrieve new payment sessions.",
      "length": 1577,
      "tokens": 247,
      "embedding": []
    },
    {
      "slug": "type PaymentSessionSource interface {",
      "content": "type PaymentSessionSource interface {\n\t// NewPaymentSession creates a new payment session that will produce\n\t// routes to the given target. An optional set of routing hints can be\n\t// provided in order to populate additional edges to explore when\n\t// finding a path to the payment's destination.\n\tNewPaymentSession(p *LightningPayment) (PaymentSession, error)\n\n\t// NewPaymentSessionEmpty creates a new paymentSession instance that is\n\t// empty, and will be exhausted immediately. Used for failure reporting\n\t// to missioncontrol for resumed payment we don't want to make more\n\t// attempts for.\n\tNewPaymentSessionEmpty() PaymentSession\n}\n\n// MissionController is an interface that exposes failure reporting and\n// probability estimation.",
      "length": 684,
      "tokens": 98,
      "embedding": []
    },
    {
      "slug": "type MissionController interface {",
      "content": "type MissionController interface {\n\t// ReportPaymentFail reports a failed payment to mission control as\n\t// input for future probability estimates. It returns a bool indicating\n\t// whether this error is a final error and no further payment attempts\n\t// need to be made.\n\tReportPaymentFail(attemptID uint64, rt *route.Route,\n\t\tfailureSourceIdx *int, failure lnwire.FailureMessage) (\n\t\t*channeldb.FailureReason, error)\n\n\t// ReportPaymentSuccess reports a successful payment to mission control as input\n\t// for future probability estimates.\n\tReportPaymentSuccess(attemptID uint64, rt *route.Route) error\n\n\t// GetProbability is expected to return the success probability of a\n\t// payment from fromNode along edge.\n\tGetProbability(fromNode, toNode route.Vertex,\n\t\tamt lnwire.MilliSatoshi, capacity btcutil.Amount) float64\n}\n\n// FeeSchema is the set fee configuration for a Lightning Node on the network.\n// Using the coefficients described within the schema, the required fee to\n// forward outgoing payments can be derived.",
      "length": 963,
      "tokens": 130,
      "embedding": []
    },
    {
      "slug": "type FeeSchema struct {",
      "content": "type FeeSchema struct {\n\t// BaseFee is the base amount of milli-satoshis that will be chained\n\t// for ANY payment forwarded.\n\tBaseFee lnwire.MilliSatoshi\n\n\t// FeeRate is the rate that will be charged for forwarding payments.\n\t// This value should be interpreted as the numerator for a fraction\n\t// (fixed point arithmetic) whose denominator is 1 million. As a result\n\t// the effective fee rate charged per mSAT will be: (amount *\n\t// FeeRate/1,000,000).\n\tFeeRate uint32\n}\n\n// ChannelPolicy holds the parameters that determine the policy we enforce\n// when forwarding payments on a channel. These parameters are communicated\n// to the rest of the network in ChannelUpdate messages.",
      "length": 642,
      "tokens": 104,
      "embedding": []
    },
    {
      "slug": "type ChannelPolicy struct {",
      "content": "type ChannelPolicy struct {\n\t// FeeSchema holds the fee configuration for a channel.\n\tFeeSchema\n\n\t// TimeLockDelta is the required HTLC timelock delta to be used\n\t// when forwarding payments.\n\tTimeLockDelta uint32\n\n\t// MaxHTLC is the maximum HTLC size including fees we are allowed to\n\t// forward over this channel.\n\tMaxHTLC lnwire.MilliSatoshi\n\n\t// MinHTLC is the minimum HTLC size including fees we are allowed to\n\t// forward over this channel.\n\tMinHTLC *lnwire.MilliSatoshi\n}\n\n// Config defines the configuration for the ChannelRouter. ALL elements within\n// the configuration MUST be non-nil for the ChannelRouter to carry out its\n// duties.",
      "length": 599,
      "tokens": 94,
      "embedding": []
    },
    {
      "slug": "type Config struct {",
      "content": "type Config struct {\n\t// Graph is the channel graph that the ChannelRouter will use to gather\n\t// metrics from and also to carry out path finding queries.\n\t// TODO(roasbeef): make into an interface\n\tGraph *channeldb.ChannelGraph\n\n\t// Chain is the router's source to the most up-to-date blockchain data.\n\t// All incoming advertised channels will be checked against the chain\n\t// to ensure that the channels advertised are still open.\n\tChain lnwallet.BlockChainIO\n\n\t// ChainView is an instance of a FilteredChainView which is used to\n\t// watch the sub-set of the UTXO set (the set of active channels) that\n\t// we need in order to properly maintain the channel graph.\n\tChainView chainview.FilteredChainView\n\n\t// Notifier is a reference to the ChainNotifier, used to grab\n\t// the latest blocks if the router is missing any.\n\tNotifier chainntnfs.ChainNotifier\n\n\t// Payer is an instance of a PaymentAttemptDispatcher and is used by\n\t// the router to send payment attempts onto the network, and receive\n\t// their results.\n\tPayer PaymentAttemptDispatcher\n\n\t// Control keeps track of the status of ongoing payments, ensuring we\n\t// can properly resume them across restarts.\n\tControl ControlTower\n\n\t// MissionControl is a shared memory of sorts that executions of\n\t// payment path finding use in order to remember which vertexes/edges\n\t// were pruned from prior attempts. During SendPayment execution,\n\t// errors sent by nodes are mapped into a vertex or edge to be pruned.\n\t// Each run will then take into account this set of pruned\n\t// vertexes/edges to reduce route failure and pass on graph information\n\t// gained to the next execution.\n\tMissionControl MissionController\n\n\t// SessionSource defines a source for the router to retrieve new payment\n\t// sessions.\n\tSessionSource PaymentSessionSource\n\n\t// ChannelPruneExpiry is the duration used to determine if a channel\n\t// should be pruned or not. If the delta between now and when the\n\t// channel was last updated is greater than ChannelPruneExpiry, then\n\t// the channel is marked as a zombie channel eligible for pruning.\n\tChannelPruneExpiry time.Duration\n\n\t// GraphPruneInterval is used as an interval to determine how often we\n\t// should examine the channel graph to garbage collect zombie channels.\n\tGraphPruneInterval time.Duration\n\n\t// FirstTimePruneDelay is the time we'll wait after startup before\n\t// attempting to prune the graph for zombie channels. We don't do it\n\t// immediately after startup to allow lnd to start up without getting\n\t// blocked by this job.\n\tFirstTimePruneDelay time.Duration\n\n\t// QueryBandwidth is a method that allows the router to query the lower\n\t// link layer to determine the up to date available bandwidth at a\n\t// prospective link to be traversed. If the  link isn't available, then\n\t// a value of zero should be returned. Otherwise, the current up to\n\t// date knowledge of the available bandwidth of the link should be\n\t// returned.\n\tGetLink getLinkQuery\n\n\t// NextPaymentID is a method that guarantees to return a new, unique ID\n\t// each time it is called. This is used by the router to generate a\n\t// unique payment ID for each payment it attempts to send, such that\n\t// the switch can properly handle the HTLC.\n\tNextPaymentID func() (uint64, error)\n\n\t// AssumeChannelValid toggles whether or not the router will check for\n\t// spentness of channel outpoints. For neutrino, this saves long rescans\n\t// from blocking initial usage of the daemon.\n\tAssumeChannelValid bool\n\n\t// PathFindingConfig defines global path finding parameters.\n\tPathFindingConfig PathFindingConfig\n\n\t// Clock is mockable time provider.\n\tClock clock.Clock\n\n\t// StrictZombiePruning determines if we attempt to prune zombie\n\t// channels according to a stricter criteria. If true, then we'll prune\n\t// a channel if only *one* of the edges is considered a zombie.\n\t// Otherwise, we'll only prune the channel when both edges have a very\n\t// dated last update.\n\tStrictZombiePruning bool\n\n\t// IsAlias returns whether a passed ShortChannelID is an alias. This is\n\t// only used for our local channels.\n\tIsAlias func(scid lnwire.ShortChannelID) bool\n}\n\n// EdgeLocator is a struct used to identify a specific edge.",
      "length": 4042,
      "tokens": 645,
      "embedding": []
    },
    {
      "slug": "type EdgeLocator struct {",
      "content": "type EdgeLocator struct {\n\t// ChannelID is the channel of this edge.\n\tChannelID uint64\n\n\t// Direction takes the value of 0 or 1 and is identical in definition to\n\t// the channel direction flag. A value of 0 means the direction from the\n\t// lower node pubkey to the higher.\n\tDirection uint8\n}\n\n// String returns a human readable version of the edgeLocator values.",
      "length": 327,
      "tokens": 60,
      "embedding": []
    },
    {
      "slug": "func (e *EdgeLocator) String() string {",
      "content": "func (e *EdgeLocator) String() string {\n\treturn fmt.Sprintf(\"%v:%v\", e.ChannelID, e.Direction)\n}\n\n// ChannelRouter is the layer 3 router within the Lightning stack. Below the\n// ChannelRouter is the HtlcSwitch, and below that is the Bitcoin blockchain\n// itself. The primary role of the ChannelRouter is to respond to queries for\n// potential routes that can support a payment amount, and also general graph\n// reachability questions. The router will prune the channel graph\n// automatically as new blocks are discovered which spend certain known funding\n// outpoints, thereby closing their respective channels.",
      "length": 562,
      "tokens": 86,
      "embedding": []
    },
    {
      "slug": "type ChannelRouter struct {",
      "content": "type ChannelRouter struct {\n\tntfnClientCounter uint64 // To be used atomically.\n\n\tstarted uint32 // To be used atomically.\n\tstopped uint32 // To be used atomically.\n\n\tbestHeight uint32 // To be used atomically.\n\n\t// cfg is a copy of the configuration struct that the ChannelRouter was\n\t// initialized with.\n\tcfg *Config\n\n\t// selfNode is the center of the star-graph centered around the\n\t// ChannelRouter. The ChannelRouter uses this node as a starting point\n\t// when doing any path finding.\n\tselfNode *channeldb.LightningNode\n\n\t// cachedGraph is an instance of routingGraph that caches the source node as\n\t// well as the channel graph itself in memory.\n\tcachedGraph routingGraph\n\n\t// newBlocks is a channel in which new blocks connected to the end of\n\t// the main chain are sent over, and blocks updated after a call to\n\t// UpdateFilter.\n\tnewBlocks <-chan *chainview.FilteredBlock\n\n\t// staleBlocks is a channel in which blocks disconnected from the end\n\t// of our currently known best chain are sent over.\n\tstaleBlocks <-chan *chainview.FilteredBlock\n\n\t// networkUpdates is a channel that carries new topology updates\n\t// messages from outside the ChannelRouter to be processed by the\n\t// networkHandler.\n\tnetworkUpdates chan *routingMsg\n\n\t// topologyClients maps a client's unique notification ID to a\n\t// topologyClient client that contains its notification dispatch\n\t// channel.\n\ttopologyClients *lnutils.SyncMap[uint64, *topologyClient]\n\n\t// ntfnClientUpdates is a channel that's used to send new updates to\n\t// topology notification clients to the ChannelRouter. Updates either\n\t// add a new notification client, or cancel notifications for an\n\t// existing client.\n\tntfnClientUpdates chan *topologyClientUpdate\n\n\t// channelEdgeMtx is a mutex we use to make sure we process only one\n\t// ChannelEdgePolicy at a time for a given channelID, to ensure\n\t// consistency between the various database accesses.\n\tchannelEdgeMtx *multimutex.Mutex\n\n\t// statTicker is a resumable ticker that logs the router's progress as\n\t// it discovers channels or receives updates.\n\tstatTicker ticker.Ticker\n\n\t// stats tracks newly processed channels, updates, and node\n\t// announcements over a window of defaultStatInterval.\n\tstats *routerStats\n\n\tquit chan struct{}\n\twg   sync.WaitGroup\n}\n\n// A compile time check to ensure ChannelRouter implements the\n// ChannelGraphSource interface.\nvar _ ChannelGraphSource = (*ChannelRouter)(nil)\n\n// New creates a new instance of the ChannelRouter with the specified\n// configuration parameters. As part of initialization, if the router detects\n// that the channel graph isn't fully in sync with the latest UTXO (since the\n// channel graph is a subset of the UTXO set) set, then the router will proceed\n// to fully sync to the latest state of the UTXO set.",
      "length": 2677,
      "tokens": 408,
      "embedding": []
    },
    {
      "slug": "func New(cfg Config) (*ChannelRouter, error) {",
      "content": "func New(cfg Config) (*ChannelRouter, error) {\n\tselfNode, err := cfg.Graph.SourceNode()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tr := &ChannelRouter{\n\t\tcfg: &cfg,\n\t\tcachedGraph: &CachedGraph{\n\t\t\tgraph:  cfg.Graph,\n\t\t\tsource: selfNode.PubKeyBytes,\n\t\t},\n\t\tnetworkUpdates:    make(chan *routingMsg),\n\t\ttopologyClients:   &lnutils.SyncMap[uint64, *topologyClient]{},\n\t\tntfnClientUpdates: make(chan *topologyClientUpdate),\n\t\tchannelEdgeMtx:    multimutex.NewMutex(),\n\t\tselfNode:          selfNode,\n\t\tstatTicker:        ticker.New(defaultStatInterval),\n\t\tstats:             new(routerStats),\n\t\tquit:              make(chan struct{}),\n\t}\n\n\treturn r, nil\n}\n\n// Start launches all the goroutines the ChannelRouter requires to carry out\n// its duties. If the router has already been started, then this method is a\n// noop.",
      "length": 738,
      "tokens": 79,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) Start() error {",
      "content": "func (r *ChannelRouter) Start() error {\n\tif !atomic.CompareAndSwapUint32(&r.started, 0, 1) {\n\t\treturn nil\n\t}\n\n\tlog.Info(\"Channel Router starting\")\n\n\tbestHash, bestHeight, err := r.cfg.Chain.GetBestBlock()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// If the graph has never been pruned, or hasn't fully been created yet,\n\t// then we don't treat this as an explicit error.\n\tif _, _, err := r.cfg.Graph.PruneTip(); err != nil {\n\t\tswitch {\n\t\tcase err == channeldb.ErrGraphNeverPruned:\n\t\t\tfallthrough\n\t\tcase err == channeldb.ErrGraphNotFound:\n\t\t\t// If the graph has never been pruned, then we'll set\n\t\t\t// the prune height to the current best height of the\n\t\t\t// chain backend.\n\t\t\t_, err = r.cfg.Graph.PruneGraph(\n\t\t\t\tnil, bestHash, uint32(bestHeight),\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\tdefault:\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// If AssumeChannelValid is present, then we won't rely on pruning\n\t// channels from the graph based on their spentness, but whether they\n\t// are considered zombies or not. We will start zombie pruning after a\n\t// small delay, to avoid slowing down startup of lnd.\n\tif r.cfg.AssumeChannelValid {\n\t\ttime.AfterFunc(r.cfg.FirstTimePruneDelay, func() {\n\t\t\tselect {\n\t\t\tcase <-r.quit:\n\t\t\t\treturn\n\t\t\tdefault:\n\t\t\t}\n\n\t\t\tlog.Info(\"Initial zombie prune starting\")\n\t\t\tif err := r.pruneZombieChans(); err != nil {\n\t\t\t\tlog.Errorf(\"Unable to prune zombies: %v\", err)\n\t\t\t}\n\t\t})\n\t} else {\n\t\t// Otherwise, we'll use our filtered chain view to prune\n\t\t// channels as soon as they are detected as spent on-chain.\n\t\tif err := r.cfg.ChainView.Start(); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Once the instance is active, we'll fetch the channel we'll\n\t\t// receive notifications over.\n\t\tr.newBlocks = r.cfg.ChainView.FilteredBlocks()\n\t\tr.staleBlocks = r.cfg.ChainView.DisconnectedBlocks()\n\n\t\t// Before we perform our manual block pruning, we'll construct\n\t\t// and apply a fresh chain filter to the active\n\t\t// FilteredChainView instance.  We do this before, as otherwise\n\t\t// we may miss on-chain events as the filter hasn't properly\n\t\t// been applied.\n\t\tchannelView, err := r.cfg.Graph.ChannelView()\n\t\tif err != nil && err != channeldb.ErrGraphNoEdgesFound {\n\t\t\treturn err\n\t\t}\n\n\t\tlog.Infof(\"Filtering chain using %v channels active\",\n\t\t\tlen(channelView))\n\n\t\tif len(channelView) != 0 {\n\t\t\terr = r.cfg.ChainView.UpdateFilter(\n\t\t\t\tchannelView, uint32(bestHeight),\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\t// The graph pruning might have taken a while and there could be\n\t\t// new blocks available.\n\t\t_, bestHeight, err = r.cfg.Chain.GetBestBlock()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tr.bestHeight = uint32(bestHeight)\n\n\t\t// Before we begin normal operation of the router, we first need\n\t\t// to synchronize the channel graph to the latest state of the\n\t\t// UTXO set.\n\t\tif err := r.syncGraphWithChain(); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Finally, before we proceed, we'll prune any unconnected nodes\n\t\t// from the graph in order to ensure we maintain a tight graph\n\t\t// of \"useful\" nodes.\n\t\terr = r.cfg.Graph.PruneGraphNodes()\n\t\tif err != nil && err != channeldb.ErrGraphNodesNotFound {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// If any payments are still in flight, we resume, to make sure their\n\t// results are properly handled.\n\tpayments, err := r.cfg.Control.FetchInFlightPayments()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Before we restart existing payments and start accepting more\n\t// payments to be made, we clean the network result store of the\n\t// Switch. We do this here at startup to ensure no more payments can be\n\t// made concurrently, so we know the toKeep map will be up-to-date\n\t// until the cleaning has finished.\n\ttoKeep := make(map[uint64]struct{})\n\tfor _, p := range payments {\n\t\tfor _, a := range p.HTLCs {\n\t\t\ttoKeep[a.AttemptID] = struct{}{}\n\t\t}\n\t}\n\n\tlog.Debugf(\"Cleaning network result store.\")\n\tif err := r.cfg.Payer.CleanStore(toKeep); err != nil {\n\t\treturn err\n\t}\n\n\tfor _, payment := range payments {\n\t\tlog.Infof(\"Resuming payment %v\", payment.Info.PaymentIdentifier)\n\t\tr.wg.Add(1)\n\t\tgo func(payment *channeldb.MPPayment) {\n\t\t\tdefer r.wg.Done()\n\n\t\t\t// Get the hashes used for the outstanding HTLCs.\n\t\t\thtlcs := make(map[uint64]lntypes.Hash)\n\t\t\tfor _, a := range payment.HTLCs {\n\t\t\t\ta := a\n\n\t\t\t\t// We check whether the individual attempts\n\t\t\t\t// have their HTLC hash set, if not we'll fall\n\t\t\t\t// back to the overall payment hash.\n\t\t\t\thash := payment.Info.PaymentIdentifier\n\t\t\t\tif a.Hash != nil {\n\t\t\t\t\thash = *a.Hash\n\t\t\t\t}\n\n\t\t\t\thtlcs[a.AttemptID] = hash\n\t\t\t}\n\n\t\t\t// Since we are not supporting creating more shards\n\t\t\t// after a restart (only receiving the result of the\n\t\t\t// shards already outstanding), we create a simple\n\t\t\t// shard tracker that will map the attempt IDs to\n\t\t\t// hashes used for the HTLCs. This will be enough also\n\t\t\t// for AMP payments, since we only need the hashes for\n\t\t\t// the individual HTLCs to regenerate the circuits, and\n\t\t\t// we don't currently persist the root share necessary\n\t\t\t// to re-derive them.\n\t\t\tshardTracker := shards.NewSimpleShardTracker(\n\t\t\t\tpayment.Info.PaymentIdentifier, htlcs,\n\t\t\t)\n\n\t\t\t// We create a dummy, empty payment session such that\n\t\t\t// we won't make another payment attempt when the\n\t\t\t// result for the in-flight attempt is received.\n\t\t\tpaySession := r.cfg.SessionSource.NewPaymentSessionEmpty()\n\n\t\t\t// We pass in a zero timeout value, to indicate we\n\t\t\t// don't need it to timeout. It will stop immediately\n\t\t\t// after the existing attempt has finished anyway. We\n\t\t\t// also set a zero fee limit, as no more routes should\n\t\t\t// be tried.\n\t\t\t_, _, err := r.sendPayment(\n\t\t\t\t0, payment.Info.PaymentIdentifier, 0,\n\t\t\t\tpaySession, shardTracker,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"Resuming payment %v failed: %v.\",\n\t\t\t\t\tpayment.Info.PaymentIdentifier, err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tlog.Infof(\"Resumed payment %v completed.\",\n\t\t\t\tpayment.Info.PaymentIdentifier)\n\t\t}(payment)\n\t}\n\n\tr.wg.Add(1)\n\tgo r.networkHandler()\n\n\treturn nil\n}\n\n// Stop signals the ChannelRouter to gracefully halt all routines. This method\n// will *block* until all goroutines have excited. If the channel router has\n// already stopped then this method will return immediately.",
      "length": 5875,
      "tokens": 888,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) Stop() error {",
      "content": "func (r *ChannelRouter) Stop() error {\n\tif !atomic.CompareAndSwapUint32(&r.stopped, 0, 1) {\n\t\treturn nil\n\t}\n\n\tlog.Info(\"Channel Router shutting down\")\n\n\t// Our filtered chain view could've only been started if\n\t// AssumeChannelValid isn't present.\n\tif !r.cfg.AssumeChannelValid {\n\t\tif err := r.cfg.ChainView.Stop(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tclose(r.quit)\n\tr.wg.Wait()\n\n\treturn nil\n}\n\n// syncGraphWithChain attempts to synchronize the current channel graph with\n// the latest UTXO set state. This process involves pruning from the channel\n// graph any channels which have been closed by spending their funding output\n// since we've been down.",
      "length": 590,
      "tokens": 87,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) syncGraphWithChain() error {",
      "content": "func (r *ChannelRouter) syncGraphWithChain() error {\n\t// First, we'll need to check to see if we're already in sync with the\n\t// latest state of the UTXO set.\n\tbestHash, bestHeight, err := r.cfg.Chain.GetBestBlock()\n\tif err != nil {\n\t\treturn err\n\t}\n\tr.bestHeight = uint32(bestHeight)\n\n\tpruneHash, pruneHeight, err := r.cfg.Graph.PruneTip()\n\tif err != nil {\n\t\tswitch {\n\t\t// If the graph has never been pruned, or hasn't fully been\n\t\t// created yet, then we don't treat this as an explicit error.\n\t\tcase err == channeldb.ErrGraphNeverPruned:\n\t\tcase err == channeldb.ErrGraphNotFound:\n\t\tdefault:\n\t\t\treturn err\n\t\t}\n\t}\n\n\tlog.Infof(\"Prune tip for Channel Graph: height=%v, hash=%v\", pruneHeight,\n\t\tpruneHash)\n\n\tswitch {\n\n\t// If the graph has never been pruned, then we can exit early as this\n\t// entails it's being created for the first time and hasn't seen any\n\t// block or created channels.\n\tcase pruneHeight == 0 || pruneHash == nil:\n\t\treturn nil\n\n\t// If the block hashes and heights match exactly, then we don't need to\n\t// prune the channel graph as we're already fully in sync.\n\tcase bestHash.IsEqual(pruneHash) && uint32(bestHeight) == pruneHeight:\n\t\treturn nil\n\t}\n\n\t// If the main chain blockhash at prune height is different from the\n\t// prune hash, this might indicate the database is on a stale branch.\n\tmainBlockHash, err := r.cfg.Chain.GetBlockHash(int64(pruneHeight))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// While we are on a stale branch of the chain, walk backwards to find\n\t// first common block.\n\tfor !pruneHash.IsEqual(mainBlockHash) {\n\t\tlog.Infof(\"channel graph is stale. Disconnecting block %v \"+\n\t\t\t\"(hash=%v)\", pruneHeight, pruneHash)\n\t\t// Prune the graph for every channel that was opened at height\n\t\t// >= pruneHeight.\n\t\t_, err := r.cfg.Graph.DisconnectBlockAtHeight(pruneHeight)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tpruneHash, pruneHeight, err = r.cfg.Graph.PruneTip()\n\t\tif err != nil {\n\t\t\tswitch {\n\t\t\t// If at this point the graph has never been pruned, we\n\t\t\t// can exit as this entails we are back to the point\n\t\t\t// where it hasn't seen any block or created channels,\n\t\t\t// alas there's nothing left to prune.\n\t\t\tcase err == channeldb.ErrGraphNeverPruned:\n\t\t\t\treturn nil\n\t\t\tcase err == channeldb.ErrGraphNotFound:\n\t\t\t\treturn nil\n\t\t\tdefault:\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tmainBlockHash, err = r.cfg.Chain.GetBlockHash(int64(pruneHeight))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tlog.Infof(\"Syncing channel graph from height=%v (hash=%v) to height=%v \"+\n\t\t\"(hash=%v)\", pruneHeight, pruneHash, bestHeight, bestHash)\n\n\t// If we're not yet caught up, then we'll walk forward in the chain\n\t// pruning the channel graph with each new block that hasn't yet been\n\t// consumed by the channel graph.\n\tvar spentOutputs []*wire.OutPoint\n\tfor nextHeight := pruneHeight + 1; nextHeight <= uint32(bestHeight); nextHeight++ {\n\t\t// Break out of the rescan early if a shutdown has been\n\t\t// requested, otherwise long rescans will block the daemon from\n\t\t// shutting down promptly.\n\t\tselect {\n\t\tcase <-r.quit:\n\t\t\treturn ErrRouterShuttingDown\n\t\tdefault:\n\t\t}\n\n\t\t// Using the next height, request a manual block pruning from\n\t\t// the chainview for the particular block hash.\n\t\tnextHash, err := r.cfg.Chain.GetBlockHash(int64(nextHeight))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfilterBlock, err := r.cfg.ChainView.FilterBlock(nextHash)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// We're only interested in all prior outputs that have been\n\t\t// spent in the block, so collate all the referenced previous\n\t\t// outpoints within each tx and input.\n\t\tfor _, tx := range filterBlock.Transactions {\n\t\t\tfor _, txIn := range tx.TxIn {\n\t\t\t\tspentOutputs = append(spentOutputs,\n\t\t\t\t\t&txIn.PreviousOutPoint)\n\t\t\t}\n\t\t}\n\t}\n\n\t// With the spent outputs gathered, attempt to prune the channel graph,\n\t// also passing in the best hash+height so the prune tip can be updated.\n\tclosedChans, err := r.cfg.Graph.PruneGraph(\n\t\tspentOutputs, bestHash, uint32(bestHeight),\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tlog.Infof(\"Graph pruning complete: %v channels were closed since \"+\n\t\t\"height %v\", len(closedChans), pruneHeight)\n\treturn nil\n}\n\n// pruneZombieChans is a method that will be called periodically to prune out\n// any \"zombie\" channels. We consider channels zombies if *both* edges haven't\n// been updated since our zombie horizon. If AssumeChannelValid is present,\n// we'll also consider channels zombies if *both* edges are disabled. This\n// usually signals that a channel has been closed on-chain. We do this\n// periodically to keep a healthy, lively routing table.",
      "length": 4352,
      "tokens": 672,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) pruneZombieChans() error {",
      "content": "func (r *ChannelRouter) pruneZombieChans() error {\n\tchansToPrune := make(map[uint64]struct{})\n\tchanExpiry := r.cfg.ChannelPruneExpiry\n\n\tlog.Infof(\"Examining channel graph for zombie channels\")\n\n\t// A helper method to detect if the channel belongs to this node\n\tisSelfChannelEdge := func(info *channeldb.ChannelEdgeInfo) bool {\n\t\treturn info.NodeKey1Bytes == r.selfNode.PubKeyBytes ||\n\t\t\tinfo.NodeKey2Bytes == r.selfNode.PubKeyBytes\n\t}\n\n\t// First, we'll collect all the channels which are eligible for garbage\n\t// collection due to being zombies.\n\tfilterPruneChans := func(info *channeldb.ChannelEdgeInfo,\n\t\te1, e2 *channeldb.ChannelEdgePolicy) error {\n\n\t\t// Exit early in case this channel is already marked to be pruned\n\t\tif _, markedToPrune := chansToPrune[info.ChannelID]; markedToPrune {\n\t\t\treturn nil\n\t\t}\n\n\t\t// We'll ensure that we don't attempt to prune our *own*\n\t\t// channels from the graph, as in any case this should be\n\t\t// re-advertised by the sub-system above us.\n\t\tif isSelfChannelEdge(info) {\n\t\t\treturn nil\n\t\t}\n\n\t\t// If either edge hasn't been updated for a period of\n\t\t// chanExpiry, then we'll mark the channel itself as eligible\n\t\t// for graph pruning.\n\t\te1Zombie := e1 == nil || time.Since(e1.LastUpdate) >= chanExpiry\n\t\te2Zombie := e2 == nil || time.Since(e2.LastUpdate) >= chanExpiry\n\n\t\tif e1Zombie {\n\t\t\tlog.Tracef(\"Node1 pubkey=%x of chan_id=%v is zombie\",\n\t\t\t\tinfo.NodeKey1Bytes, info.ChannelID)\n\t\t}\n\t\tif e2Zombie {\n\t\t\tlog.Tracef(\"Node2 pubkey=%x of chan_id=%v is zombie\",\n\t\t\t\tinfo.NodeKey2Bytes, info.ChannelID)\n\t\t}\n\n\t\t// If we're using strict zombie pruning, then a channel is only\n\t\t// considered live if both edges have a recent update we know\n\t\t// of.\n\t\tvar channelIsLive bool\n\t\tswitch {\n\t\tcase r.cfg.StrictZombiePruning:\n\t\t\tchannelIsLive = !e1Zombie && !e2Zombie\n\n\t\t// Otherwise, if we're using the less strict variant, then a\n\t\t// channel is considered live if either of the edges have a\n\t\t// recent update.\n\t\tdefault:\n\t\t\tchannelIsLive = !e1Zombie || !e2Zombie\n\t\t}\n\n\t\t// Return early if the channel is still considered to be live\n\t\t// with the current set of configuration parameters.\n\t\tif channelIsLive {\n\t\t\treturn nil\n\t\t}\n\n\t\tlog.Debugf(\"ChannelID(%v) is a zombie, collecting to prune\",\n\t\t\tinfo.ChannelID)\n\n\t\t// TODO(roasbeef): add ability to delete single directional edge\n\t\tchansToPrune[info.ChannelID] = struct{}{}\n\n\t\treturn nil\n\t}\n\n\t// If AssumeChannelValid is present we'll look at the disabled bit for both\n\t// edges. If they're both disabled, then we can interpret this as the\n\t// channel being closed and can prune it from our graph.\n\tif r.cfg.AssumeChannelValid {\n\t\tdisabledChanIDs, err := r.cfg.Graph.DisabledChannelIDs()\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to get disabled channels ids \"+\n\t\t\t\t\"chans: %v\", err)\n\t\t}\n\n\t\tdisabledEdges, err := r.cfg.Graph.FetchChanInfos(disabledChanIDs)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to fetch disabled channels edges \"+\n\t\t\t\t\"chans: %v\", err)\n\t\t}\n\n\t\t// Ensuring we won't prune our own channel from the graph.\n\t\tfor _, disabledEdge := range disabledEdges {\n\t\t\tif !isSelfChannelEdge(disabledEdge.Info) {\n\t\t\t\tchansToPrune[disabledEdge.Info.ChannelID] = struct{}{}\n\t\t\t}\n\t\t}\n\t}\n\n\tstartTime := time.Unix(0, 0)\n\tendTime := time.Now().Add(-1 * chanExpiry)\n\toldEdges, err := r.cfg.Graph.ChanUpdatesInHorizon(startTime, endTime)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to fetch expired channel updates \"+\n\t\t\t\"chans: %v\", err)\n\t}\n\n\tfor _, u := range oldEdges {\n\t\tfilterPruneChans(u.Info, u.Policy1, u.Policy2)\n\t}\n\n\tlog.Infof(\"Pruning %v zombie channels\", len(chansToPrune))\n\tif len(chansToPrune) == 0 {\n\t\treturn nil\n\t}\n\n\t// With the set of zombie-like channels obtained, we'll do another pass\n\t// to delete them from the channel graph.\n\ttoPrune := make([]uint64, 0, len(chansToPrune))\n\tfor chanID := range chansToPrune {\n\t\ttoPrune = append(toPrune, chanID)\n\t\tlog.Tracef(\"Pruning zombie channel with ChannelID(%v)\", chanID)\n\t}\n\terr = r.cfg.Graph.DeleteChannelEdges(\n\t\tr.cfg.StrictZombiePruning, true, toPrune...,\n\t)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to delete zombie channels: %v\", err)\n\t}\n\n\t// With the channels pruned, we'll also attempt to prune any nodes that\n\t// were a part of them.\n\terr = r.cfg.Graph.PruneGraphNodes()\n\tif err != nil && err != channeldb.ErrGraphNodesNotFound {\n\t\treturn fmt.Errorf(\"unable to prune graph nodes: %v\", err)\n\t}\n\n\treturn nil\n}\n\n// handleNetworkUpdate is responsible for processing the update message and\n// notifies topology changes, if any.\n//\n// NOTE: must be run inside goroutine.",
      "length": 4330,
      "tokens": 607,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) handleNetworkUpdate(vb *ValidationBarrier,",
      "content": "func (r *ChannelRouter) handleNetworkUpdate(vb *ValidationBarrier,\n\tupdate *routingMsg) {\n\n\tdefer r.wg.Done()\n\tdefer vb.CompleteJob()\n\n\t// If this message has an existing dependency, then we'll wait until\n\t// that has been fully validated before we proceed.\n\terr := vb.WaitForDependants(update.msg)\n\tif err != nil {\n\t\tswitch {\n\t\tcase IsError(err, ErrVBarrierShuttingDown):\n\t\t\tupdate.err <- err\n\n\t\tcase IsError(err, ErrParentValidationFailed):\n\t\t\tupdate.err <- newErrf(ErrIgnored, err.Error())\n\n\t\tdefault:\n\t\t\tlog.Warnf(\"unexpected error during validation \"+\n\t\t\t\t\"barrier shutdown: %v\", err)\n\t\t\tupdate.err <- err\n\t\t}\n\n\t\treturn\n\t}\n\n\t// Process the routing update to determine if this is either a new\n\t// update from our PoV or an update to a prior vertex/edge we\n\t// previously accepted.\n\terr = r.processUpdate(update.msg, update.op...)\n\tupdate.err <- err\n\n\t// If this message had any dependencies, then we can now signal them to\n\t// continue.\n\tallowDependents := err == nil || IsError(err, ErrIgnored, ErrOutdated)\n\tvb.SignalDependants(update.msg, allowDependents)\n\n\t// If the error is not nil here, there's no need to send topology\n\t// change.\n\tif err != nil {\n\t\t// We now decide to log an error or not. If allowDependents is\n\t\t// false, it means there is an error and the error is neither\n\t\t// ErrIgnored or ErrOutdated. In this case, we'll log an error.\n\t\t// Otherwise, we'll add debug log only.\n\t\tif allowDependents {\n\t\t\tlog.Debugf(\"process network updates got: %v\", err)\n\t\t} else {\n\t\t\tlog.Errorf(\"process network updates got: %v\", err)\n\t\t}\n\n\t\treturn\n\t}\n\n\t// Otherwise, we'll send off a new notification for the newly accepted\n\t// update, if any.\n\ttopChange := &TopologyChange{}\n\terr = addToTopologyChange(r.cfg.Graph, topChange, update.msg)\n\tif err != nil {\n\t\tlog.Errorf(\"unable to update topology change notification: %v\",\n\t\t\terr)\n\t\treturn\n\t}\n\n\tif !topChange.isEmpty() {\n\t\tr.notifyTopologyChange(topChange)\n\t}\n}\n\n// networkHandler is the primary goroutine for the ChannelRouter. The roles of\n// this goroutine include answering queries related to the state of the\n// network, pruning the graph on new block notification, applying network\n// updates, and registering new topology clients.\n//\n// NOTE: This MUST be run as a goroutine.",
      "length": 2097,
      "tokens": 313,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) networkHandler() {",
      "content": "func (r *ChannelRouter) networkHandler() {\n\tdefer r.wg.Done()\n\n\tgraphPruneTicker := time.NewTicker(r.cfg.GraphPruneInterval)\n\tdefer graphPruneTicker.Stop()\n\n\tdefer r.statTicker.Stop()\n\n\tr.stats.Reset()\n\n\t// We'll use this validation barrier to ensure that we process all jobs\n\t// in the proper order during parallel validation.\n\t//\n\t// NOTE: For AssumeChannelValid, we bump up the maximum number of\n\t// concurrent validation requests since there are no blocks being\n\t// fetched. This significantly increases the performance of IGD for\n\t// neutrino nodes.\n\t//\n\t// However, we dial back to use multiple of the number of cores when\n\t// fully validating, to avoid fetching up to 1000 blocks from the\n\t// backend. On bitcoind, this will empirically cause massive latency\n\t// spikes when executing this many concurrent RPC calls. Critical\n\t// subsystems or basic rpc calls that rely on calls such as GetBestBlock\n\t// will hang due to excessive load.\n\t//\n\t// See https://github.com/lightningnetwork/lnd/issues/4892.\n\tvar validationBarrier *ValidationBarrier\n\tif r.cfg.AssumeChannelValid {\n\t\tvalidationBarrier = NewValidationBarrier(1000, r.quit)\n\t} else {\n\t\tvalidationBarrier = NewValidationBarrier(\n\t\t\t4*runtime.NumCPU(), r.quit,\n\t\t)\n\t}\n\n\tfor {\n\n\t\t// If there are stats, resume the statTicker.\n\t\tif !r.stats.Empty() {\n\t\t\tr.statTicker.Resume()\n\t\t}\n\n\t\tselect {\n\t\t// A new fully validated network update has just arrived. As a\n\t\t// result we'll modify the channel graph accordingly depending\n\t\t// on the exact type of the message.\n\t\tcase update := <-r.networkUpdates:\n\t\t\t// We'll set up any dependants, and wait until a free\n\t\t\t// slot for this job opens up, this allows us to not\n\t\t\t// have thousands of goroutines active.\n\t\t\tvalidationBarrier.InitJobDependencies(update.msg)\n\n\t\t\tr.wg.Add(1)\n\t\t\tgo r.handleNetworkUpdate(validationBarrier, update)\n\n\t\t\t// TODO(roasbeef): remove all unconnected vertexes\n\t\t\t// after N blocks pass with no corresponding\n\t\t\t// announcements.\n\n\t\tcase chainUpdate, ok := <-r.staleBlocks:\n\t\t\t// If the channel has been closed, then this indicates\n\t\t\t// the daemon is shutting down, so we exit ourselves.\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Since this block is stale, we update our best height\n\t\t\t// to the previous block.\n\t\t\tblockHeight := uint32(chainUpdate.Height)\n\t\t\tatomic.StoreUint32(&r.bestHeight, blockHeight-1)\n\n\t\t\t// Update the channel graph to reflect that this block\n\t\t\t// was disconnected.\n\t\t\t_, err := r.cfg.Graph.DisconnectBlockAtHeight(blockHeight)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"unable to prune graph with stale \"+\n\t\t\t\t\t\"block: %v\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// TODO(halseth): notify client about the reorg?\n\n\t\t// A new block has arrived, so we can prune the channel graph\n\t\t// of any channels which were closed in the block.\n\t\tcase chainUpdate, ok := <-r.newBlocks:\n\t\t\t// If the channel has been closed, then this indicates\n\t\t\t// the daemon is shutting down, so we exit ourselves.\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// We'll ensure that any new blocks received attach\n\t\t\t// directly to the end of our main chain. If not, then\n\t\t\t// we've somehow missed some blocks. Here we'll catch\n\t\t\t// up the chain with the latest blocks.\n\t\t\tcurrentHeight := atomic.LoadUint32(&r.bestHeight)\n\t\t\tswitch {\n\t\t\tcase chainUpdate.Height == currentHeight+1:\n\t\t\t\terr := r.updateGraphWithClosedChannels(\n\t\t\t\t\tchainUpdate,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Errorf(\"unable to prune graph \"+\n\t\t\t\t\t\t\"with closed channels: %v\", err)\n\t\t\t\t}\n\n\t\t\tcase chainUpdate.Height > currentHeight+1:\n\t\t\t\tlog.Errorf(\"out of order block: expecting \"+\n\t\t\t\t\t\"height=%v, got height=%v\",\n\t\t\t\t\tcurrentHeight+1, chainUpdate.Height)\n\n\t\t\t\terr := r.getMissingBlocks(currentHeight, chainUpdate)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Errorf(\"unable to retrieve missing\"+\n\t\t\t\t\t\t\"blocks: %v\", err)\n\t\t\t\t}\n\n\t\t\tcase chainUpdate.Height < currentHeight+1:\n\t\t\t\tlog.Errorf(\"out of order block: expecting \"+\n\t\t\t\t\t\"height=%v, got height=%v\",\n\t\t\t\t\tcurrentHeight+1, chainUpdate.Height)\n\n\t\t\t\tlog.Infof(\"Skipping channel pruning since \"+\n\t\t\t\t\t\"received block height %v was already\"+\n\t\t\t\t\t\" processed.\", chainUpdate.Height)\n\t\t\t}\n\n\t\t// A new notification client update has arrived. We're either\n\t\t// gaining a new client, or cancelling notifications for an\n\t\t// existing client.\n\t\tcase ntfnUpdate := <-r.ntfnClientUpdates:\n\t\t\tclientID := ntfnUpdate.clientID\n\n\t\t\tif ntfnUpdate.cancel {\n\t\t\t\tclient, ok := r.topologyClients.LoadAndDelete(\n\t\t\t\t\tclientID,\n\t\t\t\t)\n\t\t\t\tif ok {\n\t\t\t\t\tclose(client.exit)\n\t\t\t\t\tclient.wg.Wait()\n\n\t\t\t\t\tclose(client.ntfnChan)\n\t\t\t\t}\n\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tr.topologyClients.Store(clientID, &topologyClient{\n\t\t\t\tntfnChan: ntfnUpdate.ntfnChan,\n\t\t\t\texit:     make(chan struct{}),\n\t\t\t})\n\n\t\t// The graph prune ticker has ticked, so we'll examine the\n\t\t// state of the known graph to filter out any zombie channels\n\t\t// for pruning.\n\t\tcase <-graphPruneTicker.C:\n\t\t\tif err := r.pruneZombieChans(); err != nil {\n\t\t\t\tlog.Errorf(\"Unable to prune zombies: %v\", err)\n\t\t\t}\n\n\t\t// Log any stats if we've processed a non-empty number of\n\t\t// channels, updates, or nodes. We'll only pause the ticker if\n\t\t// the last window contained no updates to avoid resuming and\n\t\t// pausing while consecutive windows contain new info.\n\t\tcase <-r.statTicker.Ticks():\n\t\t\tif !r.stats.Empty() {\n\t\t\t\tlog.Infof(r.stats.String())\n\t\t\t} else {\n\t\t\t\tr.statTicker.Pause()\n\t\t\t}\n\t\t\tr.stats.Reset()\n\n\t\t// The router has been signalled to exit, to we exit our main\n\t\t// loop so the wait group can be decremented.\n\t\tcase <-r.quit:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// getMissingBlocks walks through all missing blocks and updates the graph\n// closed channels accordingly.",
      "length": 5363,
      "tokens": 727,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) getMissingBlocks(currentHeight uint32,",
      "content": "func (r *ChannelRouter) getMissingBlocks(currentHeight uint32,\n\tchainUpdate *chainview.FilteredBlock) error {\n\n\toutdatedHash, err := r.cfg.Chain.GetBlockHash(int64(currentHeight))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\toutdatedBlock := &chainntnfs.BlockEpoch{\n\t\tHeight: int32(currentHeight),\n\t\tHash:   outdatedHash,\n\t}\n\n\tepochClient, err := r.cfg.Notifier.RegisterBlockEpochNtfn(\n\t\toutdatedBlock,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer epochClient.Cancel()\n\n\tblockDifference := int(chainUpdate.Height - currentHeight)\n\n\t// We'll walk through all the outdated blocks and make sure we're able\n\t// to update the graph with any closed channels from them.\n\tfor i := 0; i < blockDifference; i++ {\n\t\tvar (\n\t\t\tmissingBlock *chainntnfs.BlockEpoch\n\t\t\tok           bool\n\t\t)\n\n\t\tselect {\n\t\tcase missingBlock, ok = <-epochClient.Epochs:\n\t\t\tif !ok {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\tcase <-r.quit:\n\t\t\treturn nil\n\t\t}\n\n\t\tfilteredBlock, err := r.cfg.ChainView.FilterBlock(\n\t\t\tmissingBlock.Hash,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\terr = r.updateGraphWithClosedChannels(\n\t\t\tfilteredBlock,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// updateGraphWithClosedChannels prunes the channel graph of closed channels\n// that are no longer needed.",
      "length": 1110,
      "tokens": 149,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) updateGraphWithClosedChannels(",
      "content": "func (r *ChannelRouter) updateGraphWithClosedChannels(\n\tchainUpdate *chainview.FilteredBlock) error {\n\n\t// Once a new block arrives, we update our running track of the height\n\t// of the chain tip.\n\tblockHeight := chainUpdate.Height\n\n\tatomic.StoreUint32(&r.bestHeight, blockHeight)\n\tlog.Infof(\"Pruning channel graph using block %v (height=%v)\",\n\t\tchainUpdate.Hash, blockHeight)\n\n\t// We're only interested in all prior outputs that have been spent in\n\t// the block, so collate all the referenced previous outpoints within\n\t// each tx and input.\n\tvar spentOutputs []*wire.OutPoint\n\tfor _, tx := range chainUpdate.Transactions {\n\t\tfor _, txIn := range tx.TxIn {\n\t\t\tspentOutputs = append(spentOutputs,\n\t\t\t\t&txIn.PreviousOutPoint)\n\t\t}\n\t}\n\n\t// With the spent outputs gathered, attempt to prune the channel graph,\n\t// also passing in the hash+height of the block being pruned so the\n\t// prune tip can be updated.\n\tchansClosed, err := r.cfg.Graph.PruneGraph(spentOutputs,\n\t\t&chainUpdate.Hash, chainUpdate.Height)\n\tif err != nil {\n\t\tlog.Errorf(\"unable to prune routing table: %v\", err)\n\t\treturn err\n\t}\n\n\tlog.Infof(\"Block %v (height=%v) closed %v channels\", chainUpdate.Hash,\n\t\tblockHeight, len(chansClosed))\n\n\tif len(chansClosed) == 0 {\n\t\treturn err\n\t}\n\n\t// Notify all currently registered clients of the newly closed channels.\n\tcloseSummaries := createCloseSummaries(blockHeight, chansClosed...)\n\tr.notifyTopologyChange(&TopologyChange{\n\t\tClosedChannels: closeSummaries,\n\t})\n\n\treturn nil\n}\n\n// assertNodeAnnFreshness returns a non-nil error if we have an announcement in\n// the database for the passed node with a timestamp newer than the passed\n// timestamp. ErrIgnored will be returned if we already have the node, and\n// ErrOutdated will be returned if we have a timestamp that's after the new\n// timestamp.",
      "length": 1695,
      "tokens": 235,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) assertNodeAnnFreshness(node route.Vertex,",
      "content": "func (r *ChannelRouter) assertNodeAnnFreshness(node route.Vertex,\n\tmsgTimestamp time.Time) error {\n\n\t// If we are not already aware of this node, it means that we don't\n\t// know about any channel using this node. To avoid a DoS attack by\n\t// node announcements, we will ignore such nodes. If we do know about\n\t// this node, check that this update brings info newer than what we\n\t// already have.\n\tlastUpdate, exists, err := r.cfg.Graph.HasLightningNode(node)\n\tif err != nil {\n\t\treturn errors.Errorf(\"unable to query for the \"+\n\t\t\t\"existence of node: %v\", err)\n\t}\n\tif !exists {\n\t\treturn newErrf(ErrIgnored, \"Ignoring node announcement\"+\n\t\t\t\" for node not found in channel graph (%x)\",\n\t\t\tnode[:])\n\t}\n\n\t// If we've reached this point then we're aware of the vertex being\n\t// advertised. So we now check if the new message has a new time stamp,\n\t// if not then we won't accept the new data as it would override newer\n\t// data.\n\tif !lastUpdate.Before(msgTimestamp) {\n\t\treturn newErrf(ErrOutdated, \"Ignoring outdated \"+\n\t\t\t\"announcement for %x\", node[:])\n\t}\n\n\treturn nil\n}\n\n// addZombieEdge adds a channel that failed complete validation into the zombie\n// index so we can avoid having to re-validate it in the future.",
      "length": 1116,
      "tokens": 190,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) addZombieEdge(chanID uint64) error {",
      "content": "func (r *ChannelRouter) addZombieEdge(chanID uint64) error {\n\t// If the edge fails validation we'll mark the edge itself as a zombie\n\t// so we don't continue to request it. We use the \"zero key\" for both\n\t// node pubkeys so this edge can't be resurrected.\n\tvar zeroKey [33]byte\n\terr := r.cfg.Graph.MarkEdgeZombie(chanID, zeroKey, zeroKey)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to mark spent chan(id=%v) as a \"+\n\t\t\t\"zombie: %w\", chanID, err)\n\t}\n\n\treturn nil\n}\n\n// processUpdate processes a new relate authenticated channel/edge, node or\n// channel/edge update network update. If the update didn't affect the internal\n// state of the draft due to either being out of date, invalid, or redundant,\n// then error is returned.",
      "length": 650,
      "tokens": 110,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) processUpdate(msg interface{},",
      "content": "func (r *ChannelRouter) processUpdate(msg interface{},\n\top ...batch.SchedulerOption) error {\n\n\tswitch msg := msg.(type) {\n\tcase *channeldb.LightningNode:\n\t\t// Before we add the node to the database, we'll check to see\n\t\t// if the announcement is \"fresh\" or not. If it isn't, then\n\t\t// we'll return an error.\n\t\terr := r.assertNodeAnnFreshness(msg.PubKeyBytes, msg.LastUpdate)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif err := r.cfg.Graph.AddLightningNode(msg, op...); err != nil {\n\t\t\treturn errors.Errorf(\"unable to add node %v to the \"+\n\t\t\t\t\"graph: %v\", msg.PubKeyBytes, err)\n\t\t}\n\n\t\tlog.Tracef(\"Updated vertex data for node=%x\", msg.PubKeyBytes)\n\t\tr.stats.incNumNodeUpdates()\n\n\tcase *channeldb.ChannelEdgeInfo:\n\t\tlog.Debugf(\"Received ChannelEdgeInfo for channel %v\",\n\t\t\tmsg.ChannelID)\n\n\t\t// Prior to processing the announcement we first check if we\n\t\t// already know of this channel, if so, then we can exit early.\n\t\t_, _, exists, isZombie, err := r.cfg.Graph.HasChannelEdge(\n\t\t\tmsg.ChannelID,\n\t\t)\n\t\tif err != nil && err != channeldb.ErrGraphNoEdgesFound {\n\t\t\treturn errors.Errorf(\"unable to check for edge \"+\n\t\t\t\t\"existence: %v\", err)\n\t\t}\n\t\tif isZombie {\n\t\t\treturn newErrf(ErrIgnored, \"ignoring msg for zombie \"+\n\t\t\t\t\"chan_id=%v\", msg.ChannelID)\n\t\t}\n\t\tif exists {\n\t\t\treturn newErrf(ErrIgnored, \"ignoring msg for known \"+\n\t\t\t\t\"chan_id=%v\", msg.ChannelID)\n\t\t}\n\n\t\t// If AssumeChannelValid is present, then we are unable to\n\t\t// perform any of the expensive checks below, so we'll\n\t\t// short-circuit our path straight to adding the edge to our\n\t\t// graph. If the passed ShortChannelID is an alias, then we'll\n\t\t// skip validation as it will not map to a legitimate tx. This\n\t\t// is not a DoS vector as only we can add an alias\n\t\t// ChannelAnnouncement from the gossiper.\n\t\tscid := lnwire.NewShortChanIDFromInt(msg.ChannelID)\n\t\tif r.cfg.AssumeChannelValid || r.cfg.IsAlias(scid) {\n\t\t\tif err := r.cfg.Graph.AddChannelEdge(msg, op...); err != nil {\n\t\t\t\treturn fmt.Errorf(\"unable to add edge: %v\", err)\n\t\t\t}\n\t\t\tlog.Tracef(\"New channel discovered! Link \"+\n\t\t\t\t\"connects %x and %x with ChannelID(%v)\",\n\t\t\t\tmsg.NodeKey1Bytes, msg.NodeKey2Bytes,\n\t\t\t\tmsg.ChannelID)\n\t\t\tr.stats.incNumEdgesDiscovered()\n\n\t\t\tbreak\n\t\t}\n\n\t\t// Before we can add the channel to the channel graph, we need\n\t\t// to obtain the full funding outpoint that's encoded within\n\t\t// the channel ID.\n\t\tchannelID := lnwire.NewShortChanIDFromInt(msg.ChannelID)\n\t\tfundingTx, err := r.fetchFundingTx(&channelID)\n\t\tif err != nil {\n\t\t\t// In order to ensure we don't erroneously mark a\n\t\t\t// channel as a zombie due to an RPC failure, we'll\n\t\t\t// attempt to string match for the relevant errors.\n\t\t\t//\n\t\t\t// * btcd:\n\t\t\t//    * https://github.com/btcsuite/btcd/blob/master/rpcserver.go#L1316\n\t\t\t//    * https://github.com/btcsuite/btcd/blob/master/rpcserver.go#L1086\n\t\t\t// * bitcoind:\n\t\t\t//    * https://github.com/bitcoin/bitcoin/blob/7fcf53f7b4524572d1d0c9a5fdc388e87eb02416/src/rpc/blockchain.cpp#L770\n\t\t\t//     * https://github.com/bitcoin/bitcoin/blob/7fcf53f7b4524572d1d0c9a5fdc388e87eb02416/src/rpc/blockchain.cpp#L954\n\t\t\tswitch {\n\t\t\tcase strings.Contains(err.Error(), \"not found\"):\n\t\t\t\tfallthrough\n\n\t\t\tcase strings.Contains(err.Error(), \"out of range\"):\n\t\t\t\t// If the funding transaction isn't found at\n\t\t\t\t// all, then we'll mark the edge itself as a\n\t\t\t\t// zombie so we don't continue to request it.\n\t\t\t\t// We use the \"zero key\" for both node pubkeys\n\t\t\t\t// so this edge can't be resurrected.\n\t\t\t\tzErr := r.addZombieEdge(msg.ChannelID)\n\t\t\t\tif zErr != nil {\n\t\t\t\t\treturn zErr\n\t\t\t\t}\n\n\t\t\tdefault:\n\t\t\t}\n\n\t\t\treturn newErrf(ErrNoFundingTransaction, \"unable to \"+\n\t\t\t\t\"locate funding tx: %v\", err)\n\t\t}\n\n\t\t// Recreate witness output to be sure that declared in channel\n\t\t// edge bitcoin keys and channel value corresponds to the\n\t\t// reality.\n\t\twitnessScript, err := input.GenMultiSigScript(\n\t\t\tmsg.BitcoinKey1Bytes[:], msg.BitcoinKey2Bytes[:],\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tpkScript, err := input.WitnessScriptHash(witnessScript)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Next we'll validate that this channel is actually well\n\t\t// formed. If this check fails, then this channel either\n\t\t// doesn't exist, or isn't the one that was meant to be created\n\t\t// according to the passed channel proofs.\n\t\tfundingPoint, err := chanvalidate.Validate(&chanvalidate.Context{\n\t\t\tLocator: &chanvalidate.ShortChanIDChanLocator{\n\t\t\t\tID: channelID,\n\t\t\t},\n\t\t\tMultiSigPkScript: pkScript,\n\t\t\tFundingTx:        fundingTx,\n\t\t})\n\t\tif err != nil {\n\t\t\t// Mark the edge as a zombie so we won't try to\n\t\t\t// re-validate it on start up.\n\t\t\tif err := r.addZombieEdge(msg.ChannelID); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\treturn newErrf(ErrInvalidFundingOutput, \"output \"+\n\t\t\t\t\"failed validation: %w\", err)\n\t\t}\n\n\t\t// Now that we have the funding outpoint of the channel, ensure\n\t\t// that it hasn't yet been spent. If so, then this channel has\n\t\t// been closed so we'll ignore it.\n\t\tfundingPkScript, err := input.WitnessScriptHash(witnessScript)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tchanUtxo, err := r.cfg.Chain.GetUtxo(\n\t\t\tfundingPoint, fundingPkScript, channelID.BlockHeight,\n\t\t\tr.quit,\n\t\t)\n\t\tif err != nil {\n\t\t\tif errors.Is(err, btcwallet.ErrOutputSpent) {\n\t\t\t\tzErr := r.addZombieEdge(msg.ChannelID)\n\t\t\t\tif zErr != nil {\n\t\t\t\t\treturn zErr\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn newErrf(ErrChannelSpent, \"unable to fetch utxo \"+\n\t\t\t\t\"for chan_id=%v, chan_point=%v: %v\",\n\t\t\t\tmsg.ChannelID, fundingPoint, err)\n\t\t}\n\n\t\t// TODO(roasbeef): this is a hack, needs to be removed\n\t\t// after commitment fees are dynamic.\n\t\tmsg.Capacity = btcutil.Amount(chanUtxo.Value)\n\t\tmsg.ChannelPoint = *fundingPoint\n\t\tif err := r.cfg.Graph.AddChannelEdge(msg, op...); err != nil {\n\t\t\treturn errors.Errorf(\"unable to add edge: %v\", err)\n\t\t}\n\n\t\tlog.Debugf(\"New channel discovered! Link \"+\n\t\t\t\"connects %x and %x with ChannelPoint(%v): \"+\n\t\t\t\"chan_id=%v, capacity=%v\",\n\t\t\tmsg.NodeKey1Bytes, msg.NodeKey2Bytes,\n\t\t\tfundingPoint, msg.ChannelID, msg.Capacity)\n\t\tr.stats.incNumEdgesDiscovered()\n\n\t\t// As a new edge has been added to the channel graph, we'll\n\t\t// update the current UTXO filter within our active\n\t\t// FilteredChainView so we are notified if/when this channel is\n\t\t// closed.\n\t\tfilterUpdate := []channeldb.EdgePoint{\n\t\t\t{\n\t\t\t\tFundingPkScript: fundingPkScript,\n\t\t\t\tOutPoint:        *fundingPoint,\n\t\t\t},\n\t\t}\n\t\terr = r.cfg.ChainView.UpdateFilter(\n\t\t\tfilterUpdate, atomic.LoadUint32(&r.bestHeight),\n\t\t)\n\t\tif err != nil {\n\t\t\treturn errors.Errorf(\"unable to update chain \"+\n\t\t\t\t\"view: %v\", err)\n\t\t}\n\n\tcase *channeldb.ChannelEdgePolicy:\n\t\tlog.Debugf(\"Received ChannelEdgePolicy for channel %v\",\n\t\t\tmsg.ChannelID)\n\n\t\t// We make sure to hold the mutex for this channel ID,\n\t\t// such that no other goroutine is concurrently doing\n\t\t// database accesses for the same channel ID.\n\t\tr.channelEdgeMtx.Lock(msg.ChannelID)\n\t\tdefer r.channelEdgeMtx.Unlock(msg.ChannelID)\n\n\t\tedge1Timestamp, edge2Timestamp, exists, isZombie, err :=\n\t\t\tr.cfg.Graph.HasChannelEdge(msg.ChannelID)\n\t\tif err != nil && err != channeldb.ErrGraphNoEdgesFound {\n\t\t\treturn errors.Errorf(\"unable to check for edge \"+\n\t\t\t\t\"existence: %v\", err)\n\n\t\t}\n\n\t\t// If the channel is marked as a zombie in our database, and\n\t\t// we consider this a stale update, then we should not apply the\n\t\t// policy.\n\t\tisStaleUpdate := time.Since(msg.LastUpdate) > r.cfg.ChannelPruneExpiry\n\t\tif isZombie && isStaleUpdate {\n\t\t\treturn newErrf(ErrIgnored, \"ignoring stale update \"+\n\t\t\t\t\"(flags=%v|%v) for zombie chan_id=%v\",\n\t\t\t\tmsg.MessageFlags, msg.ChannelFlags,\n\t\t\t\tmsg.ChannelID)\n\t\t}\n\n\t\t// If the channel doesn't exist in our database, we cannot\n\t\t// apply the updated policy.\n\t\tif !exists {\n\t\t\treturn newErrf(ErrIgnored, \"ignoring update \"+\n\t\t\t\t\"(flags=%v|%v) for unknown chan_id=%v\",\n\t\t\t\tmsg.MessageFlags, msg.ChannelFlags,\n\t\t\t\tmsg.ChannelID)\n\t\t}\n\n\t\t// As edges are directional edge node has a unique policy for\n\t\t// the direction of the edge they control. Therefore we first\n\t\t// check if we already have the most up to date information for\n\t\t// that edge. If this message has a timestamp not strictly\n\t\t// newer than what we already know of we can exit early.\n\t\tswitch {\n\n\t\t// A flag set of 0 indicates this is an announcement for the\n\t\t// \"first\" node in the channel.\n\t\tcase msg.ChannelFlags&lnwire.ChanUpdateDirection == 0:\n\n\t\t\t// Ignore outdated message.\n\t\t\tif !edge1Timestamp.Before(msg.LastUpdate) {\n\t\t\t\treturn newErrf(ErrOutdated, \"Ignoring \"+\n\t\t\t\t\t\"outdated update (flags=%v|%v) for \"+\n\t\t\t\t\t\"known chan_id=%v\", msg.MessageFlags,\n\t\t\t\t\tmsg.ChannelFlags, msg.ChannelID)\n\t\t\t}\n\n\t\t// Similarly, a flag set of 1 indicates this is an announcement\n\t\t// for the \"second\" node in the channel.\n\t\tcase msg.ChannelFlags&lnwire.ChanUpdateDirection == 1:\n\n\t\t\t// Ignore outdated message.\n\t\t\tif !edge2Timestamp.Before(msg.LastUpdate) {\n\t\t\t\treturn newErrf(ErrOutdated, \"Ignoring \"+\n\t\t\t\t\t\"outdated update (flags=%v|%v) for \"+\n\t\t\t\t\t\"known chan_id=%v\", msg.MessageFlags,\n\t\t\t\t\tmsg.ChannelFlags, msg.ChannelID)\n\t\t\t}\n\t\t}\n\n\t\t// Now that we know this isn't a stale update, we'll apply the\n\t\t// new edge policy to the proper directional edge within the\n\t\t// channel graph.\n\t\tif err = r.cfg.Graph.UpdateEdgePolicy(msg, op...); err != nil {\n\t\t\terr := errors.Errorf(\"unable to add channel: %v\", err)\n\t\t\tlog.Error(err)\n\t\t\treturn err\n\t\t}\n\n\t\tlog.Debugf(\"New channel update applied: %v\",\n\t\t\tnewLogClosure(func() string { return spew.Sdump(msg) }))\n\t\tr.stats.incNumChannelUpdates()\n\n\tdefault:\n\t\treturn errors.Errorf(\"wrong routing update message type\")\n\t}\n\n\treturn nil\n}\n\n// fetchFundingTx returns the funding transaction identified by the passed\n// short channel ID.\n//\n// TODO(roasbeef): replace with call to GetBlockTransaction? (would allow to\n// later use getblocktxn)",
      "length": 9332,
      "tokens": 1217,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) fetchFundingTx(",
      "content": "func (r *ChannelRouter) fetchFundingTx(\n\tchanID *lnwire.ShortChannelID) (*wire.MsgTx, error) {\n\n\t// First fetch the block hash by the block number encoded, then use\n\t// that hash to fetch the block itself.\n\tblockNum := int64(chanID.BlockHeight)\n\tblockHash, err := r.cfg.Chain.GetBlockHash(blockNum)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfundingBlock, err := r.cfg.Chain.GetBlock(blockHash)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// As a sanity check, ensure that the advertised transaction index is\n\t// within the bounds of the total number of transactions within a\n\t// block.\n\tnumTxns := uint32(len(fundingBlock.Transactions))\n\tif chanID.TxIndex > numTxns-1 {\n\t\treturn nil, fmt.Errorf(\"tx_index=#%v \"+\n\t\t\t\"is out of range (max_index=%v), network_chan_id=%v\",\n\t\t\tchanID.TxIndex, numTxns-1, chanID)\n\t}\n\n\treturn fundingBlock.Transactions[chanID.TxIndex], nil\n}\n\n// routingMsg couples a routing related routing topology update to the\n// error channel.",
      "length": 882,
      "tokens": 121,
      "embedding": []
    },
    {
      "slug": "type routingMsg struct {",
      "content": "type routingMsg struct {\n\tmsg interface{}\n\top  []batch.SchedulerOption\n\terr chan error\n}\n\n// FindRoute attempts to query the ChannelRouter for the optimum path to a\n// particular target destination to which it is able to send `amt` after\n// factoring in channel capacities and cumulative fees along the route.",
      "length": 277,
      "tokens": 45,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) FindRoute(source, target route.Vertex,",
      "content": "func (r *ChannelRouter) FindRoute(source, target route.Vertex,\n\tamt lnwire.MilliSatoshi, timePref float64, restrictions *RestrictParams,\n\tdestCustomRecords record.CustomSet,\n\trouteHints map[route.Vertex][]*channeldb.CachedEdgePolicy,\n\tfinalExpiry uint16) (*route.Route, float64, error) {\n\n\tlog.Debugf(\"Searching for path to %v, sending %v\", target, amt)\n\n\t// We'll attempt to obtain a set of bandwidth hints that can help us\n\t// eliminate certain routes early on in the path finding process.\n\tbandwidthHints, err := newBandwidthManager(\n\t\tr.cachedGraph, r.selfNode.PubKeyBytes, r.cfg.GetLink,\n\t)\n\tif err != nil {\n\t\treturn nil, 0, err\n\t}\n\n\t// We'll fetch the current block height so we can properly calculate the\n\t// required HTLC time locks within the route.\n\t_, currentHeight, err := r.cfg.Chain.GetBestBlock()\n\tif err != nil {\n\t\treturn nil, 0, err\n\t}\n\n\t// Now that we know the destination is reachable within the graph, we'll\n\t// execute our path finding algorithm.\n\tfinalHtlcExpiry := currentHeight + int32(finalExpiry)\n\n\t// Validate time preference.\n\tif timePref < -1 || timePref > 1 {\n\t\treturn nil, 0, errors.New(\"time preference out of range\")\n\t}\n\n\tpath, probability, err := findPath(\n\t\t&graphParams{\n\t\t\tadditionalEdges: routeHints,\n\t\t\tbandwidthHints:  bandwidthHints,\n\t\t\tgraph:           r.cachedGraph,\n\t\t},\n\t\trestrictions,\n\t\t&r.cfg.PathFindingConfig,\n\t\tsource, target, amt, timePref, finalHtlcExpiry,\n\t)\n\tif err != nil {\n\t\treturn nil, 0, err\n\t}\n\n\t// Create the route with absolute time lock values.\n\troute, err := newRoute(\n\t\tsource, path, uint32(currentHeight),\n\t\tfinalHopParams{\n\t\t\tamt:       amt,\n\t\t\ttotalAmt:  amt,\n\t\t\tcltvDelta: finalExpiry,\n\t\t\trecords:   destCustomRecords,\n\t\t},\n\t)\n\tif err != nil {\n\t\treturn nil, 0, err\n\t}\n\n\tgo log.Tracef(\"Obtained path to send %v to %x: %v\",\n\t\tamt, target, newLogClosure(func() string {\n\t\t\treturn spew.Sdump(route)\n\t\t}),\n\t)\n\n\treturn route, probability, nil\n}\n\n// generateNewSessionKey generates a new ephemeral private key to be used for a\n// payment attempt.",
      "length": 1874,
      "tokens": 257,
      "embedding": []
    },
    {
      "slug": "func generateNewSessionKey() (*btcec.PrivateKey, error) {",
      "content": "func generateNewSessionKey() (*btcec.PrivateKey, error) {\n\t// Generate a new random session key to ensure that we don't trigger\n\t// any replay.\n\t//\n\t// TODO(roasbeef): add more sources of randomness?\n\treturn btcec.NewPrivateKey()\n}\n\n// generateSphinxPacket generates then encodes a sphinx packet which encodes\n// the onion route specified by the passed layer 3 route. The blob returned\n// from this function can immediately be included within an HTLC add packet to\n// be sent to the first hop within the route.",
      "length": 442,
      "tokens": 75,
      "embedding": []
    },
    {
      "slug": "func generateSphinxPacket(rt *route.Route, paymentHash []byte,",
      "content": "func generateSphinxPacket(rt *route.Route, paymentHash []byte,\n\tsessionKey *btcec.PrivateKey) ([]byte, *sphinx.Circuit, error) {\n\n\t// Now that we know we have an actual route, we'll map the route into a\n\t// sphinx payment path which includes per-hop payloads for each hop\n\t// that give each node within the route the necessary information\n\t// (fees, CLTV value, etc) to properly forward the payment.\n\tsphinxPath, err := rt.ToSphinxPath()\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tlog.Tracef(\"Constructed per-hop payloads for payment_hash=%x: %v\",\n\t\tpaymentHash[:], newLogClosure(func() string {\n\t\t\tpath := make([]sphinx.OnionHop, sphinxPath.TrueRouteLength())\n\t\t\tfor i := range path {\n\t\t\t\thopCopy := sphinxPath[i]\n\t\t\t\tpath[i] = hopCopy\n\t\t\t}\n\t\t\treturn spew.Sdump(path)\n\t\t}),\n\t)\n\n\t// Next generate the onion routing packet which allows us to perform\n\t// privacy preserving source routing across the network.\n\tsphinxPacket, err := sphinx.NewOnionPacket(\n\t\tsphinxPath, sessionKey, paymentHash,\n\t\tsphinx.DeterministicPacketFiller,\n\t)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\t// Finally, encode Sphinx packet using its wire representation to be\n\t// included within the HTLC add packet.\n\tvar onionBlob bytes.Buffer\n\tif err := sphinxPacket.Encode(&onionBlob); err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tlog.Tracef(\"Generated sphinx packet: %v\",\n\t\tnewLogClosure(func() string {\n\t\t\t// We make a copy of the ephemeral key and unset the\n\t\t\t// internal curve here in order to keep the logs from\n\t\t\t// getting noisy.\n\t\t\tkey := *sphinxPacket.EphemeralKey\n\t\t\tpacketCopy := *sphinxPacket\n\t\t\tpacketCopy.EphemeralKey = &key\n\t\t\treturn spew.Sdump(packetCopy)\n\t\t}),\n\t)\n\n\treturn onionBlob.Bytes(), &sphinx.Circuit{\n\t\tSessionKey:  sessionKey,\n\t\tPaymentPath: sphinxPath.NodeKeys(),\n\t}, nil\n}\n\n// LightningPayment describes a payment to be sent through the network to the\n// final destination.",
      "length": 1751,
      "tokens": 244,
      "embedding": []
    },
    {
      "slug": "type LightningPayment struct {",
      "content": "type LightningPayment struct {\n\t// Target is the node in which the payment should be routed towards.\n\tTarget route.Vertex\n\n\t// Amount is the value of the payment to send through the network in\n\t// milli-satoshis.\n\tAmount lnwire.MilliSatoshi\n\n\t// FeeLimit is the maximum fee in millisatoshis that the payment should\n\t// accept when sending it through the network. The payment will fail\n\t// if there isn't a route with lower fees than this limit.\n\tFeeLimit lnwire.MilliSatoshi\n\n\t// CltvLimit is the maximum time lock that is allowed for attempts to\n\t// complete this payment.\n\tCltvLimit uint32\n\n\t// paymentHash is the r-hash value to use within the HTLC extended to\n\t// the first hop. This won't be set for AMP payments.\n\tpaymentHash *lntypes.Hash\n\n\t// amp is an optional field that is set if and only if this is am AMP\n\t// payment.\n\tamp *AMPOptions\n\n\t// FinalCLTVDelta is the CTLV expiry delta to use for the _final_ hop\n\t// in the route. This means that the final hop will have a CLTV delta\n\t// of at least: currentHeight + FinalCLTVDelta.\n\tFinalCLTVDelta uint16\n\n\t// PayAttemptTimeout is a timeout value that we'll use to determine\n\t// when we should should abandon the payment attempt after consecutive\n\t// payment failure. This prevents us from attempting to send a payment\n\t// indefinitely. A zero value means the payment will never time out.\n\t//\n\t// TODO(halseth): make wallclock time to allow resume after startup.\n\tPayAttemptTimeout time.Duration\n\n\t// RouteHints represents the different routing hints that can be used to\n\t// assist a payment in reaching its destination successfully. These\n\t// hints will act as intermediate hops along the route.\n\t//\n\t// NOTE: This is optional unless required by the payment. When providing\n\t// multiple routes, ensure the hop hints within each route are chained\n\t// together and sorted in forward order in order to reach the\n\t// destination successfully.\n\tRouteHints [][]zpay32.HopHint\n\n\t// OutgoingChannelIDs is the list of channels that are allowed for the\n\t// first hop. If nil, any channel may be used.\n\tOutgoingChannelIDs []uint64\n\n\t// LastHop is the pubkey of the last node before the final destination\n\t// is reached. If nil, any node may be used.\n\tLastHop *route.Vertex\n\n\t// DestFeatures specifies the set of features we assume the final node\n\t// has for pathfinding. Typically these will be taken directly from an\n\t// invoice, but they can also be manually supplied or assumed by the\n\t// sender. If a nil feature vector is provided, the router will try to\n\t// fallback to the graph in order to load a feature vector for a node in\n\t// the public graph.\n\tDestFeatures *lnwire.FeatureVector\n\n\t// PaymentAddr is the payment address specified by the receiver. This\n\t// field should be a random 32-byte nonce presented in the receiver's\n\t// invoice to prevent probing of the destination.\n\tPaymentAddr *[32]byte\n\n\t// PaymentRequest is an optional payment request that this payment is\n\t// attempting to complete.\n\tPaymentRequest []byte\n\n\t// DestCustomRecords are TLV records that are to be sent to the final\n\t// hop in the new onion payload format. If the destination does not\n\t// understand this new onion payload format, then the payment will\n\t// fail.\n\tDestCustomRecords record.CustomSet\n\n\t// MaxParts is the maximum number of partial payments that may be used\n\t// to complete the full amount.\n\tMaxParts uint32\n\n\t// MaxShardAmt is the largest shard that we'll attempt to split using.\n\t// If this field is set, and we need to split, rather than attempting\n\t// half of the original payment amount, we'll use this value if half\n\t// the payment amount is greater than it.\n\t//\n\t// NOTE: This field is _optional_.\n\tMaxShardAmt *lnwire.MilliSatoshi\n\n\t// TimePref is the time preference for this payment. Set to -1 to\n\t// optimize for fees only, to 1 to optimize for reliability only or a\n\t// value in between for a mix.\n\tTimePref float64\n\n\t// Metadata is additional data that is sent along with the payment to\n\t// the payee.\n\tMetadata []byte\n}\n\n// AMPOptions houses information that must be known in order to send an AMP\n// payment.",
      "length": 3940,
      "tokens": 668,
      "embedding": []
    },
    {
      "slug": "type AMPOptions struct {",
      "content": "type AMPOptions struct {\n\tSetID     [32]byte\n\tRootShare [32]byte\n}\n\n// SetPaymentHash sets the given hash as the payment's overall hash. This\n// should only be used for non-AMP payments.",
      "length": 156,
      "tokens": 25,
      "embedding": []
    },
    {
      "slug": "func (l *LightningPayment) SetPaymentHash(hash lntypes.Hash) error {",
      "content": "func (l *LightningPayment) SetPaymentHash(hash lntypes.Hash) error {\n\tif l.amp != nil {\n\t\treturn fmt.Errorf(\"cannot set payment hash for AMP payment\")\n\t}\n\n\tl.paymentHash = &hash\n\treturn nil\n}\n\n// SetAMP sets the given AMP options for the payment.",
      "length": 169,
      "tokens": 30,
      "embedding": []
    },
    {
      "slug": "func (l *LightningPayment) SetAMP(amp *AMPOptions) error {",
      "content": "func (l *LightningPayment) SetAMP(amp *AMPOptions) error {\n\tif l.paymentHash != nil {\n\t\treturn fmt.Errorf(\"cannot set amp options for payment \" +\n\t\t\t\"with payment hash\")\n\t}\n\n\tl.amp = amp\n\treturn nil\n}\n\n// Identifier returns a 32-byte slice that uniquely identifies this single\n// payment. For non-AMP payments this will be the payment hash, for AMP\n// payments this will be the used SetID.",
      "length": 319,
      "tokens": 56,
      "embedding": []
    },
    {
      "slug": "func (l *LightningPayment) Identifier() [32]byte {",
      "content": "func (l *LightningPayment) Identifier() [32]byte {\n\tif l.amp != nil {\n\t\treturn l.amp.SetID\n\t}\n\n\treturn *l.paymentHash\n}\n\n// SendPayment attempts to send a payment as described within the passed\n// LightningPayment. This function is blocking and will return either: when the\n// payment is successful, or all candidates routes have been attempted and\n// resulted in a failed payment. If the payment succeeds, then a non-nil Route\n// will be returned which describes the path the successful payment traversed\n// within the network to reach the destination. Additionally, the payment\n// preimage will also be returned.",
      "length": 550,
      "tokens": 90,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) SendPayment(payment *LightningPayment) ([32]byte,",
      "content": "func (r *ChannelRouter) SendPayment(payment *LightningPayment) ([32]byte,\n\t*route.Route, error) {\n\n\tpaySession, shardTracker, err := r.preparePayment(payment)\n\tif err != nil {\n\t\treturn [32]byte{}, nil, err\n\t}\n\n\tlog.Tracef(\"Dispatching SendPayment for lightning payment: %v\",\n\t\tspewPayment(payment))\n\n\t// Since this is the first time this payment is being made, we pass nil\n\t// for the existing attempt.\n\treturn r.sendPayment(\n\t\tpayment.FeeLimit, payment.Identifier(),\n\t\tpayment.PayAttemptTimeout, paySession, shardTracker,\n\t)\n}\n\n// SendPaymentAsync is the non-blocking version of SendPayment. The payment\n// result needs to be retrieved via the control tower.",
      "length": 566,
      "tokens": 74,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) SendPaymentAsync(payment *LightningPayment) error {",
      "content": "func (r *ChannelRouter) SendPaymentAsync(payment *LightningPayment) error {\n\tpaySession, shardTracker, err := r.preparePayment(payment)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Since this is the first time this payment is being made, we pass nil\n\t// for the existing attempt.\n\tr.wg.Add(1)\n\tgo func() {\n\t\tdefer r.wg.Done()\n\n\t\tlog.Tracef(\"Dispatching SendPayment for lightning payment: %v\",\n\t\t\tspewPayment(payment))\n\n\t\t_, _, err := r.sendPayment(\n\t\t\tpayment.FeeLimit, payment.Identifier(),\n\t\t\tpayment.PayAttemptTimeout, paySession, shardTracker,\n\t\t)\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"Payment %x failed: %v\",\n\t\t\t\tpayment.Identifier(), err)\n\t\t}\n\t}()\n\n\treturn nil\n}\n\n// spewPayment returns a log closures that provides a spewed string\n// representation of the passed payment.",
      "length": 662,
      "tokens": 90,
      "embedding": []
    },
    {
      "slug": "func spewPayment(payment *LightningPayment) logClosure {",
      "content": "func spewPayment(payment *LightningPayment) logClosure {\n\treturn newLogClosure(func() string {\n\t\t// Make a copy of the payment with a nilled Curve\n\t\t// before spewing.\n\t\tvar routeHints [][]zpay32.HopHint\n\t\tfor _, routeHint := range payment.RouteHints {\n\t\t\tvar hopHints []zpay32.HopHint\n\t\t\tfor _, hopHint := range routeHint {\n\t\t\t\th := hopHint.Copy()\n\t\t\t\thopHints = append(hopHints, h)\n\t\t\t}\n\t\t\trouteHints = append(routeHints, hopHints)\n\t\t}\n\t\tp := *payment\n\t\tp.RouteHints = routeHints\n\t\treturn spew.Sdump(p)\n\t})\n}\n\n// preparePayment creates the payment session and registers the payment with the\n// control tower.",
      "length": 534,
      "tokens": 76,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) preparePayment(payment *LightningPayment) (",
      "content": "func (r *ChannelRouter) preparePayment(payment *LightningPayment) (\n\tPaymentSession, shards.ShardTracker, error) {\n\n\t// Before starting the HTLC routing attempt, we'll create a fresh\n\t// payment session which will report our errors back to mission\n\t// control.\n\tpaySession, err := r.cfg.SessionSource.NewPaymentSession(payment)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\t// Record this payment hash with the ControlTower, ensuring it is not\n\t// already in-flight.\n\t//\n\t// TODO(roasbeef): store records as part of creation info?\n\tinfo := &channeldb.PaymentCreationInfo{\n\t\tPaymentIdentifier: payment.Identifier(),\n\t\tValue:             payment.Amount,\n\t\tCreationTime:      r.cfg.Clock.Now(),\n\t\tPaymentRequest:    payment.PaymentRequest,\n\t}\n\n\t// Create a new ShardTracker that we'll use during the life cycle of\n\t// this payment.\n\tvar shardTracker shards.ShardTracker\n\tswitch {\n\t// If this is an AMP payment, we'll use the AMP shard tracker.\n\tcase payment.amp != nil:\n\t\tshardTracker = amp.NewShardTracker(\n\t\t\tpayment.amp.RootShare, payment.amp.SetID,\n\t\t\t*payment.PaymentAddr, payment.Amount,\n\t\t)\n\n\t// Otherwise we'll use the simple tracker that will map each attempt to\n\t// the same payment hash.\n\tdefault:\n\t\tshardTracker = shards.NewSimpleShardTracker(\n\t\t\tpayment.Identifier(), nil,\n\t\t)\n\t}\n\n\terr = r.cfg.Control.InitPayment(payment.Identifier(), info)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\treturn paySession, shardTracker, nil\n}\n\n// SendToRoute sends a payment using the provided route and fails the payment\n// when an error is returned from the attempt.",
      "length": 1443,
      "tokens": 192,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) SendToRoute(htlcHash lntypes.Hash,",
      "content": "func (r *ChannelRouter) SendToRoute(htlcHash lntypes.Hash,\n\trt *route.Route) (*channeldb.HTLCAttempt, error) {\n\n\treturn r.sendToRoute(htlcHash, rt, false)\n}\n\n// SendToRouteSkipTempErr sends a payment using the provided route and fails\n// the payment ONLY when a terminal error is returned from the attempt.",
      "length": 241,
      "tokens": 34,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) SendToRouteSkipTempErr(htlcHash lntypes.Hash,",
      "content": "func (r *ChannelRouter) SendToRouteSkipTempErr(htlcHash lntypes.Hash,\n\trt *route.Route) (*channeldb.HTLCAttempt, error) {\n\n\treturn r.sendToRoute(htlcHash, rt, true)\n}\n\n// sendToRoute attempts to send a payment with the given hash through the\n// provided route. This function is blocking and will return the attempt\n// information as it is stored in the database. For a successful htlc, this\n// information will contain the preimage. If an error occurs after the attempt\n// was initiated, both return values will be non-nil. If skipTempErr is true,\n// the payment won't be failed unless a terminal error has occurred.",
      "length": 536,
      "tokens": 87,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) sendToRoute(htlcHash lntypes.Hash, rt *route.Route,",
      "content": "func (r *ChannelRouter) sendToRoute(htlcHash lntypes.Hash, rt *route.Route,\n\tskipTempErr bool) (*channeldb.HTLCAttempt, error) {\n\n\t// Calculate amount paid to receiver.\n\tamt := rt.ReceiverAmt()\n\n\t// If this is meant as a MP payment shard, we set the amount\n\t// for the creating info to the total amount of the payment.\n\tfinalHop := rt.Hops[len(rt.Hops)-1]\n\tmpp := finalHop.MPP\n\tif mpp != nil {\n\t\tamt = mpp.TotalMsat()\n\t}\n\n\t// For non-AMP payments the overall payment identifier will be the same\n\t// hash as used for this HTLC.\n\tpaymentIdentifier := htlcHash\n\n\t// For AMP-payments, we'll use the setID as the unique ID for the\n\t// overall payment.\n\tamp := finalHop.AMP\n\tif amp != nil {\n\t\tpaymentIdentifier = amp.SetID()\n\t}\n\n\t// Record this payment hash with the ControlTower, ensuring it is not\n\t// already in-flight.\n\tinfo := &channeldb.PaymentCreationInfo{\n\t\tPaymentIdentifier: paymentIdentifier,\n\t\tValue:             amt,\n\t\tCreationTime:      r.cfg.Clock.Now(),\n\t\tPaymentRequest:    nil,\n\t}\n\n\terr := r.cfg.Control.InitPayment(paymentIdentifier, info)\n\tswitch {\n\t// If this is an MPP attempt and the hash is already registered with\n\t// the database, we can go on to launch the shard.\n\tcase err == channeldb.ErrPaymentInFlight && mpp != nil:\n\n\t// Any other error is not tolerated.\n\tcase err != nil:\n\t\treturn nil, err\n\t}\n\n\tlog.Tracef(\"Dispatching SendToRoute for HTLC hash %v: %v\",\n\t\thtlcHash, newLogClosure(func() string {\n\t\t\treturn spew.Sdump(rt)\n\t\t}),\n\t)\n\n\t// Since the HTLC hashes and preimages are specified manually over the\n\t// RPC for SendToRoute requests, we don't have to worry about creating\n\t// a ShardTracker that can generate hashes for AMP payments. Instead we\n\t// create a simple tracker that can just return the hash for the single\n\t// shard we'll now launch.\n\tshardTracker := shards.NewSimpleShardTracker(htlcHash, nil)\n\n\t// Launch a shard along the given route.\n\tsh := &shardHandler{\n\t\trouter:       r,\n\t\tidentifier:   paymentIdentifier,\n\t\tshardTracker: shardTracker,\n\t}\n\n\tvar shardError error\n\tattempt, outcome, err := sh.launchShard(rt, false)\n\n\t// With SendToRoute, it can happen that the route exceeds protocol\n\t// constraints. Mark the payment as failed with an internal error.\n\tif err == route.ErrMaxRouteHopsExceeded ||\n\t\terr == sphinx.ErrMaxRoutingInfoSizeExceeded {\n\n\t\tlog.Debugf(\"Invalid route provided for payment %x: %v\",\n\t\t\tpaymentIdentifier, err)\n\n\t\tcontrolErr := r.cfg.Control.FailPayment(\n\t\t\tpaymentIdentifier, channeldb.FailureReasonError,\n\t\t)\n\t\tif controlErr != nil {\n\t\t\treturn nil, controlErr\n\t\t}\n\t}\n\n\t// In any case, don't continue if there is an error.\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar htlcAttempt *channeldb.HTLCAttempt\n\tswitch {\n\t// Failed to launch shard.\n\tcase outcome.err != nil:\n\t\tshardError = outcome.err\n\t\thtlcAttempt = outcome.attempt\n\n\t// Shard successfully launched, wait for the result to be available.\n\tdefault:\n\t\tresult, err := sh.collectResult(attempt)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// We got a successful result.\n\t\tif result.err == nil {\n\t\t\treturn result.attempt, nil\n\t\t}\n\n\t\t// The shard failed, break switch to handle it.\n\t\tshardError = result.err\n\t\thtlcAttempt = result.attempt\n\t}\n\n\t// Since for SendToRoute we won't retry in case the shard fails, we'll\n\t// mark the payment failed with the control tower immediately. Process\n\t// the error to check if it maps into a terminal error code, if not use\n\t// a generic NO_ROUTE error.\n\tvar failureReason *channeldb.FailureReason\n\terr = sh.handleSendError(attempt, shardError)\n\n\tswitch {\n\t// If a non-terminal error is returned and `skipTempErr` is false, then\n\t// we'll use the normal no route error.\n\tcase err == nil && !skipTempErr:\n\t\terr = r.cfg.Control.FailPayment(\n\t\t\tpaymentIdentifier, channeldb.FailureReasonNoRoute,\n\t\t)\n\n\t// If this is a failure reason, then we'll apply the failure directly\n\t// to the control tower, and return the normal response to the caller.\n\tcase goErrors.As(err, &failureReason):\n\t\terr = r.cfg.Control.FailPayment(\n\t\t\tpaymentIdentifier, *failureReason,\n\t\t)\n\t}\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn htlcAttempt, shardError\n}\n\n// sendPayment attempts to send a payment to the passed payment hash. This\n// function is blocking and will return either: when the payment is successful,\n// or all candidates routes have been attempted and resulted in a failed\n// payment. If the payment succeeds, then a non-nil Route will be returned\n// which describes the path the successful payment traversed within the network\n// to reach the destination. Additionally, the payment preimage will also be\n// returned.\n//\n// The existing attempt argument should be set to nil if this is a payment that\n// haven't had any payment attempt sent to the switch yet. If it has had an\n// attempt already, it should be passed such that the result can be retrieved.\n//\n// This method relies on the ControlTower's internal payment state machine to\n// carry out its execution. After restarts it is safe, and assumed, that the\n// router will call this method for every payment still in-flight according to\n// the ControlTower.",
      "length": 4828,
      "tokens": 742,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) sendPayment(feeLimit lnwire.MilliSatoshi,",
      "content": "func (r *ChannelRouter) sendPayment(feeLimit lnwire.MilliSatoshi,\n\tidentifier lntypes.Hash, timeout time.Duration,\n\tpaySession PaymentSession,\n\tshardTracker shards.ShardTracker) ([32]byte, *route.Route, error) {\n\n\t// We'll also fetch the current block height so we can properly\n\t// calculate the required HTLC time locks within the route.\n\t_, currentHeight, err := r.cfg.Chain.GetBestBlock()\n\tif err != nil {\n\t\treturn [32]byte{}, nil, err\n\t}\n\n\t// Now set up a paymentLifecycle struct with these params, such that we\n\t// can resume the payment from the current state.\n\tp := &paymentLifecycle{\n\t\trouter:        r,\n\t\tfeeLimit:      feeLimit,\n\t\tidentifier:    identifier,\n\t\tpaySession:    paySession,\n\t\tshardTracker:  shardTracker,\n\t\tcurrentHeight: currentHeight,\n\t}\n\n\t// If a timeout is specified, create a timeout channel. If no timeout is\n\t// specified, the channel is left nil and will never abort the payment\n\t// loop.\n\tif timeout != 0 {\n\t\tp.timeoutChan = time.After(timeout)\n\t}\n\n\treturn p.resumePayment()\n\n}\n\n// extractChannelUpdate examines the error and extracts the channel update.",
      "length": 987,
      "tokens": 138,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) extractChannelUpdate(",
      "content": "func (r *ChannelRouter) extractChannelUpdate(\n\tfailure lnwire.FailureMessage) *lnwire.ChannelUpdate {\n\n\tvar update *lnwire.ChannelUpdate\n\tswitch onionErr := failure.(type) {\n\tcase *lnwire.FailExpiryTooSoon:\n\t\tupdate = &onionErr.Update\n\tcase *lnwire.FailAmountBelowMinimum:\n\t\tupdate = &onionErr.Update\n\tcase *lnwire.FailFeeInsufficient:\n\t\tupdate = &onionErr.Update\n\tcase *lnwire.FailIncorrectCltvExpiry:\n\t\tupdate = &onionErr.Update\n\tcase *lnwire.FailChannelDisabled:\n\t\tupdate = &onionErr.Update\n\tcase *lnwire.FailTemporaryChannelFailure:\n\t\tupdate = onionErr.Update\n\t}\n\n\treturn update\n}\n\n// applyChannelUpdate validates a channel update and if valid, applies it to the\n// database. It returns a bool indicating whether the updates were successful.",
      "length": 677,
      "tokens": 71,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) applyChannelUpdate(msg *lnwire.ChannelUpdate) bool {",
      "content": "func (r *ChannelRouter) applyChannelUpdate(msg *lnwire.ChannelUpdate) bool {\n\tch, _, _, err := r.GetChannelByID(msg.ShortChannelID)\n\tif err != nil {\n\t\tlog.Errorf(\"Unable to retrieve channel by id: %v\", err)\n\t\treturn false\n\t}\n\n\tvar pubKey *btcec.PublicKey\n\n\tswitch msg.ChannelFlags & lnwire.ChanUpdateDirection {\n\tcase 0:\n\t\tpubKey, _ = ch.NodeKey1()\n\n\tcase 1:\n\t\tpubKey, _ = ch.NodeKey2()\n\t}\n\n\t// Exit early if the pubkey cannot be decided.\n\tif pubKey == nil {\n\t\tlog.Errorf(\"Unable to decide pubkey with ChannelFlags=%v\",\n\t\t\tmsg.ChannelFlags)\n\t\treturn false\n\t}\n\n\terr = ValidateChannelUpdateAnn(pubKey, ch.Capacity, msg)\n\tif err != nil {\n\t\tlog.Errorf(\"Unable to validate channel update: %v\", err)\n\t\treturn false\n\t}\n\n\terr = r.UpdateEdge(&channeldb.ChannelEdgePolicy{\n\t\tSigBytes:                  msg.Signature.ToSignatureBytes(),\n\t\tChannelID:                 msg.ShortChannelID.ToUint64(),\n\t\tLastUpdate:                time.Unix(int64(msg.Timestamp), 0),\n\t\tMessageFlags:              msg.MessageFlags,\n\t\tChannelFlags:              msg.ChannelFlags,\n\t\tTimeLockDelta:             msg.TimeLockDelta,\n\t\tMinHTLC:                   msg.HtlcMinimumMsat,\n\t\tMaxHTLC:                   msg.HtlcMaximumMsat,\n\t\tFeeBaseMSat:               lnwire.MilliSatoshi(msg.BaseFee),\n\t\tFeeProportionalMillionths: lnwire.MilliSatoshi(msg.FeeRate),\n\t})\n\tif err != nil && !IsError(err, ErrIgnored, ErrOutdated) {\n\t\tlog.Errorf(\"Unable to apply channel update: %v\", err)\n\t\treturn false\n\t}\n\n\treturn true\n}\n\n// AddNode is used to add information about a node to the router database. If\n// the node with this pubkey is not present in an existing channel, it will\n// be ignored.\n//\n// NOTE: This method is part of the ChannelGraphSource interface.",
      "length": 1580,
      "tokens": 178,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) AddNode(node *channeldb.LightningNode,",
      "content": "func (r *ChannelRouter) AddNode(node *channeldb.LightningNode,\n\top ...batch.SchedulerOption) error {\n\n\trMsg := &routingMsg{\n\t\tmsg: node,\n\t\top:  op,\n\t\terr: make(chan error, 1),\n\t}\n\n\tselect {\n\tcase r.networkUpdates <- rMsg:\n\t\tselect {\n\t\tcase err := <-rMsg.err:\n\t\t\treturn err\n\t\tcase <-r.quit:\n\t\t\treturn ErrRouterShuttingDown\n\t\t}\n\tcase <-r.quit:\n\t\treturn ErrRouterShuttingDown\n\t}\n}\n\n// AddEdge is used to add edge/channel to the topology of the router, after all\n// information about channel will be gathered this edge/channel might be used\n// in construction of payment path.\n//\n// NOTE: This method is part of the ChannelGraphSource interface.",
      "length": 553,
      "tokens": 85,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) AddEdge(edge *channeldb.ChannelEdgeInfo,",
      "content": "func (r *ChannelRouter) AddEdge(edge *channeldb.ChannelEdgeInfo,\n\top ...batch.SchedulerOption) error {\n\n\trMsg := &routingMsg{\n\t\tmsg: edge,\n\t\top:  op,\n\t\terr: make(chan error, 1),\n\t}\n\n\tselect {\n\tcase r.networkUpdates <- rMsg:\n\t\tselect {\n\t\tcase err := <-rMsg.err:\n\t\t\treturn err\n\t\tcase <-r.quit:\n\t\t\treturn ErrRouterShuttingDown\n\t\t}\n\tcase <-r.quit:\n\t\treturn ErrRouterShuttingDown\n\t}\n}\n\n// UpdateEdge is used to update edge information, without this message edge\n// considered as not fully constructed.\n//\n// NOTE: This method is part of the ChannelGraphSource interface.",
      "length": 476,
      "tokens": 70,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) UpdateEdge(update *channeldb.ChannelEdgePolicy,",
      "content": "func (r *ChannelRouter) UpdateEdge(update *channeldb.ChannelEdgePolicy,\n\top ...batch.SchedulerOption) error {\n\n\trMsg := &routingMsg{\n\t\tmsg: update,\n\t\top:  op,\n\t\terr: make(chan error, 1),\n\t}\n\n\tselect {\n\tcase r.networkUpdates <- rMsg:\n\t\tselect {\n\t\tcase err := <-rMsg.err:\n\t\t\treturn err\n\t\tcase <-r.quit:\n\t\t\treturn ErrRouterShuttingDown\n\t\t}\n\tcase <-r.quit:\n\t\treturn ErrRouterShuttingDown\n\t}\n}\n\n// CurrentBlockHeight returns the block height from POV of the router subsystem.\n//\n// NOTE: This method is part of the ChannelGraphSource interface.",
      "length": 444,
      "tokens": 64,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) CurrentBlockHeight() (uint32, error) {",
      "content": "func (r *ChannelRouter) CurrentBlockHeight() (uint32, error) {\n\t_, height, err := r.cfg.Chain.GetBestBlock()\n\treturn uint32(height), err\n}\n\n// SyncedHeight returns the block height to which the router subsystem currently\n// is synced to. This can differ from the above chain height if the goroutine\n// responsible for processing the blocks isn't yet up to speed.",
      "length": 293,
      "tokens": 47,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) SyncedHeight() uint32 {",
      "content": "func (r *ChannelRouter) SyncedHeight() uint32 {\n\treturn atomic.LoadUint32(&r.bestHeight)\n}\n\n// GetChannelByID return the channel by the channel id.\n//\n// NOTE: This method is part of the ChannelGraphSource interface.",
      "length": 163,
      "tokens": 23,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) GetChannelByID(chanID lnwire.ShortChannelID) (",
      "content": "func (r *ChannelRouter) GetChannelByID(chanID lnwire.ShortChannelID) (\n\t*channeldb.ChannelEdgeInfo,\n\t*channeldb.ChannelEdgePolicy,\n\t*channeldb.ChannelEdgePolicy, error) {\n\n\treturn r.cfg.Graph.FetchChannelEdgesByID(chanID.ToUint64())\n}\n\n// FetchLightningNode attempts to look up a target node by its identity public\n// key. channeldb.ErrGraphNodeNotFound is returned if the node doesn't exist\n// within the graph.\n//\n// NOTE: This method is part of the ChannelGraphSource interface.",
      "length": 399,
      "tokens": 46,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) FetchLightningNode(",
      "content": "func (r *ChannelRouter) FetchLightningNode(\n\tnode route.Vertex) (*channeldb.LightningNode, error) {\n\n\treturn r.cfg.Graph.FetchLightningNode(node)\n}\n\n// ForEachNode is used to iterate over every node in router topology.\n//\n// NOTE: This method is part of the ChannelGraphSource interface.",
      "length": 236,
      "tokens": 31,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) ForEachNode(cb func(*channeldb.LightningNode) error) error {",
      "content": "func (r *ChannelRouter) ForEachNode(cb func(*channeldb.LightningNode) error) error {\n\treturn r.cfg.Graph.ForEachNode(func(_ kvdb.RTx, n *channeldb.LightningNode) error {\n\t\treturn cb(n)\n\t})\n}\n\n// ForAllOutgoingChannels is used to iterate over all outgoing channels owned by\n// the router.\n//\n// NOTE: This method is part of the ChannelGraphSource interface.",
      "length": 263,
      "tokens": 37,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) ForAllOutgoingChannels(cb func(kvdb.RTx,",
      "content": "func (r *ChannelRouter) ForAllOutgoingChannels(cb func(kvdb.RTx,\n\t*channeldb.ChannelEdgeInfo, *channeldb.ChannelEdgePolicy) error) error {\n\n\treturn r.selfNode.ForEachChannel(nil, func(tx kvdb.RTx,\n\t\tc *channeldb.ChannelEdgeInfo,\n\t\te, _ *channeldb.ChannelEdgePolicy) error {\n\n\t\tif e == nil {\n\t\t\treturn fmt.Errorf(\"channel from self node has no policy\")\n\t\t}\n\n\t\treturn cb(tx, c, e)\n\t})\n}\n\n// AddProof updates the channel edge info with proof which is needed to\n// properly announce the edge to the rest of the network.\n//\n// NOTE: This method is part of the ChannelGraphSource interface.",
      "length": 502,
      "tokens": 71,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) AddProof(chanID lnwire.ShortChannelID,",
      "content": "func (r *ChannelRouter) AddProof(chanID lnwire.ShortChannelID,\n\tproof *channeldb.ChannelAuthProof) error {\n\n\tinfo, _, _, err := r.cfg.Graph.FetchChannelEdgesByID(chanID.ToUint64())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tinfo.AuthProof = proof\n\treturn r.cfg.Graph.UpdateChannelEdge(info)\n}\n\n// IsStaleNode returns true if the graph source has a node announcement for the\n// target node with a more recent timestamp.\n//\n// NOTE: This method is part of the ChannelGraphSource interface.",
      "length": 402,
      "tokens": 57,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) IsStaleNode(node route.Vertex,",
      "content": "func (r *ChannelRouter) IsStaleNode(node route.Vertex,\n\ttimestamp time.Time) bool {\n\n\t// If our attempt to assert that the node announcement is fresh fails,\n\t// then we know that this is actually a stale announcement.\n\terr := r.assertNodeAnnFreshness(node, timestamp)\n\tif err != nil {\n\t\tlog.Debugf(\"Checking stale node %x got %v\", node, err)\n\t\treturn true\n\t}\n\n\treturn false\n}\n\n// IsPublicNode determines whether the given vertex is seen as a public node in\n// the graph from the graph's source node's point of view.\n//\n// NOTE: This method is part of the ChannelGraphSource interface.",
      "length": 513,
      "tokens": 87,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) IsPublicNode(node route.Vertex) (bool, error) {",
      "content": "func (r *ChannelRouter) IsPublicNode(node route.Vertex) (bool, error) {\n\treturn r.cfg.Graph.IsPublicNode(node)\n}\n\n// IsKnownEdge returns true if the graph source already knows of the passed\n// channel ID either as a live or zombie edge.\n//\n// NOTE: This method is part of the ChannelGraphSource interface.",
      "length": 227,
      "tokens": 37,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) IsKnownEdge(chanID lnwire.ShortChannelID) bool {",
      "content": "func (r *ChannelRouter) IsKnownEdge(chanID lnwire.ShortChannelID) bool {\n\t_, _, exists, isZombie, _ := r.cfg.Graph.HasChannelEdge(chanID.ToUint64())\n\treturn exists || isZombie\n}\n\n// IsStaleEdgePolicy returns true if the graph source has a channel edge for\n// the passed channel ID (and flags) that have a more recent timestamp.\n//\n// NOTE: This method is part of the ChannelGraphSource interface.",
      "length": 316,
      "tokens": 49,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) IsStaleEdgePolicy(chanID lnwire.ShortChannelID,",
      "content": "func (r *ChannelRouter) IsStaleEdgePolicy(chanID lnwire.ShortChannelID,\n\ttimestamp time.Time, flags lnwire.ChanUpdateChanFlags) bool {\n\n\tedge1Timestamp, edge2Timestamp, exists, isZombie, err :=\n\t\tr.cfg.Graph.HasChannelEdge(chanID.ToUint64())\n\tif err != nil {\n\t\tlog.Debugf(\"Check stale edge policy got error: %v\", err)\n\t\treturn false\n\n\t}\n\n\t// If we know of the edge as a zombie, then we'll make some additional\n\t// checks to determine if the new policy is fresh.\n\tif isZombie {\n\t\t// When running with AssumeChannelValid, we also prune channels\n\t\t// if both of their edges are disabled. We'll mark the new\n\t\t// policy as stale if it remains disabled.\n\t\tif r.cfg.AssumeChannelValid {\n\t\t\tisDisabled := flags&lnwire.ChanUpdateDisabled ==\n\t\t\t\tlnwire.ChanUpdateDisabled\n\t\t\tif isDisabled {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\n\t\t// Otherwise, we'll fall back to our usual ChannelPruneExpiry.\n\t\treturn time.Since(timestamp) > r.cfg.ChannelPruneExpiry\n\t}\n\n\t// If we don't know of the edge, then it means it's fresh (thus not\n\t// stale).\n\tif !exists {\n\t\treturn false\n\t}\n\n\t// As edges are directional edge node has a unique policy for the\n\t// direction of the edge they control. Therefore we first check if we\n\t// already have the most up to date information for that edge. If so,\n\t// then we can exit early.\n\tswitch {\n\t// A flag set of 0 indicates this is an announcement for the \"first\"\n\t// node in the channel.\n\tcase flags&lnwire.ChanUpdateDirection == 0:\n\t\treturn !edge1Timestamp.Before(timestamp)\n\n\t// Similarly, a flag set of 1 indicates this is an announcement for the\n\t// \"second\" node in the channel.\n\tcase flags&lnwire.ChanUpdateDirection == 1:\n\t\treturn !edge2Timestamp.Before(timestamp)\n\t}\n\n\treturn false\n}\n\n// MarkEdgeLive clears an edge from our zombie index, deeming it as live.\n//\n// NOTE: This method is part of the ChannelGraphSource interface.",
      "length": 1711,
      "tokens": 265,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) MarkEdgeLive(chanID lnwire.ShortChannelID) error {",
      "content": "func (r *ChannelRouter) MarkEdgeLive(chanID lnwire.ShortChannelID) error {\n\treturn r.cfg.Graph.MarkEdgeLive(chanID.ToUint64())\n}\n\n// ErrNoChannel is returned when a route cannot be built because there are no\n// channels that satisfy all requirements.",
      "length": 171,
      "tokens": 23,
      "embedding": []
    },
    {
      "slug": "type ErrNoChannel struct {",
      "content": "type ErrNoChannel struct {\n\tposition int\n\tfromNode route.Vertex\n}\n\n// Error returns a human readable string describing the error.",
      "length": 98,
      "tokens": 15,
      "embedding": []
    },
    {
      "slug": "func (e ErrNoChannel) Error() string {",
      "content": "func (e ErrNoChannel) Error() string {\n\treturn fmt.Sprintf(\"no matching outgoing channel available for \"+\n\t\t\"node %v (%v)\", e.position, e.fromNode)\n}\n\n// BuildRoute returns a fully specified route based on a list of pubkeys. If\n// amount is nil, the minimum routable amount is used. To force a specific\n// outgoing channel, use the outgoingChan parameter.",
      "length": 310,
      "tokens": 49,
      "embedding": []
    },
    {
      "slug": "func (r *ChannelRouter) BuildRoute(amt *lnwire.MilliSatoshi,",
      "content": "func (r *ChannelRouter) BuildRoute(amt *lnwire.MilliSatoshi,\n\thops []route.Vertex, outgoingChan *uint64,\n\tfinalCltvDelta int32, payAddr *[32]byte) (*route.Route, error) {\n\n\tlog.Tracef(\"BuildRoute called: hopsCount=%v, amt=%v\",\n\t\tlen(hops), amt)\n\n\tvar outgoingChans map[uint64]struct{}\n\tif outgoingChan != nil {\n\t\toutgoingChans = map[uint64]struct{}{\n\t\t\t*outgoingChan: {},\n\t\t}\n\t}\n\n\t// If no amount is specified, we need to build a route for the minimum\n\t// amount that this route can carry.\n\tuseMinAmt := amt == nil\n\n\t// We'll attempt to obtain a set of bandwidth hints that helps us select\n\t// the best outgoing channel to use in case no outgoing channel is set.\n\tbandwidthHints, err := newBandwidthManager(\n\t\tr.cachedGraph, r.selfNode.PubKeyBytes, r.cfg.GetLink,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Fetch the current block height outside the routing transaction, to\n\t// prevent the rpc call blocking the database.\n\t_, height, err := r.cfg.Chain.GetBestBlock()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Allocate a list that will contain the edge unifiers for this route.\n\tunifiers := make([]*edgeUnifier, len(hops))\n\n\tvar runningAmt lnwire.MilliSatoshi\n\tif useMinAmt {\n\t\t// For minimum amount routes, aim to deliver at least 1 msat to\n\t\t// the destination. There are nodes in the wild that have a\n\t\t// min_htlc channel policy of zero, which could lead to a zero\n\t\t// amount payment being made.\n\t\trunningAmt = 1\n\t} else {\n\t\t// If an amount is specified, we need to build a route that\n\t\t// delivers exactly this amount to the final destination.\n\t\trunningAmt = *amt\n\t}\n\n\t// Traverse hops backwards to accumulate fees in the running amounts.\n\tsource := r.selfNode.PubKeyBytes\n\tfor i := len(hops) - 1; i >= 0; i-- {\n\t\ttoNode := hops[i]\n\n\t\tvar fromNode route.Vertex\n\t\tif i == 0 {\n\t\t\tfromNode = source\n\t\t} else {\n\t\t\tfromNode = hops[i-1]\n\t\t}\n\n\t\tlocalChan := i == 0\n\n\t\t// Build unified edges for this hop based on the channels known\n\t\t// in the graph.\n\t\tu := newNodeEdgeUnifier(source, toNode, outgoingChans)\n\n\t\terr := u.addGraphPolicies(r.cachedGraph)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// Exit if there are no channels.\n\t\tedgeUnifier, ok := u.edgeUnifiers[fromNode]\n\t\tif !ok {\n\t\t\tlog.Errorf(\"Cannot find policy for node %v\", fromNode)\n\t\t\treturn nil, ErrNoChannel{\n\t\t\t\tfromNode: fromNode,\n\t\t\t\tposition: i,\n\t\t\t}\n\t\t}\n\n\t\t// If using min amt, increase amt if needed.\n\t\tif useMinAmt {\n\t\t\tmin := edgeUnifier.minAmt()\n\t\t\tif min > runningAmt {\n\t\t\t\trunningAmt = min\n\t\t\t}\n\t\t}\n\n\t\t// Get an edge for the specific amount that we want to forward.\n\t\tedge := edgeUnifier.getEdge(runningAmt, bandwidthHints)\n\t\tif edge == nil {\n\t\t\tlog.Errorf(\"Cannot find policy with amt=%v for node %v\",\n\t\t\t\trunningAmt, fromNode)\n\n\t\t\treturn nil, ErrNoChannel{\n\t\t\t\tfromNode: fromNode,\n\t\t\t\tposition: i,\n\t\t\t}\n\t\t}\n\n\t\t// Add fee for this hop.\n\t\tif !localChan {\n\t\t\trunningAmt += edge.policy.ComputeFee(runningAmt)\n\t\t}\n\n\t\tlog.Tracef(\"Select channel %v at position %v\",\n\t\t\tedge.policy.ChannelID, i)\n\n\t\tunifiers[i] = edgeUnifier\n\t}\n\n\t// Now that we arrived at the start of the route and found out the route\n\t// total amount, we make a forward pass. Because the amount may have\n\t// been increased in the backward pass, fees need to be recalculated and\n\t// amount ranges re-checked.\n\tvar pathEdges []*channeldb.CachedEdgePolicy\n\treceiverAmt := runningAmt\n\tfor i, unifier := range unifiers {\n\t\tedge := unifier.getEdge(receiverAmt, bandwidthHints)\n\t\tif edge == nil {\n\t\t\treturn nil, ErrNoChannel{\n\t\t\t\tfromNode: hops[i-1],\n\t\t\t\tposition: i,\n\t\t\t}\n\t\t}\n\n\t\tif i > 0 {\n\t\t\t// Decrease the amount to send while going forward.\n\t\t\treceiverAmt -= edge.policy.ComputeFeeFromIncoming(\n\t\t\t\treceiverAmt,\n\t\t\t)\n\t\t}\n\n\t\tpathEdges = append(pathEdges, edge.policy)\n\t}\n\n\t// Build and return the final route.\n\treturn newRoute(\n\t\tsource, pathEdges, uint32(height),\n\t\tfinalHopParams{\n\t\t\tamt:         receiverAmt,\n\t\t\ttotalAmt:    receiverAmt,\n\t\t\tcltvDelta:   uint16(finalCltvDelta),\n\t\t\trecords:     nil,\n\t\t\tpaymentAddr: payAddr,\n\t\t},\n\t)\n}\n",
      "length": 3765,
      "tokens": 567,
      "embedding": []
    }
  ]
}