{
  "filepath": "../implementations/go/lnd/tlv/stream.go",
  "package": "tlv",
  "sections": [
    {
      "slug": "type Stream struct {",
      "content": "type Stream struct {\n\trecords []Record\n\tbuf     [8]byte\n}\n\n// NewStream creates a new TLV Stream given an encoding codec, a decoding codec,\n// and a set of known records.",
      "length": 144,
      "tokens": 26,
      "embedding": []
    },
    {
      "slug": "func NewStream(records ...Record) (*Stream, error) {",
      "content": "func NewStream(records ...Record) (*Stream, error) {\n\t// Assert that the ordering of the Records is canonical and appear in\n\t// ascending order of type.\n\tvar (\n\t\tmin      Type\n\t\toverflow bool\n\t)\n\tfor _, record := range records {\n\t\tif overflow || record.typ < min {\n\t\t\treturn nil, ErrStreamNotCanonical\n\t\t}\n\t\tif record.encoder == nil {\n\t\t\trecord.encoder = ENOP\n\t\t}\n\t\tif record.decoder == nil {\n\t\t\trecord.decoder = DNOP\n\t\t}\n\t\tif record.typ == math.MaxUint64 {\n\t\t\toverflow = true\n\t\t}\n\t\tmin = record.typ + 1\n\t}\n\n\treturn &Stream{\n\t\trecords: records,\n\t}, nil\n}\n\n// MustNewStream creates a new TLV Stream given an encoding codec, a decoding\n// codec, and a set of known records. If an error is encountered in creating the\n// stream, this method will panic instead of returning the error.",
      "length": 698,
      "tokens": 123,
      "embedding": []
    },
    {
      "slug": "func MustNewStream(records ...Record) *Stream {",
      "content": "func MustNewStream(records ...Record) *Stream {\n\tstream, err := NewStream(records...)\n\tif err != nil {\n\t\tpanic(err.Error())\n\t}\n\treturn stream\n}\n\n// Encode writes a Stream to the passed io.Writer. Each of the Records known to\n// the Stream is written in ascending order of their type so as to be canonical.\n//\n// The stream is constructed by concatenating the individual, serialized Records\n// where each record has the following format:\n//\n//\t[varint: type]\n//\t[varint: length]\n//\t[length: value]\n//\n// An error is returned if the io.Writer fails to accept bytes from the\n// encoding, and nothing else. The ordering of the Records is asserted upon the\n// creation of a Stream, and thus the output will be by definition canonical.",
      "length": 662,
      "tokens": 118,
      "embedding": []
    },
    {
      "slug": "func (s *Stream) Encode(w io.Writer) error {",
      "content": "func (s *Stream) Encode(w io.Writer) error {\n\t// Iterate through all known records, if any, serializing each record's\n\t// type, length and value.\n\tfor i := range s.records {\n\t\trec := &s.records[i]\n\n\t\t// Write the record's type as a varint.\n\t\terr := WriteVarInt(w, uint64(rec.typ), &s.buf)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Write the record's length as a varint.\n\t\terr = WriteVarInt(w, rec.Size(), &s.buf)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Encode the current record's value using the stream's codec.\n\t\terr = rec.encoder(w, rec.value, &s.buf)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Decode deserializes TLV Stream from the passed io.Reader for non-P2P\n// settings. The Stream will inspect each record that is parsed and check to\n// see if it has a corresponding Record to facilitate deserialization of that\n// field. If the record is unknown, the Stream will discard the record's bytes\n// and proceed to the subsequent record.\n//\n// Each record has the following format:\n//\n//\t[varint: type]\n//\t[varint: length]\n//\t[length: value]\n//\n// A series of (possibly zero) records are concatenated into a stream, this\n// example contains two records:\n//\n//\t(t: 0x01, l: 0x04, v: 0xff, 0xff, 0xff, 0xff)\n//\t(t: 0x02, l: 0x01, v: 0x01)\n//\n// This method asserts that the byte stream is canonical, namely that each\n// record is unique and that all records are sorted in ascending order. An\n// ErrNotCanonicalStream error is returned if the encoded TLV stream is not.\n//\n// We permit an io.EOF error only when reading the type byte which signals that\n// the last record was read cleanly and we should stop parsing. All other io.EOF\n// or io.ErrUnexpectedEOF errors are returned.",
      "length": 1602,
      "tokens": 285,
      "embedding": []
    },
    {
      "slug": "func (s *Stream) Decode(r io.Reader) error {",
      "content": "func (s *Stream) Decode(r io.Reader) error {\n\t_, err := s.decode(r, nil, false)\n\treturn err\n}\n\n// DecodeP2P is identical to Decode except that the maximum record size is\n// capped at 65535.",
      "length": 139,
      "tokens": 26,
      "embedding": []
    },
    {
      "slug": "func (s *Stream) DecodeP2P(r io.Reader) error {",
      "content": "func (s *Stream) DecodeP2P(r io.Reader) error {\n\t_, err := s.decode(r, nil, true)\n\treturn err\n}\n\n// DecodeWithParsedTypes is identical to Decode, but if successful, returns a\n// TypeMap containing the types of all records that were decoded or ignored from\n// the stream.",
      "length": 216,
      "tokens": 37,
      "embedding": []
    },
    {
      "slug": "func (s *Stream) DecodeWithParsedTypes(r io.Reader) (TypeMap, error) {",
      "content": "func (s *Stream) DecodeWithParsedTypes(r io.Reader) (TypeMap, error) {\n\treturn s.decode(r, make(TypeMap), false)\n}\n\n// DecodeWithParsedTypesP2P is identical to DecodeWithParsedTypes except that\n// the record size is capped at 65535. This should only be called from a p2p\n// setting where untrusted input is being deserialized.",
      "length": 250,
      "tokens": 37,
      "embedding": []
    },
    {
      "slug": "func (s *Stream) DecodeWithParsedTypesP2P(r io.Reader) (TypeMap, error) {",
      "content": "func (s *Stream) DecodeWithParsedTypesP2P(r io.Reader) (TypeMap, error) {\n\treturn s.decode(r, make(TypeMap), true)\n}\n\n// decode is a helper function that performs the basis of stream decoding. If\n// the caller needs the set of parsed types, it must provide an initialized\n// parsedTypes, otherwise the returned TypeMap will be nil. If the p2p bool is\n// true, then the record size is capped at 65535. Otherwise, it is not capped.",
      "length": 349,
      "tokens": 62,
      "embedding": []
    },
    {
      "slug": "func (s *Stream) decode(r io.Reader, parsedTypes TypeMap, p2p bool) (TypeMap,",
      "content": "func (s *Stream) decode(r io.Reader, parsedTypes TypeMap, p2p bool) (TypeMap,\n\terror) {\n\n\tvar (\n\t\ttyp       Type\n\t\tmin       Type\n\t\trecordIdx int\n\t\toverflow  bool\n\t)\n\n\t// Iterate through all possible type identifiers. As types are read from\n\t// the io.Reader, min will skip forward to the last read type.\n\tfor {\n\t\t// Read the next varint type.\n\t\tt, err := ReadVarInt(r, &s.buf)\n\t\tswitch {\n\n\t\t// We'll silence an EOF when zero bytes remain, meaning the\n\t\t// stream was cleanly encoded.\n\t\tcase err == io.EOF:\n\t\t\treturn parsedTypes, nil\n\n\t\t// Other unexpected errors.\n\t\tcase err != nil:\n\t\t\treturn nil, err\n\t\t}\n\n\t\ttyp = Type(t)\n\n\t\t// Assert that this type is greater than any previously read.\n\t\t// If we've already overflowed and we parsed another type, the\n\t\t// stream is not canonical. This check prevents us from accepts\n\t\t// encodings that have duplicate records or from accepting an\n\t\t// unsorted series.\n\t\tif overflow || typ < min {\n\t\t\treturn nil, ErrStreamNotCanonical\n\t\t}\n\n\t\t// Read the varint length.\n\t\tlength, err := ReadVarInt(r, &s.buf)\n\t\tswitch {\n\n\t\t// We'll convert any EOFs to ErrUnexpectedEOF, since this\n\t\t// results in an invalid record.\n\t\tcase err == io.EOF:\n\t\t\treturn nil, io.ErrUnexpectedEOF\n\n\t\t// Other unexpected errors.\n\t\tcase err != nil:\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// Place a soft limit on the size of a sane record, which\n\t\t// prevents malicious encoders from causing us to allocate an\n\t\t// unbounded amount of memory when decoding variable-sized\n\t\t// fields. This is only checked when the p2p bool is true.\n\t\tif p2p && length > MaxRecordSize {\n\t\t\treturn nil, ErrRecordTooLarge\n\t\t}\n\n\t\t// Search the records known to the stream for this type. We'll\n\t\t// begin the search and recordIdx and walk forward until we find\n\t\t// it or the next record's type is larger.\n\t\trec, newIdx, ok := s.getRecord(typ, recordIdx)\n\t\tswitch {\n\n\t\t// We know of this record type, proceed to decode the value.\n\t\t// This method asserts that length bytes are read in the\n\t\t// process, and returns an error if the number of bytes is not\n\t\t// exactly length.\n\t\tcase ok:\n\t\t\terr := rec.decoder(r, rec.value, &s.buf, length)\n\t\t\tswitch {\n\n\t\t\t// We'll convert any EOFs to ErrUnexpectedEOF, since this\n\t\t\t// results in an invalid record.\n\t\t\tcase err == io.EOF:\n\t\t\t\treturn nil, io.ErrUnexpectedEOF\n\n\t\t\t// Other unexpected errors.\n\t\t\tcase err != nil:\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\t// Record the successfully decoded type if the caller\n\t\t\t// provided an initialized TypeMap.\n\t\t\tif parsedTypes != nil {\n\t\t\t\tparsedTypes[typ] = nil\n\t\t\t}\n\n\t\t// Otherwise, the record type is unknown and is odd, discard the\n\t\t// number of bytes specified by length.\n\t\tdefault:\n\t\t\t// If the caller provided an initialized TypeMap, record\n\t\t\t// the encoded bytes.\n\t\t\tvar b *bytes.Buffer\n\t\t\twriter := ioutil.Discard\n\t\t\tif parsedTypes != nil {\n\t\t\t\tb = bytes.NewBuffer(make([]byte, 0, length))\n\t\t\t\twriter = b\n\t\t\t}\n\n\t\t\t_, err := io.CopyN(writer, r, int64(length))\n\t\t\tswitch {\n\n\t\t\t// We'll convert any EOFs to ErrUnexpectedEOF, since this\n\t\t\t// results in an invalid record.\n\t\t\tcase err == io.EOF:\n\t\t\t\treturn nil, io.ErrUnexpectedEOF\n\n\t\t\t// Other unexpected errors.\n\t\t\tcase err != nil:\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tif parsedTypes != nil {\n\t\t\t\tparsedTypes[typ] = b.Bytes()\n\t\t\t}\n\t\t}\n\n\t\t// Update our record index so that we can begin our next search\n\t\t// from where we left off.\n\t\trecordIdx = newIdx\n\n\t\t// If we've parsed the largest possible type, the next loop will\n\t\t// overflow back to zero. However, we need to attempt parsing\n\t\t// the next type to ensure that the stream is empty.\n\t\tif typ == math.MaxUint64 {\n\t\t\toverflow = true\n\t\t}\n\n\t\t// Finally, set our lower bound on the next accepted type.\n\t\tmin = typ + 1\n\t}\n}\n\n// getRecord searches for a record matching typ known to the stream. The boolean\n// return value indicates whether the record is known to the stream. The integer\n// return value carries the index from where getRecord should be invoked on the\n// subsequent call. The first call to getRecord should always use an idx of 0.",
      "length": 3777,
      "tokens": 639,
      "embedding": []
    },
    {
      "slug": "func (s *Stream) getRecord(typ Type, idx int) (Record, int, bool) {",
      "content": "func (s *Stream) getRecord(typ Type, idx int) (Record, int, bool) {\n\tfor idx < len(s.records) {\n\t\trecord := s.records[idx]\n\t\tswitch {\n\n\t\t// Found target record, return it to the caller. The next index\n\t\t// returned points to the immediately following record.\n\t\tcase record.typ == typ:\n\t\t\treturn record, idx + 1, true\n\n\t\t// This record's type is lower than the target. Advance our\n\t\t// index and continue to the next record which will have a\n\t\t// strictly higher type.\n\t\tcase record.typ < typ:\n\t\t\tidx++\n\t\t\tcontinue\n\n\t\t// This record's type is larger than the target, hence we have\n\t\t// no record matching the current type. Return the current index\n\t\t// so that we can start our search from here when processing the\n\t\t// next tlv record.\n\t\tdefault:\n\t\t\treturn Record{}, idx, false\n\t\t}\n\t}\n\n\t// All known records are exhausted.\n\treturn Record{}, idx, false\n}\n",
      "length": 758,
      "tokens": 131,
      "embedding": []
    }
  ]
}