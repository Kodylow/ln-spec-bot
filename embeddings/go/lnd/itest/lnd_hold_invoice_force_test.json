{
  "filepath": "../implementations/go/lnd/itest/lnd_hold_invoice_force_test.go",
  "package": "itest",
  "sections": [
    {
      "slug": "func testHoldInvoiceForceClose(ht *lntest.HarnessTest) {",
      "content": "func testHoldInvoiceForceClose(ht *lntest.HarnessTest) {\n\t// Open a channel between alice and bob.\n\talice, bob := ht.Alice, ht.Bob\n\tchanPoint := ht.OpenChannel(\n\t\talice, bob, lntest.OpenChannelParams{Amt: 300000},\n\t)\n\n\t// Create a non-dust hold invoice for bob.\n\tvar (\n\t\tpreimage = lntypes.Preimage{1, 2, 3}\n\t\tpayHash  = preimage.Hash()\n\t)\n\tinvoiceReq := &invoicesrpc.AddHoldInvoiceRequest{\n\t\tValue:      30000,\n\t\tCltvExpiry: 40,\n\t\tHash:       payHash[:],\n\t}\n\tbobInvoice := bob.RPC.AddHoldInvoice(invoiceReq)\n\n\t// Subscribe the invoice.\n\tstream := bob.RPC.SubscribeSingleInvoice(payHash[:])\n\n\t// Pay this invoice from Alice -> Bob, we should achieve this with a\n\t// single htlc.\n\treq := &routerrpc.SendPaymentRequest{\n\t\tPaymentRequest: bobInvoice.PaymentRequest,\n\t\tTimeoutSeconds: 60,\n\t\tFeeLimitMsat:   noFeeLimitMsat,\n\t}\n\talice.RPC.SendPayment(req)\n\n\tht.AssertInvoiceState(stream, lnrpc.Invoice_ACCEPTED)\n\n\t// Once the HTLC has cleared, alice and bob should both have a single\n\t// htlc locked in.\n\tht.AssertActiveHtlcs(alice, payHash[:])\n\tht.AssertActiveHtlcs(bob, payHash[:])\n\n\t// Get our htlc expiry height and current block height so that we\n\t// can mine the exact number of blocks required to expire the htlc.\n\tchannel := ht.QueryChannelByChanPoint(alice, chanPoint)\n\trequire.Len(ht, channel.PendingHtlcs, 1)\n\tactiveHtlc := channel.PendingHtlcs[0]\n\n\t_, currentHeight := ht.Miner.GetBestBlock()\n\n\t// Now we will mine blocks until the htlc expires, and wait for each\n\t// node to sync to our latest height. Sanity check that we won't\n\t// underflow.\n\trequire.Greater(ht, activeHtlc.ExpirationHeight, uint32(currentHeight),\n\t\t\"expected expiry after current height\")\n\tblocksTillExpiry := activeHtlc.ExpirationHeight - uint32(currentHeight)\n\n\t// Alice will go to chain with some delta, sanity check that we won't\n\t// underflow and subtract this from our mined blocks.\n\trequire.Greater(ht, blocksTillExpiry,\n\t\tuint32(lncfg.DefaultOutgoingBroadcastDelta))\n\n\t// blocksTillForce is the number of blocks should be mined to\n\t// trigger a force close from Alice iff the invoice cancelation\n\t// failed. This value is 48 in current test setup.\n\tblocksTillForce := blocksTillExpiry -\n\t\tlncfg.DefaultOutgoingBroadcastDelta\n\n\t// blocksTillCancel is the number of blocks should be mined to trigger\n\t// an invoice cancelation from Bob. This value is 30 in current test\n\t// setup.\n\tblocksTillCancel := blocksTillExpiry -\n\t\tlncfg.DefaultHoldInvoiceExpiryDelta\n\n\t// When using ht.MineBlocks, for bitcoind backend, the block height\n\t// synced differ significantly among subsystems. From observation, the\n\t// LNWL syncs much faster than other subsystems, with more than 10\n\t// blocks ahead. For this test case, CRTR may be lagging behind for\n\t// more than 20 blocks. Thus we use slow mining instead.\n\t// TODO(yy): fix block height asymmetry among all the subsystems.\n\t//\n\t// We first mine enough blocks to trigger an invoice cancelation.\n\tht.MineBlocks(blocksTillCancel)\n\n\t// Wait for the nodes to be synced.\n\tht.WaitForBlockchainSync(alice)\n\tht.WaitForBlockchainSync(bob)\n\n\t// Check that the invoice is canceled by Bob.\n\terr := wait.NoError(func() error {\n\t\tinv := bob.RPC.LookupInvoice(payHash[:])\n\n\t\tif inv.State != lnrpc.Invoice_CANCELED {\n\t\t\treturn fmt.Errorf(\"expected canceled invoice, got: %v\",\n\t\t\t\tinv.State)\n\t\t}\n\n\t\tfor _, htlc := range inv.Htlcs {\n\t\t\tif htlc.State != lnrpc.InvoiceHTLCState_CANCELED {\n\t\t\t\treturn fmt.Errorf(\"expected htlc canceled, \"+\n\t\t\t\t\t\"got: %v\", htlc.State)\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t}, defaultTimeout)\n\trequire.NoError(ht, err, \"expected canceled invoice\")\n\n\t// We now continue to mine more blocks to the point where it could have\n\t// triggered a force close if the invoice cancelation was failed.\n\t//\n\t// NOTE: we need to mine blocks in two sections because of a following\n\t// case has happened frequently with bitcoind backend,\n\t// - when mining all the blocks together, subsystems were syncing\n\t// blocks under very different speed.\n\t// - Bob would cancel the invoice in INVC, and send an UpdateFailHTLC\n\t// in PEER.\n\t// - Alice, however, would need to receive the message before her\n\t// subsystem CNCT being synced to the force close height. This didn't\n\t// happen in bitcoind backend, as Alice's CNCT was syncing way faster\n\t// than Bob's INVC, causing the channel being force closed before the\n\t// invoice cancelation message was received by Alice.\n\tht.MineBlocks(blocksTillForce - blocksTillCancel)\n\n\t// Wait for the nodes to be synced.\n\tht.WaitForBlockchainSync(alice)\n\tht.WaitForBlockchainSync(bob)\n\n\t// Check that Alice has not closed the channel because there are no\n\t// outgoing HTLCs in her channel as the only HTLC has already been\n\t// canceled.\n\tht.AssertNumPendingForceClose(alice, 0)\n\n\t// Clean up the channel.\n\tht.CloseChannel(alice, chanPoint)\n}\n",
      "length": 4587,
      "tokens": 627,
      "embedding": []
    }
  ]
}