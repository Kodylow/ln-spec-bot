{
  "filepath": "../implementations/go/lnd/itest/lnd_channel_backup_test.go",
  "package": "itest",
  "sections": [
    {
      "slug": "type (",
      "content": "type (\n\t// nodeRestorer is a function closure that allows each test case to\n\t// control exactly *how* the prior node is restored. This might be\n\t// using an backup obtained over RPC, or the file system, etc.\n\tnodeRestorer func() *node.HarnessNode\n\n\t// restoreMethod takes an old node, then returns a function closure\n\t// that'll return the same node, but with its state restored via a\n\t// custom method. We use this to abstract away _how_ a node is restored\n\t// from our assertions once the node has been fully restored itself.\n\trestoreMethodType func(ht *lntest.HarnessTest,\n\t\toldNode *node.HarnessNode, backupFilePath string,\n\t\tpassword []byte, mnemonic []string) nodeRestorer\n)\n\n// revocationWindow is used when we specify the revocation window used when\n// restoring node.\nconst revocationWindow = 100\n\n// chanRestoreScenario represents a test case used by testing the channel\n// restore methods.",
      "length": 874,
      "tokens": 135,
      "embedding": []
    },
    {
      "slug": "type chanRestoreScenario struct {",
      "content": "type chanRestoreScenario struct {\n\tcarol    *node.HarnessNode\n\tdave     *node.HarnessNode\n\tpassword []byte\n\tmnemonic []string\n\tparams   lntest.OpenChannelParams\n}\n\n// newChanRestoreScenario creates a new scenario that has two nodes, Carol and\n// Dave, connected and funded.",
      "length": 231,
      "tokens": 28,
      "embedding": []
    },
    {
      "slug": "func newChanRestoreScenario(ht *lntest.HarnessTest, ct lnrpc.CommitmentType,",
      "content": "func newChanRestoreScenario(ht *lntest.HarnessTest, ct lnrpc.CommitmentType,\n\tzeroConf bool) *chanRestoreScenario {\n\n\tconst (\n\t\tchanAmt = btcutil.Amount(10000000)\n\t\tpushAmt = btcutil.Amount(5000000)\n\t)\n\n\tpassword := []byte(\"El Psy Kongroo\")\n\tnodeArgs := []string{\n\t\t\"--minbackoff=50ms\",\n\t\t\"--maxbackoff=1s\",\n\t}\n\n\tif ct != lnrpc.CommitmentType_UNKNOWN_COMMITMENT_TYPE {\n\t\targs := lntest.NodeArgsForCommitType(ct)\n\t\tnodeArgs = append(nodeArgs, args...)\n\t}\n\n\tif zeroConf {\n\t\tnodeArgs = append(\n\t\t\tnodeArgs, \"--protocol.option-scid-alias\",\n\t\t\t\"--protocol.zero-conf\",\n\t\t)\n\t}\n\n\t// First, we'll create a brand new node we'll use within the test. If\n\t// we have a custom backup file specified, then we'll also create that\n\t// for use.\n\tdave, mnemonic, _ := ht.NewNodeWithSeed(\n\t\t\"dave\", nodeArgs, password, false,\n\t)\n\tcarol := ht.NewNode(\"carol\", nodeArgs)\n\n\t// Now that our new nodes are created, we'll give them some coins for\n\t// channel opening and anchor sweeping.\n\tht.FundCoinsUnconfirmed(btcutil.SatoshiPerBitcoin, carol)\n\tht.FundCoinsUnconfirmed(btcutil.SatoshiPerBitcoin, dave)\n\n\t// Mine a block to confirm the funds.\n\tht.MineBlocks(1)\n\n\t// For the anchor output case we need two UTXOs for Carol so she can\n\t// sweep both the local and remote anchor.\n\tif lntest.CommitTypeHasAnchors(ct) {\n\t\tht.FundCoins(btcutil.SatoshiPerBitcoin, carol)\n\t}\n\n\t// Next, we'll connect Dave to Carol, and open a new channel to her\n\t// with a portion pushed.\n\tht.ConnectNodes(dave, carol)\n\n\treturn &chanRestoreScenario{\n\t\tcarol:    carol,\n\t\tdave:     dave,\n\t\tmnemonic: mnemonic,\n\t\tpassword: password,\n\t\tparams: lntest.OpenChannelParams{\n\t\t\tAmt:            chanAmt,\n\t\t\tPushAmt:        pushAmt,\n\t\t\tZeroConf:       zeroConf,\n\t\t\tCommitmentType: ct,\n\t\t},\n\t}\n}\n\n// restoreDave will call the `nodeRestorer` and asserts Dave is restored by\n// checking his wallet balance against zero.",
      "length": 1713,
      "tokens": 217,
      "embedding": []
    },
    {
      "slug": "func (c *chanRestoreScenario) restoreDave(ht *lntest.HarnessTest,",
      "content": "func (c *chanRestoreScenario) restoreDave(ht *lntest.HarnessTest,\n\trestoredNodeFunc nodeRestorer) *node.HarnessNode {\n\n\t// Next, we'll make a new Dave and start the bulk of our recovery\n\t// workflow.\n\tdave := restoredNodeFunc()\n\n\t// First ensure that the on-chain balance is restored.\n\terr := wait.NoError(func() error {\n\t\tdaveBalResp := dave.RPC.WalletBalance()\n\t\tdaveBal := daveBalResp.ConfirmedBalance\n\t\tif daveBal <= 0 {\n\t\t\treturn fmt.Errorf(\"expected positive balance, had %v\",\n\t\t\t\tdaveBal)\n\t\t}\n\n\t\treturn nil\n\t}, defaultTimeout)\n\trequire.NoError(ht, err, \"On-chain balance not restored\")\n\n\treturn dave\n}\n\n// testScenario runs a test case with a given setup and asserts the DLP is\n// executed as expected, in details, it will,\n//  1. shutdown Dave.\n//  2. suspend Carol.\n//  3. restore Dave.\n//  4. validate pending channel state and check we cannot force close it.\n//  5. validate Carol's UTXOs.\n//  6. assert DLP is executed.",
      "length": 836,
      "tokens": 128,
      "embedding": []
    },
    {
      "slug": "func (c *chanRestoreScenario) testScenario(ht *lntest.HarnessTest,",
      "content": "func (c *chanRestoreScenario) testScenario(ht *lntest.HarnessTest,\n\trestoredNodeFunc nodeRestorer) {\n\n\tcarol, dave := c.carol, c.dave\n\n\t// Before we start the recovery, we'll record the balances of both\n\t// Carol and Dave to ensure they both sweep their coins at the end.\n\tcarolBalResp := carol.RPC.WalletBalance()\n\tcarolStartingBalance := carolBalResp.ConfirmedBalance\n\n\tdaveBalance := dave.RPC.WalletBalance()\n\tdaveStartingBalance := daveBalance.ConfirmedBalance\n\n\t// Now that we're able to make our restored now, we'll shutdown the old\n\t// Dave node as we'll be storing it shortly below.\n\tht.Shutdown(dave)\n\n\t// To make sure the channel state is advanced correctly if the channel\n\t// peer is not online at first, we also shutdown Carol.\n\trestartCarol := ht.SuspendNode(carol)\n\n\t// We now restore Dave.\n\tdave = c.restoreDave(ht, restoredNodeFunc)\n\n\t// We now check that the restored channel is in the proper state. It\n\t// should not yet be force closing as no connection with the remote\n\t// peer was established yet. We should also not be able to close the\n\t// channel.\n\tchannel := ht.AssertNumWaitingClose(dave, 1)[0]\n\tchanPointStr := channel.Channel.ChannelPoint\n\n\t// We also want to make sure we cannot force close in this state. That\n\t// would get the state machine in a weird state.\n\tchanPointParts := strings.Split(chanPointStr, \":\")\n\tchanPointIndex, _ := strconv.ParseUint(chanPointParts[1], 10, 32)\n\n\t// We don't get an error directly but only when reading the first\n\t// message of the stream.\n\terr := ht.CloseChannelAssertErr(\n\t\tdave, &lnrpc.ChannelPoint{\n\t\t\tFundingTxid: &lnrpc.ChannelPoint_FundingTxidStr{\n\t\t\t\tFundingTxidStr: chanPointParts[0],\n\t\t\t},\n\t\t\tOutputIndex: uint32(chanPointIndex),\n\t\t}, true,\n\t)\n\trequire.Contains(ht, err.Error(), \"cannot close channel with state: \")\n\trequire.Contains(ht, err.Error(), \"ChanStatusRestored\")\n\n\t// Increase the fee estimate so that the following force close tx will\n\t// be cpfp'ed in case of anchor commitments.\n\tht.SetFeeEstimate(30000)\n\n\t// Now that we have ensured that the channels restored by the backup\n\t// are in the correct state even without the remote peer telling us so,\n\t// let's start up Carol again.\n\trequire.NoError(ht, restartCarol(), \"restart carol failed\")\n\n\tif lntest.CommitTypeHasAnchors(c.params.CommitmentType) {\n\t\tht.AssertNumUTXOs(carol, 2)\n\t} else {\n\t\tht.AssertNumUTXOs(carol, 1)\n\t}\n\n\t// Now we'll assert that both sides properly execute the DLP protocol.\n\t// We grab their balances now to ensure that they're made whole at the\n\t// end of the protocol.\n\tassertDLPExecuted(\n\t\tht, carol, carolStartingBalance, dave,\n\t\tdaveStartingBalance, c.params.CommitmentType,\n\t)\n}\n\n// testChannelBackupRestore tests that we're able to recover from, and initiate\n// the DLP protocol via: the RPC restore command, restoring on unlock, and\n// restoring from initial wallet creation. We'll also alternate between\n// restoring form the on disk file, and restoring from the exported RPC command\n// as well.",
      "length": 2822,
      "tokens": 398,
      "embedding": []
    },
    {
      "slug": "func testChannelBackupRestoreBasic(ht *lntest.HarnessTest) {",
      "content": "func testChannelBackupRestoreBasic(ht *lntest.HarnessTest) {\n\tvar testCases = []struct {\n\t\tname          string\n\t\trestoreMethod restoreMethodType\n\t}{\n\t\t// Restore from backups obtained via the RPC interface. Dave\n\t\t// was the initiator, of the non-advertised channel.\n\t\t{\n\t\t\tname: \"restore from RPC backup\",\n\t\t\trestoreMethod: func(st *lntest.HarnessTest,\n\t\t\t\toldNode *node.HarnessNode,\n\t\t\t\tbackupFilePath string,\n\t\t\t\tpassword []byte,\n\t\t\t\tmnemonic []string) nodeRestorer {\n\n\t\t\t\t// For this restoration method, we'll grab the\n\t\t\t\t// current multi-channel backup from the old\n\t\t\t\t// node, and use it to restore a new node\n\t\t\t\t// within the closure.\n\t\t\t\tchanBackup := oldNode.RPC.ExportAllChanBackups()\n\n\t\t\t\tmulti := chanBackup.MultiChanBackup.\n\t\t\t\t\tMultiChanBackup\n\n\t\t\t\t// In our nodeRestorer function, we'll restore\n\t\t\t\t// the node from seed, then manually recover\n\t\t\t\t// the channel backup.\n\t\t\t\treturn chanRestoreViaRPC(\n\t\t\t\t\tst, password, mnemonic, multi, oldNode,\n\t\t\t\t)\n\t\t\t},\n\t\t},\n\n\t\t// Restore the backup from the on-disk file, using the RPC\n\t\t// interface.\n\t\t{\n\t\t\tname: \"restore from backup file\",\n\t\t\trestoreMethod: func(st *lntest.HarnessTest,\n\t\t\t\toldNode *node.HarnessNode,\n\t\t\t\tbackupFilePath string,\n\t\t\t\tpassword []byte,\n\t\t\t\tmnemonic []string) nodeRestorer {\n\n\t\t\t\t// Read the entire Multi backup stored within\n\t\t\t\t// this node's channel.backup file.\n\t\t\t\tmulti, err := ioutil.ReadFile(backupFilePath)\n\t\t\t\trequire.NoError(st, err)\n\n\t\t\t\t// Now that we have Dave's backup file, we'll\n\t\t\t\t// create a new nodeRestorer that will restore\n\t\t\t\t// using the on-disk channel.backup.\n\t\t\t\treturn chanRestoreViaRPC(\n\t\t\t\t\tst, password, mnemonic, multi, oldNode,\n\t\t\t\t)\n\t\t\t},\n\t\t},\n\n\t\t// Restore the backup as part of node initialization with the\n\t\t// prior mnemonic and new backup seed.\n\t\t{\n\t\t\tname: \"restore during creation\",\n\t\t\trestoreMethod: func(st *lntest.HarnessTest,\n\t\t\t\toldNode *node.HarnessNode,\n\t\t\t\tbackupFilePath string,\n\t\t\t\tpassword []byte,\n\t\t\t\tmnemonic []string) nodeRestorer {\n\n\t\t\t\t// First, fetch the current backup state as is,\n\t\t\t\t// to obtain our latest Multi.\n\t\t\t\tchanBackup := oldNode.RPC.ExportAllChanBackups()\n\t\t\t\tbackupSnapshot := &lnrpc.ChanBackupSnapshot{\n\t\t\t\t\tMultiChanBackup: chanBackup.\n\t\t\t\t\t\tMultiChanBackup,\n\t\t\t\t}\n\n\t\t\t\t// Create a new nodeRestorer that will restore\n\t\t\t\t// the node using the Multi backup we just\n\t\t\t\t// obtained above.\n\t\t\t\treturn func() *node.HarnessNode {\n\t\t\t\t\treturn st.RestoreNodeWithSeed(\n\t\t\t\t\t\t\"dave\", nil, password, mnemonic,\n\t\t\t\t\t\t\"\", revocationWindow,\n\t\t\t\t\t\tbackupSnapshot,\n\t\t\t\t\t\tcopyPorts(oldNode),\n\t\t\t\t\t)\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\n\t\t// Restore the backup once the node has already been\n\t\t// re-created, using the Unlock call.\n\t\t{\n\t\t\tname: \"restore during unlock\",\n\t\t\trestoreMethod: func(st *lntest.HarnessTest,\n\t\t\t\toldNode *node.HarnessNode,\n\t\t\t\tbackupFilePath string,\n\t\t\t\tpassword []byte,\n\t\t\t\tmnemonic []string) nodeRestorer {\n\n\t\t\t\t// First, fetch the current backup state as is,\n\t\t\t\t// to obtain our latest Multi.\n\t\t\t\tchanBackup := oldNode.RPC.ExportAllChanBackups()\n\t\t\t\tbackupSnapshot := &lnrpc.ChanBackupSnapshot{\n\t\t\t\t\tMultiChanBackup: chanBackup.\n\t\t\t\t\t\tMultiChanBackup,\n\t\t\t\t}\n\n\t\t\t\t// Create a new nodeRestorer that will restore\n\t\t\t\t// the node with its seed, but no channel\n\t\t\t\t// backup, shutdown this initialized node, then\n\t\t\t\t// restart it again using Unlock.\n\t\t\t\treturn func() *node.HarnessNode {\n\t\t\t\t\tnewNode := st.RestoreNodeWithSeed(\n\t\t\t\t\t\t\"dave\", nil, password, mnemonic,\n\t\t\t\t\t\t\"\", revocationWindow, nil,\n\t\t\t\t\t\tcopyPorts(oldNode),\n\t\t\t\t\t)\n\t\t\t\t\tst.RestartNodeWithChanBackups(\n\t\t\t\t\t\tnewNode, backupSnapshot,\n\t\t\t\t\t)\n\n\t\t\t\t\treturn newNode\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\n\t\t// Restore the backup from the on-disk file a second time to\n\t\t// make sure imports can be canceled and later resumed.\n\t\t{\n\t\t\tname: \"restore from backup file twice\",\n\t\t\trestoreMethod: func(st *lntest.HarnessTest,\n\t\t\t\toldNode *node.HarnessNode,\n\t\t\t\tbackupFilePath string,\n\t\t\t\tpassword []byte,\n\t\t\t\tmnemonic []string) nodeRestorer {\n\n\t\t\t\t// Read the entire Multi backup stored within\n\t\t\t\t// this node's channel.backup file.\n\t\t\t\tmulti, err := ioutil.ReadFile(backupFilePath)\n\t\t\t\trequire.NoError(st, err)\n\n\t\t\t\t// Now that we have Dave's backup file, we'll\n\t\t\t\t// create a new nodeRestorer that will restore\n\t\t\t\t// using the on-disk channel.backup.\n\t\t\t\t//\n\t\t\t\t//nolint:lll\n\t\t\t\tbackup := &lnrpc.RestoreChanBackupRequest_MultiChanBackup{\n\t\t\t\t\tMultiChanBackup: multi,\n\t\t\t\t}\n\n\t\t\t\treturn func() *node.HarnessNode {\n\t\t\t\t\tnewNode := st.RestoreNodeWithSeed(\n\t\t\t\t\t\t\"dave\", nil, password, mnemonic,\n\t\t\t\t\t\t\"\", revocationWindow, nil,\n\t\t\t\t\t\tcopyPorts(oldNode),\n\t\t\t\t\t)\n\n\t\t\t\t\treq := &lnrpc.RestoreChanBackupRequest{\n\t\t\t\t\t\tBackup: backup,\n\t\t\t\t\t}\n\t\t\t\t\tnewNode.RPC.RestoreChanBackups(req)\n\n\t\t\t\t\treq = &lnrpc.RestoreChanBackupRequest{\n\t\t\t\t\t\tBackup: backup,\n\t\t\t\t\t}\n\t\t\t\t\tnewNode.RPC.RestoreChanBackups(req)\n\n\t\t\t\t\treturn newNode\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, testCase := range testCases {\n\t\ttc := testCase\n\t\tsuccess := ht.Run(tc.name, func(t *testing.T) {\n\t\t\th := ht.Subtest(t)\n\n\t\t\trunChanRestoreScenarioBasic(h, tc.restoreMethod)\n\t\t})\n\t\tif !success {\n\t\t\tbreak\n\t\t}\n\t}\n}\n\n// runChanRestoreScenarioBasic executes a given test case from end to end,\n// ensuring that after Dave restores his channel state according to the\n// testCase, the DLP protocol is executed properly and both nodes are made\n// whole.",
      "length": 5029,
      "tokens": 602,
      "embedding": []
    },
    {
      "slug": "func runChanRestoreScenarioBasic(ht *lntest.HarnessTest,",
      "content": "func runChanRestoreScenarioBasic(ht *lntest.HarnessTest,\n\trestoreMethod restoreMethodType) {\n\n\t// Create a new retore scenario.\n\tcrs := newChanRestoreScenario(\n\t\tht, lnrpc.CommitmentType_UNKNOWN_COMMITMENT_TYPE, false,\n\t)\n\tcarol, dave := crs.carol, crs.dave\n\n\t// Open a channel from Dave to Carol.\n\tht.OpenChannel(dave, carol, crs.params)\n\n\t// At this point, we'll now execute the restore method to give us the\n\t// new node we should attempt our assertions against.\n\tbackupFilePath := dave.Cfg.ChanBackupPath()\n\trestoredNodeFunc := restoreMethod(\n\t\tht, dave, backupFilePath, crs.password, crs.mnemonic,\n\t)\n\n\t// Test the scenario.\n\tcrs.testScenario(ht, restoredNodeFunc)\n}\n\n// testChannelBackupRestoreUnconfirmed tests that we're able to restore from\n// disk file and the exported RPC command for unconfirmed channel.",
      "length": 736,
      "tokens": 94,
      "embedding": []
    },
    {
      "slug": "func testChannelBackupRestoreUnconfirmed(ht *lntest.HarnessTest) {",
      "content": "func testChannelBackupRestoreUnconfirmed(ht *lntest.HarnessTest) {\n\t// Use the channel backup file that contains an unconfirmed channel and\n\t// make sure recovery works as well.\n\tht.Run(\"restore unconfirmed channel file\", func(t *testing.T) {\n\t\tst := ht.Subtest(t)\n\t\trunChanRestoreScenarioUnConfirmed(st, true)\n\t})\n\n\t// Create a backup using RPC that contains an unconfirmed channel and\n\t// make sure recovery works as well.\n\tht.Run(\"restore unconfirmed channel RPC\", func(t *testing.T) {\n\t\tst := ht.Subtest(t)\n\t\trunChanRestoreScenarioUnConfirmed(st, false)\n\t})\n}\n\n// runChanRestoreScenarioUnConfirmed checks that Dave is able to restore for an\n// unconfirmed channel.",
      "length": 585,
      "tokens": 79,
      "embedding": []
    },
    {
      "slug": "func runChanRestoreScenarioUnConfirmed(ht *lntest.HarnessTest, useFile bool) {",
      "content": "func runChanRestoreScenarioUnConfirmed(ht *lntest.HarnessTest, useFile bool) {\n\t// Create a new retore scenario.\n\tcrs := newChanRestoreScenario(\n\t\tht, lnrpc.CommitmentType_UNKNOWN_COMMITMENT_TYPE, false,\n\t)\n\tcarol, dave := crs.carol, crs.dave\n\n\t// Open a pending channel.\n\tht.OpenChannelAssertPending(dave, carol, crs.params)\n\n\t// Give the pubsub some time to update the channel backup.\n\terr := wait.NoError(func() error {\n\t\tfi, err := os.Stat(dave.Cfg.ChanBackupPath())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif fi.Size() <= chanbackup.NilMultiSizePacked {\n\t\t\treturn fmt.Errorf(\"backup file empty\")\n\t\t}\n\n\t\treturn nil\n\t}, defaultTimeout)\n\trequire.NoError(ht, err, \"channel backup not updated in time\")\n\n\t// At this point, we'll now execute the restore method to give us the\n\t// new node we should attempt our assertions against.\n\tvar multi []byte\n\tif useFile {\n\t\tbackupFilePath := dave.Cfg.ChanBackupPath()\n\t\t// Read the entire Multi backup stored within this node's\n\t\t// channel.backup file.\n\t\tmulti, err = ioutil.ReadFile(backupFilePath)\n\t\trequire.NoError(ht, err)\n\t} else {\n\t\t// For this restoration method, we'll grab the current\n\t\t// multi-channel backup from the old node. The channel should\n\t\t// be included, even if it is not confirmed yet.\n\t\tchanBackup := dave.RPC.ExportAllChanBackups()\n\t\tchanPoints := chanBackup.MultiChanBackup.ChanPoints\n\t\trequire.NotEmpty(ht, chanPoints,\n\t\t\t\"unconfirmed channel not found\")\n\t\tmulti = chanBackup.MultiChanBackup.MultiChanBackup\n\t}\n\n\t// Let's assume time passes, the channel confirms in the meantime but\n\t// for some reason the backup we made while it was still unconfirmed is\n\t// the only backup we have. We should still be able to restore it. To\n\t// simulate time passing, we mine some blocks to get the channel\n\t// confirmed _after_ we saved the backup.\n\tht.MineBlocksAndAssertNumTxes(6, 1)\n\n\t// In our nodeRestorer function, we'll restore the node from seed, then\n\t// manually recover the channel backup.\n\trestoredNodeFunc := chanRestoreViaRPC(\n\t\tht, crs.password, crs.mnemonic, multi, dave,\n\t)\n\n\t// Test the scenario.\n\tcrs.testScenario(ht, restoredNodeFunc)\n}\n\n// testChannelBackupRestoreCommitTypes tests that we're able to recover from,\n// and initiate the DLP protocol for different channel commitment types and\n// zero-conf channel.",
      "length": 2145,
      "tokens": 295,
      "embedding": []
    },
    {
      "slug": "func testChannelBackupRestoreCommitTypes(ht *lntest.HarnessTest) {",
      "content": "func testChannelBackupRestoreCommitTypes(ht *lntest.HarnessTest) {\n\tvar testCases = []struct {\n\t\tname     string\n\t\tct       lnrpc.CommitmentType\n\t\tzeroConf bool\n\t}{\n\t\t// Restore the backup from the on-disk file, using the RPC\n\t\t// interface, for anchor commitment channels.\n\t\t{\n\t\t\tname: \"restore from backup file anchors\",\n\t\t\tct:   lnrpc.CommitmentType_ANCHORS,\n\t\t},\n\n\t\t// Restore the backup from the on-disk file, using the RPC\n\t\t// interface, for script-enforced leased channels.\n\t\t{\n\t\t\tname: \"restore from backup file script \" +\n\t\t\t\t\"enforced lease\",\n\t\t\tct: lnrpc.CommitmentType_SCRIPT_ENFORCED_LEASE,\n\t\t},\n\n\t\t// Restore the backup from the on-disk file, using the RPC\n\t\t// interface, for zero-conf anchor channels.\n\t\t{\n\t\t\tname: \"restore from backup file for zero-conf \" +\n\t\t\t\t\"anchors channel\",\n\t\t\tct:       lnrpc.CommitmentType_ANCHORS,\n\t\t\tzeroConf: true,\n\t\t},\n\n\t\t// Restore the backup from the on-disk file, using the RPC\n\t\t// interface for a zero-conf script-enforced leased channel.\n\t\t{\n\t\t\tname: \"restore from backup file zero-conf \" +\n\t\t\t\t\"script-enforced leased channel\",\n\t\t\tct:       lnrpc.CommitmentType_SCRIPT_ENFORCED_LEASE,\n\t\t\tzeroConf: true,\n\t\t},\n\t}\n\n\tfor _, testCase := range testCases {\n\t\ttc := testCase\n\t\tsuccess := ht.Run(tc.name, func(t *testing.T) {\n\t\t\th := ht.Subtest(t)\n\n\t\t\trunChanRestoreScenarioCommitTypes(\n\t\t\t\th, tc.ct, tc.zeroConf,\n\t\t\t)\n\t\t})\n\t\tif !success {\n\t\t\tbreak\n\t\t}\n\t}\n}\n\n// runChanRestoreScenarioCommitTypes tests that the DLP is applied for\n// different channel commitment types and zero-conf channel.",
      "length": 1414,
      "tokens": 190,
      "embedding": []
    },
    {
      "slug": "func runChanRestoreScenarioCommitTypes(ht *lntest.HarnessTest,",
      "content": "func runChanRestoreScenarioCommitTypes(ht *lntest.HarnessTest,\n\tct lnrpc.CommitmentType, zeroConf bool) {\n\n\t// Create a new retore scenario.\n\tcrs := newChanRestoreScenario(ht, ct, zeroConf)\n\tcarol, dave := crs.carol, crs.dave\n\n\t// If we are testing zero-conf channels, setup a ChannelAcceptor for\n\t// the fundee.\n\tvar cancelAcceptor context.CancelFunc\n\tif zeroConf {\n\t\t// Setup a ChannelAcceptor.\n\t\tacceptStream, cancel := carol.RPC.ChannelAcceptor()\n\t\tcancelAcceptor = cancel\n\t\tgo acceptChannel(ht.T, true, acceptStream)\n\t}\n\n\tvar fundingShim *lnrpc.FundingShim\n\tif ct == lnrpc.CommitmentType_SCRIPT_ENFORCED_LEASE {\n\t\t_, minerHeight := ht.Miner.GetBestBlock()\n\t\tthawHeight := uint32(minerHeight + thawHeightDelta)\n\n\t\tfundingShim, _ = deriveFundingShim(\n\t\t\tht, dave, carol, crs.params.Amt, thawHeight, true,\n\t\t)\n\t\tcrs.params.FundingShim = fundingShim\n\t}\n\tht.OpenChannel(dave, carol, crs.params)\n\n\t// Remove the ChannelAcceptor.\n\tif zeroConf {\n\t\tcancelAcceptor()\n\t}\n\n\t// At this point, we'll now execute the restore method to give us the\n\t// new node we should attempt our assertions against.\n\tbackupFilePath := dave.Cfg.ChanBackupPath()\n\n\t// Read the entire Multi backup stored within this node's\n\t// channels.backup file.\n\tmulti, err := ioutil.ReadFile(backupFilePath)\n\trequire.NoError(ht, err)\n\n\t// Now that we have Dave's backup file, we'll create a new nodeRestorer\n\t// that we'll restore using the on-disk channels.backup.\n\trestoredNodeFunc := chanRestoreViaRPC(\n\t\tht, crs.password, crs.mnemonic, multi, dave,\n\t)\n\n\t// Test the scenario.\n\tcrs.testScenario(ht, restoredNodeFunc)\n}\n\n// testChannelBackupRestoreLegacy checks a channel with the legacy revocation\n// producer format and makes sure old SCBs can still be recovered.",
      "length": 1613,
      "tokens": 204,
      "embedding": []
    },
    {
      "slug": "func testChannelBackupRestoreLegacy(ht *lntest.HarnessTest) {",
      "content": "func testChannelBackupRestoreLegacy(ht *lntest.HarnessTest) {\n\t// Create a new retore scenario.\n\tcrs := newChanRestoreScenario(\n\t\tht, lnrpc.CommitmentType_UNKNOWN_COMMITMENT_TYPE, false,\n\t)\n\tcarol, dave := crs.carol, crs.dave\n\n\tcreateLegacyRevocationChannel(\n\t\tht, crs.params.Amt, crs.params.PushAmt, dave, carol,\n\t)\n\n\t// For this restoration method, we'll grab the current multi-channel\n\t// backup from the old node, and use it to restore a new node within\n\t// the closure.\n\tchanBackup := dave.RPC.ExportAllChanBackups()\n\tmulti := chanBackup.MultiChanBackup.MultiChanBackup\n\n\t// In our nodeRestorer function, we'll restore the node from seed, then\n\t// manually recover the channel backup.\n\trestoredNodeFunc := chanRestoreViaRPC(\n\t\tht, crs.password, crs.mnemonic, multi, dave,\n\t)\n\n\t// Test the scenario.\n\tcrs.testScenario(ht, restoredNodeFunc)\n}\n\n// testChannelBackupRestoreForceClose checks that Dave can restore from force\n// closed channels.",
      "length": 855,
      "tokens": 105,
      "embedding": []
    },
    {
      "slug": "func testChannelBackupRestoreForceClose(ht *lntest.HarnessTest) {",
      "content": "func testChannelBackupRestoreForceClose(ht *lntest.HarnessTest) {\n\t// Restore a channel that was force closed by dave just before going\n\t// offline.\n\tsuccess := ht.Run(\"from backup file anchors\", func(t *testing.T) {\n\t\tst := ht.Subtest(t)\n\t\trunChanRestoreScenarioForceClose(st, false)\n\t})\n\n\t// Only run the second test if the first passed.\n\tif !success {\n\t\treturn\n\t}\n\n\t// Restore a zero-conf anchors channel that was force closed by dave\n\t// just before going offline.\n\tht.Run(\"from backup file anchors w/ zero-conf\", func(t *testing.T) {\n\t\tst := ht.Subtest(t)\n\t\trunChanRestoreScenarioForceClose(st, true)\n\t})\n}\n\n// runChanRestoreScenarioForceClose creates anchor-enabled force close channels\n// and checks that Dave is able to restore from them.",
      "length": 659,
      "tokens": 96,
      "embedding": []
    },
    {
      "slug": "func runChanRestoreScenarioForceClose(ht *lntest.HarnessTest, zeroConf bool) {",
      "content": "func runChanRestoreScenarioForceClose(ht *lntest.HarnessTest, zeroConf bool) {\n\tcrs := newChanRestoreScenario(\n\t\tht, lnrpc.CommitmentType_ANCHORS, zeroConf,\n\t)\n\tcarol, dave := crs.carol, crs.dave\n\n\t// For neutrino backend, we give Dave once more UTXO to fund the anchor\n\t// sweep.\n\tif ht.IsNeutrinoBackend() {\n\t\tht.FundCoins(btcutil.SatoshiPerBitcoin, dave)\n\t}\n\n\t// If we are testing zero-conf channels, setup a ChannelAcceptor for\n\t// the fundee.\n\tvar cancelAcceptor context.CancelFunc\n\tif zeroConf {\n\t\t// Setup a ChannelAcceptor.\n\t\tacceptStream, cancel := carol.RPC.ChannelAcceptor()\n\t\tcancelAcceptor = cancel\n\t\tgo acceptChannel(ht.T, true, acceptStream)\n\t}\n\n\tchanPoint := ht.OpenChannel(dave, carol, crs.params)\n\n\t// Remove the ChannelAcceptor.\n\tif zeroConf {\n\t\tcancelAcceptor()\n\t}\n\n\t// If we're testing that locally force closed channels can be restored\n\t// then we issue the force close now.\n\tht.CloseChannelAssertPending(dave, chanPoint, true)\n\n\t// Dave should see one waiting close channel.\n\tht.AssertNumWaitingClose(dave, 1)\n\n\t// Now we need to make sure that the channel is still in the backup.\n\t// Otherwise restoring won't work later.\n\tdave.RPC.ExportChanBackup(chanPoint)\n\n\t// Before we start the recovery, we'll record the balances of both\n\t// Carol and Dave to ensure they both sweep their coins at the end.\n\tcarolBalResp := carol.RPC.WalletBalance()\n\tcarolStartingBalance := carolBalResp.ConfirmedBalance\n\n\tdaveBalance := dave.RPC.WalletBalance()\n\tdaveStartingBalance := daveBalance.ConfirmedBalance\n\n\t// At this point, we'll now execute the restore method to give us the\n\t// new node we should attempt our assertions against.\n\tbackupFilePath := dave.Cfg.ChanBackupPath()\n\n\t// Read the entire Multi backup stored within this node's\n\t// channel.backup file.\n\tmulti, err := ioutil.ReadFile(backupFilePath)\n\trequire.NoError(ht, err)\n\n\t// Now that we have Dave's backup file, we'll create a new nodeRestorer\n\t// that will restore using the on-disk channel.backup.\n\trestoredNodeFunc := chanRestoreViaRPC(\n\t\tht, crs.password, crs.mnemonic, multi, dave,\n\t)\n\n\t// We now wait until both Dave's closing tx and sweep tx have shown in\n\t// mempool.\n\tht.Miner.AssertNumTxsInMempool(2)\n\n\t// Now that we're able to make our restored now, we'll shutdown the old\n\t// Dave node as we'll be storing it shortly below.\n\tht.Shutdown(dave)\n\n\t// Mine a block to confirm the closing tx from Dave.\n\tht.MineBlocksAndAssertNumTxes(1, 2)\n\n\t// To make sure the channel state is advanced correctly if the channel\n\t// peer is not online at first, we also shutdown Carol.\n\trestartCarol := ht.SuspendNode(carol)\n\n\tdave = crs.restoreDave(ht, restoredNodeFunc)\n\n\t// For our force close scenario we don't need the channel to be closed\n\t// by Carol since it was already force closed before we started the\n\t// recovery. All we need is for Carol to send us over the commit height\n\t// so we can sweep the time locked output with the correct commit\n\t// point.\n\tht.AssertNumPendingForceClose(dave, 1)\n\n\trequire.NoError(ht, restartCarol(), \"restart carol failed\")\n\n\t// Now that we have our new node up, we expect that it'll re-connect to\n\t// Carol automatically based on the restored backup.\n\tht.EnsureConnected(dave, carol)\n\n\tassertTimeLockSwept(\n\t\tht, carol, dave, carolStartingBalance, daveStartingBalance,\n\t)\n}\n\n// testChannelBackupUpdates tests that both the streaming channel update RPC,\n// and the on-disk channel.backup are updated each time a channel is\n// opened/closed.",
      "length": 3272,
      "tokens": 460,
      "embedding": []
    },
    {
      "slug": "func testChannelBackupUpdates(ht *lntest.HarnessTest) {",
      "content": "func testChannelBackupUpdates(ht *lntest.HarnessTest) {\n\talice := ht.Alice\n\n\t// First, we'll make a temp directory that we'll use to store our\n\t// backup file, so we can check in on it during the test easily.\n\tbackupDir := ht.T.TempDir()\n\n\t// First, we'll create a new node, Carol. We'll also create a temporary\n\t// file that Carol will use to store her channel backups.\n\tbackupFilePath := filepath.Join(\n\t\tbackupDir, chanbackup.DefaultBackupFileName,\n\t)\n\tcarolArgs := fmt.Sprintf(\"--backupfilepath=%v\", backupFilePath)\n\tcarol := ht.NewNode(\"carol\", []string{carolArgs})\n\n\t// Next, we'll register for streaming notifications for changes to the\n\t// backup file.\n\tbackupStream := carol.RPC.SubscribeChannelBackups()\n\n\t// We'll use this goroutine to proxy any updates to a channel we can\n\t// easily use below.\n\tvar wg sync.WaitGroup\n\tbackupUpdates := make(chan *lnrpc.ChanBackupSnapshot)\n\tstreamErr := make(chan error)\n\tstreamQuit := make(chan struct{})\n\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\tfor {\n\t\t\tsnapshot, err := backupStream.Recv()\n\t\t\tif err != nil {\n\t\t\t\tselect {\n\t\t\t\tcase streamErr <- err:\n\t\t\t\tcase <-streamQuit:\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tselect {\n\t\t\tcase backupUpdates <- snapshot:\n\t\t\tcase <-streamQuit:\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\tdefer close(streamQuit)\n\n\t// With Carol up, we'll now connect her to Alice, and open a channel\n\t// between them.\n\tht.ConnectNodes(carol, alice)\n\n\t// Next, we'll open two channels between Alice and Carol back to back.\n\tvar chanPoints []*lnrpc.ChannelPoint\n\tnumChans := 2\n\tchanAmt := btcutil.Amount(1000000)\n\tfor i := 0; i < numChans; i++ {\n\t\tchanPoint := ht.OpenChannel(\n\t\t\talice, carol, lntest.OpenChannelParams{Amt: chanAmt},\n\t\t)\n\t\tchanPoints = append(chanPoints, chanPoint)\n\t}\n\n\t// Using this helper function, we'll maintain a pointer to the latest\n\t// channel backup so we can compare it to the on disk state.\n\tvar currentBackup *lnrpc.ChanBackupSnapshot\n\tassertBackupNtfns := func(numNtfns int) {\n\t\tfor i := 0; i < numNtfns; i++ {\n\t\t\tselect {\n\t\t\tcase err := <-streamErr:\n\t\t\t\trequire.Failf(ht, \"stream err\",\n\t\t\t\t\t\"error with backup stream: %v\", err)\n\n\t\t\tcase currentBackup = <-backupUpdates:\n\n\t\t\tcase <-time.After(time.Second * 5):\n\t\t\t\trequire.Failf(ht, \"timeout\", \"didn't \"+\n\t\t\t\t\t\"receive channel backup \"+\n\t\t\t\t\t\"notification %v\", i+1)\n\t\t\t}\n\t\t}\n\t}\n\n\t// assertBackupFileState is a helper function that we'll use to compare\n\t// the on disk back up file to our currentBackup pointer above.\n\tassertBackupFileState := func() {\n\t\terr := wait.NoError(func() error {\n\t\t\tpackedBackup, err := ioutil.ReadFile(backupFilePath)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"unable to read backup \"+\n\t\t\t\t\t\"file: %v\", err)\n\t\t\t}\n\n\t\t\t// As each back up file will be encrypted with a fresh\n\t\t\t// nonce, we can't compare them directly, so instead\n\t\t\t// we'll compare the length which is a proxy for the\n\t\t\t// number of channels that the multi-backup contains.\n\t\t\tbackup := currentBackup.MultiChanBackup.MultiChanBackup\n\t\t\tif len(backup) != len(packedBackup) {\n\t\t\t\treturn fmt.Errorf(\"backup files don't match: \"+\n\t\t\t\t\t\"expected %x got %x\", backup,\n\t\t\t\t\tpackedBackup)\n\t\t\t}\n\n\t\t\t// Additionally, we'll assert that both backups up\n\t\t\t// returned are valid.\n\t\t\tfor _, backup := range [][]byte{backup, packedBackup} {\n\t\t\t\tsnapshot := &lnrpc.ChanBackupSnapshot{\n\t\t\t\t\tMultiChanBackup: &lnrpc.MultiChanBackup{\n\t\t\t\t\t\tMultiChanBackup: backup,\n\t\t\t\t\t},\n\t\t\t\t}\n\n\t\t\t\tcarol.RPC.VerifyChanBackup(snapshot)\n\t\t\t}\n\n\t\t\treturn nil\n\t\t}, defaultTimeout)\n\t\trequire.NoError(ht, err, \"timeout while checking \"+\n\t\t\t\"backup state: %v\", err)\n\t}\n\n\t// As these two channels were just opened, we should've got two times\n\t// the pending and open notifications for channel backups.\n\tassertBackupNtfns(2 * 2)\n\n\t// The on disk file should also exactly match the latest backup that we\n\t// have.\n\tassertBackupFileState()\n\n\t// Next, we'll close the channels one by one. After each channel\n\t// closure, we should get a notification, and the on-disk state should\n\t// match this state as well.\n\tfor i := 0; i < numChans; i++ {\n\t\t// To ensure force closes also trigger an update, we'll force\n\t\t// close half of the channels.\n\t\tforceClose := i%2 == 0\n\n\t\tchanPoint := chanPoints[i]\n\n\t\t// If we force closed the channel, then we'll mine enough\n\t\t// blocks to ensure all outputs have been swept.\n\t\tif forceClose {\n\t\t\tht.ForceCloseChannel(alice, chanPoint)\n\n\t\t\t// A local force closed channel will trigger a\n\t\t\t// notification once the commitment TX confirms on\n\t\t\t// chain. But that won't remove the channel from the\n\t\t\t// backup just yet, that will only happen once the time\n\t\t\t// locked contract was fully resolved on chain.\n\t\t\tassertBackupNtfns(1)\n\n\t\t\t// Now that the channel's been fully resolved, we\n\t\t\t// expect another notification.\n\t\t\tassertBackupNtfns(1)\n\t\t\tassertBackupFileState()\n\t\t} else {\n\t\t\tht.CloseChannel(alice, chanPoint)\n\t\t\t// We should get a single notification after closing,\n\t\t\t// and the on-disk state should match this latest\n\t\t\t// notifications.\n\t\t\tassertBackupNtfns(1)\n\t\t\tassertBackupFileState()\n\t\t}\n\t}\n}\n\n// testExportChannelBackup tests that we're able to properly export either a\n// targeted channel's backup, or export backups of all the currents open\n// channels.",
      "length": 4943,
      "tokens": 711,
      "embedding": []
    },
    {
      "slug": "func testExportChannelBackup(ht *lntest.HarnessTest) {",
      "content": "func testExportChannelBackup(ht *lntest.HarnessTest) {\n\t// First, we'll create our primary test node: Carol. We'll use Carol to\n\t// open channels and also export backups that we'll examine throughout\n\t// the test.\n\tcarol := ht.NewNode(\"carol\", nil)\n\n\t// With Carol up, we'll now connect her to Alice, and open a channel\n\t// between them.\n\talice := ht.Alice\n\tht.ConnectNodes(carol, alice)\n\n\t// Next, we'll open two channels between Alice and Carol back to back.\n\tvar chanPoints []*lnrpc.ChannelPoint\n\tnumChans := 2\n\tchanAmt := btcutil.Amount(1000000)\n\tfor i := 0; i < numChans; i++ {\n\t\tchanPoint := ht.OpenChannel(\n\t\t\talice, carol, lntest.OpenChannelParams{Amt: chanAmt},\n\t\t)\n\t\tchanPoints = append(chanPoints, chanPoint)\n\t}\n\n\t// Now that the channels are open, we should be able to fetch the\n\t// backups of each of the channels.\n\tfor _, chanPoint := range chanPoints {\n\t\tchanBackup := carol.RPC.ExportChanBackup(chanPoint)\n\n\t\t// The returned backup should be full populated. Since it's\n\t\t// encrypted, we can't assert any more than that atm.\n\t\trequire.NotEmptyf(ht, chanBackup.ChanBackup,\n\t\t\t\"obtained empty backup for channel: %v\", chanPoint)\n\n\t\t// The specified chanPoint in the response should match our\n\t\t// requested chanPoint.\n\t\trequire.Equal(ht, chanBackup.ChanPoint.String(),\n\t\t\tchanPoint.String())\n\t}\n\n\t// Before we proceed, we'll make two utility methods we'll use below\n\t// for our primary assertions.\n\tassertNumSingleBackups := func(numSingles int) {\n\t\terr := wait.NoError(func() error {\n\t\t\tchanSnapshot := carol.RPC.ExportAllChanBackups()\n\n\t\t\tif chanSnapshot.SingleChanBackups == nil {\n\t\t\t\treturn fmt.Errorf(\"single chan backups not \" +\n\t\t\t\t\t\"populated\")\n\t\t\t}\n\n\t\t\tbackups := chanSnapshot.SingleChanBackups.ChanBackups\n\t\t\tif len(backups) != numSingles {\n\t\t\t\treturn fmt.Errorf(\"expected %v singles, \"+\n\t\t\t\t\t\"got %v\", len(backups), numSingles)\n\t\t\t}\n\n\t\t\treturn nil\n\t\t}, defaultTimeout)\n\t\trequire.NoError(ht, err, \"timeout checking num single backup\")\n\t}\n\n\tassertMultiBackupFound := func() func(bool,\n\t\tmap[wire.OutPoint]struct{}) {\n\n\t\tchanSnapshot := carol.RPC.ExportAllChanBackups()\n\n\t\treturn func(found bool, chanPoints map[wire.OutPoint]struct{}) {\n\t\t\tnum := len(chanSnapshot.MultiChanBackup.MultiChanBackup)\n\n\t\t\tswitch {\n\t\t\tcase found && chanSnapshot.MultiChanBackup == nil:\n\t\t\t\trequire.Fail(ht, \"multi-backup not present\")\n\n\t\t\tcase !found && chanSnapshot.MultiChanBackup != nil &&\n\t\t\t\tnum != chanbackup.NilMultiSizePacked:\n\n\t\t\t\trequire.Fail(ht, \"found multi-backup when \"+\n\t\t\t\t\t\"non should be found\")\n\t\t\t}\n\n\t\t\tif !found {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tbackedUpChans := chanSnapshot.MultiChanBackup.ChanPoints\n\t\t\trequire.Len(ht, backedUpChans, len(chanPoints))\n\n\t\t\tfor _, chanPoint := range backedUpChans {\n\t\t\t\twp := ht.OutPointFromChannelPoint(chanPoint)\n\t\t\t\t_, ok := chanPoints[wp]\n\t\t\t\trequire.True(ht, ok, \"unexpected \"+\n\t\t\t\t\t\"backup: %v\", wp)\n\t\t\t}\n\t\t}\n\t}\n\n\tchans := make(map[wire.OutPoint]struct{})\n\tfor _, chanPoint := range chanPoints {\n\t\tchans[ht.OutPointFromChannelPoint(chanPoint)] = struct{}{}\n\t}\n\n\t// We should have exactly two single channel backups contained, and we\n\t// should also have a multi-channel backup.\n\tassertNumSingleBackups(2)\n\tassertMultiBackupFound()(true, chans)\n\n\t// We'll now close each channel on by one. After we close a channel, we\n\t// shouldn't be able to find that channel as a backup still. We should\n\t// also have one less single written to disk.\n\tfor i, chanPoint := range chanPoints {\n\t\tht.CloseChannel(alice, chanPoint)\n\n\t\tassertNumSingleBackups(len(chanPoints) - i - 1)\n\n\t\tdelete(chans, ht.OutPointFromChannelPoint(chanPoint))\n\t\tassertMultiBackupFound()(true, chans)\n\t}\n\n\t// At this point we shouldn't have any single or multi-chan backups at\n\t// all.\n\tassertNumSingleBackups(0)\n\tassertMultiBackupFound()(false, nil)\n}\n\n// testDataLossProtection tests that if one of the nodes in a channel\n// relationship lost state, they will detect this during channel sync, and the\n// up-to-date party will force close the channel, giving the outdated party the\n// opportunity to sweep its output.",
      "length": 3847,
      "tokens": 489,
      "embedding": []
    },
    {
      "slug": "func testDataLossProtection(ht *lntest.HarnessTest) {",
      "content": "func testDataLossProtection(ht *lntest.HarnessTest) {\n\tconst (\n\t\tchanAmt     = funding.MaxBtcFundingAmount\n\t\tpaymentAmt  = 10000\n\t\tnumInvoices = 6\n\t)\n\n\t// Carol will be the up-to-date party. We set --nolisten to ensure Dave\n\t// won't be able to connect to her and trigger the channel data\n\t// protection logic automatically. We also can't have Carol\n\t// automatically re-connect too early, otherwise DLP would be initiated\n\t// at the wrong moment.\n\tcarol := ht.NewNode(\"Carol\", []string{\"--nolisten\", \"--minbackoff=1h\"})\n\n\t// Dave will be the party losing his state.\n\tdave := ht.NewNode(\"Dave\", nil)\n\n\t// Before we make a channel, we'll load up Carol with some coins sent\n\t// directly from the miner.\n\tht.FundCoins(btcutil.SatoshiPerBitcoin, carol)\n\n\t// timeTravelDave is a method that will make Carol open a channel to\n\t// Dave, settle a series of payments, then Dave back to the state\n\t// before the payments happened. When this method returns Dave will\n\t// be unaware of the new state updates. The returned function can be\n\t// used to restart Dave in this state.\n\ttimeTravelDave := func() (func() error, *lnrpc.ChannelPoint, int64) {\n\t\t// We must let the node communicate with Carol before they are\n\t\t// able to open channel, so we connect them.\n\t\tht.EnsureConnected(carol, dave)\n\n\t\t// We'll first open up a channel between them with a 0.5 BTC\n\t\t// value.\n\t\tchanPoint := ht.OpenChannel(\n\t\t\tcarol, dave, lntest.OpenChannelParams{\n\t\t\t\tAmt: chanAmt,\n\t\t\t},\n\t\t)\n\n\t\t// With the channel open, we'll create a few invoices for the\n\t\t// node that Carol will pay to in order to advance the state of\n\t\t// the channel.\n\t\t// TODO(halseth): have dangling HTLCs on the commitment, able to\n\t\t// retrieve funds?\n\t\tpayReqs, _, _ := ht.CreatePayReqs(dave, paymentAmt, numInvoices)\n\n\t\t// Send payments from Carol using 3 of the payment hashes\n\t\t// generated above.\n\t\tht.CompletePaymentRequests(carol, payReqs[:numInvoices/2])\n\n\t\t// Next query for Dave's channel state, as we sent 3 payments\n\t\t// of 10k satoshis each, it should now see his balance as being\n\t\t// 30k satoshis.\n\t\tnodeChan := ht.AssertChannelLocalBalance(\n\t\t\tdave, chanPoint, 30_000,\n\t\t)\n\n\t\t// Grab the current commitment height (update number), we'll\n\t\t// later revert him to this state after additional updates to\n\t\t// revoke this state.\n\t\tstateNumPreCopy := nodeChan.NumUpdates\n\n\t\t// With the temporary file created, copy the current state into\n\t\t// the temporary file we created above. Later after more\n\t\t// updates, we'll restore this state.\n\t\tht.BackupDB(dave)\n\n\t\t// Reconnect the peers after the restart that was needed for\n\t\t// the db backup.\n\t\tht.EnsureConnected(carol, dave)\n\n\t\t// Finally, send more payments from Carol, using the remaining\n\t\t// payment hashes.\n\t\tht.CompletePaymentRequests(carol, payReqs[numInvoices/2:])\n\n\t\t// TODO(yy): remove the sleep once the following bug is fixed.\n\t\t//\n\t\t// While the payment is reported as settled, the commitment\n\t\t// dance may not be finished, which leaves several HTLCs in the\n\t\t// commitment. Later on, when Carol force closes this channel,\n\t\t// she would have HTLCs there and the test won't pass.\n\t\ttime.Sleep(2 * time.Second)\n\n\t\t// Now we shutdown Dave, copying over the its temporary\n\t\t// database state which has the *prior* channel state over his\n\t\t// current most up to date state. With this, we essentially\n\t\t// force Dave to travel back in time within the channel's\n\t\t// history.\n\t\tht.RestartNodeAndRestoreDB(dave)\n\n\t\t// Make sure the channel is still there from the PoV of Dave.\n\t\tht.AssertNodeNumChannels(dave, 1)\n\n\t\t// Now query for the channel state, it should show that it's at\n\t\t// a state number in the past, not the *latest* state.\n\t\tht.AssertChannelNumUpdates(dave, stateNumPreCopy, chanPoint)\n\n\t\tbalResp := dave.RPC.WalletBalance()\n\t\trestart := ht.SuspendNode(dave)\n\n\t\treturn restart, chanPoint, balResp.ConfirmedBalance\n\t}\n\n\t// Reset Dave to a state where he has an outdated channel state.\n\trestartDave, _, daveStartingBalance := timeTravelDave()\n\n\t// We make a note of the nodes' current on-chain balances, to make sure\n\t// they are able to retrieve the channel funds eventually,\n\tcarolBalResp := carol.RPC.WalletBalance()\n\tcarolStartingBalance := carolBalResp.ConfirmedBalance\n\n\t// Restart Dave to trigger a channel resync.\n\trequire.NoError(ht, restartDave(), \"unable to restart dave\")\n\n\t// Assert that once Dave comes up, they reconnect, Carol force closes\n\t// on chain, and both of them properly carry out the DLP protocol.\n\tassertDLPExecuted(\n\t\tht, carol, carolStartingBalance, dave,\n\t\tdaveStartingBalance, lnrpc.CommitmentType_STATIC_REMOTE_KEY,\n\t)\n\n\t// As a second part of this test, we will test the scenario where a\n\t// channel is closed while Dave is offline, loses his state and comes\n\t// back online. In this case the node should attempt to resync the\n\t// channel, and the peer should resend a channel sync message for the\n\t// closed channel, such that Dave can retrieve his funds.\n\t//\n\t// We start by letting Dave time travel back to an outdated state.\n\trestartDave, chanPoint2, daveStartingBalance := timeTravelDave()\n\n\tcarolBalResp = carol.RPC.WalletBalance()\n\tcarolStartingBalance = carolBalResp.ConfirmedBalance\n\n\t// Now let Carol force close the channel while Dave is offline.\n\tht.ForceCloseChannel(carol, chanPoint2)\n\n\t// Make sure Carol got her balance back.\n\tcarolBalResp = carol.RPC.WalletBalance()\n\tcarolBalance := carolBalResp.ConfirmedBalance\n\trequire.Greater(ht, carolBalance, carolStartingBalance,\n\t\t\"expected carol to have balance increased\")\n\n\tht.AssertNodeNumChannels(carol, 0)\n\n\t// When Dave comes online, he will reconnect to Carol, try to resync\n\t// the channel, but it will already be closed. Carol should resend the\n\t// information Dave needs to sweep his funds.\n\trequire.NoError(ht, restartDave(), \"unable to restart Eve\")\n\n\t// Dave should sweep his funds.\n\tht.Miner.AssertNumTxsInMempool(1)\n\n\t// Mine a block to confirm the sweep, and make sure Dave got his\n\t// balance back.\n\tht.MineBlocksAndAssertNumTxes(1, 1)\n\tht.AssertNodeNumChannels(dave, 0)\n\n\terr := wait.NoError(func() error {\n\t\tdaveBalResp := dave.RPC.WalletBalance()\n\t\tdaveBalance := daveBalResp.ConfirmedBalance\n\t\tif daveBalance <= daveStartingBalance {\n\t\t\treturn fmt.Errorf(\"expected dave to have balance \"+\n\t\t\t\t\"above %d, intead had %v\", daveStartingBalance,\n\t\t\t\tdaveBalance)\n\t\t}\n\n\t\treturn nil\n\t}, defaultTimeout)\n\trequire.NoError(ht, err, \"timeout while checking dave's balance\")\n}\n\n// createLegacyRevocationChannel creates a single channel using the legacy\n// revocation producer format by using PSBT to signal a special pending channel\n// ID.",
      "length": 6340,
      "tokens": 917,
      "embedding": []
    },
    {
      "slug": "func createLegacyRevocationChannel(ht *lntest.HarnessTest,",
      "content": "func createLegacyRevocationChannel(ht *lntest.HarnessTest,\n\tchanAmt, pushAmt btcutil.Amount, from, to *node.HarnessNode) {\n\n\t// We'll signal to the wallet that we also want to create a channel\n\t// with the legacy revocation producer format that relies on deriving a\n\t// private key from the key ring. This is only available during itests\n\t// to make sure we don't hard depend on the DerivePrivKey method of the\n\t// key ring. We can signal the wallet by setting a custom pending\n\t// channel ID. To be able to do that, we need to set a funding shim\n\t// which is easiest by using PSBT funding. The ID is the hex\n\t// representation of the string \"legacy-revocation\".\n\titestLegacyFormatChanID := [32]byte{\n\t\t0x6c, 0x65, 0x67, 0x61, 0x63, 0x79, 0x2d, 0x72, 0x65, 0x76,\n\t\t0x6f, 0x63, 0x61, 0x74, 0x69, 0x6f, 0x6e,\n\t}\n\tshim := &lnrpc.FundingShim{\n\t\tShim: &lnrpc.FundingShim_PsbtShim{\n\t\t\tPsbtShim: &lnrpc.PsbtShim{\n\t\t\t\tPendingChanId: itestLegacyFormatChanID[:],\n\t\t\t},\n\t\t},\n\t}\n\topenChannelReq := lntest.OpenChannelParams{\n\t\tAmt:         chanAmt,\n\t\tPushAmt:     pushAmt,\n\t\tFundingShim: shim,\n\t}\n\tchanUpdates, tempPsbt := ht.OpenChannelPsbt(from, to, openChannelReq)\n\n\t// Fund the PSBT by using the source node's wallet.\n\tfundReq := &walletrpc.FundPsbtRequest{\n\t\tTemplate: &walletrpc.FundPsbtRequest_Psbt{\n\t\t\tPsbt: tempPsbt,\n\t\t},\n\t\tFees: &walletrpc.FundPsbtRequest_SatPerVbyte{\n\t\t\tSatPerVbyte: 2,\n\t\t},\n\t}\n\tfundResp := from.RPC.FundPsbt(fundReq)\n\n\t// We have a PSBT that has no witness data yet, which is exactly what\n\t// we need for the next step of verifying the PSBT with the funding\n\t// intents.\n\tmsg := &lnrpc.FundingTransitionMsg{\n\t\tTrigger: &lnrpc.FundingTransitionMsg_PsbtVerify{\n\t\t\tPsbtVerify: &lnrpc.FundingPsbtVerify{\n\t\t\t\tPendingChanId: itestLegacyFormatChanID[:],\n\t\t\t\tFundedPsbt:    fundResp.FundedPsbt,\n\t\t\t},\n\t\t},\n\t}\n\tfrom.RPC.FundingStateStep(msg)\n\n\t// Now we'll ask the source node's wallet to sign the PSBT so we can\n\t// finish the funding flow.\n\tfinalizeReq := &walletrpc.FinalizePsbtRequest{\n\t\tFundedPsbt: fundResp.FundedPsbt,\n\t}\n\tfinalizeRes := from.RPC.FinalizePsbt(finalizeReq)\n\n\t// We've signed our PSBT now, let's pass it to the intent again.\n\tmsg = &lnrpc.FundingTransitionMsg{\n\t\tTrigger: &lnrpc.FundingTransitionMsg_PsbtFinalize{\n\t\t\tPsbtFinalize: &lnrpc.FundingPsbtFinalize{\n\t\t\t\tPendingChanId: itestLegacyFormatChanID[:],\n\t\t\t\tSignedPsbt:    finalizeRes.SignedPsbt,\n\t\t\t},\n\t\t},\n\t}\n\tfrom.RPC.FundingStateStep(msg)\n\n\t// Consume the \"channel pending\" update. This waits until the funding\n\t// transaction was fully compiled.\n\tupdateResp := ht.ReceiveOpenChannelUpdate(chanUpdates)\n\tupd, ok := updateResp.Update.(*lnrpc.OpenStatusUpdate_ChanPending)\n\trequire.True(ht, ok)\n\tchanPoint := &lnrpc.ChannelPoint{\n\t\tFundingTxid: &lnrpc.ChannelPoint_FundingTxidBytes{\n\t\t\tFundingTxidBytes: upd.ChanPending.Txid,\n\t\t},\n\t\tOutputIndex: upd.ChanPending.OutputIndex,\n\t}\n\n\tht.MineBlocksAndAssertNumTxes(6, 1)\n\tht.AssertTopologyChannelOpen(from, chanPoint)\n\tht.AssertTopologyChannelOpen(to, chanPoint)\n}\n\n// chanRestoreViaRPC is a helper test method that returns a nodeRestorer\n// instance which will restore the target node from a password+seed, then\n// trigger a SCB restore using the RPC interface.",
      "length": 3041,
      "tokens": 362,
      "embedding": []
    },
    {
      "slug": "func chanRestoreViaRPC(ht *lntest.HarnessTest, password []byte,",
      "content": "func chanRestoreViaRPC(ht *lntest.HarnessTest, password []byte,\n\tmnemonic []string, multi []byte,\n\toldNode *node.HarnessNode) nodeRestorer {\n\n\tbackup := &lnrpc.RestoreChanBackupRequest_MultiChanBackup{\n\t\tMultiChanBackup: multi,\n\t}\n\n\treturn func() *node.HarnessNode {\n\t\tnewNode := ht.RestoreNodeWithSeed(\n\t\t\t\"dave\", nil, password, mnemonic, \"\", revocationWindow,\n\t\t\tnil, copyPorts(oldNode),\n\t\t)\n\t\treq := &lnrpc.RestoreChanBackupRequest{Backup: backup}\n\t\tnewNode.RPC.RestoreChanBackups(req)\n\n\t\treturn newNode\n\t}\n}\n\n// copyPorts returns a node option function that copies the ports of an existing\n// node over to the newly created one.",
      "length": 548,
      "tokens": 61,
      "embedding": []
    },
    {
      "slug": "func copyPorts(oldNode *node.HarnessNode) node.Option {",
      "content": "func copyPorts(oldNode *node.HarnessNode) node.Option {\n\treturn func(cfg *node.BaseNodeConfig) {\n\t\tcfg.P2PPort = oldNode.Cfg.P2PPort\n\t\tcfg.RPCPort = oldNode.Cfg.RPCPort\n\t\tcfg.RESTPort = oldNode.Cfg.RESTPort\n\t\tcfg.ProfilePort = oldNode.Cfg.ProfilePort\n\t}\n}\n\n// assertTimeLockSwept when dave's outputs matures, he should claim them. This\n// function will advance 2 blocks such that all the pending closing\n// transactions would be swept in the end.\n//\n// Note: this function is only used in this test file and has been made\n// specifically for testChanRestoreScenario.",
      "length": 497,
      "tokens": 69,
      "embedding": []
    },
    {
      "slug": "func assertTimeLockSwept(ht *lntest.HarnessTest, carol, dave *node.HarnessNode,",
      "content": "func assertTimeLockSwept(ht *lntest.HarnessTest, carol, dave *node.HarnessNode,\n\tcarolStartingBalance, daveStartingBalance int64) {\n\n\t// We expect Carol to sweep her funds and also the anchor tx.\n\texpectedTxes := 2\n\n\t// Carol should sweep her funds immediately, as they are not\n\t// timelocked.\n\tht.Miner.AssertNumTxsInMempool(expectedTxes)\n\n\t// Carol should consider the channel pending force close (since she is\n\t// waiting for her sweep to confirm).\n\tht.AssertNumPendingForceClose(carol, 1)\n\n\t// Dave is considering it \"pending force close\", as we must wait before\n\t// he can sweep her outputs.\n\tht.AssertNumPendingForceClose(dave, 1)\n\n\t// Mine the sweep (and anchor) tx(ns).\n\tht.MineBlocksAndAssertNumTxes(1, expectedTxes)\n\n\t// Now Carol should consider the channel fully closed.\n\tht.AssertNumPendingForceClose(carol, 0)\n\n\t// We query Carol's balance to make sure it increased after the channel\n\t// closed. This checks that she was able to sweep the funds she had in\n\t// the channel.\n\tcarolBalResp := carol.RPC.WalletBalance()\n\tcarolBalance := carolBalResp.ConfirmedBalance\n\trequire.Greater(ht, carolBalance, carolStartingBalance,\n\t\t\"balance not increased\")\n\n\t// After the Dave's output matures, he should reclaim his funds.\n\t//\n\t// The commit sweep resolver publishes the sweep tx at defaultCSV-1 and\n\t// we already mined one block after the commitment was published, so\n\t// take that into account.\n\tht.MineBlocks(defaultCSV - 1 - 1)\n\tdaveSweep := ht.Miner.AssertNumTxsInMempool(1)[0]\n\tblock := ht.MineBlocksAndAssertNumTxes(1, 1)[0]\n\tht.Miner.AssertTxInBlock(block, daveSweep)\n\n\t// Now the channel should be fully closed also from Dave's POV.\n\tht.AssertNumPendingForceClose(dave, 0)\n\n\t// Make sure Dave got his balance back.\n\terr := wait.NoError(func() error {\n\t\tdaveBalResp := dave.RPC.WalletBalance()\n\t\tdaveBalance := daveBalResp.ConfirmedBalance\n\t\tif daveBalance <= daveStartingBalance {\n\t\t\treturn fmt.Errorf(\"expected dave to have balance \"+\n\t\t\t\t\"above %d, instead had %v\", daveStartingBalance,\n\t\t\t\tdaveBalance)\n\t\t}\n\n\t\treturn nil\n\t}, defaultTimeout)\n\trequire.NoError(ht, err)\n\n\tht.AssertNodeNumChannels(dave, 0)\n\tht.AssertNodeNumChannels(carol, 0)\n}\n\n// assertDLPExecuted asserts that Dave is a node that has recovered their state\n// form scratch. Carol should then force close on chain, with Dave sweeping his\n// funds immediately, and Carol sweeping her fund after her CSV delay is up. If\n// the blankSlate value is true, then this means that Dave won't need to sweep\n// on chain as he has no funds in the channel.",
      "length": 2379,
      "tokens": 326,
      "embedding": []
    },
    {
      "slug": "func assertDLPExecuted(ht *lntest.HarnessTest,",
      "content": "func assertDLPExecuted(ht *lntest.HarnessTest,\n\tcarol *node.HarnessNode, carolStartingBalance int64,\n\tdave *node.HarnessNode, daveStartingBalance int64,\n\tcommitType lnrpc.CommitmentType) {\n\n\tht.Helper()\n\n\t// Increase the fee estimate so that the following force close tx will\n\t// be cpfp'ed.\n\tht.SetFeeEstimate(30000)\n\n\t// We disabled auto-reconnect for some tests to avoid timing issues.\n\t// To make sure the nodes are initiating DLP now, we have to manually\n\t// re-connect them.\n\tht.EnsureConnected(carol, dave)\n\n\t// Upon reconnection, the nodes should detect that Dave is out of sync.\n\t// Carol should force close the channel using her latest commitment.\n\texpectedTxes := 1\n\tif lntest.CommitTypeHasAnchors(commitType) {\n\t\texpectedTxes = 2\n\t}\n\tht.Miner.AssertNumTxsInMempool(expectedTxes)\n\n\t// Channel should be in the state \"waiting close\" for Carol since she\n\t// broadcasted the force close tx.\n\tht.AssertNumWaitingClose(carol, 1)\n\n\t// Dave should also consider the channel \"waiting close\", as he noticed\n\t// the channel was out of sync, and is now waiting for a force close to\n\t// hit the chain.\n\tht.AssertNumWaitingClose(dave, 1)\n\n\t// Restart Dave to make sure he is able to sweep the funds after\n\t// shutdown.\n\tht.RestartNode(dave)\n\n\t// Generate a single block, which should confirm the closing tx.\n\tht.MineBlocksAndAssertNumTxes(1, expectedTxes)\n\n\t// Dave should consider the channel pending force close (since he is\n\t// waiting for his sweep to confirm).\n\tht.AssertNumPendingForceClose(dave, 1)\n\n\t// Carol is considering it \"pending force close\", as we must wait\n\t// before she can sweep her outputs.\n\tht.AssertNumPendingForceClose(carol, 1)\n\n\tif commitType == lnrpc.CommitmentType_SCRIPT_ENFORCED_LEASE {\n\t\t// Dave should sweep his anchor only, since he still has the\n\t\t// lease CLTV constraint on his commitment output.\n\t\tht.Miner.AssertNumTxsInMempool(1)\n\n\t\t// Mine Dave's anchor sweep tx.\n\t\tht.MineBlocksAndAssertNumTxes(1, 1)\n\n\t\t// After Carol's output matures, she should also reclaim her\n\t\t// funds.\n\t\t//\n\t\t// The commit sweep resolver publishes the sweep tx at\n\t\t// defaultCSV-1 and we already mined one block after the\n\t\t// commitmment was published, so take that into account.\n\t\tht.MineBlocks(defaultCSV - 1 - 1)\n\t\tht.MineBlocksAndAssertNumTxes(1, 1)\n\n\t\t// Now the channel should be fully closed also from Carol's POV.\n\t\tht.AssertNumPendingForceClose(carol, 0)\n\n\t\t// We'll now mine the remaining blocks to prompt Dave to sweep\n\t\t// his CLTV-constrained output.\n\t\tresp := dave.RPC.PendingChannels()\n\t\tblocksTilMaturity :=\n\t\t\tresp.PendingForceClosingChannels[0].BlocksTilMaturity\n\t\trequire.Positive(ht, blocksTilMaturity)\n\n\t\tht.MineBlocks(uint32(blocksTilMaturity))\n\t\tht.MineBlocksAndAssertNumTxes(1, 1)\n\n\t\t// Now Dave should consider the channel fully closed.\n\t\tht.AssertNumPendingForceClose(dave, 0)\n\t} else {\n\t\t// Dave should sweep his funds immediately, as they are not\n\t\t// timelocked. We also expect Dave to sweep his anchor, if\n\t\t// present.\n\t\tht.Miner.AssertNumTxsInMempool(expectedTxes)\n\n\t\t// Mine the sweep tx.\n\t\tht.MineBlocksAndAssertNumTxes(1, expectedTxes)\n\n\t\t// Now Dave should consider the channel fully closed.\n\t\tht.AssertNumPendingForceClose(dave, 0)\n\n\t\t// After Carol's output matures, she should also reclaim her\n\t\t// funds.\n\t\t//\n\t\t// The commit sweep resolver publishes the sweep tx at\n\t\t// defaultCSV-1 and we already mined one block after the\n\t\t// commitmment was published, so take that into account.\n\t\tht.MineBlocks(defaultCSV - 1 - 1)\n\t\tht.MineBlocksAndAssertNumTxes(1, 1)\n\n\t\t// Now the channel should be fully closed also from Carol's\n\t\t// POV.\n\t\tht.AssertNumPendingForceClose(carol, 0)\n\t}\n\n\t// We query Dave's balance to make sure it increased after the channel\n\t// closed. This checks that he was able to sweep the funds he had in\n\t// the channel.\n\tdaveBalResp := dave.RPC.WalletBalance()\n\tdaveBalance := daveBalResp.ConfirmedBalance\n\trequire.Greater(ht, daveBalance, daveStartingBalance,\n\t\t\"balance not increased\")\n\n\t// Make sure Carol got her balance back.\n\terr := wait.NoError(func() error {\n\t\tcarolBalResp := carol.RPC.WalletBalance()\n\t\tcarolBalance := carolBalResp.ConfirmedBalance\n\n\t\t// With Neutrino we don't get a backend error when trying to\n\t\t// publish an orphan TX (which is what the sweep for the remote\n\t\t// anchor is since the remote commitment TX was not broadcast).\n\t\t// That's why the wallet still sees that as unconfirmed and we\n\t\t// need to count the total balance instead of the confirmed.\n\t\tif ht.IsNeutrinoBackend() {\n\t\t\tcarolBalance = carolBalResp.TotalBalance\n\t\t}\n\n\t\tif carolBalance <= carolStartingBalance {\n\t\t\treturn fmt.Errorf(\"expected carol to have balance \"+\n\t\t\t\t\"above %d, instead had %v\",\n\t\t\t\tcarolStartingBalance, carolBalance)\n\t\t}\n\n\t\treturn nil\n\t}, defaultTimeout)\n\trequire.NoError(ht, err, \"timeout while checking carol's balance\")\n\n\tht.AssertNodeNumChannels(dave, 0)\n\tht.AssertNodeNumChannels(carol, 0)\n}\n",
      "length": 4700,
      "tokens": 631,
      "embedding": []
    }
  ]
}