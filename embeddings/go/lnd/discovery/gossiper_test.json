{
  "filepath": "../implementations/go/lnd/discovery/gossiper_test.go",
  "package": "discovery",
  "sections": [
    {
      "slug": "func makeTestDB(t *testing.T) (*channeldb.DB, error) {",
      "content": "func makeTestDB(t *testing.T) (*channeldb.DB, error) {\n\t// Create channeldb for the first time.\n\tcdb, err := channeldb.Open(t.TempDir())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tt.Cleanup(func() {\n\t\tcdb.Close()\n\t})\n\n\treturn cdb, nil\n}\n",
      "length": 167,
      "tokens": 28,
      "embedding": []
    },
    {
      "slug": "type mockGraphSource struct {",
      "content": "type mockGraphSource struct {\n\tbestHeight uint32\n\n\tmu            sync.Mutex\n\tnodes         []channeldb.LightningNode\n\tinfos         map[uint64]channeldb.ChannelEdgeInfo\n\tedges         map[uint64][]channeldb.ChannelEdgePolicy\n\tzombies       map[uint64][][33]byte\n\tchansToReject map[uint64]struct{}\n}\n",
      "length": 260,
      "tokens": 15,
      "embedding": []
    },
    {
      "slug": "func newMockRouter(height uint32) *mockGraphSource {",
      "content": "func newMockRouter(height uint32) *mockGraphSource {\n\treturn &mockGraphSource{\n\t\tbestHeight:    height,\n\t\tinfos:         make(map[uint64]channeldb.ChannelEdgeInfo),\n\t\tedges:         make(map[uint64][]channeldb.ChannelEdgePolicy),\n\t\tzombies:       make(map[uint64][][33]byte),\n\t\tchansToReject: make(map[uint64]struct{}),\n\t}\n}\n\nvar _ routing.ChannelGraphSource = (*mockGraphSource)(nil)\n",
      "length": 322,
      "tokens": 19,
      "embedding": []
    },
    {
      "slug": "func (r *mockGraphSource) AddNode(node *channeldb.LightningNode,",
      "content": "func (r *mockGraphSource) AddNode(node *channeldb.LightningNode,\n\t_ ...batch.SchedulerOption) error {\n\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\n\tr.nodes = append(r.nodes, *node)\n\treturn nil\n}\n",
      "length": 113,
      "tokens": 14,
      "embedding": []
    },
    {
      "slug": "func (r *mockGraphSource) AddEdge(info *channeldb.ChannelEdgeInfo,",
      "content": "func (r *mockGraphSource) AddEdge(info *channeldb.ChannelEdgeInfo,\n\t_ ...batch.SchedulerOption) error {\n\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\n\tif _, ok := r.infos[info.ChannelID]; ok {\n\t\treturn errors.New(\"info already exist\")\n\t}\n\n\tif _, ok := r.chansToReject[info.ChannelID]; ok {\n\t\treturn errors.New(\"validation failed\")\n\t}\n\n\tr.infos[info.ChannelID] = *info\n\treturn nil\n}\n",
      "length": 289,
      "tokens": 36,
      "embedding": []
    },
    {
      "slug": "func (r *mockGraphSource) queueValidationFail(chanID uint64) {",
      "content": "func (r *mockGraphSource) queueValidationFail(chanID uint64) {\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\n\tr.chansToReject[chanID] = struct{}{}\n}\n",
      "length": 70,
      "tokens": 7,
      "embedding": []
    },
    {
      "slug": "func (r *mockGraphSource) UpdateEdge(edge *channeldb.ChannelEdgePolicy,",
      "content": "func (r *mockGraphSource) UpdateEdge(edge *channeldb.ChannelEdgePolicy,\n\t_ ...batch.SchedulerOption) error {\n\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\n\tif len(r.edges[edge.ChannelID]) == 0 {\n\t\tr.edges[edge.ChannelID] = make([]channeldb.ChannelEdgePolicy, 2)\n\t}\n\n\tif edge.ChannelFlags&lnwire.ChanUpdateDirection == 0 {\n\t\tr.edges[edge.ChannelID][0] = *edge\n\t} else {\n\t\tr.edges[edge.ChannelID][1] = *edge\n\t}\n\n\treturn nil\n}\n",
      "length": 325,
      "tokens": 35,
      "embedding": []
    },
    {
      "slug": "func (r *mockGraphSource) CurrentBlockHeight() (uint32, error) {",
      "content": "func (r *mockGraphSource) CurrentBlockHeight() (uint32, error) {\n\treturn r.bestHeight, nil\n}\n",
      "length": 26,
      "tokens": 4,
      "embedding": []
    },
    {
      "slug": "func (r *mockGraphSource) AddProof(chanID lnwire.ShortChannelID,",
      "content": "func (r *mockGraphSource) AddProof(chanID lnwire.ShortChannelID,\n\tproof *channeldb.ChannelAuthProof) error {\n\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\n\tchanIDInt := chanID.ToUint64()\n\tinfo, ok := r.infos[chanIDInt]\n\tif !ok {\n\t\treturn errors.New(\"channel does not exist\")\n\t}\n\n\tinfo.AuthProof = proof\n\tr.infos[chanIDInt] = info\n\n\treturn nil\n}\n",
      "length": 254,
      "tokens": 32,
      "embedding": []
    },
    {
      "slug": "func (r *mockGraphSource) ForEachNode(func(node *channeldb.LightningNode) error) error {",
      "content": "func (r *mockGraphSource) ForEachNode(func(node *channeldb.LightningNode) error) error {\n\treturn nil\n}\n",
      "length": 12,
      "tokens": 3,
      "embedding": []
    },
    {
      "slug": "func (r *mockGraphSource) ForAllOutgoingChannels(cb func(tx kvdb.RTx,",
      "content": "func (r *mockGraphSource) ForAllOutgoingChannels(cb func(tx kvdb.RTx,\n\ti *channeldb.ChannelEdgeInfo,\n\tc *channeldb.ChannelEdgePolicy) error) error {\n\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\n\tchans := make(map[uint64]channeldb.ChannelEdge)\n\tfor _, info := range r.infos {\n\t\tinfo := info\n\n\t\tedgeInfo := chans[info.ChannelID]\n\t\tedgeInfo.Info = &info\n\t\tchans[info.ChannelID] = edgeInfo\n\t}\n\tfor _, edges := range r.edges {\n\t\tedges := edges\n\n\t\tedge := chans[edges[0].ChannelID]\n\t\tedge.Policy1 = &edges[0]\n\t\tchans[edges[0].ChannelID] = edge\n\t}\n\n\tfor _, channel := range chans {\n\t\tif err := cb(nil, channel.Info, channel.Policy1); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n",
      "length": 567,
      "tokens": 77,
      "embedding": []
    },
    {
      "slug": "func (r *mockGraphSource) ForEachChannel(func(chanInfo *channeldb.ChannelEdgeInfo,",
      "content": "func (r *mockGraphSource) ForEachChannel(func(chanInfo *channeldb.ChannelEdgeInfo,\n\te1, e2 *channeldb.ChannelEdgePolicy) error) error {\n\treturn nil\n}\n",
      "length": 64,
      "tokens": 9,
      "embedding": []
    },
    {
      "slug": "func (r *mockGraphSource) GetChannelByID(chanID lnwire.ShortChannelID) (",
      "content": "func (r *mockGraphSource) GetChannelByID(chanID lnwire.ShortChannelID) (\n\t*channeldb.ChannelEdgeInfo,\n\t*channeldb.ChannelEdgePolicy,\n\t*channeldb.ChannelEdgePolicy, error) {\n\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\n\tchanIDInt := chanID.ToUint64()\n\tchanInfo, ok := r.infos[chanIDInt]\n\tif !ok {\n\t\tpubKeys, isZombie := r.zombies[chanIDInt]\n\t\tif !isZombie {\n\t\t\treturn nil, nil, nil, channeldb.ErrEdgeNotFound\n\t\t}\n\n\t\treturn &channeldb.ChannelEdgeInfo{\n\t\t\tNodeKey1Bytes: pubKeys[0],\n\t\t\tNodeKey2Bytes: pubKeys[1],\n\t\t}, nil, nil, channeldb.ErrZombieEdge\n\t}\n\n\tedges := r.edges[chanID.ToUint64()]\n\tif len(edges) == 0 {\n\t\treturn &chanInfo, nil, nil, nil\n\t}\n\n\tvar edge1 *channeldb.ChannelEdgePolicy\n\tif !reflect.DeepEqual(edges[0], channeldb.ChannelEdgePolicy{}) {\n\t\tedge1 = &edges[0]\n\t}\n\n\tvar edge2 *channeldb.ChannelEdgePolicy\n\tif !reflect.DeepEqual(edges[1], channeldb.ChannelEdgePolicy{}) {\n\t\tedge2 = &edges[1]\n\t}\n\n\treturn &chanInfo, edge1, edge2, nil\n}\n",
      "length": 829,
      "tokens": 84,
      "embedding": []
    },
    {
      "slug": "func (r *mockGraphSource) FetchLightningNode(",
      "content": "func (r *mockGraphSource) FetchLightningNode(\n\tnodePub route.Vertex) (*channeldb.LightningNode, error) {\n\n\tfor _, node := range r.nodes {\n\t\tif bytes.Equal(nodePub[:], node.PubKeyBytes[:]) {\n\t\t\treturn &node, nil\n\t\t}\n\t}\n\n\treturn nil, channeldb.ErrGraphNodeNotFound\n}\n\n// IsStaleNode returns true if the graph source has a node announcement for the\n// target node with a more recent timestamp.",
      "length": 332,
      "tokens": 47,
      "embedding": []
    },
    {
      "slug": "func (r *mockGraphSource) IsStaleNode(nodePub route.Vertex, timestamp time.Time) bool {",
      "content": "func (r *mockGraphSource) IsStaleNode(nodePub route.Vertex, timestamp time.Time) bool {\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\n\tfor _, node := range r.nodes {\n\t\tif node.PubKeyBytes == nodePub {\n\t\t\treturn node.LastUpdate.After(timestamp) ||\n\t\t\t\tnode.LastUpdate.Equal(timestamp)\n\t\t}\n\t}\n\n\t// If we did not find the node among our existing graph nodes, we\n\t// require the node to already have a channel in the graph to not be\n\t// considered stale.\n\tfor _, info := range r.infos {\n\t\tif info.NodeKey1Bytes == nodePub {\n\t\t\treturn false\n\t\t}\n\t\tif info.NodeKey2Bytes == nodePub {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// IsPublicNode determines whether the given vertex is seen as a public node in\n// the graph from the graph's source node's point of view.",
      "length": 630,
      "tokens": 105,
      "embedding": []
    },
    {
      "slug": "func (r *mockGraphSource) IsPublicNode(node route.Vertex) (bool, error) {",
      "content": "func (r *mockGraphSource) IsPublicNode(node route.Vertex) (bool, error) {\n\tfor _, info := range r.infos {\n\t\tif !bytes.Equal(node[:], info.NodeKey1Bytes[:]) &&\n\t\t\t!bytes.Equal(node[:], info.NodeKey2Bytes[:]) {\n\t\t\tcontinue\n\t\t}\n\n\t\tif info.AuthProof != nil {\n\t\t\treturn true, nil\n\t\t}\n\t}\n\treturn false, nil\n}\n\n// IsKnownEdge returns true if the graph source already knows of the passed\n// channel ID either as a live or zombie channel.",
      "length": 341,
      "tokens": 53,
      "embedding": []
    },
    {
      "slug": "func (r *mockGraphSource) IsKnownEdge(chanID lnwire.ShortChannelID) bool {",
      "content": "func (r *mockGraphSource) IsKnownEdge(chanID lnwire.ShortChannelID) bool {\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\n\tchanIDInt := chanID.ToUint64()\n\t_, exists := r.infos[chanIDInt]\n\t_, isZombie := r.zombies[chanIDInt]\n\treturn exists || isZombie\n}\n\n// IsStaleEdgePolicy returns true if the graph source has a channel edge for\n// the passed channel ID (and flags) that have a more recent timestamp.",
      "length": 305,
      "tokens": 45,
      "embedding": []
    },
    {
      "slug": "func (r *mockGraphSource) IsStaleEdgePolicy(chanID lnwire.ShortChannelID,",
      "content": "func (r *mockGraphSource) IsStaleEdgePolicy(chanID lnwire.ShortChannelID,\n\ttimestamp time.Time, flags lnwire.ChanUpdateChanFlags) bool {\n\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\n\tchanIDInt := chanID.ToUint64()\n\tedges, ok := r.edges[chanIDInt]\n\tif !ok {\n\t\t// Since the edge doesn't exist, we'll check our zombie index as\n\t\t// well.\n\t\t_, isZombie := r.zombies[chanIDInt]\n\t\tif !isZombie {\n\t\t\treturn false\n\t\t}\n\n\t\t// Since it exists within our zombie index, we'll check that it\n\t\t// respects the router's live edge horizon to determine whether\n\t\t// it is stale or not.\n\t\treturn time.Since(timestamp) > routing.DefaultChannelPruneExpiry\n\t}\n\n\tswitch {\n\tcase flags&lnwire.ChanUpdateDirection == 0 &&\n\t\t!reflect.DeepEqual(edges[0], channeldb.ChannelEdgePolicy{}):\n\n\t\treturn !timestamp.After(edges[0].LastUpdate)\n\n\tcase flags&lnwire.ChanUpdateDirection == 1 &&\n\t\t!reflect.DeepEqual(edges[1], channeldb.ChannelEdgePolicy{}):\n\n\t\treturn !timestamp.After(edges[1].LastUpdate)\n\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// MarkEdgeLive clears an edge from our zombie index, deeming it as live.\n//\n// NOTE: This method is part of the ChannelGraphSource interface.",
      "length": 1018,
      "tokens": 125,
      "embedding": []
    },
    {
      "slug": "func (r *mockGraphSource) MarkEdgeLive(chanID lnwire.ShortChannelID) error {",
      "content": "func (r *mockGraphSource) MarkEdgeLive(chanID lnwire.ShortChannelID) error {\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\tdelete(r.zombies, chanID.ToUint64())\n\treturn nil\n}\n\n// MarkEdgeZombie marks an edge as a zombie within our zombie index.",
      "length": 149,
      "tokens": 20,
      "embedding": []
    },
    {
      "slug": "func (r *mockGraphSource) MarkEdgeZombie(chanID lnwire.ShortChannelID, pubKey1,",
      "content": "func (r *mockGraphSource) MarkEdgeZombie(chanID lnwire.ShortChannelID, pubKey1,\n\tpubKey2 [33]byte) error {\n\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\n\tr.zombies[chanID.ToUint64()] = [][33]byte{pubKey1, pubKey2}\n\n\treturn nil\n}\n",
      "length": 130,
      "tokens": 14,
      "embedding": []
    },
    {
      "slug": "type mockNotifier struct {",
      "content": "type mockNotifier struct {\n\tclientCounter uint32\n\tepochClients  map[uint32]chan *chainntnfs.BlockEpoch\n\n\tsync.RWMutex\n}\n",
      "length": 88,
      "tokens": 7,
      "embedding": []
    },
    {
      "slug": "func newMockNotifier() *mockNotifier {",
      "content": "func newMockNotifier() *mockNotifier {\n\treturn &mockNotifier{\n\t\tepochClients: make(map[uint32]chan *chainntnfs.BlockEpoch),\n\t}\n}\n",
      "length": 86,
      "tokens": 7,
      "embedding": []
    },
    {
      "slug": "func (m *mockNotifier) RegisterConfirmationsNtfn(txid *chainhash.Hash,",
      "content": "func (m *mockNotifier) RegisterConfirmationsNtfn(txid *chainhash.Hash,\n\t_ []byte, numConfs, _ uint32,\n\topts ...chainntnfs.NotifierOption) (*chainntnfs.ConfirmationEvent, error) {\n\n\treturn nil, nil\n}\n",
      "length": 123,
      "tokens": 14,
      "embedding": []
    },
    {
      "slug": "func (m *mockNotifier) RegisterSpendNtfn(outpoint *wire.OutPoint, _ []byte,",
      "content": "func (m *mockNotifier) RegisterSpendNtfn(outpoint *wire.OutPoint, _ []byte,\n\t_ uint32) (*chainntnfs.SpendEvent, error) {\n\treturn nil, nil\n}\n",
      "length": 61,
      "tokens": 9,
      "embedding": []
    },
    {
      "slug": "func (m *mockNotifier) notifyBlock(hash chainhash.Hash, height uint32) {",
      "content": "func (m *mockNotifier) notifyBlock(hash chainhash.Hash, height uint32) {\n\tm.RLock()\n\tdefer m.RUnlock()\n\n\tfor _, client := range m.epochClients {\n\t\tclient <- &chainntnfs.BlockEpoch{\n\t\t\tHeight: int32(height),\n\t\t\tHash:   &hash,\n\t\t}\n\t}\n}\n",
      "length": 151,
      "tokens": 20,
      "embedding": []
    },
    {
      "slug": "func (m *mockNotifier) RegisterBlockEpochNtfn(",
      "content": "func (m *mockNotifier) RegisterBlockEpochNtfn(\n\tbestBlock *chainntnfs.BlockEpoch) (*chainntnfs.BlockEpochEvent, error) {\n\tm.RLock()\n\tdefer m.RUnlock()\n\n\tepochChan := make(chan *chainntnfs.BlockEpoch)\n\tclientID := m.clientCounter\n\tm.clientCounter++\n\tm.epochClients[clientID] = epochChan\n\n\treturn &chainntnfs.BlockEpochEvent{\n\t\tEpochs: epochChan,\n\t\tCancel: func() {},\n\t}, nil\n}\n",
      "length": 315,
      "tokens": 29,
      "embedding": []
    },
    {
      "slug": "func (m *mockNotifier) Start() error {",
      "content": "func (m *mockNotifier) Start() error {\n\treturn nil\n}\n",
      "length": 12,
      "tokens": 3,
      "embedding": []
    },
    {
      "slug": "func (m *mockNotifier) Started() bool {",
      "content": "func (m *mockNotifier) Started() bool {\n\treturn true\n}\n",
      "length": 13,
      "tokens": 3,
      "embedding": []
    },
    {
      "slug": "func (m *mockNotifier) Stop() error {",
      "content": "func (m *mockNotifier) Stop() error {\n\treturn nil\n}\n",
      "length": 12,
      "tokens": 3,
      "embedding": []
    },
    {
      "slug": "type annBatch struct {",
      "content": "type annBatch struct {\n\tnodeAnn1 *lnwire.NodeAnnouncement\n\tnodeAnn2 *lnwire.NodeAnnouncement\n\n\tchanAnn *lnwire.ChannelAnnouncement\n\n\tchanUpdAnn1 *lnwire.ChannelUpdate\n\tchanUpdAnn2 *lnwire.ChannelUpdate\n\n\tlocalProofAnn  *lnwire.AnnounceSignatures\n\tremoteProofAnn *lnwire.AnnounceSignatures\n}\n",
      "length": 257,
      "tokens": 15,
      "embedding": []
    },
    {
      "slug": "func createLocalAnnouncements(blockHeight uint32) (*annBatch, error) {",
      "content": "func createLocalAnnouncements(blockHeight uint32) (*annBatch, error) {\n\treturn createAnnouncements(blockHeight, selfKeyPriv, remoteKeyPriv1)\n}\n",
      "length": 70,
      "tokens": 5,
      "embedding": []
    },
    {
      "slug": "func createRemoteAnnouncements(blockHeight uint32) (*annBatch, error) {",
      "content": "func createRemoteAnnouncements(blockHeight uint32) (*annBatch, error) {\n\treturn createAnnouncements(blockHeight, remoteKeyPriv1, remoteKeyPriv2)\n}\n",
      "length": 73,
      "tokens": 5,
      "embedding": []
    },
    {
      "slug": "func createAnnouncements(blockHeight uint32, key1, key2 *btcec.PrivateKey) (*annBatch, error) {",
      "content": "func createAnnouncements(blockHeight uint32, key1, key2 *btcec.PrivateKey) (*annBatch, error) {\n\tvar err error\n\tvar batch annBatch\n\ttimestamp := testTimestamp\n\n\tbatch.nodeAnn1, err = createNodeAnnouncement(key1, timestamp)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tbatch.nodeAnn2, err = createNodeAnnouncement(key2, timestamp)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tbatch.chanAnn, err = createChannelAnnouncement(blockHeight, key1, key2)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tbatch.remoteProofAnn = &lnwire.AnnounceSignatures{\n\t\tShortChannelID: lnwire.ShortChannelID{\n\t\t\tBlockHeight: blockHeight,\n\t\t},\n\t\tNodeSignature:    batch.chanAnn.NodeSig2,\n\t\tBitcoinSignature: batch.chanAnn.BitcoinSig2,\n\t}\n\n\tbatch.localProofAnn = &lnwire.AnnounceSignatures{\n\t\tShortChannelID: lnwire.ShortChannelID{\n\t\t\tBlockHeight: blockHeight,\n\t\t},\n\t\tNodeSignature:    batch.chanAnn.NodeSig1,\n\t\tBitcoinSignature: batch.chanAnn.BitcoinSig1,\n\t}\n\n\tbatch.chanUpdAnn1, err = createUpdateAnnouncement(\n\t\tblockHeight, 0, key1, timestamp,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tbatch.chanUpdAnn2, err = createUpdateAnnouncement(\n\t\tblockHeight, 1, key2, timestamp,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &batch, nil\n\n}\n",
      "length": 1052,
      "tokens": 118,
      "embedding": []
    },
    {
      "slug": "func createNodeAnnouncement(priv *btcec.PrivateKey,",
      "content": "func createNodeAnnouncement(priv *btcec.PrivateKey,\n\ttimestamp uint32, extraBytes ...[]byte) (*lnwire.NodeAnnouncement, error) {\n\n\tvar err error\n\tk := hex.EncodeToString(priv.Serialize())\n\talias, err := lnwire.NewNodeAlias(\"kek\" + k[:10])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ta := &lnwire.NodeAnnouncement{\n\t\tTimestamp: timestamp,\n\t\tAddresses: testAddrs,\n\t\tAlias:     alias,\n\t\tFeatures:  testFeatures,\n\t}\n\tcopy(a.NodeID[:], priv.PubKey().SerializeCompressed())\n\tif len(extraBytes) == 1 {\n\t\ta.ExtraOpaqueData = extraBytes[0]\n\t}\n\n\tsigner := mock.SingleSigner{Privkey: priv}\n\tsig, err := netann.SignAnnouncement(&signer, testKeyLoc, a)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ta.Signature, err = lnwire.NewSigFromSignature(sig)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn a, nil\n}\n",
      "length": 698,
      "tokens": 87,
      "embedding": []
    },
    {
      "slug": "func createUpdateAnnouncement(blockHeight uint32,",
      "content": "func createUpdateAnnouncement(blockHeight uint32,\n\tflags lnwire.ChanUpdateChanFlags,\n\tnodeKey *btcec.PrivateKey, timestamp uint32,\n\textraBytes ...[]byte) (*lnwire.ChannelUpdate, error) {\n\n\tvar err error\n\n\thtlcMinMsat := lnwire.MilliSatoshi(prand.Int63())\n\ta := &lnwire.ChannelUpdate{\n\t\tShortChannelID: lnwire.ShortChannelID{\n\t\t\tBlockHeight: blockHeight,\n\t\t},\n\t\tTimestamp:       timestamp,\n\t\tMessageFlags:    lnwire.ChanUpdateRequiredMaxHtlc,\n\t\tChannelFlags:    flags,\n\t\tTimeLockDelta:   uint16(prand.Int63()),\n\t\tHtlcMinimumMsat: htlcMinMsat,\n\n\t\t// Since the max HTLC must be greater than the min HTLC to pass channel\n\t\t// update validation, set it to double the min htlc.\n\t\tHtlcMaximumMsat: 2 * htlcMinMsat,\n\t\tFeeRate:         uint32(prand.Int31()),\n\t\tBaseFee:         uint32(prand.Int31()),\n\t}\n\tif len(extraBytes) == 1 {\n\t\ta.ExtraOpaqueData = extraBytes[0]\n\t}\n\n\terr = signUpdate(nodeKey, a)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn a, nil\n}\n",
      "length": 864,
      "tokens": 95,
      "embedding": []
    },
    {
      "slug": "func signUpdate(nodeKey *btcec.PrivateKey, a *lnwire.ChannelUpdate) error {",
      "content": "func signUpdate(nodeKey *btcec.PrivateKey, a *lnwire.ChannelUpdate) error {\n\tsigner := mock.SingleSigner{Privkey: nodeKey}\n\tsig, err := netann.SignAnnouncement(&signer, testKeyLoc, a)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\ta.Signature, err = lnwire.NewSigFromSignature(sig)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n",
      "length": 229,
      "tokens": 33,
      "embedding": []
    },
    {
      "slug": "func createAnnouncementWithoutProof(blockHeight uint32,",
      "content": "func createAnnouncementWithoutProof(blockHeight uint32,\n\tkey1, key2 *btcec.PublicKey,\n\textraBytes ...[]byte) *lnwire.ChannelAnnouncement {\n\n\ta := &lnwire.ChannelAnnouncement{\n\t\tShortChannelID: lnwire.ShortChannelID{\n\t\t\tBlockHeight: blockHeight,\n\t\t\tTxIndex:     0,\n\t\t\tTxPosition:  0,\n\t\t},\n\t\tFeatures: testFeatures,\n\t}\n\tcopy(a.NodeID1[:], key1.SerializeCompressed())\n\tcopy(a.NodeID2[:], key2.SerializeCompressed())\n\tcopy(a.BitcoinKey1[:], bitcoinKeyPub1.SerializeCompressed())\n\tcopy(a.BitcoinKey2[:], bitcoinKeyPub2.SerializeCompressed())\n\tif len(extraBytes) == 1 {\n\t\ta.ExtraOpaqueData = extraBytes[0]\n\t}\n\n\treturn a\n}\n",
      "length": 539,
      "tokens": 42,
      "embedding": []
    },
    {
      "slug": "func createRemoteChannelAnnouncement(blockHeight uint32,",
      "content": "func createRemoteChannelAnnouncement(blockHeight uint32,\n\textraBytes ...[]byte) (*lnwire.ChannelAnnouncement, error) {\n\n\treturn createChannelAnnouncement(blockHeight, remoteKeyPriv1, remoteKeyPriv2, extraBytes...)\n}\n",
      "length": 155,
      "tokens": 11,
      "embedding": []
    },
    {
      "slug": "func createChannelAnnouncement(blockHeight uint32, key1, key2 *btcec.PrivateKey,",
      "content": "func createChannelAnnouncement(blockHeight uint32, key1, key2 *btcec.PrivateKey,\n\textraBytes ...[]byte) (*lnwire.ChannelAnnouncement, error) {\n\n\ta := createAnnouncementWithoutProof(blockHeight, key1.PubKey(), key2.PubKey(), extraBytes...)\n\n\tsigner := mock.SingleSigner{Privkey: key1}\n\tsig, err := netann.SignAnnouncement(&signer, testKeyLoc, a)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ta.NodeSig1, err = lnwire.NewSigFromSignature(sig)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tsigner = mock.SingleSigner{Privkey: key2}\n\tsig, err = netann.SignAnnouncement(&signer, testKeyLoc, a)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ta.NodeSig2, err = lnwire.NewSigFromSignature(sig)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tsigner = mock.SingleSigner{Privkey: bitcoinKeyPriv1}\n\tsig, err = netann.SignAnnouncement(&signer, testKeyLoc, a)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ta.BitcoinSig1, err = lnwire.NewSigFromSignature(sig)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tsigner = mock.SingleSigner{Privkey: bitcoinKeyPriv2}\n\tsig, err = netann.SignAnnouncement(&signer, testKeyLoc, a)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ta.BitcoinSig2, err = lnwire.NewSigFromSignature(sig)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn a, nil\n}\n",
      "length": 1084,
      "tokens": 143,
      "embedding": []
    },
    {
      "slug": "func mockFindChannel(node *btcec.PublicKey, chanID lnwire.ChannelID) (",
      "content": "func mockFindChannel(node *btcec.PublicKey, chanID lnwire.ChannelID) (\n\t*channeldb.OpenChannel, error) {\n\n\treturn nil, nil\n}\n",
      "length": 50,
      "tokens": 7,
      "embedding": []
    },
    {
      "slug": "type testCtx struct {",
      "content": "type testCtx struct {\n\tgossiper           *AuthenticatedGossiper\n\trouter             *mockGraphSource\n\tnotifier           *mockNotifier\n\tbroadcastedMessage chan msgWithSenders\n}\n",
      "length": 151,
      "tokens": 10,
      "embedding": []
    },
    {
      "slug": "func createTestCtx(t *testing.T, startHeight uint32) (*testCtx, error) {",
      "content": "func createTestCtx(t *testing.T, startHeight uint32) (*testCtx, error) {\n\t// Next we'll initialize an instance of the channel router with mock\n\t// versions of the chain and channel notifier. As we don't need to test\n\t// any p2p functionality, the peer send and switch send,\n\t// broadcast functions won't be populated.\n\tnotifier := newMockNotifier()\n\trouter := newMockRouter(startHeight)\n\n\tdb, err := makeTestDB(t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\twaitingProofStore, err := channeldb.NewWaitingProofStore(db)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tbroadcastedMessage := make(chan msgWithSenders, 10)\n\n\tisAlias := func(lnwire.ShortChannelID) bool {\n\t\treturn false\n\t}\n\n\tsignAliasUpdate := func(*lnwire.ChannelUpdate) (*ecdsa.Signature,\n\t\terror) {\n\n\t\treturn nil, nil\n\t}\n\n\tfindBaseByAlias := func(lnwire.ShortChannelID) (lnwire.ShortChannelID,\n\t\terror) {\n\n\t\treturn lnwire.ShortChannelID{}, fmt.Errorf(\"no base scid\")\n\t}\n\n\tgetAlias := func(lnwire.ChannelID) (lnwire.ShortChannelID, error) {\n\t\treturn lnwire.ShortChannelID{}, fmt.Errorf(\"no peer alias\")\n\t}\n\n\tgossiper := New(Config{\n\t\tNotifier: notifier,\n\t\tBroadcast: func(senders map[route.Vertex]struct{},\n\t\t\tmsgs ...lnwire.Message) error {\n\n\t\t\tfor _, msg := range msgs {\n\t\t\t\tbroadcastedMessage <- msgWithSenders{\n\t\t\t\t\tmsg:     msg,\n\t\t\t\t\tsenders: senders,\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn nil\n\t\t},\n\t\tNotifyWhenOnline: func(target [33]byte,\n\t\t\tpeerChan chan<- lnpeer.Peer) {\n\n\t\t\tpk, _ := btcec.ParsePubKey(target[:])\n\t\t\tpeerChan <- &mockPeer{pk, nil, nil}\n\t\t},\n\t\tNotifyWhenOffline: func(_ [33]byte) <-chan struct{} {\n\t\t\tc := make(chan struct{})\n\t\t\treturn c\n\t\t},\n\t\tSelfNodeAnnouncement: func(bool) (lnwire.NodeAnnouncement, error) {\n\t\t\treturn lnwire.NodeAnnouncement{\n\t\t\t\tTimestamp: testTimestamp,\n\t\t\t}, nil\n\t\t},\n\t\tRouter:                router,\n\t\tTrickleDelay:          trickleDelay,\n\t\tRetransmitTicker:      ticker.NewForce(retransmitDelay),\n\t\tRebroadcastInterval:   rebroadcastInterval,\n\t\tProofMatureDelta:      proofMatureDelta,\n\t\tWaitingProofStore:     waitingProofStore,\n\t\tMessageStore:          newMockMessageStore(),\n\t\tRotateTicker:          ticker.NewForce(DefaultSyncerRotationInterval),\n\t\tHistoricalSyncTicker:  ticker.NewForce(DefaultHistoricalSyncInterval),\n\t\tNumActiveSyncers:      3,\n\t\tAnnSigner:             &mock.SingleSigner{Privkey: selfKeyPriv},\n\t\tSubBatchDelay:         1 * time.Millisecond,\n\t\tMinimumBatchSize:      10,\n\t\tMaxChannelUpdateBurst: DefaultMaxChannelUpdateBurst,\n\t\tChannelUpdateInterval: DefaultChannelUpdateInterval,\n\t\tIsAlias:               isAlias,\n\t\tSignAliasUpdate:       signAliasUpdate,\n\t\tFindBaseByAlias:       findBaseByAlias,\n\t\tGetAlias:              getAlias,\n\t\tFindChannel:           mockFindChannel,\n\t}, selfKeyDesc)\n\n\tif err := gossiper.Start(); err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to start router: %v\", err)\n\t}\n\n\t// Mark the graph as synced in order to allow the announcements to be\n\t// broadcast.\n\tgossiper.syncMgr.markGraphSynced()\n\n\tt.Cleanup(func() {\n\t\tgossiper.Stop()\n\t})\n\n\treturn &testCtx{\n\t\trouter:             router,\n\t\tnotifier:           notifier,\n\t\tgossiper:           gossiper,\n\t\tbroadcastedMessage: broadcastedMessage,\n\t}, nil\n}\n\n// TestProcessAnnouncement checks that mature announcements are propagated to\n// the router subsystem.",
      "length": 3060,
      "tokens": 303,
      "embedding": []
    },
    {
      "slug": "func TestProcessAnnouncement(t *testing.T) {",
      "content": "func TestProcessAnnouncement(t *testing.T) {\n\tt.Parallel()\n\n\ttimestamp := testTimestamp\n\tctx, err := createTestCtx(t, 0)\n\trequire.NoError(t, err, \"can't create context\")\n\n\tassertSenderExistence := func(sender *btcec.PublicKey, msg msgWithSenders) {\n\t\tt.Helper()\n\n\t\tif _, ok := msg.senders[route.NewVertex(sender)]; !ok {\n\t\t\tt.Fatalf(\"sender=%x not present in %v\",\n\t\t\t\tsender.SerializeCompressed(), spew.Sdump(msg))\n\t\t}\n\t}\n\n\tnodePeer := &mockPeer{remoteKeyPriv1.PubKey(), nil, nil}\n\n\t// First, we'll craft a valid remote channel announcement and send it to\n\t// the gossiper so that it can be processed.\n\tca, err := createRemoteChannelAnnouncement(0)\n\trequire.NoError(t, err, \"can't create channel announcement\")\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(ca, nodePeer):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"remote announcement not processed\")\n\t}\n\trequire.NoError(t, err, \"can't process remote announcement\")\n\n\t// The announcement should be broadcast and included in our local view\n\t// of the graph.\n\tselect {\n\tcase msg := <-ctx.broadcastedMessage:\n\t\tassertSenderExistence(nodePeer.IdentityKey(), msg)\n\tcase <-time.After(2 * trickleDelay):\n\t\tt.Fatal(\"announcement wasn't proceeded\")\n\t}\n\n\tif len(ctx.router.infos) != 1 {\n\t\tt.Fatalf(\"edge wasn't added to router: %v\", err)\n\t}\n\n\t// We'll craft an invalid channel update, setting no message flags.\n\tua, err := createUpdateAnnouncement(0, 0, remoteKeyPriv1, timestamp)\n\trequire.NoError(t, err, \"can't create update announcement\")\n\tua.MessageFlags = 0\n\n\t// We send an invalid channel update and expect it to fail.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(ua, nodePeer):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"remote announcement not processed\")\n\t}\n\trequire.ErrorContains(t, err, \"max htlc flag not set for channel \"+\n\t\t\"update\")\n\n\t// We should not broadcast the channel update.\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"gossiper should not have broadcast channel update\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\t// We'll then craft the channel policy of the remote party and also send\n\t// it to the gossiper.\n\tua, err = createUpdateAnnouncement(0, 0, remoteKeyPriv1, timestamp)\n\trequire.NoError(t, err, \"can't create update announcement\")\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(ua, nodePeer):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"remote announcement not processed\")\n\t}\n\trequire.NoError(t, err, \"can't process remote announcement\")\n\n\t// The channel policy should be broadcast to the rest of the network.\n\tselect {\n\tcase msg := <-ctx.broadcastedMessage:\n\t\tassertSenderExistence(nodePeer.IdentityKey(), msg)\n\tcase <-time.After(2 * trickleDelay):\n\t\tt.Fatal(\"announcement wasn't proceeded\")\n\t}\n\n\tif len(ctx.router.edges) != 1 {\n\t\tt.Fatalf(\"edge update wasn't added to router: %v\", err)\n\t}\n\n\t// Finally, we'll craft the remote party's node announcement.\n\tna, err := createNodeAnnouncement(remoteKeyPriv1, timestamp)\n\trequire.NoError(t, err, \"can't create node announcement\")\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(na, nodePeer):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"remote announcement not processed\")\n\t}\n\trequire.NoError(t, err, \"can't process remote announcement\")\n\n\t// It should also be broadcast to the network and included in our local\n\t// view of the graph.\n\tselect {\n\tcase msg := <-ctx.broadcastedMessage:\n\t\tassertSenderExistence(nodePeer.IdentityKey(), msg)\n\tcase <-time.After(2 * trickleDelay):\n\t\tt.Fatal(\"announcement wasn't proceeded\")\n\t}\n\n\tif len(ctx.router.nodes) != 1 {\n\t\tt.Fatalf(\"node wasn't added to router: %v\", err)\n\t}\n}\n\n// TestPrematureAnnouncement checks that premature announcements are not\n// propagated to the router subsystem.",
      "length": 3579,
      "tokens": 434,
      "embedding": []
    },
    {
      "slug": "func TestPrematureAnnouncement(t *testing.T) {",
      "content": "func TestPrematureAnnouncement(t *testing.T) {\n\tt.Parallel()\n\n\ttimestamp := testTimestamp\n\n\tctx, err := createTestCtx(t, 0)\n\trequire.NoError(t, err, \"can't create context\")\n\n\t_, err = createNodeAnnouncement(remoteKeyPriv1, timestamp)\n\trequire.NoError(t, err, \"can't create node announcement\")\n\n\tnodePeer := &mockPeer{remoteKeyPriv1.PubKey(), nil, nil}\n\n\t// Pretending that we receive the valid channel announcement from\n\t// remote side, but block height of this announcement is greater than\n\t// highest know to us, for that reason it should be ignored and not\n\t// added to the router.\n\tca, err := createRemoteChannelAnnouncement(1)\n\trequire.NoError(t, err, \"can't create channel announcement\")\n\n\tselect {\n\tcase <-ctx.gossiper.ProcessRemoteAnnouncement(ca, nodePeer):\n\tcase <-time.After(time.Second):\n\t\tt.Fatal(\"announcement was not processed\")\n\t}\n\n\tif len(ctx.router.infos) != 0 {\n\t\tt.Fatal(\"edge was added to router\")\n\t}\n}\n\n// TestSignatureAnnouncementLocalFirst ensures that the AuthenticatedGossiper\n// properly processes partial and fully announcement signatures message.",
      "length": 997,
      "tokens": 120,
      "embedding": []
    },
    {
      "slug": "func TestSignatureAnnouncementLocalFirst(t *testing.T) {",
      "content": "func TestSignatureAnnouncementLocalFirst(t *testing.T) {\n\tt.Parallel()\n\n\tctx, err := createTestCtx(t, proofMatureDelta)\n\trequire.NoError(t, err, \"can't create context\")\n\n\t// Set up a channel that we can use to inspect the messages sent\n\t// directly from the gossiper.\n\tsentMsgs := make(chan lnwire.Message, 10)\n\tctx.gossiper.reliableSender.cfg.NotifyWhenOnline = func(target [33]byte,\n\t\tpeerChan chan<- lnpeer.Peer) {\n\n\t\tpk, _ := btcec.ParsePubKey(target[:])\n\n\t\tselect {\n\t\tcase peerChan <- &mockPeer{pk, sentMsgs, ctx.gossiper.quit}:\n\t\tcase <-ctx.gossiper.quit:\n\t\t}\n\t}\n\n\tbatch, err := createLocalAnnouncements(0)\n\trequire.NoError(t, err, \"can't generate announcements\")\n\n\tremoteKey, err := btcec.ParsePubKey(batch.nodeAnn2.NodeID[:])\n\trequire.NoError(t, err, \"unable to parse pubkey\")\n\tremotePeer := &mockPeer{remoteKey, sentMsgs, ctx.gossiper.quit}\n\n\t// Recreate lightning network topology. Initialize router with channel\n\t// between two nodes.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(batch.chanAnn):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process channel ann\")\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"channel announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(batch.chanUpdAnn1):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process channel update\")\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"channel update announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(batch.nodeAnn1):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process node ann\")\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"node announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\t// The local ChannelUpdate should now be sent directly to the remote peer,\n\t// such that the edge can be used for routing, regardless if this channel\n\t// is announced or not (private channel).\n\tselect {\n\tcase msg := <-sentMsgs:\n\t\tassertMessage(t, batch.chanUpdAnn1, msg)\n\tcase <-time.After(1 * time.Second):\n\t\tt.Fatal(\"gossiper did not send channel update to peer\")\n\t}\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.chanUpdAnn2, remotePeer,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process channel update\")\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"channel update announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.nodeAnn2, remotePeer,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process node ann\")\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"node announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\t// Pretending that we receive local channel announcement from funding\n\t// manager, thereby kick off the announcement exchange process.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(batch.localProofAnn):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process local proof\")\n\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"announcements were broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\tnumber := 0\n\tif err := ctx.gossiper.cfg.WaitingProofStore.ForAll(\n\t\tfunc(*channeldb.WaitingProof) error {\n\t\t\tnumber++\n\t\t\treturn nil\n\t\t},\n\t\tfunc() {\n\t\t\tnumber = 0\n\t\t},\n\t); err != nil {\n\t\tt.Fatalf(\"unable to retrieve objects from store: %v\", err)\n\t}\n\n\tif number != 1 {\n\t\tt.Fatal(\"wrong number of objects in storage\")\n\t}\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.remoteProofAnn, remotePeer,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process remote proof\")\n\n\tfor i := 0; i < 5; i++ {\n\t\tselect {\n\t\tcase <-ctx.broadcastedMessage:\n\t\tcase <-time.After(time.Second):\n\t\t\tt.Fatal(\"announcement wasn't broadcast\")\n\t\t}\n\t}\n\n\tnumber = 0\n\tif err := ctx.gossiper.cfg.WaitingProofStore.ForAll(\n\t\tfunc(*channeldb.WaitingProof) error {\n\t\t\tnumber++\n\t\t\treturn nil\n\t\t},\n\t\tfunc() {\n\t\t\tnumber = 0\n\t\t},\n\t); err != nil && err != channeldb.ErrWaitingProofNotFound {\n\t\tt.Fatalf(\"unable to retrieve objects from store: %v\", err)\n\t}\n\n\tif number != 0 {\n\t\tt.Fatal(\"waiting proof should be removed from storage\")\n\t}\n}\n\n// TestOrphanSignatureAnnouncement ensures that the gossiper properly\n// processes announcement with unknown channel ids.",
      "length": 4647,
      "tokens": 552,
      "embedding": []
    },
    {
      "slug": "func TestOrphanSignatureAnnouncement(t *testing.T) {",
      "content": "func TestOrphanSignatureAnnouncement(t *testing.T) {\n\tt.Parallel()\n\n\tctx, err := createTestCtx(t, proofMatureDelta)\n\trequire.NoError(t, err, \"can't create context\")\n\n\t// Set up a channel that we can use to inspect the messages sent\n\t// directly from the gossiper.\n\tsentMsgs := make(chan lnwire.Message, 10)\n\tctx.gossiper.reliableSender.cfg.NotifyWhenOnline = func(target [33]byte,\n\t\tpeerChan chan<- lnpeer.Peer) {\n\n\t\tpk, _ := btcec.ParsePubKey(target[:])\n\n\t\tselect {\n\t\tcase peerChan <- &mockPeer{pk, sentMsgs, ctx.gossiper.quit}:\n\t\tcase <-ctx.gossiper.quit:\n\t\t}\n\t}\n\n\tbatch, err := createLocalAnnouncements(0)\n\trequire.NoError(t, err, \"can't generate announcements\")\n\n\tremoteKey, err := btcec.ParsePubKey(batch.nodeAnn2.NodeID[:])\n\trequire.NoError(t, err, \"unable to parse pubkey\")\n\tremotePeer := &mockPeer{remoteKey, sentMsgs, ctx.gossiper.quit}\n\n\t// Pretending that we receive local channel announcement from funding\n\t// manager, thereby kick off the announcement exchange process, in\n\t// this case the announcement should be added in the orphan batch\n\t// because we haven't announce the channel yet.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(batch.remoteProofAnn,\n\t\tremotePeer):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to proceed announcement\")\n\n\tnumber := 0\n\tif err := ctx.gossiper.cfg.WaitingProofStore.ForAll(\n\t\tfunc(*channeldb.WaitingProof) error {\n\t\t\tnumber++\n\t\t\treturn nil\n\t\t},\n\t\tfunc() {\n\t\t\tnumber = 0\n\t\t},\n\t); err != nil {\n\t\tt.Fatalf(\"unable to retrieve objects from store: %v\", err)\n\t}\n\n\tif number != 1 {\n\t\tt.Fatal(\"wrong number of objects in storage\")\n\t}\n\n\t// Recreate lightning network topology. Initialize router with channel\n\t// between two nodes.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(batch.chanAnn):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local announcement\")\n\t}\n\n\trequire.NoError(t, err, \"unable to process\")\n\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"channel announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(batch.chanUpdAnn1):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process\")\n\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"channel update announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(batch.nodeAnn1):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process node ann\")\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"node announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\t// The local ChannelUpdate should now be sent directly to the remote peer,\n\t// such that the edge can be used for routing, regardless if this channel\n\t// is announced or not (private channel).\n\tselect {\n\tcase msg := <-sentMsgs:\n\t\tassertMessage(t, batch.chanUpdAnn1, msg)\n\tcase <-time.After(1 * time.Second):\n\t\tt.Fatal(\"gossiper did not send channel update to peer\")\n\t}\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(batch.chanUpdAnn2,\n\t\tremotePeer):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process node ann\")\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"channel update announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.nodeAnn2, remotePeer,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process\")\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"node announcement announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\t// After that we process local announcement, and waiting to receive\n\t// the channel announcement.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(batch.localProofAnn):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process\")\n\n\t// The local proof should be sent to the remote peer.\n\tselect {\n\tcase msg := <-sentMsgs:\n\t\tassertMessage(t, batch.localProofAnn, msg)\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatalf(\"local proof was not sent to peer\")\n\t}\n\n\t// And since both remote and local announcements are processed, we\n\t// should be broadcasting the final channel announcements.\n\tfor i := 0; i < 5; i++ {\n\t\tselect {\n\t\tcase <-ctx.broadcastedMessage:\n\t\tcase <-time.After(time.Second):\n\t\t\tt.Fatal(\"announcement wasn't broadcast\")\n\t\t}\n\t}\n\n\tnumber = 0\n\tif err := ctx.gossiper.cfg.WaitingProofStore.ForAll(\n\t\tfunc(p *channeldb.WaitingProof) error {\n\t\t\tnumber++\n\t\t\treturn nil\n\t\t},\n\t\tfunc() {\n\t\t\tnumber = 0\n\t\t},\n\t); err != nil {\n\t\tt.Fatalf(\"unable to retrieve objects from store: %v\", err)\n\t}\n\n\tif number != 0 {\n\t\tt.Fatalf(\"wrong number of objects in storage: %v\", number)\n\t}\n}\n\n// TestSignatureAnnouncementRetryAtStartup tests that if we restart the\n// gossiper, it will retry sending the AnnounceSignatures to the peer if it did\n// not succeed before shutting down, and the full channel proof is not yet\n// assembled.",
      "length": 5113,
      "tokens": 637,
      "embedding": []
    },
    {
      "slug": "func TestSignatureAnnouncementRetryAtStartup(t *testing.T) {",
      "content": "func TestSignatureAnnouncementRetryAtStartup(t *testing.T) {\n\tt.Parallel()\n\n\tctx, err := createTestCtx(t, proofMatureDelta)\n\trequire.NoError(t, err, \"can't create context\")\n\n\tbatch, err := createLocalAnnouncements(0)\n\trequire.NoError(t, err, \"can't generate announcements\")\n\n\tremoteKey, err := btcec.ParsePubKey(batch.nodeAnn2.NodeID[:])\n\trequire.NoError(t, err, \"unable to parse pubkey\")\n\n\t// Set up a channel to intercept the messages sent to the remote peer.\n\tsentToPeer := make(chan lnwire.Message, 1)\n\tremotePeer := &mockPeer{remoteKey, sentToPeer, ctx.gossiper.quit}\n\n\t// Since the reliable send to the remote peer of the local channel proof\n\t// requires a notification when the peer comes online, we'll capture the\n\t// channel through which it gets sent to control exactly when to\n\t// dispatch it.\n\tnotifyPeers := make(chan chan<- lnpeer.Peer, 1)\n\tctx.gossiper.reliableSender.cfg.NotifyWhenOnline = func(peer [33]byte,\n\t\tconnectedChan chan<- lnpeer.Peer) {\n\t\tnotifyPeers <- connectedChan\n\t}\n\n\t// Recreate lightning network topology. Initialize router with channel\n\t// between two nodes.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(batch.chanAnn):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process channel ann\")\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"channel announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\t// Pretending that we receive local channel announcement from funding\n\t// manager, thereby kick off the announcement exchange process.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(\n\t\tbatch.localProofAnn,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\tif err != nil {\n\t\tt.Fatalf(\"unable to process :%v\", err)\n\t}\n\n\t// The gossiper should register for a notification for when the peer is\n\t// online.\n\tselect {\n\tcase <-notifyPeers:\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatalf(\"gossiper did not ask to get notified when \" +\n\t\t\t\"peer is online\")\n\t}\n\n\t// The proof should not be broadcast yet since we're still missing the\n\t// remote party's.\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"announcements were broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\t// And it shouldn't be sent to the peer either as they are offline.\n\tselect {\n\tcase msg := <-sentToPeer:\n\t\tt.Fatalf(\"received unexpected message: %v\", spew.Sdump(msg))\n\tcase <-time.After(time.Second):\n\t}\n\n\tnumber := 0\n\tif err := ctx.gossiper.cfg.WaitingProofStore.ForAll(\n\t\tfunc(*channeldb.WaitingProof) error {\n\t\t\tnumber++\n\t\t\treturn nil\n\t\t},\n\t\tfunc() {\n\t\t\tnumber = 0\n\t\t},\n\t); err != nil {\n\t\tt.Fatalf(\"unable to retrieve objects from store: %v\", err)\n\t}\n\n\tif number != 1 {\n\t\tt.Fatal(\"wrong number of objects in storage\")\n\t}\n\n\t// Restart the gossiper and restore its original NotifyWhenOnline and\n\t// NotifyWhenOffline methods. This should trigger a new attempt to send\n\t// the message to the peer.\n\tctx.gossiper.Stop()\n\n\tisAlias := func(lnwire.ShortChannelID) bool {\n\t\treturn false\n\t}\n\n\tsignAliasUpdate := func(*lnwire.ChannelUpdate) (*ecdsa.Signature,\n\t\terror) {\n\n\t\treturn nil, nil\n\t}\n\n\tfindBaseByAlias := func(lnwire.ShortChannelID) (lnwire.ShortChannelID,\n\t\terror) {\n\n\t\treturn lnwire.ShortChannelID{}, fmt.Errorf(\"no base scid\")\n\t}\n\n\tgetAlias := func(lnwire.ChannelID) (lnwire.ShortChannelID, error) {\n\t\treturn lnwire.ShortChannelID{}, fmt.Errorf(\"no peer alias\")\n\t}\n\n\tgossiper := New(Config{\n\t\tNotifier:             ctx.gossiper.cfg.Notifier,\n\t\tBroadcast:            ctx.gossiper.cfg.Broadcast,\n\t\tNotifyWhenOnline:     ctx.gossiper.reliableSender.cfg.NotifyWhenOnline,\n\t\tNotifyWhenOffline:    ctx.gossiper.reliableSender.cfg.NotifyWhenOffline,\n\t\tSelfNodeAnnouncement: ctx.gossiper.cfg.SelfNodeAnnouncement,\n\t\tRouter:               ctx.gossiper.cfg.Router,\n\t\tTrickleDelay:         trickleDelay,\n\t\tRetransmitTicker:     ticker.NewForce(retransmitDelay),\n\t\tRebroadcastInterval:  rebroadcastInterval,\n\t\tProofMatureDelta:     proofMatureDelta,\n\t\tWaitingProofStore:    ctx.gossiper.cfg.WaitingProofStore,\n\t\tMessageStore:         ctx.gossiper.cfg.MessageStore,\n\t\tRotateTicker:         ticker.NewForce(DefaultSyncerRotationInterval),\n\t\tHistoricalSyncTicker: ticker.NewForce(DefaultHistoricalSyncInterval),\n\t\tNumActiveSyncers:     3,\n\t\tMinimumBatchSize:     10,\n\t\tSubBatchDelay:        time.Second * 5,\n\t\tIsAlias:              isAlias,\n\t\tSignAliasUpdate:      signAliasUpdate,\n\t\tFindBaseByAlias:      findBaseByAlias,\n\t\tGetAlias:             getAlias,\n\t}, &keychain.KeyDescriptor{\n\t\tPubKey:     ctx.gossiper.selfKey,\n\t\tKeyLocator: ctx.gossiper.selfKeyLoc,\n\t})\n\trequire.NoError(t, err, \"unable to recreate gossiper\")\n\tif err := gossiper.Start(); err != nil {\n\t\tt.Fatalf(\"unable to start recreated gossiper: %v\", err)\n\t}\n\tdefer gossiper.Stop()\n\n\t// Mark the graph as synced in order to allow the announcements to be\n\t// broadcast.\n\tgossiper.syncMgr.markGraphSynced()\n\n\tctx.gossiper = gossiper\n\tremotePeer.quit = ctx.gossiper.quit\n\n\t// After starting up, the gossiper will see that it has a proof in the\n\t// WaitingProofStore, and will retry sending its part to the remote.\n\t// It should register for a notification for when the peer is online.\n\tvar peerChan chan<- lnpeer.Peer\n\tselect {\n\tcase peerChan = <-notifyPeers:\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatalf(\"gossiper did not ask to get notified when \" +\n\t\t\t\"peer is online\")\n\t}\n\n\t// Notify that peer is now online. This should allow the proof to be\n\t// sent.\n\tpeerChan <- remotePeer\n\nout:\n\tfor {\n\t\tselect {\n\t\tcase msg := <-sentToPeer:\n\t\t\t// Since the ChannelUpdate will also be resent as it is\n\t\t\t// sent reliably, we'll need to filter it out.\n\t\t\tif _, ok := msg.(*lnwire.AnnounceSignatures); !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tassertMessage(t, batch.localProofAnn, msg)\n\t\t\tbreak out\n\t\tcase <-time.After(2 * time.Second):\n\t\t\tt.Fatalf(\"gossiper did not send message when peer \" +\n\t\t\t\t\"came online\")\n\t\t}\n\t}\n\n\t// Now exchanging the remote channel proof, the channel announcement\n\t// broadcast should continue as normal.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.remoteProofAnn, remotePeer,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\tif err != nil {\n\t\tt.Fatalf(\"unable to process :%v\", err)\n\t}\n\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\tcase <-time.After(time.Second):\n\t\tt.Fatal(\"announcement wasn't broadcast\")\n\t}\n\n\tnumber = 0\n\tif err := ctx.gossiper.cfg.WaitingProofStore.ForAll(\n\t\tfunc(*channeldb.WaitingProof) error {\n\t\t\tnumber++\n\t\t\treturn nil\n\t\t},\n\t\tfunc() {\n\t\t\tnumber = 0\n\t\t},\n\t); err != nil && err != channeldb.ErrWaitingProofNotFound {\n\t\tt.Fatalf(\"unable to retrieve objects from store: %v\", err)\n\t}\n\n\tif number != 0 {\n\t\tt.Fatal(\"waiting proof should be removed from storage\")\n\t}\n}\n\n// TestSignatureAnnouncementFullProofWhenRemoteProof tests that if a remote\n// proof is received when we already have the full proof, the gossiper will send\n// the full proof (ChannelAnnouncement) to the remote peer.",
      "length": 6723,
      "tokens": 805,
      "embedding": []
    },
    {
      "slug": "func TestSignatureAnnouncementFullProofWhenRemoteProof(t *testing.T) {",
      "content": "func TestSignatureAnnouncementFullProofWhenRemoteProof(t *testing.T) {\n\tt.Parallel()\n\n\tctx, err := createTestCtx(t, proofMatureDelta)\n\trequire.NoError(t, err, \"can't create context\")\n\n\tbatch, err := createLocalAnnouncements(0)\n\trequire.NoError(t, err, \"can't generate announcements\")\n\n\tremoteKey, err := btcec.ParsePubKey(batch.nodeAnn2.NodeID[:])\n\trequire.NoError(t, err, \"unable to parse pubkey\")\n\n\t// Set up a channel we can use to inspect messages sent by the\n\t// gossiper to the remote peer.\n\tsentToPeer := make(chan lnwire.Message, 1)\n\tremotePeer := &mockPeer{remoteKey, sentToPeer, ctx.gossiper.quit}\n\n\t// Override NotifyWhenOnline to return the remote peer which we expect\n\t// meesages to be sent to.\n\tctx.gossiper.reliableSender.cfg.NotifyWhenOnline = func(peer [33]byte,\n\t\tpeerChan chan<- lnpeer.Peer) {\n\n\t\tpeerChan <- remotePeer\n\t}\n\n\t// Recreate lightning network topology. Initialize router with channel\n\t// between two nodes.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(\n\t\tbatch.chanAnn,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process channel ann\")\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"channel announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(\n\t\tbatch.chanUpdAnn1,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process channel update\")\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"channel update announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\tselect {\n\tcase msg := <-sentToPeer:\n\t\tassertMessage(t, batch.chanUpdAnn1, msg)\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"gossiper did not send channel update to remove peer\")\n\t}\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(\n\t\tbatch.nodeAnn1,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local announcement\")\n\t}\n\tif err != nil {\n\t\tt.Fatalf(\"unable to process node ann:%v\", err)\n\t}\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"node announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.chanUpdAnn2, remotePeer,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process channel update\")\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"channel update announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.nodeAnn2, remotePeer,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process node ann\")\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"node announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\t// Pretending that we receive local channel announcement from funding\n\t// manager, thereby kick off the announcement exchange process.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(\n\t\tbatch.localProofAnn,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process local proof\")\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.remoteProofAnn, remotePeer,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process remote proof\")\n\n\t// We expect the gossiper to send this message to the remote peer.\n\tselect {\n\tcase msg := <-sentToPeer:\n\t\tassertMessage(t, batch.localProofAnn, msg)\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not send local proof to peer\")\n\t}\n\n\t// All channel and node announcements should be broadcast.\n\tfor i := 0; i < 5; i++ {\n\t\tselect {\n\t\tcase <-ctx.broadcastedMessage:\n\t\tcase <-time.After(time.Second):\n\t\t\tt.Fatal(\"announcement wasn't broadcast\")\n\t\t}\n\t}\n\n\tnumber := 0\n\tif err := ctx.gossiper.cfg.WaitingProofStore.ForAll(\n\t\tfunc(*channeldb.WaitingProof) error {\n\t\t\tnumber++\n\t\t\treturn nil\n\t\t},\n\t\tfunc() {\n\t\t\tnumber = 0\n\t\t},\n\t); err != nil && err != channeldb.ErrWaitingProofNotFound {\n\t\tt.Fatalf(\"unable to retrieve objects from store: %v\", err)\n\t}\n\n\tif number != 0 {\n\t\tt.Fatal(\"waiting proof should be removed from storage\")\n\t}\n\n\t// Now give the gossiper the remote proof yet again. This should\n\t// trigger a send of the full ChannelAnnouncement.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.remoteProofAnn, remotePeer,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process remote proof\")\n\n\t// We expect the gossiper to send this message to the remote peer.\n\tselect {\n\tcase msg := <-sentToPeer:\n\t\t_, ok := msg.(*lnwire.ChannelAnnouncement)\n\t\tif !ok {\n\t\t\tt.Fatalf(\"expected ChannelAnnouncement, instead got %T\", msg)\n\t\t}\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not send local proof to peer\")\n\t}\n}\n\n// TestDeDuplicatedAnnouncements ensures that the deDupedAnnouncements struct\n// properly stores and delivers the set of de-duplicated announcements.",
      "length": 5031,
      "tokens": 617,
      "embedding": []
    },
    {
      "slug": "func TestDeDuplicatedAnnouncements(t *testing.T) {",
      "content": "func TestDeDuplicatedAnnouncements(t *testing.T) {\n\tt.Parallel()\n\n\ttimestamp := testTimestamp\n\tannouncements := deDupedAnnouncements{}\n\tannouncements.Reset()\n\n\t// Ensure that after new deDupedAnnouncements struct is created and\n\t// reset that storage of each announcement type is empty.\n\tif len(announcements.channelAnnouncements) != 0 {\n\t\tt.Fatal(\"channel announcements map not empty after reset\")\n\t}\n\tif len(announcements.channelUpdates) != 0 {\n\t\tt.Fatal(\"channel updates map not empty after reset\")\n\t}\n\tif len(announcements.nodeAnnouncements) != 0 {\n\t\tt.Fatal(\"node announcements map not empty after reset\")\n\t}\n\n\t// Ensure that remote channel announcements are properly stored\n\t// and de-duplicated.\n\tca, err := createRemoteChannelAnnouncement(0)\n\trequire.NoError(t, err, \"can't create remote channel announcement\")\n\n\tnodePeer := &mockPeer{bitcoinKeyPub2, nil, nil}\n\tannouncements.AddMsgs(networkMsg{\n\t\tmsg:    ca,\n\t\tpeer:   nodePeer,\n\t\tsource: nodePeer.IdentityKey(),\n\t})\n\tif len(announcements.channelAnnouncements) != 1 {\n\t\tt.Fatal(\"new channel announcement not stored in batch\")\n\t}\n\n\t// We'll create a second instance of the same announcement with the\n\t// same channel ID. Adding this shouldn't cause an increase in the\n\t// number of items as they should be de-duplicated.\n\tca2, err := createRemoteChannelAnnouncement(0)\n\trequire.NoError(t, err, \"can't create remote channel announcement\")\n\tannouncements.AddMsgs(networkMsg{\n\t\tmsg:    ca2,\n\t\tpeer:   nodePeer,\n\t\tsource: nodePeer.IdentityKey(),\n\t})\n\tif len(announcements.channelAnnouncements) != 1 {\n\t\tt.Fatal(\"channel announcement not replaced in batch\")\n\t}\n\n\t// Next, we'll ensure that channel update announcements are properly\n\t// stored and de-duplicated. We do this by creating two updates\n\t// announcements with the same short ID and flag.\n\tua, err := createUpdateAnnouncement(0, 0, remoteKeyPriv1, timestamp)\n\trequire.NoError(t, err, \"can't create update announcement\")\n\tannouncements.AddMsgs(networkMsg{\n\t\tmsg:    ua,\n\t\tpeer:   nodePeer,\n\t\tsource: nodePeer.IdentityKey(),\n\t})\n\tif len(announcements.channelUpdates) != 1 {\n\t\tt.Fatal(\"new channel update not stored in batch\")\n\t}\n\n\t// Adding the very same announcement shouldn't cause an increase in the\n\t// number of ChannelUpdate announcements stored.\n\tua2, err := createUpdateAnnouncement(0, 0, remoteKeyPriv1, timestamp)\n\trequire.NoError(t, err, \"can't create update announcement\")\n\tannouncements.AddMsgs(networkMsg{\n\t\tmsg:    ua2,\n\t\tpeer:   nodePeer,\n\t\tsource: nodePeer.IdentityKey(),\n\t})\n\tif len(announcements.channelUpdates) != 1 {\n\t\tt.Fatal(\"channel update not replaced in batch\")\n\t}\n\n\t// Adding an announcement with a later timestamp should replace the\n\t// stored one.\n\tua3, err := createUpdateAnnouncement(0, 0, remoteKeyPriv1, timestamp+1)\n\trequire.NoError(t, err, \"can't create update announcement\")\n\tannouncements.AddMsgs(networkMsg{\n\t\tmsg:    ua3,\n\t\tpeer:   nodePeer,\n\t\tsource: nodePeer.IdentityKey(),\n\t})\n\tif len(announcements.channelUpdates) != 1 {\n\t\tt.Fatal(\"channel update not replaced in batch\")\n\t}\n\n\tassertChannelUpdate := func(channelUpdate *lnwire.ChannelUpdate) {\n\t\tchannelKey := channelUpdateID{\n\t\t\tua3.ShortChannelID,\n\t\t\tua3.ChannelFlags,\n\t\t}\n\n\t\tmws, ok := announcements.channelUpdates[channelKey]\n\t\tif !ok {\n\t\t\tt.Fatal(\"channel update not in batch\")\n\t\t}\n\t\tif mws.msg != channelUpdate {\n\t\t\tt.Fatalf(\"expected channel update %v, got %v)\",\n\t\t\t\tchannelUpdate, mws.msg)\n\t\t}\n\t}\n\n\t// Check that ua3 is the currently stored channel update.\n\tassertChannelUpdate(ua3)\n\n\t// Adding a channel update with an earlier timestamp should NOT\n\t// replace the one stored.\n\tua4, err := createUpdateAnnouncement(0, 0, remoteKeyPriv1, timestamp)\n\trequire.NoError(t, err, \"can't create update announcement\")\n\tannouncements.AddMsgs(networkMsg{\n\t\tmsg:    ua4,\n\t\tpeer:   nodePeer,\n\t\tsource: nodePeer.IdentityKey(),\n\t})\n\tif len(announcements.channelUpdates) != 1 {\n\t\tt.Fatal(\"channel update not in batch\")\n\t}\n\tassertChannelUpdate(ua3)\n\n\t// Next well ensure that node announcements are properly de-duplicated.\n\t// We'll first add a single instance with a node's private key.\n\tna, err := createNodeAnnouncement(remoteKeyPriv1, timestamp)\n\trequire.NoError(t, err, \"can't create node announcement\")\n\tannouncements.AddMsgs(networkMsg{\n\t\tmsg:    na,\n\t\tpeer:   nodePeer,\n\t\tsource: nodePeer.IdentityKey(),\n\t})\n\tif len(announcements.nodeAnnouncements) != 1 {\n\t\tt.Fatal(\"new node announcement not stored in batch\")\n\t}\n\n\t// We'll now add another node to the batch.\n\tna2, err := createNodeAnnouncement(remoteKeyPriv2, timestamp)\n\trequire.NoError(t, err, \"can't create node announcement\")\n\tannouncements.AddMsgs(networkMsg{\n\t\tmsg:    na2,\n\t\tpeer:   nodePeer,\n\t\tsource: nodePeer.IdentityKey(),\n\t})\n\tif len(announcements.nodeAnnouncements) != 2 {\n\t\tt.Fatal(\"second node announcement not stored in batch\")\n\t}\n\n\t// Adding a new instance of the _same_ node shouldn't increase the size\n\t// of the node ann batch.\n\tna3, err := createNodeAnnouncement(remoteKeyPriv2, timestamp)\n\trequire.NoError(t, err, \"can't create node announcement\")\n\tannouncements.AddMsgs(networkMsg{\n\t\tmsg:    na3,\n\t\tpeer:   nodePeer,\n\t\tsource: nodePeer.IdentityKey(),\n\t})\n\tif len(announcements.nodeAnnouncements) != 2 {\n\t\tt.Fatal(\"second node announcement not replaced in batch\")\n\t}\n\n\t// Ensure that node announcement with different pointer to same public\n\t// key is still de-duplicated.\n\tnewNodeKeyPointer := remoteKeyPriv2\n\tna4, err := createNodeAnnouncement(newNodeKeyPointer, timestamp)\n\trequire.NoError(t, err, \"can't create node announcement\")\n\tannouncements.AddMsgs(networkMsg{\n\t\tmsg:    na4,\n\t\tpeer:   nodePeer,\n\t\tsource: nodePeer.IdentityKey(),\n\t})\n\tif len(announcements.nodeAnnouncements) != 2 {\n\t\tt.Fatal(\"second node announcement not replaced again in batch\")\n\t}\n\n\t// Ensure that node announcement with increased timestamp replaces\n\t// what is currently stored.\n\tna5, err := createNodeAnnouncement(remoteKeyPriv2, timestamp+1)\n\trequire.NoError(t, err, \"can't create node announcement\")\n\tannouncements.AddMsgs(networkMsg{\n\t\tmsg:    na5,\n\t\tpeer:   nodePeer,\n\t\tsource: nodePeer.IdentityKey(),\n\t})\n\tif len(announcements.nodeAnnouncements) != 2 {\n\t\tt.Fatal(\"node announcement not replaced in batch\")\n\t}\n\tnodeID := route.NewVertex(remoteKeyPriv2.PubKey())\n\tstored, ok := announcements.nodeAnnouncements[nodeID]\n\tif !ok {\n\t\tt.Fatalf(\"node announcement not found in batch\")\n\t}\n\tif stored.msg != na5 {\n\t\tt.Fatalf(\"expected de-duped node announcement to be %v, got %v\",\n\t\t\tna5, stored.msg)\n\t}\n\n\t// Ensure that announcement batch delivers channel announcements,\n\t// channel updates, and node announcements in proper order.\n\tbatch := announcements.Emit()\n\tif batch.length() != 4 {\n\t\tt.Fatal(\"announcement batch incorrect length\")\n\t}\n\n\tif !reflect.DeepEqual(batch.localMsgs[0].msg, ca2) {\n\t\tt.Fatalf(\"channel announcement not first in batch: got %v, \"+\n\t\t\t\"expected %v\", spew.Sdump(batch.localMsgs[0].msg),\n\t\t\tspew.Sdump(ca2))\n\t}\n\n\tif !reflect.DeepEqual(batch.localMsgs[1].msg, ua3) {\n\t\tt.Fatalf(\"channel update not next in batch: got %v, \"+\n\t\t\t\"expected %v\", spew.Sdump(batch.localMsgs[1].msg),\n\t\t\tspew.Sdump(ua2))\n\t}\n\n\t// We'll ensure that both node announcements are present. We check both\n\t// indexes as due to the randomized order of map iteration they may be\n\t// in either place.\n\tif !reflect.DeepEqual(batch.localMsgs[2].msg, na) &&\n\t\t!reflect.DeepEqual(batch.localMsgs[3].msg, na) {\n\n\t\tt.Fatalf(\"first node announcement not in last part of batch: \"+\n\t\t\t\"got %v, expected %v\", batch.localMsgs[2].msg,\n\t\t\tna)\n\t}\n\tif !reflect.DeepEqual(batch.localMsgs[2].msg, na5) &&\n\t\t!reflect.DeepEqual(batch.localMsgs[3].msg, na5) {\n\n\t\tt.Fatalf(\"second node announcement not in last part of batch: \"+\n\t\t\t\"got %v, expected %v\", batch.localMsgs[3].msg,\n\t\t\tna5)\n\t}\n\n\t// Ensure that after reset, storage of each announcement type\n\t// in deDupedAnnouncements struct is empty again.\n\tannouncements.Reset()\n\tif len(announcements.channelAnnouncements) != 0 {\n\t\tt.Fatal(\"channel announcements map not empty after reset\")\n\t}\n\tif len(announcements.channelUpdates) != 0 {\n\t\tt.Fatal(\"channel updates map not empty after reset\")\n\t}\n\tif len(announcements.nodeAnnouncements) != 0 {\n\t\tt.Fatal(\"node announcements map not empty after reset\")\n\t}\n}\n\n// TestForwardPrivateNodeAnnouncement ensures that we do not forward node\n// announcements for nodes who do not intend to publicly advertise themselves.",
      "length": 8035,
      "tokens": 941,
      "embedding": []
    },
    {
      "slug": "func TestForwardPrivateNodeAnnouncement(t *testing.T) {",
      "content": "func TestForwardPrivateNodeAnnouncement(t *testing.T) {\n\tt.Parallel()\n\n\tconst (\n\t\tstartingHeight = 100\n\t\ttimestamp      = 123456\n\t)\n\n\tctx, err := createTestCtx(t, startingHeight)\n\trequire.NoError(t, err, \"can't create context\")\n\n\t// We'll start off by processing a channel announcement without a proof\n\t// (i.e., an unadvertised channel), followed by a node announcement for\n\t// this same channel announcement.\n\tchanAnn := createAnnouncementWithoutProof(\n\t\tstartingHeight-2, selfKeyDesc.PubKey, remoteKeyPub1,\n\t)\n\tpubKey := remoteKeyPriv1.PubKey()\n\n\tselect {\n\tcase err := <-ctx.gossiper.ProcessLocalAnnouncement(chanAnn):\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to process local announcement: %v\", err)\n\t\t}\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatalf(\"local announcement not processed\")\n\t}\n\n\t// The gossiper should not broadcast the announcement due to it not\n\t// having its announcement signatures.\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"gossiper should not have broadcast channel announcement\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\tnodeAnn, err := createNodeAnnouncement(remoteKeyPriv1, timestamp)\n\trequire.NoError(t, err, \"unable to create node announcement\")\n\n\tselect {\n\tcase err := <-ctx.gossiper.ProcessLocalAnnouncement(nodeAnn):\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to process remote announcement: %v\", err)\n\t\t}\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"remote announcement not processed\")\n\t}\n\n\t// The gossiper should also not broadcast the node announcement due to\n\t// it not being part of any advertised channels.\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"gossiper should not have broadcast node announcement\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\t// Now, we'll attempt to forward the NodeAnnouncement for the same node\n\t// by opening a public channel on the network. We'll create a\n\t// ChannelAnnouncement and hand it off to the gossiper in order to\n\t// process it.\n\tremoteChanAnn, err := createRemoteChannelAnnouncement(startingHeight - 1)\n\trequire.NoError(t, err, \"unable to create remote channel announcement\")\n\tpeer := &mockPeer{pubKey, nil, nil}\n\n\tselect {\n\tcase err := <-ctx.gossiper.ProcessRemoteAnnouncement(remoteChanAnn, peer):\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to process remote announcement: %v\", err)\n\t\t}\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"remote announcement not processed\")\n\t}\n\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\tcase <-time.After(2 * trickleDelay):\n\t\tt.Fatal(\"gossiper should have broadcast the channel announcement\")\n\t}\n\n\t// We'll recreate the NodeAnnouncement with an updated timestamp to\n\t// prevent a stale update. The NodeAnnouncement should now be forwarded.\n\tnodeAnn, err = createNodeAnnouncement(remoteKeyPriv1, timestamp+1)\n\trequire.NoError(t, err, \"unable to create node announcement\")\n\n\tselect {\n\tcase err := <-ctx.gossiper.ProcessRemoteAnnouncement(nodeAnn, peer):\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to process remote announcement: %v\", err)\n\t\t}\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"remote announcement not processed\")\n\t}\n\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\tcase <-time.After(2 * trickleDelay):\n\t\tt.Fatal(\"gossiper should have broadcast the node announcement\")\n\t}\n}\n\n// TestRejectZombieEdge ensures that we properly reject any announcements for\n// zombie edges.",
      "length": 3137,
      "tokens": 391,
      "embedding": []
    },
    {
      "slug": "func TestRejectZombieEdge(t *testing.T) {",
      "content": "func TestRejectZombieEdge(t *testing.T) {\n\tt.Parallel()\n\n\t// We'll start by creating our test context with a batch of\n\t// announcements.\n\tctx, err := createTestCtx(t, 0)\n\trequire.NoError(t, err, \"unable to create test context\")\n\n\tbatch, err := createRemoteAnnouncements(0)\n\trequire.NoError(t, err, \"unable to create announcements\")\n\tremotePeer := &mockPeer{pk: remoteKeyPriv2.PubKey()}\n\n\t// processAnnouncements is a helper closure we'll use to test that we\n\t// properly process/reject announcements based on whether they're for a\n\t// zombie edge or not.\n\tprocessAnnouncements := func(isZombie bool) {\n\t\tt.Helper()\n\n\t\terrChan := ctx.gossiper.ProcessRemoteAnnouncement(\n\t\t\tbatch.chanAnn, remotePeer,\n\t\t)\n\t\tselect {\n\t\tcase err := <-errChan:\n\t\t\tif isZombie && err != nil {\n\t\t\t\tt.Fatalf(\"expected to reject live channel \"+\n\t\t\t\t\t\"announcement with nil error: %v\", err)\n\t\t\t}\n\t\t\tif !isZombie && err != nil {\n\t\t\t\tt.Fatalf(\"expected to process live channel \"+\n\t\t\t\t\t\"announcement: %v\", err)\n\t\t\t}\n\t\tcase <-time.After(time.Second):\n\t\t\tt.Fatal(\"expected to process channel announcement\")\n\t\t}\n\t\tselect {\n\t\tcase <-ctx.broadcastedMessage:\n\t\t\tif isZombie {\n\t\t\t\tt.Fatal(\"expected to not broadcast zombie \" +\n\t\t\t\t\t\"channel announcement\")\n\t\t\t}\n\t\tcase <-time.After(2 * trickleDelay):\n\t\t\tif !isZombie {\n\t\t\t\tt.Fatal(\"expected to broadcast live channel \" +\n\t\t\t\t\t\"announcement\")\n\t\t\t}\n\t\t}\n\n\t\terrChan = ctx.gossiper.ProcessRemoteAnnouncement(\n\t\t\tbatch.chanUpdAnn2, remotePeer,\n\t\t)\n\t\tselect {\n\t\tcase err := <-errChan:\n\t\t\tif isZombie && err != nil {\n\t\t\t\tt.Fatalf(\"expected to reject zombie channel \"+\n\t\t\t\t\t\"update with nil error: %v\", err)\n\t\t\t}\n\t\t\tif !isZombie && err != nil {\n\t\t\t\tt.Fatalf(\"expected to process live channel \"+\n\t\t\t\t\t\"update: %v\", err)\n\t\t\t}\n\t\tcase <-time.After(time.Second):\n\t\t\tt.Fatal(\"expected to process channel update\")\n\t\t}\n\t\tselect {\n\t\tcase <-ctx.broadcastedMessage:\n\t\t\tif isZombie {\n\t\t\t\tt.Fatal(\"expected to not broadcast zombie \" +\n\t\t\t\t\t\"channel update\")\n\t\t\t}\n\t\tcase <-time.After(2 * trickleDelay):\n\t\t\tif !isZombie {\n\t\t\t\tt.Fatal(\"expected to broadcast live channel \" +\n\t\t\t\t\t\"update\")\n\t\t\t}\n\t\t}\n\t}\n\n\t// We'll mark the edge for which we'll process announcements for as a\n\t// zombie within the router. This should reject any announcements for\n\t// this edge while it remains as a zombie.\n\tchanID := batch.chanAnn.ShortChannelID\n\terr = ctx.router.MarkEdgeZombie(\n\t\tchanID, batch.chanAnn.NodeID1, batch.chanAnn.NodeID2,\n\t)\n\tif err != nil {\n\t\tt.Fatalf(\"unable to mark channel %v as zombie: %v\", chanID, err)\n\t}\n\n\tprocessAnnouncements(true)\n\n\t// If we then mark the edge as live, the edge's zombie status should be\n\t// overridden and the announcements should be processed.\n\tif err := ctx.router.MarkEdgeLive(chanID); err != nil {\n\t\tt.Fatalf(\"unable mark channel %v as zombie: %v\", chanID, err)\n\t}\n\n\tprocessAnnouncements(false)\n}\n\n// TestProcessZombieEdgeNowLive ensures that we can detect when a zombie edge\n// becomes live by receiving a fresh update.",
      "length": 2795,
      "tokens": 379,
      "embedding": []
    },
    {
      "slug": "func TestProcessZombieEdgeNowLive(t *testing.T) {",
      "content": "func TestProcessZombieEdgeNowLive(t *testing.T) {\n\tt.Parallel()\n\n\t// We'll start by creating our test context with a batch of\n\t// announcements.\n\tctx, err := createTestCtx(t, 0)\n\trequire.NoError(t, err, \"unable to create test context\")\n\n\tbatch, err := createRemoteAnnouncements(0)\n\trequire.NoError(t, err, \"unable to create announcements\")\n\n\tremotePeer := &mockPeer{pk: remoteKeyPriv1.PubKey()}\n\n\t// processAnnouncement is a helper closure we'll use to ensure an\n\t// announcement is properly processed/rejected based on whether the edge\n\t// is a zombie or not. The expectsErr boolean can be used to determine\n\t// whether we should expect an error when processing the message, while\n\t// the isZombie boolean can be used to determine whether the\n\t// announcement should be or not be broadcast.\n\tprocessAnnouncement := func(ann lnwire.Message, isZombie, expectsErr bool) {\n\t\tt.Helper()\n\n\t\terrChan := ctx.gossiper.ProcessRemoteAnnouncement(\n\t\t\tann, remotePeer,\n\t\t)\n\n\t\tvar err error\n\t\tselect {\n\t\tcase err = <-errChan:\n\t\tcase <-time.After(time.Second):\n\t\t\tt.Fatal(\"expected to process announcement\")\n\t\t}\n\t\tif expectsErr && err == nil {\n\t\t\tt.Fatal(\"expected error when processing announcement\")\n\t\t}\n\t\tif !expectsErr && err != nil {\n\t\t\tt.Fatalf(\"received unexpected error when processing \"+\n\t\t\t\t\"announcement: %v\", err)\n\t\t}\n\n\t\tselect {\n\t\tcase msgWithSenders := <-ctx.broadcastedMessage:\n\t\t\tif isZombie {\n\t\t\t\tt.Fatal(\"expected to not broadcast zombie \" +\n\t\t\t\t\t\"channel message\")\n\t\t\t}\n\t\t\tassertMessage(t, ann, msgWithSenders.msg)\n\n\t\tcase <-time.After(2 * trickleDelay):\n\t\t\tif !isZombie {\n\t\t\t\tt.Fatal(\"expected to broadcast live channel \" +\n\t\t\t\t\t\"message\")\n\t\t\t}\n\t\t}\n\t}\n\n\t// We'll generate a channel update with a timestamp far enough in the\n\t// past to consider it a zombie.\n\tzombieTimestamp := time.Now().Add(-routing.DefaultChannelPruneExpiry)\n\tbatch.chanUpdAnn2.Timestamp = uint32(zombieTimestamp.Unix())\n\tif err := signUpdate(remoteKeyPriv2, batch.chanUpdAnn2); err != nil {\n\t\tt.Fatalf(\"unable to sign update with new timestamp: %v\", err)\n\t}\n\n\t// We'll also add the edge to our zombie index, provide a blank pubkey\n\t// for the first node as we're simulating the situation where the first\n\t// node is updating but the second node isn't. In this case we only\n\t// want to allow a new update from the second node to allow the entire\n\t// edge to be resurrected.\n\tchanID := batch.chanAnn.ShortChannelID\n\terr = ctx.router.MarkEdgeZombie(\n\t\tchanID, [33]byte{}, batch.chanAnn.NodeID2,\n\t)\n\tif err != nil {\n\t\tt.Fatalf(\"unable mark channel %v as zombie: %v\", chanID, err)\n\t}\n\n\t// If we send a new update but for the other direction of the channel,\n\t// then it should still be rejected as we want a fresh update from the\n\t// one that was considered stale.\n\tbatch.chanUpdAnn1.Timestamp = uint32(time.Now().Unix())\n\tif err := signUpdate(remoteKeyPriv1, batch.chanUpdAnn1); err != nil {\n\t\tt.Fatalf(\"unable to sign update with new timestamp: %v\", err)\n\t}\n\tprocessAnnouncement(batch.chanUpdAnn1, true, true)\n\n\t// At this point, the channel should still be considered a zombie.\n\t_, _, _, err = ctx.router.GetChannelByID(chanID)\n\tif err != channeldb.ErrZombieEdge {\n\t\tt.Fatalf(\"channel should still be a zombie\")\n\t}\n\n\t// Attempting to process the current channel update should fail due to\n\t// its edge being considered a zombie and its timestamp not being within\n\t// the live horizon. We should not expect an error here since it is just\n\t// a stale update.\n\tprocessAnnouncement(batch.chanUpdAnn2, true, false)\n\n\t// Now we'll generate a new update with a fresh timestamp. This should\n\t// allow the channel update to be processed even though it is still\n\t// marked as a zombie within the index, since it is a fresh new update.\n\t// This won't work however since we'll sign it with the wrong private\n\t// key (remote key 1 rather than remote key 2).\n\tbatch.chanUpdAnn2.Timestamp = uint32(time.Now().Unix())\n\tif err := signUpdate(remoteKeyPriv1, batch.chanUpdAnn2); err != nil {\n\t\tt.Fatalf(\"unable to sign update with new timestamp: %v\", err)\n\t}\n\n\t// We should expect an error due to the signature being invalid.\n\tprocessAnnouncement(batch.chanUpdAnn2, true, true)\n\n\t// Signing it with the correct private key should allow it to be\n\t// processed.\n\tif err := signUpdate(remoteKeyPriv2, batch.chanUpdAnn2); err != nil {\n\t\tt.Fatalf(\"unable to sign update with new timestamp: %v\", err)\n\t}\n\n\t// The channel update cannot be successfully processed and broadcast\n\t// until the channel announcement is. Since the channel update indicates\n\t// a fresh new update, the gossiper should stash it until it sees the\n\t// corresponding channel announcement.\n\tupdateErrChan := ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.chanUpdAnn2, remotePeer,\n\t)\n\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"expected to not broadcast live channel update \" +\n\t\t\t\"without announcement\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\t// We'll go ahead and process the channel announcement to ensure the\n\t// channel update is processed thereafter.\n\tprocessAnnouncement(batch.chanAnn, false, false)\n\n\t// After successfully processing the announcement, the channel update\n\t// should have been processed and broadcast successfully as well.\n\tselect {\n\tcase err := <-updateErrChan:\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"expected to process live channel update: %v\",\n\t\t\t\terr)\n\t\t}\n\tcase <-time.After(time.Second):\n\t\tt.Fatal(\"expected to process announcement\")\n\t}\n\n\tselect {\n\tcase msgWithSenders := <-ctx.broadcastedMessage:\n\t\tassertMessage(t, batch.chanUpdAnn2, msgWithSenders.msg)\n\tcase <-time.After(2 * trickleDelay):\n\t\tt.Fatal(\"expected to broadcast live channel update\")\n\t}\n}\n\n// TestReceiveRemoteChannelUpdateFirst tests that if we receive a ChannelUpdate\n// from the remote before we have processed our own ChannelAnnouncement, it will\n// be reprocessed later, after our ChannelAnnouncement.",
      "length": 5617,
      "tokens": 794,
      "embedding": []
    },
    {
      "slug": "func TestReceiveRemoteChannelUpdateFirst(t *testing.T) {",
      "content": "func TestReceiveRemoteChannelUpdateFirst(t *testing.T) {\n\tt.Parallel()\n\n\tctx, err := createTestCtx(t, proofMatureDelta)\n\trequire.NoError(t, err, \"can't create context\")\n\n\tbatch, err := createLocalAnnouncements(0)\n\trequire.NoError(t, err, \"can't generate announcements\")\n\n\tremoteKey, err := btcec.ParsePubKey(batch.nodeAnn2.NodeID[:])\n\trequire.NoError(t, err, \"unable to parse pubkey\")\n\n\t// Set up a channel that we can use to inspect the messages sent\n\t// directly from the gossiper.\n\tsentMsgs := make(chan lnwire.Message, 10)\n\tremotePeer := &mockPeer{remoteKey, sentMsgs, ctx.gossiper.quit}\n\n\t// Override NotifyWhenOnline to return the remote peer which we expect\n\t// messages to be sent to.\n\tctx.gossiper.reliableSender.cfg.NotifyWhenOnline = func(peer [33]byte,\n\t\tpeerChan chan<- lnpeer.Peer) {\n\n\t\tpeerChan <- remotePeer\n\t}\n\n\t// Recreate the case where the remote node is sending us its ChannelUpdate\n\t// before we have been able to process our own ChannelAnnouncement and\n\t// ChannelUpdate.\n\terrRemoteAnn := ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.chanUpdAnn2, remotePeer,\n\t)\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"channel update announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\terr = <-ctx.gossiper.ProcessRemoteAnnouncement(batch.nodeAnn2, remotePeer)\n\trequire.NoError(t, err, \"unable to process node ann\")\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"node announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\t// Since the remote ChannelUpdate was added for an edge that\n\t// we did not already know about, it should have been added\n\t// to the map of premature ChannelUpdates. Check that nothing\n\t// was added to the graph.\n\tchanInfo, e1, e2, err := ctx.router.GetChannelByID(batch.chanUpdAnn1.ShortChannelID)\n\tif err != channeldb.ErrEdgeNotFound {\n\t\tt.Fatalf(\"Expected ErrEdgeNotFound, got: %v\", err)\n\t}\n\tif chanInfo != nil {\n\t\tt.Fatalf(\"chanInfo was not nil\")\n\t}\n\tif e1 != nil {\n\t\tt.Fatalf(\"e1 was not nil\")\n\t}\n\tif e2 != nil {\n\t\tt.Fatalf(\"e2 was not nil\")\n\t}\n\n\t// Recreate lightning network topology. Initialize router with channel\n\t// between two nodes.\n\terr = <-ctx.gossiper.ProcessLocalAnnouncement(batch.chanAnn)\n\tif err != nil {\n\t\tt.Fatalf(\"unable to process :%v\", err)\n\t}\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"channel announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\terr = <-ctx.gossiper.ProcessLocalAnnouncement(batch.chanUpdAnn1)\n\tif err != nil {\n\t\tt.Fatalf(\"unable to process :%v\", err)\n\t}\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"channel update announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\terr = <-ctx.gossiper.ProcessLocalAnnouncement(batch.nodeAnn1)\n\tif err != nil {\n\t\tt.Fatalf(\"unable to process :%v\", err)\n\t}\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"node announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\t// The local ChannelUpdate should now be sent directly to the remote peer,\n\t// such that the edge can be used for routing, regardless if this channel\n\t// is announced or not (private channel).\n\tselect {\n\tcase msg := <-sentMsgs:\n\t\tassertMessage(t, batch.chanUpdAnn1, msg)\n\tcase <-time.After(1 * time.Second):\n\t\tt.Fatal(\"gossiper did not send channel update to peer\")\n\t}\n\n\t// At this point the remote ChannelUpdate we received earlier should\n\t// be reprocessed, as we now have the necessary edge entry in the graph.\n\tselect {\n\tcase err := <-errRemoteAnn:\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"error re-processing remote update: %v\", err)\n\t\t}\n\tcase <-time.After(2 * trickleDelay):\n\t\tt.Fatalf(\"remote update was not processed\")\n\t}\n\n\t// Check that the ChannelEdgePolicy was added to the graph.\n\tchanInfo, e1, e2, err = ctx.router.GetChannelByID(\n\t\tbatch.chanUpdAnn1.ShortChannelID,\n\t)\n\trequire.NoError(t, err, \"unable to get channel from router\")\n\tif chanInfo == nil {\n\t\tt.Fatalf(\"chanInfo was nil\")\n\t}\n\tif e1 == nil {\n\t\tt.Fatalf(\"e1 was nil\")\n\t}\n\tif e2 == nil {\n\t\tt.Fatalf(\"e2 was nil\")\n\t}\n\n\t// Pretending that we receive local channel announcement from funding\n\t// manager, thereby kick off the announcement exchange process.\n\terr = <-ctx.gossiper.ProcessLocalAnnouncement(batch.localProofAnn)\n\tif err != nil {\n\t\tt.Fatalf(\"unable to process :%v\", err)\n\t}\n\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"announcements were broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\tnumber := 0\n\tif err := ctx.gossiper.cfg.WaitingProofStore.ForAll(\n\t\tfunc(*channeldb.WaitingProof) error {\n\t\t\tnumber++\n\t\t\treturn nil\n\t\t},\n\t\tfunc() {\n\t\t\tnumber = 0\n\t\t},\n\t); err != nil {\n\t\tt.Fatalf(\"unable to retrieve objects from store: %v\", err)\n\t}\n\n\tif number != 1 {\n\t\tt.Fatal(\"wrong number of objects in storage\")\n\t}\n\n\terr = <-ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.remoteProofAnn, remotePeer,\n\t)\n\tif err != nil {\n\t\tt.Fatalf(\"unable to process :%v\", err)\n\t}\n\n\tfor i := 0; i < 4; i++ {\n\t\tselect {\n\t\tcase <-ctx.broadcastedMessage:\n\t\tcase <-time.After(time.Second):\n\t\t\tt.Fatal(\"announcement wasn't broadcast\")\n\t\t}\n\t}\n\n\tnumber = 0\n\tif err := ctx.gossiper.cfg.WaitingProofStore.ForAll(\n\t\tfunc(*channeldb.WaitingProof) error {\n\t\t\tnumber++\n\t\t\treturn nil\n\t\t},\n\t\tfunc() {\n\t\t\tnumber = 0\n\t\t},\n\t); err != nil && err != channeldb.ErrWaitingProofNotFound {\n\t\tt.Fatalf(\"unable to retrieve objects from store: %v\", err)\n\t}\n\n\tif number != 0 {\n\t\tt.Fatal(\"waiting proof should be removed from storage\")\n\t}\n}\n\n// TestExtraDataChannelAnnouncementValidation tests that we're able to properly\n// validate a ChannelAnnouncement that includes opaque bytes that we don't\n// currently know of.",
      "length": 5306,
      "tokens": 705,
      "embedding": []
    },
    {
      "slug": "func TestExtraDataChannelAnnouncementValidation(t *testing.T) {",
      "content": "func TestExtraDataChannelAnnouncementValidation(t *testing.T) {\n\tt.Parallel()\n\n\tctx, err := createTestCtx(t, 0)\n\trequire.NoError(t, err, \"can't create context\")\n\n\tremotePeer := &mockPeer{remoteKeyPriv1.PubKey(), nil, nil}\n\n\t// We'll now create an announcement that contains an extra set of bytes\n\t// that we don't know of ourselves, but should still include in the\n\t// final signature check.\n\textraBytes := []byte(\"gotta validate this still!\")\n\tca, err := createRemoteChannelAnnouncement(0, extraBytes)\n\trequire.NoError(t, err, \"can't create channel announcement\")\n\n\t// We'll now send the announcement to the main gossiper. We should be\n\t// able to validate this announcement to problem.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(ca, remotePeer):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\tif err != nil {\n\t\tt.Fatalf(\"unable to process :%v\", err)\n\t}\n}\n\n// TestExtraDataChannelUpdateValidation tests that we're able to properly\n// validate a ChannelUpdate that includes opaque bytes that we don't currently\n// know of.",
      "length": 993,
      "tokens": 136,
      "embedding": []
    },
    {
      "slug": "func TestExtraDataChannelUpdateValidation(t *testing.T) {",
      "content": "func TestExtraDataChannelUpdateValidation(t *testing.T) {\n\tt.Parallel()\n\n\ttimestamp := testTimestamp\n\tctx, err := createTestCtx(t, 0)\n\trequire.NoError(t, err, \"can't create context\")\n\n\tremotePeer := &mockPeer{remoteKeyPriv1.PubKey(), nil, nil}\n\n\t// In this scenario, we'll create two announcements, one regular\n\t// channel announcement, and another channel update announcement, that\n\t// has additional data that we won't be interpreting.\n\tchanAnn, err := createRemoteChannelAnnouncement(0)\n\trequire.NoError(t, err, \"unable to create chan ann\")\n\tchanUpdAnn1, err := createUpdateAnnouncement(\n\t\t0, 0, remoteKeyPriv1, timestamp,\n\t\t[]byte(\"must also validate\"),\n\t)\n\trequire.NoError(t, err, \"unable to create chan up\")\n\tchanUpdAnn2, err := createUpdateAnnouncement(\n\t\t0, 1, remoteKeyPriv2, timestamp,\n\t\t[]byte(\"must also validate\"),\n\t)\n\trequire.NoError(t, err, \"unable to create chan up\")\n\n\t// We should be able to properly validate all three messages without\n\t// any issue.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(chanAnn, remotePeer):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process announcement\")\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(chanUpdAnn1, remotePeer):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process announcement\")\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(chanUpdAnn2, remotePeer):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process announcement\")\n}\n\n// TestExtraDataNodeAnnouncementValidation tests that we're able to properly\n// validate a NodeAnnouncement that includes opaque bytes that we don't\n// currently know of.",
      "length": 1744,
      "tokens": 204,
      "embedding": []
    },
    {
      "slug": "func TestExtraDataNodeAnnouncementValidation(t *testing.T) {",
      "content": "func TestExtraDataNodeAnnouncementValidation(t *testing.T) {\n\tt.Parallel()\n\n\tctx, err := createTestCtx(t, 0)\n\trequire.NoError(t, err, \"can't create context\")\n\n\tremotePeer := &mockPeer{remoteKeyPriv1.PubKey(), nil, nil}\n\ttimestamp := testTimestamp\n\n\t// We'll create a node announcement that includes a set of opaque data\n\t// which we don't know of, but will store anyway in order to ensure\n\t// upgrades can flow smoothly in the future.\n\tnodeAnn, err := createNodeAnnouncement(\n\t\tremoteKeyPriv1, timestamp, []byte(\"gotta validate\"),\n\t)\n\trequire.NoError(t, err, \"can't create node announcement\")\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(nodeAnn, remotePeer):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process announcement\")\n}\n\n// assertBroadcast checks that num messages are being broadcasted from the\n// gossiper. The broadcasted messages are returned.",
      "length": 870,
      "tokens": 111,
      "embedding": []
    },
    {
      "slug": "func assertBroadcast(t *testing.T, ctx *testCtx, num int) []lnwire.Message {",
      "content": "func assertBroadcast(t *testing.T, ctx *testCtx, num int) []lnwire.Message {\n\tt.Helper()\n\n\tvar msgs []lnwire.Message\n\tfor i := 0; i < num; i++ {\n\t\tselect {\n\t\tcase msg := <-ctx.broadcastedMessage:\n\t\t\tmsgs = append(msgs, msg.msg)\n\t\tcase <-time.After(time.Second):\n\t\t\tt.Fatalf(\"expected %d messages to be broadcast, only \"+\n\t\t\t\t\"got %d\", num, i)\n\t\t}\n\t}\n\n\t// No more messages should be broadcast.\n\tselect {\n\tcase msg := <-ctx.broadcastedMessage:\n\t\tt.Fatalf(\"unexpected message was broadcast: %T\", msg.msg)\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\treturn msgs\n}\n\n// assertProcessAnnouncemnt is a helper method that checks that the result of\n// processing an announcement is successful.",
      "length": 581,
      "tokens": 84,
      "embedding": []
    },
    {
      "slug": "func assertProcessAnnouncement(t *testing.T, result chan error) {",
      "content": "func assertProcessAnnouncement(t *testing.T, result chan error) {\n\tt.Helper()\n\n\tselect {\n\tcase err := <-result:\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to process :%v\", err)\n\t\t}\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process announcement\")\n\t}\n}\n\n// TestRetransmit checks that the expected announcements are retransmitted when\n// the retransmit ticker ticks.",
      "length": 293,
      "tokens": 43,
      "embedding": []
    },
    {
      "slug": "func TestRetransmit(t *testing.T) {",
      "content": "func TestRetransmit(t *testing.T) {\n\tt.Parallel()\n\n\tctx, err := createTestCtx(t, proofMatureDelta)\n\trequire.NoError(t, err, \"can't create context\")\n\n\tbatch, err := createLocalAnnouncements(0)\n\trequire.NoError(t, err, \"can't generate announcements\")\n\n\tremoteKey, err := btcec.ParsePubKey(batch.nodeAnn2.NodeID[:])\n\trequire.NoError(t, err, \"unable to parse pubkey\")\n\tremotePeer := &mockPeer{remoteKey, nil, nil}\n\n\t// Process a local channel announcement, channel update and node\n\t// announcement. No messages should be broadcasted yet, since no proof\n\t// has been exchanged.\n\tassertProcessAnnouncement(\n\t\tt, ctx.gossiper.ProcessLocalAnnouncement(batch.chanAnn),\n\t)\n\tassertBroadcast(t, ctx, 0)\n\n\tassertProcessAnnouncement(\n\t\tt, ctx.gossiper.ProcessLocalAnnouncement(batch.chanUpdAnn1),\n\t)\n\tassertBroadcast(t, ctx, 0)\n\n\tassertProcessAnnouncement(\n\t\tt, ctx.gossiper.ProcessLocalAnnouncement(batch.nodeAnn1),\n\t)\n\tassertBroadcast(t, ctx, 0)\n\n\t// Add the remote channel update to the gossiper. Similarly, nothing\n\t// should be broadcasted.\n\tassertProcessAnnouncement(\n\t\tt, ctx.gossiper.ProcessRemoteAnnouncement(\n\t\t\tbatch.chanUpdAnn2, remotePeer,\n\t\t),\n\t)\n\tassertBroadcast(t, ctx, 0)\n\n\t// Now add the local and remote proof to the gossiper, which should\n\t// trigger a broadcast of the announcements.\n\tassertProcessAnnouncement(\n\t\tt, ctx.gossiper.ProcessLocalAnnouncement(batch.localProofAnn),\n\t)\n\tassertBroadcast(t, ctx, 0)\n\n\tassertProcessAnnouncement(\n\t\tt, ctx.gossiper.ProcessRemoteAnnouncement(\n\t\t\tbatch.remoteProofAnn, remotePeer,\n\t\t),\n\t)\n\n\t// checkAnncouncments make sure the expected number of channel\n\t// announcements + channel updates + node announcements are broadcast.\n\tcheckAnnouncements := func(t *testing.T, chanAnns, chanUpds,\n\t\tnodeAnns int) {\n\n\t\tt.Helper()\n\n\t\tnum := chanAnns + chanUpds + nodeAnns\n\t\tanns := assertBroadcast(t, ctx, num)\n\n\t\t// Count the received announcements.\n\t\tvar chanAnn, chanUpd, nodeAnn int\n\t\tfor _, msg := range anns {\n\t\t\tswitch msg.(type) {\n\t\t\tcase *lnwire.ChannelAnnouncement:\n\t\t\t\tchanAnn++\n\t\t\tcase *lnwire.ChannelUpdate:\n\t\t\t\tchanUpd++\n\t\t\tcase *lnwire.NodeAnnouncement:\n\t\t\t\tnodeAnn++\n\t\t\t}\n\t\t}\n\n\t\tif chanAnn != chanAnns || chanUpd != chanUpds ||\n\t\t\tnodeAnn != nodeAnns {\n\n\t\t\tt.Fatalf(\"unexpected number of announcements: \"+\n\t\t\t\t\"chanAnn=%d, chanUpd=%d, nodeAnn=%d\",\n\t\t\t\tchanAnn, chanUpd, nodeAnn)\n\t\t}\n\t}\n\n\t// All announcements should be broadcast, including the remote channel\n\t// update.\n\tcheckAnnouncements(t, 1, 2, 1)\n\n\t// Now let the retransmit ticker tick, which should trigger updates to\n\t// be rebroadcast.\n\tnow := time.Unix(int64(testTimestamp), 0)\n\tfuture := now.Add(rebroadcastInterval + 10*time.Second)\n\tselect {\n\tcase ctx.gossiper.cfg.RetransmitTicker.(*ticker.Force).Force <- future:\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatalf(\"unable to force tick\")\n\t}\n\n\t// The channel announcement + local channel update + node announcement\n\t// should be re-broadcast.\n\tcheckAnnouncements(t, 1, 1, 1)\n}\n\n// TestNodeAnnouncementNoChannels tests that NodeAnnouncements for nodes with\n// no existing channels in the graph do not get forwarded.",
      "length": 2936,
      "tokens": 332,
      "embedding": []
    },
    {
      "slug": "func TestNodeAnnouncementNoChannels(t *testing.T) {",
      "content": "func TestNodeAnnouncementNoChannels(t *testing.T) {\n\tt.Parallel()\n\n\tctx, err := createTestCtx(t, 0)\n\trequire.NoError(t, err, \"can't create context\")\n\n\tbatch, err := createRemoteAnnouncements(0)\n\trequire.NoError(t, err, \"can't generate announcements\")\n\n\tremoteKey, err := btcec.ParsePubKey(batch.nodeAnn2.NodeID[:])\n\trequire.NoError(t, err, \"unable to parse pubkey\")\n\tremotePeer := &mockPeer{remoteKey, nil, nil}\n\n\t// Process the remote node announcement.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(batch.nodeAnn2,\n\t\tremotePeer):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process announcement\")\n\n\t// Since no channels or node announcements were already in the graph,\n\t// the node announcement should be ignored, and not forwarded.\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"node announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\t// Now add the node's channel to the graph by processing the channel\n\t// announcement and channel update.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(batch.chanAnn,\n\t\tremotePeer):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process announcement\")\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(batch.chanUpdAnn2,\n\t\tremotePeer):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process announcement\")\n\n\t// Now process the node announcement again.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(batch.nodeAnn2, remotePeer):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process announcement\")\n\n\t// This time the node announcement should be forwarded. The same should\n\t// the channel announcement and update be.\n\tfor i := 0; i < 3; i++ {\n\t\tselect {\n\t\tcase <-ctx.broadcastedMessage:\n\t\tcase <-time.After(time.Second):\n\t\t\tt.Fatal(\"announcement wasn't broadcast\")\n\t\t}\n\t}\n\n\t// Processing the same node announcement again should be ignored, as it\n\t// is stale.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(batch.nodeAnn2,\n\t\tremotePeer):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process announcement\")\n\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"node announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n}\n\n// TestOptionalFieldsChannelUpdateValidation tests that we're able to properly\n// validate the msg flags and max HTLC field of a ChannelUpdate.",
      "length": 2585,
      "tokens": 304,
      "embedding": []
    },
    {
      "slug": "func TestOptionalFieldsChannelUpdateValidation(t *testing.T) {",
      "content": "func TestOptionalFieldsChannelUpdateValidation(t *testing.T) {\n\tt.Parallel()\n\n\tctx, err := createTestCtx(t, 0)\n\trequire.NoError(t, err, \"can't create context\")\n\n\tprocessRemoteAnnouncement := ctx.gossiper.ProcessRemoteAnnouncement\n\n\tchanUpdateHeight := uint32(0)\n\ttimestamp := uint32(123456)\n\tnodePeer := &mockPeer{remoteKeyPriv1.PubKey(), nil, nil}\n\n\t// In this scenario, we'll test whether the message flags field in a\n\t// channel update is properly handled.\n\tchanAnn, err := createRemoteChannelAnnouncement(chanUpdateHeight)\n\trequire.NoError(t, err, \"can't create channel announcement\")\n\n\tselect {\n\tcase err = <-processRemoteAnnouncement(chanAnn, nodePeer):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process announcement\")\n\n\t// The first update should fail from an invalid max HTLC field, which is\n\t// less than the min HTLC.\n\tchanUpdAnn, err := createUpdateAnnouncement(\n\t\t0, 0, remoteKeyPriv1, timestamp,\n\t)\n\trequire.NoError(t, err, \"unable to create channel update\")\n\n\tchanUpdAnn.HtlcMinimumMsat = 5000\n\tchanUpdAnn.HtlcMaximumMsat = 4000\n\tif err := signUpdate(remoteKeyPriv1, chanUpdAnn); err != nil {\n\t\tt.Fatalf(\"unable to sign channel update: %v\", err)\n\t}\n\n\tselect {\n\tcase err = <-processRemoteAnnouncement(chanUpdAnn, nodePeer):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\tif err == nil || !strings.Contains(err.Error(), \"invalid max htlc\") {\n\t\tt.Fatalf(\"expected chan update to error, instead got %v\", err)\n\t}\n\n\t// The second update should fail because the message flag is set but\n\t// the max HTLC field is 0.\n\tchanUpdAnn.HtlcMinimumMsat = 0\n\tchanUpdAnn.HtlcMaximumMsat = 0\n\tif err := signUpdate(remoteKeyPriv1, chanUpdAnn); err != nil {\n\t\tt.Fatalf(\"unable to sign channel update: %v\", err)\n\t}\n\n\tselect {\n\tcase err = <-processRemoteAnnouncement(chanUpdAnn, nodePeer):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\tif err == nil || !strings.Contains(err.Error(), \"invalid max htlc\") {\n\t\tt.Fatalf(\"expected chan update to error, instead got %v\", err)\n\t}\n\n\t// The third update should not succeed, a channel update with no message\n\t// flag set is invalid.\n\tchanUpdAnn.MessageFlags = 0\n\tif err := signUpdate(remoteKeyPriv1, chanUpdAnn); err != nil {\n\t\tt.Fatalf(\"unable to sign channel update: %v\", err)\n\t}\n\n\tselect {\n\tcase err = <-processRemoteAnnouncement(chanUpdAnn, nodePeer):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.ErrorContains(t, err, \"max htlc flag not set\")\n\n\t// The final update should succeed.\n\tchanUpdAnn, err = createUpdateAnnouncement(\n\t\t0, 0, remoteKeyPriv1, timestamp,\n\t)\n\trequire.NoError(t, err, \"unable to create channel update\")\n\n\tif err := signUpdate(remoteKeyPriv1, chanUpdAnn); err != nil {\n\t\tt.Fatalf(\"unable to sign channel update: %v\", err)\n\t}\n\n\tselect {\n\tcase err = <-processRemoteAnnouncement(chanUpdAnn, nodePeer):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"expected update to be processed\")\n}\n\n// TestSendChannelUpdateReliably ensures that the latest channel update for a\n// channel is always sent upon the remote party reconnecting.",
      "length": 3111,
      "tokens": 399,
      "embedding": []
    },
    {
      "slug": "func TestSendChannelUpdateReliably(t *testing.T) {",
      "content": "func TestSendChannelUpdateReliably(t *testing.T) {\n\tt.Parallel()\n\n\t// We'll start by creating our test context and a batch of\n\t// announcements.\n\tctx, err := createTestCtx(t, proofMatureDelta)\n\trequire.NoError(t, err, \"unable to create test context\")\n\n\tbatch, err := createLocalAnnouncements(0)\n\trequire.NoError(t, err, \"can't generate announcements\")\n\n\t// We'll also create two keys, one for ourselves and another for the\n\t// remote party.\n\n\tremoteKey, err := btcec.ParsePubKey(batch.nodeAnn2.NodeID[:])\n\trequire.NoError(t, err, \"unable to parse pubkey\")\n\n\t// Set up a channel we can use to inspect messages sent by the\n\t// gossiper to the remote peer.\n\tsentToPeer := make(chan lnwire.Message, 1)\n\tremotePeer := &mockPeer{remoteKey, sentToPeer, ctx.gossiper.quit}\n\n\t// Since we first wait to be notified of the peer before attempting to\n\t// send the message, we'll overwrite NotifyWhenOnline and\n\t// NotifyWhenOffline to instead give us access to the channel that will\n\t// receive the notification.\n\tnotifyOnline := make(chan chan<- lnpeer.Peer, 1)\n\tctx.gossiper.reliableSender.cfg.NotifyWhenOnline = func(_ [33]byte,\n\t\tpeerChan chan<- lnpeer.Peer) {\n\n\t\tnotifyOnline <- peerChan\n\t}\n\tnotifyOffline := make(chan chan struct{}, 1)\n\tctx.gossiper.reliableSender.cfg.NotifyWhenOffline = func(\n\t\t_ [33]byte) <-chan struct{} {\n\n\t\tc := make(chan struct{}, 1)\n\t\tnotifyOffline <- c\n\t\treturn c\n\t}\n\n\t// assertMsgSent is a helper closure we'll use to determine if the\n\t// correct gossip message was sent.\n\tassertMsgSent := func(msg lnwire.Message) {\n\t\tt.Helper()\n\n\t\tselect {\n\t\tcase msgSent := <-sentToPeer:\n\t\t\tassertMessage(t, msg, msgSent)\n\t\tcase <-time.After(2 * time.Second):\n\t\t\tt.Fatalf(\"did not send %v message to peer\",\n\t\t\t\tmsg.MsgType())\n\t\t}\n\t}\n\n\t// Process the channel announcement for which we'll send a channel\n\t// update for.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(batch.chanAnn):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local channel announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process local channel announcement\")\n\n\t// It should not be broadcast due to not having an announcement proof.\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"channel announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\t// Now, we'll process the channel update.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(batch.chanUpdAnn1):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local channel update\")\n\t}\n\trequire.NoError(t, err, \"unable to process local channel update\")\n\n\t// It should also not be broadcast due to the announcement not having an\n\t// announcement proof.\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"channel announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\t// It should however send it to the peer directly. In order to do so,\n\t// it'll request a notification for when the peer is online.\n\tvar peerChan chan<- lnpeer.Peer\n\tselect {\n\tcase peerChan = <-notifyOnline:\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"gossiper did not request notification upon peer \" +\n\t\t\t\"connection\")\n\t}\n\n\t// We can go ahead and notify the peer, which should trigger the message\n\t// to be sent.\n\tpeerChan <- remotePeer\n\tassertMsgSent(batch.chanUpdAnn1)\n\n\t// The gossiper should now request a notification for when the peer\n\t// disconnects. We'll also trigger this now.\n\tvar offlineChan chan struct{}\n\tselect {\n\tcase offlineChan = <-notifyOffline:\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"gossiper did not request notification upon peer \" +\n\t\t\t\"disconnection\")\n\t}\n\n\tclose(offlineChan)\n\n\t// Since it's offline, the gossiper should request another notification\n\t// for when it comes back online.\n\tselect {\n\tcase peerChan = <-notifyOnline:\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"gossiper did not request notification upon peer \" +\n\t\t\t\"connection\")\n\t}\n\n\t// Now that the remote peer is offline, we'll send a new channel update.\n\tbatch.chanUpdAnn1.Timestamp++\n\tif err := signUpdate(selfKeyPriv, batch.chanUpdAnn1); err != nil {\n\t\tt.Fatalf(\"unable to sign new channel update: %v\", err)\n\t}\n\n\t// With the new update created, we'll go ahead and process it.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(\n\t\tbatch.chanUpdAnn1,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local channel update\")\n\t}\n\trequire.NoError(t, err, \"unable to process local channel update\")\n\n\t// It should also not be broadcast due to the announcement not having an\n\t// announcement proof.\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"channel announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\t// The message should not be sent since the peer remains offline.\n\tselect {\n\tcase msg := <-sentToPeer:\n\t\tt.Fatalf(\"received unexpected message: %v\", spew.Sdump(msg))\n\tcase <-time.After(time.Second):\n\t}\n\n\t// Once again, we'll notify the peer is online and ensure the new\n\t// channel update is received. This will also cause an offline\n\t// notification to be requested again.\n\tpeerChan <- remotePeer\n\tassertMsgSent(batch.chanUpdAnn1)\n\n\tselect {\n\tcase offlineChan = <-notifyOffline:\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"gossiper did not request notification upon peer \" +\n\t\t\t\"disconnection\")\n\t}\n\n\t// We'll then exchange proofs with the remote peer in order to announce\n\t// the channel.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(\n\t\tbatch.localProofAnn,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local channel proof\")\n\t}\n\trequire.NoError(t, err, \"unable to process local channel proof\")\n\n\t// No messages should be broadcast as we don't have the full proof yet.\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"channel announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\t// Our proof should be sent to the remote peer however.\n\tassertMsgSent(batch.localProofAnn)\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.remoteProofAnn, remotePeer,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote channel proof\")\n\t}\n\trequire.NoError(t, err, \"unable to process remote channel proof\")\n\n\t// Now that we've constructed our full proof, we can assert that the\n\t// channel has been announced.\n\tfor i := 0; i < 2; i++ {\n\t\tselect {\n\t\tcase <-ctx.broadcastedMessage:\n\t\tcase <-time.After(2 * trickleDelay):\n\t\t\tt.Fatal(\"expected channel to be announced\")\n\t\t}\n\t}\n\n\t// With the channel announced, we'll generate a new channel update. This\n\t// one won't take the path of the reliable sender, as the channel has\n\t// already been announced. We'll keep track of the old message that is\n\t// now stale to use later on.\n\tstaleChannelUpdate := batch.chanUpdAnn1\n\tnewChannelUpdate := &lnwire.ChannelUpdate{}\n\t*newChannelUpdate = *staleChannelUpdate\n\tnewChannelUpdate.Timestamp++\n\tif err := signUpdate(selfKeyPriv, newChannelUpdate); err != nil {\n\t\tt.Fatalf(\"unable to sign new channel update: %v\", err)\n\t}\n\n\t// Process the new channel update. It should not be sent to the peer\n\t// directly since the reliable sender only applies when the channel is\n\t// not announced.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(\n\t\tnewChannelUpdate,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local channel update\")\n\t}\n\trequire.NoError(t, err, \"unable to process local channel update\")\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\tcase <-time.After(2 * trickleDelay):\n\t\tt.Fatal(\"channel update was not broadcast\")\n\t}\n\tselect {\n\tcase msg := <-sentToPeer:\n\t\tt.Fatalf(\"received unexpected message: %v\", spew.Sdump(msg))\n\tcase <-time.After(time.Second):\n\t}\n\n\t// Then, we'll trigger the reliable sender to send its pending messages\n\t// by triggering an offline notification for the peer, followed by an\n\t// online one.\n\tclose(offlineChan)\n\n\tselect {\n\tcase peerChan = <-notifyOnline:\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"gossiper did not request notification upon peer \" +\n\t\t\t\"connection\")\n\t}\n\n\tpeerChan <- remotePeer\n\n\t// At this point, we should have sent both the AnnounceSignatures and\n\t// stale ChannelUpdate.\n\tfor i := 0; i < 2; i++ {\n\t\tvar msg lnwire.Message\n\t\tselect {\n\t\tcase msg = <-sentToPeer:\n\t\tcase <-time.After(time.Second):\n\t\t\tt.Fatal(\"expected to send message\")\n\t\t}\n\n\t\tswitch msg := msg.(type) {\n\t\tcase *lnwire.ChannelUpdate:\n\t\t\tassertMessage(t, staleChannelUpdate, msg)\n\t\tcase *lnwire.AnnounceSignatures:\n\t\t\tassertMessage(t, batch.localProofAnn, msg)\n\t\tdefault:\n\t\t\tt.Fatalf(\"send unexpected %v message\", msg.MsgType())\n\t\t}\n\t}\n\n\t// Since the messages above are now deemed as stale, they should be\n\t// removed from the message store.\n\terr = wait.NoError(func() error {\n\t\tmsgs, err := ctx.gossiper.cfg.MessageStore.Messages()\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to retrieve pending \"+\n\t\t\t\t\"messages: %v\", err)\n\t\t}\n\t\tif len(msgs) != 0 {\n\t\t\treturn fmt.Errorf(\"expected no messages left, found %d\",\n\t\t\t\tlen(msgs))\n\t\t}\n\t\treturn nil\n\t}, time.Second)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n",
      "length": 8638,
      "tokens": 1180,
      "embedding": []
    },
    {
      "slug": "func sendLocalMsg(t *testing.T, ctx *testCtx, msg lnwire.Message,",
      "content": "func sendLocalMsg(t *testing.T, ctx *testCtx, msg lnwire.Message,\n\toptionalMsgFields ...OptionalMsgField) {\n\n\tt.Helper()\n\n\tvar err error\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(\n\t\tmsg, optionalMsgFields...,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process channel msg\")\n}\n",
      "length": 301,
      "tokens": 34,
      "embedding": []
    },
    {
      "slug": "func sendRemoteMsg(t *testing.T, ctx *testCtx, msg lnwire.Message,",
      "content": "func sendRemoteMsg(t *testing.T, ctx *testCtx, msg lnwire.Message,\n\tremotePeer lnpeer.Peer) {\n\n\tt.Helper()\n\n\tselect {\n\tcase err := <-ctx.gossiper.ProcessRemoteAnnouncement(msg, remotePeer):\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to process channel msg: %v\", err)\n\t\t}\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local announcement\")\n\t}\n}\n",
      "length": 276,
      "tokens": 35,
      "embedding": []
    },
    {
      "slug": "func assertBroadcastMsg(t *testing.T, ctx *testCtx,",
      "content": "func assertBroadcastMsg(t *testing.T, ctx *testCtx,\n\tpredicate func(lnwire.Message) error) {\n\n\tt.Helper()\n\n\t// We don't care about the order of the broadcast, only that our target\n\t// predicate returns true for any of the messages, so we'll continue to\n\t// retry until either we hit our timeout, or it returns with no error\n\t// (message found).\n\terr := wait.NoError(func() error {\n\t\tselect {\n\t\tcase msg := <-ctx.broadcastedMessage:\n\t\t\treturn predicate(msg.msg)\n\t\tcase <-time.After(2 * trickleDelay):\n\t\t\treturn fmt.Errorf(\"no message broadcast\")\n\t\t}\n\t}, time.Second*5)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\n// TestPropagateChanPolicyUpdate tests that we're able to issue requests to\n// update policies for all channels and also select target channels.\n// Additionally, we ensure that we don't propagate updates for any private\n// channels.",
      "length": 762,
      "tokens": 116,
      "embedding": []
    },
    {
      "slug": "func TestPropagateChanPolicyUpdate(t *testing.T) {",
      "content": "func TestPropagateChanPolicyUpdate(t *testing.T) {\n\tt.Parallel()\n\n\t// First, we'll make out test context and add 3 random channels to the\n\t// graph.\n\tstartingHeight := uint32(10)\n\tctx, err := createTestCtx(t, startingHeight)\n\trequire.NoError(t, err, \"unable to create test context\")\n\n\tconst numChannels = 3\n\tchannelsToAnnounce := make([]*annBatch, 0, numChannels)\n\tfor i := 0; i < numChannels; i++ {\n\t\tnewChan, err := createLocalAnnouncements(uint32(i + 1))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to make new channel ann: %v\", err)\n\t\t}\n\n\t\tchannelsToAnnounce = append(channelsToAnnounce, newChan)\n\t}\n\n\tremoteKey := remoteKeyPriv1.PubKey()\n\n\tsentMsgs := make(chan lnwire.Message, 10)\n\tremotePeer := &mockPeer{remoteKey, sentMsgs, ctx.gossiper.quit}\n\n\t// The forced code path for sending the private ChannelUpdate to the\n\t// remote peer will be hit, forcing it to request a notification that\n\t// the remote peer is active. We'll ensure that it targets the proper\n\t// pubkey, and hand it our mock peer above.\n\tnotifyErr := make(chan error, 1)\n\tctx.gossiper.reliableSender.cfg.NotifyWhenOnline = func(\n\t\ttargetPub [33]byte, peerChan chan<- lnpeer.Peer) {\n\n\t\tif !bytes.Equal(targetPub[:], remoteKey.SerializeCompressed()) {\n\t\t\tnotifyErr <- fmt.Errorf(\"reliableSender attempted to send the \"+\n\t\t\t\t\"message to the wrong peer: expected %x got %x\",\n\t\t\t\tremoteKey.SerializeCompressed(),\n\t\t\t\ttargetPub)\n\t\t}\n\n\t\tpeerChan <- remotePeer\n\t}\n\n\t// With our channel announcements created, we'll now send them all to\n\t// the gossiper in order for it to process. However, we'll hold back\n\t// the channel ann proof from the first channel in order to have it be\n\t// marked as private channel.\n\tfirstChanID := channelsToAnnounce[0].chanAnn.ShortChannelID\n\tfor i, batch := range channelsToAnnounce {\n\t\t// channelPoint ensures that each channel policy in the map\n\t\t// returned by PropagateChanPolicyUpdate has a unique key. Since\n\t\t// the map is keyed by wire.OutPoint, we want to ensure that\n\t\t// each channel has a unique channel point.\n\t\tchannelPoint := ChannelPoint(wire.OutPoint{Index: uint32(i)})\n\n\t\tsendLocalMsg(t, ctx, batch.chanAnn, channelPoint)\n\t\tsendLocalMsg(t, ctx, batch.chanUpdAnn1)\n\t\tsendLocalMsg(t, ctx, batch.nodeAnn1)\n\n\t\tsendRemoteMsg(t, ctx, batch.chanUpdAnn2, remotePeer)\n\t\tsendRemoteMsg(t, ctx, batch.nodeAnn2, remotePeer)\n\n\t\t// We'll skip sending the auth proofs from the first channel to\n\t\t// ensure that it's seen as a private channel.\n\t\tif batch.chanAnn.ShortChannelID == firstChanID {\n\t\t\tcontinue\n\t\t}\n\n\t\tsendLocalMsg(t, ctx, batch.localProofAnn)\n\t\tsendRemoteMsg(t, ctx, batch.remoteProofAnn, remotePeer)\n\t}\n\n\t// Drain out any broadcast or direct messages we might not have read up\n\t// to this point. We'll also check out notifyErr to detect if the\n\t// reliable sender had an issue sending to the remote peer.\nout:\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.broadcastedMessage:\n\t\tcase <-sentMsgs:\n\t\tcase err := <-notifyErr:\n\t\t\tt.Fatal(err)\n\n\t\t// Give it 5 seconds to drain out.\n\t\tcase <-time.After(5 * time.Second):\n\t\t\tbreak out\n\t\t}\n\t}\n\n\t// Now that all of our channels are loaded, we'll attempt to update the\n\t// policy of all of them.\n\tconst newTimeLockDelta = 100\n\tvar edgesToUpdate []EdgeWithInfo\n\terr = ctx.router.ForAllOutgoingChannels(func(\n\t\t_ kvdb.RTx,\n\t\tinfo *channeldb.ChannelEdgeInfo,\n\t\tedge *channeldb.ChannelEdgePolicy) error {\n\n\t\tedge.TimeLockDelta = uint16(newTimeLockDelta)\n\t\tedgesToUpdate = append(edgesToUpdate, EdgeWithInfo{\n\t\t\tInfo: info,\n\t\t\tEdge: edge,\n\t\t})\n\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terr = ctx.gossiper.PropagateChanPolicyUpdate(edgesToUpdate)\n\trequire.NoError(t, err, \"unable to chan policies\")\n\n\t// Two channel updates should now be broadcast, with neither of them\n\t// being the channel our first private channel.\n\tfor i := 0; i < numChannels-1; i++ {\n\t\tassertBroadcastMsg(t, ctx, func(msg lnwire.Message) error {\n\t\t\tupd, ok := msg.(*lnwire.ChannelUpdate)\n\t\t\tif !ok {\n\t\t\t\treturn fmt.Errorf(\"channel update not \"+\n\t\t\t\t\t\"broadcast, instead %T was\", msg)\n\t\t\t}\n\n\t\t\tif upd.ShortChannelID == firstChanID {\n\t\t\t\treturn fmt.Errorf(\"private channel upd \" +\n\t\t\t\t\t\"broadcast\")\n\t\t\t}\n\t\t\tif upd.TimeLockDelta != newTimeLockDelta {\n\t\t\t\treturn fmt.Errorf(\"wrong delta: expected %v, \"+\n\t\t\t\t\t\"got %v\", newTimeLockDelta,\n\t\t\t\t\tupd.TimeLockDelta)\n\t\t\t}\n\n\t\t\treturn nil\n\t\t})\n\t}\n\n\t// Finally the ChannelUpdate should have been sent directly to the\n\t// remote peer via the reliable sender.\n\tselect {\n\tcase msg := <-sentMsgs:\n\t\tupd, ok := msg.(*lnwire.ChannelUpdate)\n\t\tif !ok {\n\t\t\tt.Fatalf(\"channel update not \"+\n\t\t\t\t\"broadcast, instead %T was\", msg)\n\t\t}\n\t\tif upd.TimeLockDelta != newTimeLockDelta {\n\t\t\tt.Fatalf(\"wrong delta: expected %v, \"+\n\t\t\t\t\"got %v\", newTimeLockDelta,\n\t\t\t\tupd.TimeLockDelta)\n\t\t}\n\t\tif upd.ShortChannelID != firstChanID {\n\t\t\tt.Fatalf(\"private channel upd \" +\n\t\t\t\t\"broadcast\")\n\t\t}\n\tcase <-time.After(time.Second * 5):\n\t\tt.Fatalf(\"message not sent directly to peer\")\n\t}\n\n\t// At this point, no other ChannelUpdate messages should be broadcast\n\t// as we sent the two public ones to the network, and the private one\n\t// was sent directly to the peer.\n\tfor {\n\t\tselect {\n\t\tcase msg := <-ctx.broadcastedMessage:\n\t\t\tif upd, ok := msg.msg.(*lnwire.ChannelUpdate); ok {\n\t\t\t\tif upd.ShortChannelID == firstChanID {\n\t\t\t\t\tt.Fatalf(\"chan update msg received: %v\",\n\t\t\t\t\t\tspew.Sdump(msg))\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// TestProcessChannelAnnouncementOptionalMsgFields ensures that the gossiper can\n// properly handled optional message fields provided by the caller when\n// processing a channel announcement.",
      "length": 5305,
      "tokens": 720,
      "embedding": []
    },
    {
      "slug": "func TestProcessChannelAnnouncementOptionalMsgFields(t *testing.T) {",
      "content": "func TestProcessChannelAnnouncementOptionalMsgFields(t *testing.T) {\n\tt.Parallel()\n\n\t// We'll start by creating our test context and a set of test channel\n\t// announcements.\n\tctx, err := createTestCtx(t, 0)\n\trequire.NoError(t, err, \"unable to create test context\")\n\n\tchanAnn1 := createAnnouncementWithoutProof(\n\t\t100, selfKeyDesc.PubKey, remoteKeyPub1,\n\t)\n\tchanAnn2 := createAnnouncementWithoutProof(\n\t\t101, selfKeyDesc.PubKey, remoteKeyPub1,\n\t)\n\n\t// assertOptionalMsgFields is a helper closure that ensures the optional\n\t// message fields were set as intended.\n\tassertOptionalMsgFields := func(chanID lnwire.ShortChannelID,\n\t\tcapacity btcutil.Amount, channelPoint wire.OutPoint) {\n\n\t\tt.Helper()\n\n\t\tedge, _, _, err := ctx.router.GetChannelByID(chanID)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to get channel by id: %v\", err)\n\t\t}\n\t\tif edge.Capacity != capacity {\n\t\t\tt.Fatalf(\"expected capacity %v, got %v\", capacity,\n\t\t\t\tedge.Capacity)\n\t\t}\n\t\tif edge.ChannelPoint != channelPoint {\n\t\t\tt.Fatalf(\"expected channel point %v, got %v\",\n\t\t\t\tchannelPoint, edge.ChannelPoint)\n\t\t}\n\t}\n\n\t// We'll process the first announcement without any optional fields. We\n\t// should see the channel's capacity and outpoint have a zero value.\n\tsendLocalMsg(t, ctx, chanAnn1)\n\tassertOptionalMsgFields(chanAnn1.ShortChannelID, 0, wire.OutPoint{})\n\n\t// Providing the capacity and channel point as optional fields should\n\t// propagate them all the way down to the router.\n\tcapacity := btcutil.Amount(1000)\n\tchannelPoint := wire.OutPoint{Index: 1}\n\tsendLocalMsg(\n\t\tt, ctx, chanAnn2, ChannelCapacity(capacity),\n\t\tChannelPoint(channelPoint),\n\t)\n\tassertOptionalMsgFields(chanAnn2.ShortChannelID, capacity, channelPoint)\n}\n",
      "length": 1566,
      "tokens": 186,
      "embedding": []
    },
    {
      "slug": "func assertMessage(t *testing.T, expected, got lnwire.Message) {",
      "content": "func assertMessage(t *testing.T, expected, got lnwire.Message) {\n\tt.Helper()\n\n\tif !reflect.DeepEqual(expected, got) {\n\t\tt.Fatalf(\"expected: %v\\ngot: %v\", spew.Sdump(expected),\n\t\t\tspew.Sdump(got))\n\t}\n}\n\n// TestSplitAnnouncementsCorrectSubBatches checks that we split a given\n// sizes of announcement list into the correct number of batches.",
      "length": 265,
      "tokens": 31,
      "embedding": []
    },
    {
      "slug": "func TestSplitAnnouncementsCorrectSubBatches(t *testing.T) {",
      "content": "func TestSplitAnnouncementsCorrectSubBatches(t *testing.T) {\n\t// Create our test harness.\n\tconst blockHeight = 100\n\tctx, err := createTestCtx(t, blockHeight)\n\trequire.NoError(t, err, \"can't create context\")\n\n\tconst subBatchSize = 10\n\n\tannouncementBatchSizes := []int{2, 5, 20, 45, 80, 100, 1005}\n\texpectedNumberMiniBatches := []int{1, 1, 2, 5, 8, 10, 101}\n\n\tlengthAnnouncementBatchSizes := len(announcementBatchSizes)\n\tlengthExpectedNumberMiniBatches := len(expectedNumberMiniBatches)\n\n\tbatchSizeCalculator = func(totalDelay, subBatchDelay time.Duration,\n\t\tminimumBatchSize, batchSize int) int {\n\n\t\treturn subBatchSize\n\t}\n\n\tif lengthAnnouncementBatchSizes != lengthExpectedNumberMiniBatches {\n\t\tt.Fatal(\"Length of announcementBatchSizes and \" +\n\t\t\t\"expectedNumberMiniBatches should be equal\")\n\t}\n\n\tfor testIndex := range announcementBatchSizes {\n\t\tvar batchSize = announcementBatchSizes[testIndex]\n\t\tannouncementBatch := make([]msgWithSenders, batchSize)\n\n\t\tsplitAnnouncementBatch := ctx.gossiper.splitAnnouncementBatches(\n\t\t\tannouncementBatch,\n\t\t)\n\n\t\tlengthMiniBatches := len(splitAnnouncementBatch)\n\n\t\tif lengthMiniBatches != expectedNumberMiniBatches[testIndex] {\n\t\t\tt.Fatalf(\"Expecting %d mini batches, actual %d\",\n\t\t\t\texpectedNumberMiniBatches[testIndex],\n\t\t\t\tlengthMiniBatches)\n\t\t}\n\t}\n}\n",
      "length": 1191,
      "tokens": 114,
      "embedding": []
    },
    {
      "slug": "func assertCorrectSubBatchSize(t *testing.T, expectedSubBatchSize,",
      "content": "func assertCorrectSubBatchSize(t *testing.T, expectedSubBatchSize,\n\tactualSubBatchSize int) {\n\n\tt.Helper()\n\n\tif actualSubBatchSize != expectedSubBatchSize {\n\t\tt.Fatalf(\"Expecting subBatch size of %d, actual %d\",\n\t\t\texpectedSubBatchSize, actualSubBatchSize)\n\t}\n}\n\n// TestCalculateCorrectSubBatchSize checks that we check the correct\n// sub batch size for each of the input vectors of batch sizes.",
      "length": 317,
      "tokens": 41,
      "embedding": []
    },
    {
      "slug": "func TestCalculateCorrectSubBatchSizes(t *testing.T) {",
      "content": "func TestCalculateCorrectSubBatchSizes(t *testing.T) {\n\tt.Parallel()\n\n\tconst minimumSubBatchSize = 10\n\tconst batchDelay = time.Duration(100)\n\tconst subBatchDelay = time.Duration(10)\n\n\tbatchSizes := []int{2, 200, 250, 305, 352, 10010, 1000001}\n\texpectedSubBatchSize := []int{10, 20, 25, 31, 36, 1001, 100001}\n\n\tfor testIndex := range batchSizes {\n\t\tbatchSize := batchSizes[testIndex]\n\t\texpectedBatchSize := expectedSubBatchSize[testIndex]\n\n\t\tactualSubBatchSize := calculateSubBatchSize(\n\t\t\tbatchDelay, subBatchDelay, minimumSubBatchSize, batchSize,\n\t\t)\n\n\t\tassertCorrectSubBatchSize(t, expectedBatchSize, actualSubBatchSize)\n\t}\n}\n\n// TestCalculateCorrectSubBatchSizesDifferentDelay checks that we check the\n// correct sub batch size for each of different delay.",
      "length": 682,
      "tokens": 73,
      "embedding": []
    },
    {
      "slug": "func TestCalculateCorrectSubBatchSizesDifferentDelay(t *testing.T) {",
      "content": "func TestCalculateCorrectSubBatchSizesDifferentDelay(t *testing.T) {\n\tt.Parallel()\n\n\tconst batchSize = 100\n\tconst minimumSubBatchSize = 10\n\n\tbatchDelays := []time.Duration{100, 50, 20, 25, 5, 0}\n\tconst subBatchDelay = 10\n\n\texpectedSubBatchSize := []int{10, 20, 50, 40, 100, 100}\n\n\tfor testIndex := range batchDelays {\n\t\tbatchDelay := batchDelays[testIndex]\n\t\texpectedBatchSize := expectedSubBatchSize[testIndex]\n\n\t\tactualSubBatchSize := calculateSubBatchSize(\n\t\t\tbatchDelay, subBatchDelay, minimumSubBatchSize, batchSize,\n\t\t)\n\n\t\tassertCorrectSubBatchSize(t, expectedBatchSize, actualSubBatchSize)\n\t}\n}\n\n// markGraphSynced allows us to report that the initial historical sync has\n// completed.",
      "length": 600,
      "tokens": 68,
      "embedding": []
    },
    {
      "slug": "func (m *SyncManager) markGraphSyncing() {",
      "content": "func (m *SyncManager) markGraphSyncing() {\n\tatomic.StoreInt32(&m.initialHistoricalSyncCompleted, 0)\n}\n\n// TestBroadcastAnnsAfterGraphSynced ensures that we only broadcast\n// announcements after the graph has been considered as synced, i.e., after our\n// initial historical sync has completed.",
      "length": 244,
      "tokens": 29,
      "embedding": []
    },
    {
      "slug": "func TestBroadcastAnnsAfterGraphSynced(t *testing.T) {",
      "content": "func TestBroadcastAnnsAfterGraphSynced(t *testing.T) {\n\tt.Parallel()\n\n\tctx, err := createTestCtx(t, 10)\n\trequire.NoError(t, err, \"can't create context\")\n\n\t// We'll mark the graph as not synced. This should prevent us from\n\t// broadcasting any messages we've received as part of our initial\n\t// historical sync.\n\tctx.gossiper.syncMgr.markGraphSyncing()\n\n\tassertBroadcast := func(msg lnwire.Message, isRemote bool,\n\t\tshouldBroadcast bool) {\n\n\t\tt.Helper()\n\n\t\tnodePeer := &mockPeer{remoteKeyPriv1.PubKey(), nil, nil}\n\t\tvar errChan chan error\n\t\tif isRemote {\n\t\t\terrChan = ctx.gossiper.ProcessRemoteAnnouncement(\n\t\t\t\tmsg, nodePeer,\n\t\t\t)\n\t\t} else {\n\t\t\terrChan = ctx.gossiper.ProcessLocalAnnouncement(msg)\n\t\t}\n\n\t\tselect {\n\t\tcase err := <-errChan:\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"unable to process gossip message: %v\",\n\t\t\t\t\terr)\n\t\t\t}\n\t\tcase <-time.After(2 * time.Second):\n\t\t\tt.Fatal(\"gossip message not processed\")\n\t\t}\n\n\t\tselect {\n\t\tcase <-ctx.broadcastedMessage:\n\t\t\tif !shouldBroadcast {\n\t\t\t\tt.Fatal(\"gossip message was broadcast\")\n\t\t\t}\n\t\tcase <-time.After(2 * trickleDelay):\n\t\t\tif shouldBroadcast {\n\t\t\t\tt.Fatal(\"gossip message wasn't broadcast\")\n\t\t\t}\n\t\t}\n\t}\n\n\t// A remote channel announcement should not be broadcast since the graph\n\t// has not yet been synced.\n\tchanAnn1, err := createRemoteChannelAnnouncement(0)\n\trequire.NoError(t, err, \"unable to create channel announcement\")\n\tassertBroadcast(chanAnn1, true, false)\n\n\t// A local channel announcement should be broadcast though, regardless\n\t// of whether we've synced our graph or not.\n\tchanUpd, err := createUpdateAnnouncement(0, 0, remoteKeyPriv1, 1)\n\trequire.NoError(t, err, \"unable to create channel announcement\")\n\tassertBroadcast(chanUpd, false, true)\n\n\t// Mark the graph as synced, which should allow the channel announcement\n\t// should to be broadcast.\n\tctx.gossiper.syncMgr.markGraphSynced()\n\n\tchanAnn2, err := createRemoteChannelAnnouncement(1)\n\trequire.NoError(t, err, \"unable to create channel announcement\")\n\tassertBroadcast(chanAnn2, true, true)\n}\n\n// TestRateLimitChannelUpdates ensures that we properly rate limit incoming\n// channel updates.",
      "length": 1982,
      "tokens": 241,
      "embedding": []
    },
    {
      "slug": "func TestRateLimitChannelUpdates(t *testing.T) {",
      "content": "func TestRateLimitChannelUpdates(t *testing.T) {\n\tt.Parallel()\n\n\t// Create our test harness.\n\tconst blockHeight = 100\n\tctx, err := createTestCtx(t, blockHeight)\n\trequire.NoError(t, err, \"can't create context\")\n\tctx.gossiper.cfg.RebroadcastInterval = time.Hour\n\tctx.gossiper.cfg.MaxChannelUpdateBurst = 5\n\tctx.gossiper.cfg.ChannelUpdateInterval = 5 * time.Second\n\n\t// The graph should start empty.\n\trequire.Empty(t, ctx.router.infos)\n\trequire.Empty(t, ctx.router.edges)\n\n\t// We'll create a batch of signed announcements, including updates for\n\t// both sides, for a channel and process them. They should all be\n\t// forwarded as this is our first time learning about the channel.\n\tbatch, err := createRemoteAnnouncements(blockHeight)\n\trequire.NoError(t, err)\n\n\tnodePeer1 := &mockPeer{remoteKeyPriv1.PubKey(), nil, nil}\n\tselect {\n\tcase err := <-ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.chanAnn, nodePeer1,\n\t):\n\t\trequire.NoError(t, err)\n\tcase <-time.After(time.Second):\n\t\tt.Fatal(\"remote announcement not processed\")\n\t}\n\n\tselect {\n\tcase err := <-ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.chanUpdAnn1, nodePeer1,\n\t):\n\t\trequire.NoError(t, err)\n\tcase <-time.After(time.Second):\n\t\tt.Fatal(\"remote announcement not processed\")\n\t}\n\n\tnodePeer2 := &mockPeer{remoteKeyPriv2.PubKey(), nil, nil}\n\tselect {\n\tcase err := <-ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.chanUpdAnn2, nodePeer2,\n\t):\n\t\trequire.NoError(t, err)\n\tcase <-time.After(time.Second):\n\t\tt.Fatal(\"remote announcement not processed\")\n\t}\n\n\ttimeout := time.After(2 * trickleDelay)\n\tfor i := 0; i < 3; i++ {\n\t\tselect {\n\t\tcase <-ctx.broadcastedMessage:\n\t\tcase <-timeout:\n\t\t\tt.Fatal(\"expected announcement to be broadcast\")\n\t\t}\n\t}\n\n\tshortChanID := batch.chanAnn.ShortChannelID.ToUint64()\n\trequire.Contains(t, ctx.router.infos, shortChanID)\n\trequire.Contains(t, ctx.router.edges, shortChanID)\n\n\t// We'll define a helper to assert whether updates should be rate\n\t// limited or not depending on their contents.\n\tassertRateLimit := func(update *lnwire.ChannelUpdate, peer lnpeer.Peer,\n\t\tshouldRateLimit bool) {\n\n\t\tt.Helper()\n\n\t\tselect {\n\t\tcase err := <-ctx.gossiper.ProcessRemoteAnnouncement(update, peer):\n\t\t\trequire.NoError(t, err)\n\t\tcase <-time.After(time.Second):\n\t\t\tt.Fatal(\"remote announcement not processed\")\n\t\t}\n\n\t\tselect {\n\t\tcase <-ctx.broadcastedMessage:\n\t\t\tif shouldRateLimit {\n\t\t\t\tt.Fatal(\"unexpected channel update broadcast\")\n\t\t\t}\n\t\tcase <-time.After(2 * trickleDelay):\n\t\t\tif !shouldRateLimit {\n\t\t\t\tt.Fatal(\"expected channel update broadcast\")\n\t\t\t}\n\t\t}\n\t}\n\n\t// We'll start with the keep alive case.\n\t//\n\t// We rate limit any keep alive updates that have not at least spanned\n\t// our rebroadcast interval.\n\trateLimitKeepAliveUpdate := *batch.chanUpdAnn1\n\trateLimitKeepAliveUpdate.Timestamp++\n\trequire.NoError(t, signUpdate(remoteKeyPriv1, &rateLimitKeepAliveUpdate))\n\tassertRateLimit(&rateLimitKeepAliveUpdate, nodePeer1, true)\n\n\tkeepAliveUpdate := *batch.chanUpdAnn1\n\tkeepAliveUpdate.Timestamp = uint32(\n\t\ttime.Unix(int64(batch.chanUpdAnn1.Timestamp), 0).\n\t\t\tAdd(ctx.gossiper.cfg.RebroadcastInterval).Unix(),\n\t)\n\trequire.NoError(t, signUpdate(remoteKeyPriv1, &keepAliveUpdate))\n\tassertRateLimit(&keepAliveUpdate, nodePeer1, false)\n\n\t// Then, we'll move on to the non keep alive cases.\n\t//\n\t// For this test, non keep alive updates are rate limited to one per 5\n\t// seconds with a max burst of 5 per direction. We'll process the max\n\t// burst of one direction first. None of these should be rate limited.\n\tupdateSameDirection := keepAliveUpdate\n\tfor i := uint32(0); i < uint32(ctx.gossiper.cfg.MaxChannelUpdateBurst); i++ {\n\t\tupdateSameDirection.Timestamp++\n\t\tupdateSameDirection.BaseFee++\n\t\trequire.NoError(t, signUpdate(remoteKeyPriv1, &updateSameDirection))\n\t\tassertRateLimit(&updateSameDirection, nodePeer1, false)\n\t}\n\n\t// Following with another update should be rate limited as the max burst\n\t// has been reached and we haven't ticked at the next interval yet.\n\tupdateSameDirection.Timestamp++\n\tupdateSameDirection.BaseFee++\n\trequire.NoError(t, signUpdate(remoteKeyPriv1, &updateSameDirection))\n\tassertRateLimit(&updateSameDirection, nodePeer1, true)\n\n\t// An update for the other direction should not be rate limited.\n\tupdateDiffDirection := *batch.chanUpdAnn2\n\tupdateDiffDirection.Timestamp++\n\tupdateDiffDirection.BaseFee++\n\trequire.NoError(t, signUpdate(remoteKeyPriv2, &updateDiffDirection))\n\tassertRateLimit(&updateDiffDirection, nodePeer2, false)\n\n\t// Wait for the next interval to tick. Since we've only waited for one,\n\t// only one more update is allowed.\n\t<-time.After(ctx.gossiper.cfg.ChannelUpdateInterval)\n\tfor i := 0; i < ctx.gossiper.cfg.MaxChannelUpdateBurst; i++ {\n\t\tupdateSameDirection.Timestamp++\n\t\tupdateSameDirection.BaseFee++\n\t\trequire.NoError(t, signUpdate(remoteKeyPriv1, &updateSameDirection))\n\n\t\tshouldRateLimit := i != 0\n\t\tassertRateLimit(&updateSameDirection, nodePeer1, shouldRateLimit)\n\t}\n}\n\n// TestIgnoreOwnAnnouncement tests that the gossiper will ignore announcements\n// about our own channels when coming from a remote peer.",
      "length": 4836,
      "tokens": 506,
      "embedding": []
    },
    {
      "slug": "func TestIgnoreOwnAnnouncement(t *testing.T) {",
      "content": "func TestIgnoreOwnAnnouncement(t *testing.T) {\n\tt.Parallel()\n\n\tctx, err := createTestCtx(t, proofMatureDelta)\n\trequire.NoError(t, err, \"can't create context\")\n\n\tbatch, err := createLocalAnnouncements(0)\n\trequire.NoError(t, err, \"can't generate announcements\")\n\n\tremoteKey, err := btcec.ParsePubKey(batch.nodeAnn2.NodeID[:])\n\trequire.NoError(t, err, \"unable to parse pubkey\")\n\tremotePeer := &mockPeer{remoteKey, nil, nil}\n\n\t// Try to let the remote peer tell us about the channel we are part of.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.chanAnn, remotePeer,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\t// It should be ignored, since the gossiper only cares about local\n\t// announcements for its own channels.\n\tif err == nil || !strings.Contains(err.Error(), \"ignoring\") {\n\t\tt.Fatalf(\"expected gossiper to ignore announcement, got: %v\", err)\n\t}\n\n\t// Now do the local channelannouncement, node announcement, and channel\n\t// update. No messages should be brodcasted yet, since we don't have\n\t// the announcement signatures.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(batch.chanAnn):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process channel ann\")\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"channel announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(batch.chanUpdAnn1):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process channel update\")\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"channel update announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(batch.nodeAnn1):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process local announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process node ann\")\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"node announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\t// We should accept the remote's channel update and node announcement.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.chanUpdAnn2, remotePeer,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process channel update\")\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"channel update announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.nodeAnn2, remotePeer,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process node ann\")\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"node announcement was broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\t// Now we exchange the proofs, the messages will be broadcasted to the\n\t// network.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessLocalAnnouncement(batch.localProofAnn):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process local proof\")\n\n\tselect {\n\tcase <-ctx.broadcastedMessage:\n\t\tt.Fatal(\"announcements were broadcast\")\n\tcase <-time.After(2 * trickleDelay):\n\t}\n\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.remoteProofAnn, remotePeer,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\trequire.NoError(t, err, \"unable to process remote proof\")\n\n\tfor i := 0; i < 5; i++ {\n\t\tselect {\n\t\tcase <-ctx.broadcastedMessage:\n\t\tcase <-time.After(time.Second):\n\t\t\tt.Fatal(\"announcement wasn't broadcast\")\n\t\t}\n\t}\n\n\t// Finally, we again check that we'll ignore the remote giving us\n\t// announcements about our own channel.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.chanAnn, remotePeer,\n\t):\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\tif err == nil || !strings.Contains(err.Error(), \"ignoring\") {\n\t\tt.Fatalf(\"expected gossiper to ignore announcement, got: %v\", err)\n\t}\n}\n\n// TestRejectCacheChannelAnn checks that if we reject a channel announcement,\n// then if we attempt to validate it again, we'll reject it with the proper\n// error.",
      "length": 4269,
      "tokens": 508,
      "embedding": []
    },
    {
      "slug": "func TestRejectCacheChannelAnn(t *testing.T) {",
      "content": "func TestRejectCacheChannelAnn(t *testing.T) {\n\tt.Parallel()\n\n\tctx, err := createTestCtx(t, proofMatureDelta)\n\trequire.NoError(t, err, \"can't create context\")\n\n\t// First, we create a channel announcement to send over to our test\n\t// peer.\n\tbatch, err := createRemoteAnnouncements(0)\n\trequire.NoError(t, err, \"can't generate announcements\")\n\n\tremoteKey, err := btcec.ParsePubKey(batch.nodeAnn2.NodeID[:])\n\trequire.NoError(t, err, \"unable to parse pubkey\")\n\tremotePeer := &mockPeer{remoteKey, nil, nil}\n\n\t// Before sending over the announcement, we'll modify it such that we\n\t// know it will always fail.\n\tchanID := batch.chanAnn.ShortChannelID.ToUint64()\n\tctx.router.queueValidationFail(chanID)\n\n\t// If we process the batch the first time we should get an error.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.chanAnn, remotePeer,\n\t):\n\t\trequire.NotNil(t, err)\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n\n\t// If we process it a *second* time, then we should get an error saying\n\t// we rejected it already.\n\tselect {\n\tcase err = <-ctx.gossiper.ProcessRemoteAnnouncement(\n\t\tbatch.chanAnn, remotePeer,\n\t):\n\t\terrStr := err.Error()\n\t\trequire.Contains(t, errStr, \"recently rejected\")\n\tcase <-time.After(2 * time.Second):\n\t\tt.Fatal(\"did not process remote announcement\")\n\t}\n}\n",
      "length": 1250,
      "tokens": 154,
      "embedding": []
    }
  ]
}