{
  "filepath": "../implementations/go/lnd/discovery/gossiper.go",
  "package": "discovery",
  "sections": [
    {
      "slug": "type optionalMsgFields struct {",
      "content": "type optionalMsgFields struct {\n\tcapacity     *btcutil.Amount\n\tchannelPoint *wire.OutPoint\n\tremoteAlias  *lnwire.ShortChannelID\n}\n\n// apply applies the optional fields within the functional options.",
      "length": 161,
      "tokens": 17,
      "embedding": []
    },
    {
      "slug": "func (f *optionalMsgFields) apply(optionalMsgFields ...OptionalMsgField) {",
      "content": "func (f *optionalMsgFields) apply(optionalMsgFields ...OptionalMsgField) {\n\tfor _, optionalMsgField := range optionalMsgFields {\n\t\toptionalMsgField(f)\n\t}\n}\n\n// OptionalMsgField is a functional option parameter that can be used to provide\n// external information that is not included within a network message but serves\n// useful when processing it.",
      "length": 266,
      "tokens": 41,
      "embedding": []
    },
    {
      "slug": "type OptionalMsgField func(*optionalMsgFields)",
      "content": "type OptionalMsgField func(*optionalMsgFields)\n\n// ChannelCapacity is an optional field that lets the gossiper know of the\n// capacity of a channel.",
      "length": 99,
      "tokens": 18,
      "embedding": []
    },
    {
      "slug": "func ChannelCapacity(capacity btcutil.Amount) OptionalMsgField {",
      "content": "func ChannelCapacity(capacity btcutil.Amount) OptionalMsgField {\n\treturn func(f *optionalMsgFields) {\n\t\tf.capacity = &capacity\n\t}\n}\n\n// ChannelPoint is an optional field that lets the gossiper know of the outpoint\n// of a channel.",
      "length": 159,
      "tokens": 27,
      "embedding": []
    },
    {
      "slug": "func ChannelPoint(op wire.OutPoint) OptionalMsgField {",
      "content": "func ChannelPoint(op wire.OutPoint) OptionalMsgField {\n\treturn func(f *optionalMsgFields) {\n\t\tf.channelPoint = &op\n\t}\n}\n\n// RemoteAlias is an optional field that lets the gossiper know that a locally\n// sent channel update is actually an update for the peer that should replace\n// the ShortChannelID field with the remote's alias. This is only used for\n// channels with peers where the option-scid-alias feature bit was negotiated.\n// The channel update will be added to the graph under the original SCID, but\n// will be modified and re-signed with this alias.",
      "length": 495,
      "tokens": 85,
      "embedding": []
    },
    {
      "slug": "func RemoteAlias(alias *lnwire.ShortChannelID) OptionalMsgField {",
      "content": "func RemoteAlias(alias *lnwire.ShortChannelID) OptionalMsgField {\n\treturn func(f *optionalMsgFields) {\n\t\tf.remoteAlias = alias\n\t}\n}\n\n// networkMsg couples a routing related wire message with the peer that\n// originally sent it.",
      "length": 155,
      "tokens": 25,
      "embedding": []
    },
    {
      "slug": "type networkMsg struct {",
      "content": "type networkMsg struct {\n\tpeer              lnpeer.Peer\n\tsource            *btcec.PublicKey\n\tmsg               lnwire.Message\n\toptionalMsgFields *optionalMsgFields\n\n\tisRemote bool\n\n\terr chan error\n}\n\n// chanPolicyUpdateRequest is a request that is sent to the server when a caller\n// wishes to update a particular set of channels. New ChannelUpdate messages\n// will be crafted to be sent out during the next broadcast epoch and the fee\n// updates committed to the lower layer.",
      "length": 438,
      "tokens": 63,
      "embedding": []
    },
    {
      "slug": "type chanPolicyUpdateRequest struct {",
      "content": "type chanPolicyUpdateRequest struct {\n\tedgesToUpdate []EdgeWithInfo\n\terrChan       chan error\n}\n\n// PinnedSyncers is a set of node pubkeys for which we will maintain an active\n// syncer at all times.",
      "length": 156,
      "tokens": 26,
      "embedding": []
    },
    {
      "slug": "type PinnedSyncers map[route.Vertex]struct{}",
      "content": "type PinnedSyncers map[route.Vertex]struct{}\n\n// Config defines the configuration for the service. ALL elements within the\n// configuration MUST be non-nil for the service to carry out its duties.",
      "length": 149,
      "tokens": 25,
      "embedding": []
    },
    {
      "slug": "type Config struct {",
      "content": "type Config struct {\n\t// ChainHash is a hash that indicates which resident chain of the\n\t// AuthenticatedGossiper. Any announcements that don't match this\n\t// chain hash will be ignored.\n\t//\n\t// TODO(roasbeef): eventually make into map so can de-multiplex\n\t// incoming announcements\n\t//   * also need to do same for Notifier\n\tChainHash chainhash.Hash\n\n\t// Router is the subsystem which is responsible for managing the\n\t// topology of lightning network. After incoming channel, node, channel\n\t// updates announcements are validated they are sent to the router in\n\t// order to be included in the LN graph.\n\tRouter routing.ChannelGraphSource\n\n\t// ChanSeries is an interfaces that provides access to a time series\n\t// view of the current known channel graph. Each GossipSyncer enabled\n\t// peer will utilize this in order to create and respond to channel\n\t// graph time series queries.\n\tChanSeries ChannelGraphTimeSeries\n\n\t// Notifier is used for receiving notifications of incoming blocks.\n\t// With each new incoming block found we process previously premature\n\t// announcements.\n\t//\n\t// TODO(roasbeef): could possibly just replace this with an epoch\n\t// channel.\n\tNotifier chainntnfs.ChainNotifier\n\n\t// Broadcast broadcasts a particular set of announcements to all peers\n\t// that the daemon is connected to. If supplied, the exclude parameter\n\t// indicates that the target peer should be excluded from the\n\t// broadcast.\n\tBroadcast func(skips map[route.Vertex]struct{},\n\t\tmsg ...lnwire.Message) error\n\n\t// NotifyWhenOnline is a function that allows the gossiper to be\n\t// notified when a certain peer comes online, allowing it to\n\t// retry sending a peer message.\n\t//\n\t// NOTE: The peerChan channel must be buffered.\n\tNotifyWhenOnline func(peerPubKey [33]byte, peerChan chan<- lnpeer.Peer)\n\n\t// NotifyWhenOffline is a function that allows the gossiper to be\n\t// notified when a certain peer disconnects, allowing it to request a\n\t// notification for when it reconnects.\n\tNotifyWhenOffline func(peerPubKey [33]byte) <-chan struct{}\n\n\t// SelfNodeAnnouncement is a function that fetches our own current node\n\t// announcement, for use when determining whether we should update our\n\t// peers about our presence on the network. If the refresh is true, a\n\t// new and updated announcement will be returned.\n\tSelfNodeAnnouncement func(refresh bool) (lnwire.NodeAnnouncement, error)\n\n\t// ProofMatureDelta the number of confirmations which is needed before\n\t// exchange the channel announcement proofs.\n\tProofMatureDelta uint32\n\n\t// TrickleDelay the period of trickle timer which flushes to the\n\t// network the pending batch of new announcements we've received since\n\t// the last trickle tick.\n\tTrickleDelay time.Duration\n\n\t// RetransmitTicker is a ticker that ticks with a period which\n\t// indicates that we should check if we need re-broadcast any of our\n\t// personal channels.\n\tRetransmitTicker ticker.Ticker\n\n\t// RebroadcastInterval is the maximum time we wait between sending out\n\t// channel updates for our active channels and our own node\n\t// announcement. We do this to ensure our active presence on the\n\t// network is known, and we are not being considered a zombie node or\n\t// having zombie channels.\n\tRebroadcastInterval time.Duration\n\n\t// WaitingProofStore is a persistent storage of partial channel proof\n\t// announcement messages. We use it to buffer half of the material\n\t// needed to reconstruct a full authenticated channel announcement.\n\t// Once we receive the other half the channel proof, we'll be able to\n\t// properly validate it and re-broadcast it out to the network.\n\t//\n\t// TODO(wilmer): make interface to prevent channeldb dependency.\n\tWaitingProofStore *channeldb.WaitingProofStore\n\n\t// MessageStore is a persistent storage of gossip messages which we will\n\t// use to determine which messages need to be resent for a given peer.\n\tMessageStore GossipMessageStore\n\n\t// AnnSigner is an instance of the MessageSigner interface which will\n\t// be used to manually sign any outgoing channel updates. The signer\n\t// implementation should be backed by the public key of the backing\n\t// Lightning node.\n\t//\n\t// TODO(roasbeef): extract ann crafting + sign from fundingMgr into\n\t// here?\n\tAnnSigner lnwallet.MessageSigner\n\n\t// NumActiveSyncers is the number of peers for which we should have\n\t// active syncers with. After reaching NumActiveSyncers, any future\n\t// gossip syncers will be passive.\n\tNumActiveSyncers int\n\n\t// RotateTicker is a ticker responsible for notifying the SyncManager\n\t// when it should rotate its active syncers. A single active syncer with\n\t// a chansSynced state will be exchanged for a passive syncer in order\n\t// to ensure we don't keep syncing with the same peers.\n\tRotateTicker ticker.Ticker\n\n\t// HistoricalSyncTicker is a ticker responsible for notifying the\n\t// syncManager when it should attempt a historical sync with a gossip\n\t// sync peer.\n\tHistoricalSyncTicker ticker.Ticker\n\n\t// ActiveSyncerTimeoutTicker is a ticker responsible for notifying the\n\t// syncManager when it should attempt to start the next pending\n\t// activeSyncer due to the current one not completing its state machine\n\t// within the timeout.\n\tActiveSyncerTimeoutTicker ticker.Ticker\n\n\t// MinimumBatchSize is minimum size of a sub batch of announcement\n\t// messages.\n\tMinimumBatchSize int\n\n\t// SubBatchDelay is the delay between sending sub batches of\n\t// gossip messages.\n\tSubBatchDelay time.Duration\n\n\t// IgnoreHistoricalFilters will prevent syncers from replying with\n\t// historical data when the remote peer sets a gossip_timestamp_range.\n\t// This prevents ranges with old start times from causing us to dump the\n\t// graph on connect.\n\tIgnoreHistoricalFilters bool\n\n\t// PinnedSyncers is a set of peers that will always transition to\n\t// ActiveSync upon connection. These peers will never transition to\n\t// PassiveSync.\n\tPinnedSyncers PinnedSyncers\n\n\t// MaxChannelUpdateBurst specifies the maximum number of updates for a\n\t// specific channel and direction that we'll accept over an interval.\n\tMaxChannelUpdateBurst int\n\n\t// ChannelUpdateInterval specifies the interval we'll use to determine\n\t// how often we should allow a new update for a specific channel and\n\t// direction.\n\tChannelUpdateInterval time.Duration\n\n\t// IsAlias returns true if a given ShortChannelID is an alias for\n\t// option_scid_alias channels.\n\tIsAlias func(scid lnwire.ShortChannelID) bool\n\n\t// SignAliasUpdate is used to re-sign a channel update using the\n\t// remote's alias if the option-scid-alias feature bit was negotiated.\n\tSignAliasUpdate func(u *lnwire.ChannelUpdate) (*ecdsa.Signature,\n\t\terror)\n\n\t// FindBaseByAlias finds the SCID stored in the graph by an alias SCID.\n\t// This is used for channels that have negotiated the option-scid-alias\n\t// feature bit.\n\tFindBaseByAlias func(alias lnwire.ShortChannelID) (\n\t\tlnwire.ShortChannelID, error)\n\n\t// GetAlias allows the gossiper to look up the peer's alias for a given\n\t// ChannelID. This is used to sign updates for them if the channel has\n\t// no AuthProof and the option-scid-alias feature bit was negotiated.\n\tGetAlias func(lnwire.ChannelID) (lnwire.ShortChannelID, error)\n\n\t// FindChannel allows the gossiper to find a channel that we're party\n\t// to without iterating over the entire set of open channels.\n\tFindChannel func(node *btcec.PublicKey, chanID lnwire.ChannelID) (\n\t\t*channeldb.OpenChannel, error)\n}\n\n// processedNetworkMsg is a wrapper around networkMsg and a boolean. It is\n// used to let the caller of the lru.Cache know if a message has already been\n// processed or not.",
      "length": 7330,
      "tokens": 1091,
      "embedding": []
    },
    {
      "slug": "type processedNetworkMsg struct {",
      "content": "type processedNetworkMsg struct {\n\tprocessed bool\n\tmsg       *networkMsg\n}\n\n// cachedNetworkMsg is a wrapper around a network message that can be used with\n// *lru.Cache.",
      "length": 131,
      "tokens": 21,
      "embedding": []
    },
    {
      "slug": "type cachedNetworkMsg struct {",
      "content": "type cachedNetworkMsg struct {\n\tmsgs []*processedNetworkMsg\n}\n\n// Size returns the \"size\" of an entry. We return the number of items as we\n// just want to limit the total amount of entries rather than do accurate size\n// accounting.",
      "length": 196,
      "tokens": 36,
      "embedding": []
    },
    {
      "slug": "func (c *cachedNetworkMsg) Size() (uint64, error) {",
      "content": "func (c *cachedNetworkMsg) Size() (uint64, error) {\n\treturn uint64(len(c.msgs)), nil\n}\n\n// rejectCacheKey is the cache key that we'll use to track announcements we've\n// recently rejected.",
      "length": 132,
      "tokens": 20,
      "embedding": []
    },
    {
      "slug": "type rejectCacheKey struct {",
      "content": "type rejectCacheKey struct {\n\tpubkey [33]byte\n\tchanID uint64\n}\n\n// newRejectCacheKey returns a new cache key for the reject cache.",
      "length": 97,
      "tokens": 16,
      "embedding": []
    },
    {
      "slug": "func newRejectCacheKey(cid uint64, pub [33]byte) rejectCacheKey {",
      "content": "func newRejectCacheKey(cid uint64, pub [33]byte) rejectCacheKey {\n\tk := rejectCacheKey{\n\t\tchanID: cid,\n\t\tpubkey: pub,\n\t}\n\n\treturn k\n}\n\n// sourceToPub returns a serialized-compressed public key for use in the reject\n// cache.",
      "length": 149,
      "tokens": 25,
      "embedding": []
    },
    {
      "slug": "func sourceToPub(pk *btcec.PublicKey) [33]byte {",
      "content": "func sourceToPub(pk *btcec.PublicKey) [33]byte {\n\tvar pub [33]byte\n\tcopy(pub[:], pk.SerializeCompressed())\n\treturn pub\n}\n\n// cachedReject is the empty value used to track the value for rejects.",
      "length": 139,
      "tokens": 21,
      "embedding": []
    },
    {
      "slug": "type cachedReject struct {",
      "content": "type cachedReject struct {\n}\n\n// Size returns the \"size\" of an entry. We return 1 as we just want to limit\n// the total size.",
      "length": 95,
      "tokens": 22,
      "embedding": []
    },
    {
      "slug": "func (c *cachedReject) Size() (uint64, error) {",
      "content": "func (c *cachedReject) Size() (uint64, error) {\n\treturn 1, nil\n}\n\n// AuthenticatedGossiper is a subsystem which is responsible for receiving\n// announcements, validating them and applying the changes to router, syncing\n// lightning network with newly connected nodes, broadcasting announcements\n// after validation, negotiating the channel announcement proofs exchange and\n// handling the premature announcements. All outgoing announcements are\n// expected to be properly signed as dictated in BOLT#7, additionally, all\n// incoming message are expected to be well formed and signed. Invalid messages\n// will be rejected by this struct.",
      "length": 577,
      "tokens": 85,
      "embedding": []
    },
    {
      "slug": "type AuthenticatedGossiper struct {",
      "content": "type AuthenticatedGossiper struct {\n\t// Parameters which are needed to properly handle the start and stop of\n\t// the service.\n\tstarted sync.Once\n\tstopped sync.Once\n\n\t// bestHeight is the height of the block at the tip of the main chain\n\t// as we know it. Accesses *MUST* be done with the gossiper's lock\n\t// held.\n\tbestHeight uint32\n\n\tquit chan struct{}\n\twg   sync.WaitGroup\n\n\t// cfg is a copy of the configuration struct that the gossiper service\n\t// was initialized with.\n\tcfg *Config\n\n\t// blockEpochs encapsulates a stream of block epochs that are sent at\n\t// every new block height.\n\tblockEpochs *chainntnfs.BlockEpochEvent\n\n\t// prematureChannelUpdates is a map of ChannelUpdates we have received\n\t// that wasn't associated with any channel we know about.  We store\n\t// them temporarily, such that we can reprocess them when a\n\t// ChannelAnnouncement for the channel is received.\n\tprematureChannelUpdates *lru.Cache[uint64, *cachedNetworkMsg]\n\n\t// networkMsgs is a channel that carries new network broadcasted\n\t// message from outside the gossiper service to be processed by the\n\t// networkHandler.\n\tnetworkMsgs chan *networkMsg\n\n\t// futureMsgs is a list of premature network messages that have a block\n\t// height specified in the future. We will save them and resend it to\n\t// the chan networkMsgs once the block height has reached. The cached\n\t// map format is,\n\t//   {blockHeight: [msg1, msg2, ...], ...}\n\tfutureMsgs *lru.Cache[uint32, *cachedNetworkMsg]\n\n\t// chanPolicyUpdates is a channel that requests to update the\n\t// forwarding policy of a set of channels is sent over.\n\tchanPolicyUpdates chan *chanPolicyUpdateRequest\n\n\t// selfKey is the identity public key of the backing Lightning node.\n\tselfKey *btcec.PublicKey\n\n\t// selfKeyLoc is the locator for the identity public key of the backing\n\t// Lightning node.\n\tselfKeyLoc keychain.KeyLocator\n\n\t// channelMtx is used to restrict the database access to one\n\t// goroutine per channel ID. This is done to ensure that when\n\t// the gossiper is handling an announcement, the db state stays\n\t// consistent between when the DB is first read until it's written.\n\tchannelMtx *multimutex.Mutex\n\n\trecentRejects *lru.Cache[rejectCacheKey, *cachedReject]\n\n\t// syncMgr is a subsystem responsible for managing the gossip syncers\n\t// for peers currently connected. When a new peer is connected, the\n\t// manager will create its accompanying gossip syncer and determine\n\t// whether it should have an activeSync or passiveSync sync type based\n\t// on how many other gossip syncers are currently active. Any activeSync\n\t// gossip syncers are started in a round-robin manner to ensure we're\n\t// not syncing with multiple peers at the same time.\n\tsyncMgr *SyncManager\n\n\t// reliableSender is a subsystem responsible for handling reliable\n\t// message send requests to peers. This should only be used for channels\n\t// that are unadvertised at the time of handling the message since if it\n\t// is advertised, then peers should be able to get the message from the\n\t// network.\n\treliableSender *reliableSender\n\n\t// chanUpdateRateLimiter contains rate limiters for each direction of\n\t// a channel update we've processed. We'll use these to determine\n\t// whether we should accept a new update for a specific channel and\n\t// direction.\n\t//\n\t// NOTE: This map must be synchronized with the main\n\t// AuthenticatedGossiper lock.\n\tchanUpdateRateLimiter map[uint64][2]*rate.Limiter\n\n\tsync.Mutex\n}\n\n// New creates a new AuthenticatedGossiper instance, initialized with the\n// passed configuration parameters.",
      "length": 3407,
      "tokens": 526,
      "embedding": []
    },
    {
      "slug": "func New(cfg Config, selfKeyDesc *keychain.KeyDescriptor) *AuthenticatedGossiper {",
      "content": "func New(cfg Config, selfKeyDesc *keychain.KeyDescriptor) *AuthenticatedGossiper {\n\tgossiper := &AuthenticatedGossiper{\n\t\tselfKey:     selfKeyDesc.PubKey,\n\t\tselfKeyLoc:  selfKeyDesc.KeyLocator,\n\t\tcfg:         &cfg,\n\t\tnetworkMsgs: make(chan *networkMsg),\n\t\tfutureMsgs: lru.NewCache[uint32, *cachedNetworkMsg](\n\t\t\tmaxPrematureUpdates,\n\t\t),\n\t\tquit:              make(chan struct{}),\n\t\tchanPolicyUpdates: make(chan *chanPolicyUpdateRequest),\n\t\tprematureChannelUpdates: lru.NewCache[uint64, *cachedNetworkMsg]( //nolint: lll\n\t\t\tmaxPrematureUpdates,\n\t\t),\n\t\tchannelMtx: multimutex.NewMutex(),\n\t\trecentRejects: lru.NewCache[rejectCacheKey, *cachedReject](\n\t\t\tmaxRejectedUpdates,\n\t\t),\n\t\tchanUpdateRateLimiter: make(map[uint64][2]*rate.Limiter),\n\t}\n\n\tgossiper.syncMgr = newSyncManager(&SyncManagerCfg{\n\t\tChainHash:               cfg.ChainHash,\n\t\tChanSeries:              cfg.ChanSeries,\n\t\tRotateTicker:            cfg.RotateTicker,\n\t\tHistoricalSyncTicker:    cfg.HistoricalSyncTicker,\n\t\tNumActiveSyncers:        cfg.NumActiveSyncers,\n\t\tIgnoreHistoricalFilters: cfg.IgnoreHistoricalFilters,\n\t\tBestHeight:              gossiper.latestHeight,\n\t\tPinnedSyncers:           cfg.PinnedSyncers,\n\t})\n\n\tgossiper.reliableSender = newReliableSender(&reliableSenderCfg{\n\t\tNotifyWhenOnline:  cfg.NotifyWhenOnline,\n\t\tNotifyWhenOffline: cfg.NotifyWhenOffline,\n\t\tMessageStore:      cfg.MessageStore,\n\t\tIsMsgStale:        gossiper.isMsgStale,\n\t})\n\n\treturn gossiper\n}\n\n// EdgeWithInfo contains the information that is required to update an edge.",
      "length": 1391,
      "tokens": 87,
      "embedding": []
    },
    {
      "slug": "type EdgeWithInfo struct {",
      "content": "type EdgeWithInfo struct {\n\t// Info describes the channel.\n\tInfo *channeldb.ChannelEdgeInfo\n\n\t// Edge describes the policy in one direction of the channel.\n\tEdge *channeldb.ChannelEdgePolicy\n}\n\n// PropagateChanPolicyUpdate signals the AuthenticatedGossiper to perform the\n// specified edge updates. Updates are done in two stages: first, the\n// AuthenticatedGossiper ensures the update has been committed by dependent\n// sub-systems, then it signs and broadcasts new updates to the network. A\n// mapping between outpoints and updated channel policies is returned, which is\n// used to update the forwarding policies of the underlying links.",
      "length": 600,
      "tokens": 87,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) PropagateChanPolicyUpdate(",
      "content": "func (d *AuthenticatedGossiper) PropagateChanPolicyUpdate(\n\tedgesToUpdate []EdgeWithInfo) error {\n\n\terrChan := make(chan error, 1)\n\tpolicyUpdate := &chanPolicyUpdateRequest{\n\t\tedgesToUpdate: edgesToUpdate,\n\t\terrChan:       errChan,\n\t}\n\n\tselect {\n\tcase d.chanPolicyUpdates <- policyUpdate:\n\t\terr := <-errChan\n\t\treturn err\n\tcase <-d.quit:\n\t\treturn fmt.Errorf(\"AuthenticatedGossiper shutting down\")\n\t}\n}\n\n// Start spawns network messages handler goroutine and registers on new block\n// notifications in order to properly handle the premature announcements.",
      "length": 476,
      "tokens": 58,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) Start() error {",
      "content": "func (d *AuthenticatedGossiper) Start() error {\n\tvar err error\n\td.started.Do(func() {\n\t\tlog.Info(\"Authenticated Gossiper starting\")\n\t\terr = d.start()\n\t})\n\treturn err\n}\n",
      "length": 113,
      "tokens": 15,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) start() error {",
      "content": "func (d *AuthenticatedGossiper) start() error {\n\t// First we register for new notifications of newly discovered blocks.\n\t// We do this immediately so we'll later be able to consume any/all\n\t// blocks which were discovered.\n\tblockEpochs, err := d.cfg.Notifier.RegisterBlockEpochNtfn(nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.blockEpochs = blockEpochs\n\n\theight, err := d.cfg.Router.CurrentBlockHeight()\n\tif err != nil {\n\t\treturn err\n\t}\n\td.bestHeight = height\n\n\t// Start the reliable sender. In case we had any pending messages ready\n\t// to be sent when the gossiper was last shut down, we must continue on\n\t// our quest to deliver them to their respective peers.\n\tif err := d.reliableSender.Start(); err != nil {\n\t\treturn err\n\t}\n\n\td.syncMgr.Start()\n\n\t// Start receiving blocks in its dedicated goroutine.\n\td.wg.Add(2)\n\tgo d.syncBlockHeight()\n\tgo d.networkHandler()\n\n\treturn nil\n}\n\n// syncBlockHeight syncs the best block height for the gossiper by reading\n// blockEpochs.\n//\n// NOTE: must be run as a goroutine.",
      "length": 925,
      "tokens": 148,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) syncBlockHeight() {",
      "content": "func (d *AuthenticatedGossiper) syncBlockHeight() {\n\tdefer d.wg.Done()\n\n\tfor {\n\t\tselect {\n\t\t// A new block has arrived, so we can re-process the previously\n\t\t// premature announcements.\n\t\tcase newBlock, ok := <-d.blockEpochs.Epochs:\n\t\t\t// If the channel has been closed, then this indicates\n\t\t\t// the daemon is shutting down, so we exit ourselves.\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Once a new block arrives, we update our running\n\t\t\t// track of the height of the chain tip.\n\t\t\td.Lock()\n\t\t\tblockHeight := uint32(newBlock.Height)\n\t\t\td.bestHeight = blockHeight\n\t\t\td.Unlock()\n\n\t\t\tlog.Debugf(\"New block: height=%d, hash=%s\", blockHeight,\n\t\t\t\tnewBlock.Hash)\n\n\t\t\t// Resend future messages, if any.\n\t\t\td.resendFutureMessages(blockHeight)\n\n\t\tcase <-d.quit:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// resendFutureMessages takes a block height, resends all the future messages\n// found at that height and deletes those messages found in the gossiper's\n// futureMsgs.",
      "length": 850,
      "tokens": 123,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) resendFutureMessages(height uint32) {",
      "content": "func (d *AuthenticatedGossiper) resendFutureMessages(height uint32) {\n\tresult, err := d.futureMsgs.Get(height)\n\n\t// Return early if no messages found.\n\tif err == cache.ErrElementNotFound {\n\t\treturn\n\t}\n\n\t// The error must nil, we will log an error and exit.\n\tif err != nil {\n\t\tlog.Errorf(\"Reading future messages got error: %v\", err)\n\t\treturn\n\t}\n\n\tmsgs := result.msgs\n\n\tlog.Debugf(\"Resending %d network messages at height %d\",\n\t\tlen(msgs), height)\n\n\tfor _, pMsg := range msgs {\n\t\tselect {\n\t\tcase d.networkMsgs <- pMsg.msg:\n\t\tcase <-d.quit:\n\t\t\tpMsg.msg.err <- ErrGossiperShuttingDown\n\t\t}\n\t}\n}\n\n// Stop signals any active goroutines for a graceful closure.",
      "length": 556,
      "tokens": 87,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) Stop() error {",
      "content": "func (d *AuthenticatedGossiper) Stop() error {\n\td.stopped.Do(func() {\n\t\tlog.Info(\"Authenticated gossiper shutting down\")\n\t\td.stop()\n\t})\n\treturn nil\n}\n",
      "length": 97,
      "tokens": 11,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) stop() {",
      "content": "func (d *AuthenticatedGossiper) stop() {\n\tlog.Info(\"Authenticated Gossiper is stopping\")\n\tdefer log.Info(\"Authenticated Gossiper stopped\")\n\n\td.blockEpochs.Cancel()\n\n\td.syncMgr.Stop()\n\n\tclose(d.quit)\n\td.wg.Wait()\n\n\t// We'll stop our reliable sender after all of the gossiper's goroutines\n\t// have exited to ensure nothing can cause it to continue executing.\n\td.reliableSender.Stop()\n}\n\n// TODO(roasbeef): need method to get current gossip timestamp?\n//  * using mtx, check time rotate forward is needed?\n\n// ProcessRemoteAnnouncement sends a new remote announcement message along with\n// the peer that sent the routing message. The announcement will be processed\n// then added to a queue for batched trickled announcement to all connected\n// peers.  Remote channel announcements should contain the announcement proof\n// and be fully validated.",
      "length": 779,
      "tokens": 108,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) ProcessRemoteAnnouncement(msg lnwire.Message,",
      "content": "func (d *AuthenticatedGossiper) ProcessRemoteAnnouncement(msg lnwire.Message,\n\tpeer lnpeer.Peer) chan error {\n\n\terrChan := make(chan error, 1)\n\n\t// For messages in the known set of channel series queries, we'll\n\t// dispatch the message directly to the GossipSyncer, and skip the main\n\t// processing loop.\n\tswitch m := msg.(type) {\n\tcase *lnwire.QueryShortChanIDs,\n\t\t*lnwire.QueryChannelRange,\n\t\t*lnwire.ReplyChannelRange,\n\t\t*lnwire.ReplyShortChanIDsEnd:\n\n\t\tsyncer, ok := d.syncMgr.GossipSyncer(peer.PubKey())\n\t\tif !ok {\n\t\t\tlog.Warnf(\"Gossip syncer for peer=%x not found\",\n\t\t\t\tpeer.PubKey())\n\n\t\t\terrChan <- ErrGossipSyncerNotFound\n\t\t\treturn errChan\n\t\t}\n\n\t\t// If we've found the message target, then we'll dispatch the\n\t\t// message directly to it.\n\t\tsyncer.ProcessQueryMsg(m, peer.QuitSignal())\n\n\t\terrChan <- nil\n\t\treturn errChan\n\n\t// If a peer is updating its current update horizon, then we'll dispatch\n\t// that directly to the proper GossipSyncer.\n\tcase *lnwire.GossipTimestampRange:\n\t\tsyncer, ok := d.syncMgr.GossipSyncer(peer.PubKey())\n\t\tif !ok {\n\t\t\tlog.Warnf(\"Gossip syncer for peer=%x not found\",\n\t\t\t\tpeer.PubKey())\n\n\t\t\terrChan <- ErrGossipSyncerNotFound\n\t\t\treturn errChan\n\t\t}\n\n\t\t// If we've found the message target, then we'll dispatch the\n\t\t// message directly to it.\n\t\tif err := syncer.ApplyGossipFilter(m); err != nil {\n\t\t\tlog.Warnf(\"Unable to apply gossip filter for peer=%x: \"+\n\t\t\t\t\"%v\", peer.PubKey(), err)\n\n\t\t\terrChan <- err\n\t\t\treturn errChan\n\t\t}\n\n\t\terrChan <- nil\n\t\treturn errChan\n\n\t// To avoid inserting edges in the graph for our own channels that we\n\t// have already closed, we ignore such channel announcements coming\n\t// from the remote.\n\tcase *lnwire.ChannelAnnouncement:\n\t\townKey := d.selfKey.SerializeCompressed()\n\t\townErr := fmt.Errorf(\"ignoring remote ChannelAnnouncement \" +\n\t\t\t\"for own channel\")\n\n\t\tif bytes.Equal(m.NodeID1[:], ownKey) ||\n\t\t\tbytes.Equal(m.NodeID2[:], ownKey) {\n\n\t\t\tlog.Warn(ownErr)\n\t\t\terrChan <- ownErr\n\t\t\treturn errChan\n\t\t}\n\t}\n\n\tnMsg := &networkMsg{\n\t\tmsg:      msg,\n\t\tisRemote: true,\n\t\tpeer:     peer,\n\t\tsource:   peer.IdentityKey(),\n\t\terr:      errChan,\n\t}\n\n\tselect {\n\tcase d.networkMsgs <- nMsg:\n\n\t// If the peer that sent us this error is quitting, then we don't need\n\t// to send back an error and can return immediately.\n\tcase <-peer.QuitSignal():\n\t\treturn nil\n\tcase <-d.quit:\n\t\tnMsg.err <- ErrGossiperShuttingDown\n\t}\n\n\treturn nMsg.err\n}\n\n// ProcessLocalAnnouncement sends a new remote announcement message along with\n// the peer that sent the routing message. The announcement will be processed\n// then added to a queue for batched trickled announcement to all connected\n// peers.  Local channel announcements don't contain the announcement proof and\n// will not be fully validated. Once the channel proofs are received, the\n// entire channel announcement and update messages will be re-constructed and\n// broadcast to the rest of the network.",
      "length": 2717,
      "tokens": 373,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) ProcessLocalAnnouncement(msg lnwire.Message,",
      "content": "func (d *AuthenticatedGossiper) ProcessLocalAnnouncement(msg lnwire.Message,\n\toptionalFields ...OptionalMsgField) chan error {\n\n\toptionalMsgFields := &optionalMsgFields{}\n\toptionalMsgFields.apply(optionalFields...)\n\n\tnMsg := &networkMsg{\n\t\tmsg:               msg,\n\t\toptionalMsgFields: optionalMsgFields,\n\t\tisRemote:          false,\n\t\tsource:            d.selfKey,\n\t\terr:               make(chan error, 1),\n\t}\n\n\tselect {\n\tcase d.networkMsgs <- nMsg:\n\tcase <-d.quit:\n\t\tnMsg.err <- ErrGossiperShuttingDown\n\t}\n\n\treturn nMsg.err\n}\n\n// channelUpdateID is a unique identifier for ChannelUpdate messages, as\n// channel updates can be identified by the (ShortChannelID, ChannelFlags)\n// tuple.",
      "length": 583,
      "tokens": 62,
      "embedding": []
    },
    {
      "slug": "type channelUpdateID struct {",
      "content": "type channelUpdateID struct {\n\t// channelID represents the set of data which is needed to\n\t// retrieve all necessary data to validate the channel existence.\n\tchannelID lnwire.ShortChannelID\n\n\t// Flags least-significant bit must be set to 0 if the creating node\n\t// corresponds to the first node in the previously sent channel\n\t// announcement and 1 otherwise.\n\tflags lnwire.ChanUpdateChanFlags\n}\n\n// msgWithSenders is a wrapper struct around a message, and the set of peers\n// that originally sent us this message. Using this struct, we can ensure that\n// we don't re-send a message to the peer that sent it to us in the first\n// place.",
      "length": 593,
      "tokens": 102,
      "embedding": []
    },
    {
      "slug": "type msgWithSenders struct {",
      "content": "type msgWithSenders struct {\n\t// msg is the wire message itself.\n\tmsg lnwire.Message\n\n\t// isLocal is true if this was a message that originated locally. We'll\n\t// use this to bypass our normal checks to ensure we prioritize sending\n\t// out our own updates.\n\tisLocal bool\n\n\t// sender is the set of peers that sent us this message.\n\tsenders map[route.Vertex]struct{}\n}\n\n// mergeSyncerMap is used to merge the set of senders of a particular message\n// with peers that we have an active GossipSyncer with. We do this to ensure\n// that we don't broadcast messages to any peers that we have active gossip\n// syncers for.",
      "length": 570,
      "tokens": 103,
      "embedding": []
    },
    {
      "slug": "func (m *msgWithSenders) mergeSyncerMap(syncers map[route.Vertex]*GossipSyncer) {",
      "content": "func (m *msgWithSenders) mergeSyncerMap(syncers map[route.Vertex]*GossipSyncer) {\n\tfor peerPub := range syncers {\n\t\tm.senders[peerPub] = struct{}{}\n\t}\n}\n\n// deDupedAnnouncements de-duplicates announcements that have been added to the\n// batch. Internally, announcements are stored in three maps\n// (one each for channel announcements, channel updates, and node\n// announcements). These maps keep track of unique announcements and ensure no\n// announcements are duplicated. We keep the three message types separate, such\n// that we can send channel announcements first, then channel updates, and\n// finally node announcements when it's time to broadcast them.",
      "length": 565,
      "tokens": 86,
      "embedding": []
    },
    {
      "slug": "type deDupedAnnouncements struct {",
      "content": "type deDupedAnnouncements struct {\n\t// channelAnnouncements are identified by the short channel id field.\n\tchannelAnnouncements map[lnwire.ShortChannelID]msgWithSenders\n\n\t// channelUpdates are identified by the channel update id field.\n\tchannelUpdates map[channelUpdateID]msgWithSenders\n\n\t// nodeAnnouncements are identified by the Vertex field.\n\tnodeAnnouncements map[route.Vertex]msgWithSenders\n\n\tsync.Mutex\n}\n\n// Reset operates on deDupedAnnouncements to reset the storage of\n// announcements.",
      "length": 448,
      "tokens": 48,
      "embedding": []
    },
    {
      "slug": "func (d *deDupedAnnouncements) Reset() {",
      "content": "func (d *deDupedAnnouncements) Reset() {\n\td.Lock()\n\tdefer d.Unlock()\n\n\td.reset()\n}\n\n// reset is the private version of the Reset method. We have this so we can\n// call this method within method that are already holding the lock.",
      "length": 180,
      "tokens": 33,
      "embedding": []
    },
    {
      "slug": "func (d *deDupedAnnouncements) reset() {",
      "content": "func (d *deDupedAnnouncements) reset() {\n\t// Storage of each type of announcement (channel announcements, channel\n\t// updates, node announcements) is set to an empty map where the\n\t// appropriate key points to the corresponding lnwire.Message.\n\td.channelAnnouncements = make(map[lnwire.ShortChannelID]msgWithSenders)\n\td.channelUpdates = make(map[channelUpdateID]msgWithSenders)\n\td.nodeAnnouncements = make(map[route.Vertex]msgWithSenders)\n}\n\n// addMsg adds a new message to the current batch. If the message is already\n// present in the current batch, then this new instance replaces the latter,\n// and the set of senders is updated to reflect which node sent us this\n// message.",
      "length": 627,
      "tokens": 85,
      "embedding": []
    },
    {
      "slug": "func (d *deDupedAnnouncements) addMsg(message networkMsg) {",
      "content": "func (d *deDupedAnnouncements) addMsg(message networkMsg) {\n\tlog.Tracef(\"Adding network message: %v to batch\", message.msg.MsgType())\n\n\t// Depending on the message type (channel announcement, channel update,\n\t// or node announcement), the message is added to the corresponding map\n\t// in deDupedAnnouncements. Because each identifying key can have at\n\t// most one value, the announcements are de-duplicated, with newer ones\n\t// replacing older ones.\n\tswitch msg := message.msg.(type) {\n\n\t// Channel announcements are identified by the short channel id field.\n\tcase *lnwire.ChannelAnnouncement:\n\t\tdeDupKey := msg.ShortChannelID\n\t\tsender := route.NewVertex(message.source)\n\n\t\tmws, ok := d.channelAnnouncements[deDupKey]\n\t\tif !ok {\n\t\t\tmws = msgWithSenders{\n\t\t\t\tmsg:     msg,\n\t\t\t\tisLocal: !message.isRemote,\n\t\t\t\tsenders: make(map[route.Vertex]struct{}),\n\t\t\t}\n\t\t\tmws.senders[sender] = struct{}{}\n\n\t\t\td.channelAnnouncements[deDupKey] = mws\n\n\t\t\treturn\n\t\t}\n\n\t\tmws.msg = msg\n\t\tmws.senders[sender] = struct{}{}\n\t\td.channelAnnouncements[deDupKey] = mws\n\n\t// Channel updates are identified by the (short channel id,\n\t// channelflags) tuple.\n\tcase *lnwire.ChannelUpdate:\n\t\tsender := route.NewVertex(message.source)\n\t\tdeDupKey := channelUpdateID{\n\t\t\tmsg.ShortChannelID,\n\t\t\tmsg.ChannelFlags,\n\t\t}\n\n\t\toldTimestamp := uint32(0)\n\t\tmws, ok := d.channelUpdates[deDupKey]\n\t\tif ok {\n\t\t\t// If we already have seen this message, record its\n\t\t\t// timestamp.\n\t\t\toldTimestamp = mws.msg.(*lnwire.ChannelUpdate).Timestamp\n\t\t}\n\n\t\t// If we already had this message with a strictly newer\n\t\t// timestamp, then we'll just discard the message we got.\n\t\tif oldTimestamp > msg.Timestamp {\n\t\t\tlog.Debugf(\"Ignored outdated network message: \"+\n\t\t\t\t\"peer=%v, msg=%s\", message.peer, msg.MsgType())\n\t\t\treturn\n\t\t}\n\n\t\t// If the message we just got is newer than what we previously\n\t\t// have seen, or this is the first time we see it, then we'll\n\t\t// add it to our map of announcements.\n\t\tif oldTimestamp < msg.Timestamp {\n\t\t\tmws = msgWithSenders{\n\t\t\t\tmsg:     msg,\n\t\t\t\tisLocal: !message.isRemote,\n\t\t\t\tsenders: make(map[route.Vertex]struct{}),\n\t\t\t}\n\n\t\t\t// We'll mark the sender of the message in the\n\t\t\t// senders map.\n\t\t\tmws.senders[sender] = struct{}{}\n\n\t\t\td.channelUpdates[deDupKey] = mws\n\n\t\t\treturn\n\t\t}\n\n\t\t// Lastly, if we had seen this exact message from before, with\n\t\t// the same timestamp, we'll add the sender to the map of\n\t\t// senders, such that we can skip sending this message back in\n\t\t// the next batch.\n\t\tmws.msg = msg\n\t\tmws.senders[sender] = struct{}{}\n\t\td.channelUpdates[deDupKey] = mws\n\n\t// Node announcements are identified by the Vertex field.  Use the\n\t// NodeID to create the corresponding Vertex.\n\tcase *lnwire.NodeAnnouncement:\n\t\tsender := route.NewVertex(message.source)\n\t\tdeDupKey := route.Vertex(msg.NodeID)\n\n\t\t// We do the same for node announcements as we did for channel\n\t\t// updates, as they also carry a timestamp.\n\t\toldTimestamp := uint32(0)\n\t\tmws, ok := d.nodeAnnouncements[deDupKey]\n\t\tif ok {\n\t\t\toldTimestamp = mws.msg.(*lnwire.NodeAnnouncement).Timestamp\n\t\t}\n\n\t\t// Discard the message if it's old.\n\t\tif oldTimestamp > msg.Timestamp {\n\t\t\treturn\n\t\t}\n\n\t\t// Replace if it's newer.\n\t\tif oldTimestamp < msg.Timestamp {\n\t\t\tmws = msgWithSenders{\n\t\t\t\tmsg:     msg,\n\t\t\t\tisLocal: !message.isRemote,\n\t\t\t\tsenders: make(map[route.Vertex]struct{}),\n\t\t\t}\n\n\t\t\tmws.senders[sender] = struct{}{}\n\n\t\t\td.nodeAnnouncements[deDupKey] = mws\n\n\t\t\treturn\n\t\t}\n\n\t\t// Add to senders map if it's the same as we had.\n\t\tmws.msg = msg\n\t\tmws.senders[sender] = struct{}{}\n\t\td.nodeAnnouncements[deDupKey] = mws\n\t}\n}\n\n// AddMsgs is a helper method to add multiple messages to the announcement\n// batch.",
      "length": 3467,
      "tokens": 460,
      "embedding": []
    },
    {
      "slug": "func (d *deDupedAnnouncements) AddMsgs(msgs ...networkMsg) {",
      "content": "func (d *deDupedAnnouncements) AddMsgs(msgs ...networkMsg) {\n\td.Lock()\n\tdefer d.Unlock()\n\n\tfor _, msg := range msgs {\n\t\td.addMsg(msg)\n\t}\n}\n\n// msgsToBroadcast is returned by Emit() and partitions the messages we'd like\n// to broadcast next into messages that are locally sourced and those that are\n// sourced remotely.",
      "length": 247,
      "tokens": 42,
      "embedding": []
    },
    {
      "slug": "type msgsToBroadcast struct {",
      "content": "type msgsToBroadcast struct {\n\t// localMsgs is the set of messages we created locally.\n\tlocalMsgs []msgWithSenders\n\n\t// remoteMsgs is the set of messages that we received from a remote\n\t// party.\n\tremoteMsgs []msgWithSenders\n}\n\n// addMsg adds a new message to the appropriate sub-slice.",
      "length": 248,
      "tokens": 40,
      "embedding": []
    },
    {
      "slug": "func (m *msgsToBroadcast) addMsg(msg msgWithSenders) {",
      "content": "func (m *msgsToBroadcast) addMsg(msg msgWithSenders) {\n\tif msg.isLocal {\n\t\tm.localMsgs = append(m.localMsgs, msg)\n\t} else {\n\t\tm.remoteMsgs = append(m.remoteMsgs, msg)\n\t}\n}\n\n// isEmpty returns true if the batch is empty.",
      "length": 157,
      "tokens": 25,
      "embedding": []
    },
    {
      "slug": "func (m *msgsToBroadcast) isEmpty() bool {",
      "content": "func (m *msgsToBroadcast) isEmpty() bool {\n\treturn len(m.localMsgs) == 0 && len(m.remoteMsgs) == 0\n}\n\n// length returns the length of the combined message set.",
      "length": 113,
      "tokens": 19,
      "embedding": []
    },
    {
      "slug": "func (m *msgsToBroadcast) length() int {",
      "content": "func (m *msgsToBroadcast) length() int {\n\treturn len(m.localMsgs) + len(m.remoteMsgs)\n}\n\n// Emit returns the set of de-duplicated announcements to be sent out during\n// the next announcement epoch, in the order of channel announcements, channel\n// updates, and node announcements. Each message emitted, contains the set of\n// peers that sent us the message. This way, we can ensure that we don't waste\n// bandwidth by re-sending a message to the peer that sent it to us in the\n// first place. Additionally, the set of stored messages are reset.",
      "length": 495,
      "tokens": 85,
      "embedding": []
    },
    {
      "slug": "func (d *deDupedAnnouncements) Emit() msgsToBroadcast {",
      "content": "func (d *deDupedAnnouncements) Emit() msgsToBroadcast {\n\td.Lock()\n\tdefer d.Unlock()\n\n\t// Get the total number of announcements.\n\tnumAnnouncements := len(d.channelAnnouncements) + len(d.channelUpdates) +\n\t\tlen(d.nodeAnnouncements)\n\n\t// Create an empty array of lnwire.Messages with a length equal to\n\t// the total number of announcements.\n\tmsgs := msgsToBroadcast{\n\t\tlocalMsgs:  make([]msgWithSenders, 0, numAnnouncements),\n\t\tremoteMsgs: make([]msgWithSenders, 0, numAnnouncements),\n\t}\n\n\t// Add the channel announcements to the array first.\n\tfor _, message := range d.channelAnnouncements {\n\t\tmsgs.addMsg(message)\n\t}\n\n\t// Then add the channel updates.\n\tfor _, message := range d.channelUpdates {\n\t\tmsgs.addMsg(message)\n\t}\n\n\t// Finally add the node announcements.\n\tfor _, message := range d.nodeAnnouncements {\n\t\tmsgs.addMsg(message)\n\t}\n\n\td.reset()\n\n\t// Return the array of lnwire.messages.\n\treturn msgs\n}\n\n// calculateSubBatchSize is a helper function that calculates the size to break\n// down the batchSize into.",
      "length": 920,
      "tokens": 122,
      "embedding": []
    },
    {
      "slug": "func calculateSubBatchSize(totalDelay, subBatchDelay time.Duration,",
      "content": "func calculateSubBatchSize(totalDelay, subBatchDelay time.Duration,\n\tminimumBatchSize, batchSize int) int {\n\tif subBatchDelay > totalDelay {\n\t\treturn batchSize\n\t}\n\n\tsubBatchSize := (batchSize*int(subBatchDelay) +\n\t\tint(totalDelay) - 1) / int(totalDelay)\n\n\tif subBatchSize < minimumBatchSize {\n\t\treturn minimumBatchSize\n\t}\n\n\treturn subBatchSize\n}\n\n// batchSizeCalculator maps to the function `calculateSubBatchSize`. We create\n// this variable so the function can be mocked in our test.\nvar batchSizeCalculator = calculateSubBatchSize\n\n// splitAnnouncementBatches takes an exiting list of announcements and\n// decomposes it into sub batches controlled by the `subBatchSize`.",
      "length": 585,
      "tokens": 77,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) splitAnnouncementBatches(",
      "content": "func (d *AuthenticatedGossiper) splitAnnouncementBatches(\n\tannouncementBatch []msgWithSenders) [][]msgWithSenders {\n\n\tsubBatchSize := batchSizeCalculator(\n\t\td.cfg.TrickleDelay, d.cfg.SubBatchDelay,\n\t\td.cfg.MinimumBatchSize, len(announcementBatch),\n\t)\n\n\tvar splitAnnouncementBatch [][]msgWithSenders\n\n\tfor subBatchSize < len(announcementBatch) {\n\t\t// For slicing with minimal allocation\n\t\t// https://github.com/golang/go/wiki/SliceTricks\n\t\tannouncementBatch, splitAnnouncementBatch =\n\t\t\tannouncementBatch[subBatchSize:],\n\t\t\tappend(splitAnnouncementBatch,\n\t\t\t\tannouncementBatch[0:subBatchSize:subBatchSize])\n\t}\n\tsplitAnnouncementBatch = append(\n\t\tsplitAnnouncementBatch, announcementBatch,\n\t)\n\n\treturn splitAnnouncementBatch\n}\n\n// splitAndSendAnnBatch takes a batch of messages, computes the proper batch\n// split size, and then sends out all items to the set of target peers. Locally\n// generated announcements are always sent before remotely generated\n// announcements.",
      "length": 884,
      "tokens": 82,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) splitAndSendAnnBatch(",
      "content": "func (d *AuthenticatedGossiper) splitAndSendAnnBatch(\n\tannBatch msgsToBroadcast) {\n\n\t// delayNextBatch is a helper closure that blocks for `SubBatchDelay`\n\t// duration to delay the sending of next announcement batch.\n\tdelayNextBatch := func() {\n\t\tselect {\n\t\tcase <-time.After(d.cfg.SubBatchDelay):\n\t\tcase <-d.quit:\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Fetch the local and remote announcements.\n\tlocalBatches := d.splitAnnouncementBatches(annBatch.localMsgs)\n\tremoteBatches := d.splitAnnouncementBatches(annBatch.remoteMsgs)\n\n\td.wg.Add(1)\n\tgo func() {\n\t\tdefer d.wg.Done()\n\n\t\tlog.Debugf(\"Broadcasting %v new local announcements in %d \"+\n\t\t\t\"sub batches\", len(annBatch.localMsgs),\n\t\t\tlen(localBatches))\n\n\t\t// Send out the local announcements first.\n\t\tfor _, annBatch := range localBatches {\n\t\t\td.sendLocalBatch(annBatch)\n\t\t\tdelayNextBatch()\n\t\t}\n\n\t\tlog.Debugf(\"Broadcasting %v new remote announcements in %d \"+\n\t\t\t\"sub batches\", len(annBatch.remoteMsgs),\n\t\t\tlen(remoteBatches))\n\n\t\t// Now send the remote announcements.\n\t\tfor _, annBatch := range remoteBatches {\n\t\t\td.sendRemoteBatch(annBatch)\n\t\t\tdelayNextBatch()\n\t\t}\n\t}()\n}\n\n// sendLocalBatch broadcasts a list of locally generated announcements to our\n// peers. For local announcements, we skip the filter and dedup logic and just\n// send the announcements out to all our coonnected peers.",
      "length": 1223,
      "tokens": 149,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) sendLocalBatch(annBatch []msgWithSenders) {",
      "content": "func (d *AuthenticatedGossiper) sendLocalBatch(annBatch []msgWithSenders) {\n\tmsgsToSend := lnutils.Map(\n\t\tannBatch, func(m msgWithSenders) lnwire.Message {\n\t\t\treturn m.msg\n\t\t},\n\t)\n\n\terr := d.cfg.Broadcast(nil, msgsToSend...)\n\tif err != nil {\n\t\tlog.Errorf(\"Unable to send local batch announcements: %v\", err)\n\t}\n}\n\n// sendRemoteBatch broadcasts a list of remotely generated announcements to our\n// peers.",
      "length": 314,
      "tokens": 44,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) sendRemoteBatch(annBatch []msgWithSenders) {",
      "content": "func (d *AuthenticatedGossiper) sendRemoteBatch(annBatch []msgWithSenders) {\n\tsyncerPeers := d.syncMgr.GossipSyncers()\n\n\t// We'll first attempt to filter out this new message for all peers\n\t// that have active gossip syncers active.\n\tfor _, syncer := range syncerPeers {\n\t\tsyncer.FilterGossipMsgs(annBatch...)\n\t}\n\n\tfor _, msgChunk := range annBatch {\n\t\tmsgChunk := msgChunk\n\n\t\t// With the syncers taken care of, we'll merge the sender map\n\t\t// with the set of syncers, so we don't send out duplicate\n\t\t// messages.\n\t\tmsgChunk.mergeSyncerMap(syncerPeers)\n\n\t\terr := d.cfg.Broadcast(msgChunk.senders, msgChunk.msg)\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"Unable to send batch \"+\n\t\t\t\t\"announcements: %v\", err)\n\t\t\tcontinue\n\t\t}\n\t}\n}\n\n// networkHandler is the primary goroutine that drives this service. The roles\n// of this goroutine includes answering queries related to the state of the\n// network, syncing up newly connected peers, and also periodically\n// broadcasting our latest topology state to all connected peers.\n//\n// NOTE: This MUST be run as a goroutine.",
      "length": 947,
      "tokens": 145,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) networkHandler() {",
      "content": "func (d *AuthenticatedGossiper) networkHandler() {\n\tdefer d.wg.Done()\n\n\t// Initialize empty deDupedAnnouncements to store announcement batch.\n\tannouncements := deDupedAnnouncements{}\n\tannouncements.Reset()\n\n\td.cfg.RetransmitTicker.Resume()\n\tdefer d.cfg.RetransmitTicker.Stop()\n\n\ttrickleTimer := time.NewTicker(d.cfg.TrickleDelay)\n\tdefer trickleTimer.Stop()\n\n\t// To start, we'll first check to see if there are any stale channel or\n\t// node announcements that we need to re-transmit.\n\tif err := d.retransmitStaleAnns(time.Now()); err != nil {\n\t\tlog.Errorf(\"Unable to rebroadcast stale announcements: %v\", err)\n\t}\n\n\t// We'll use this validation to ensure that we process jobs in their\n\t// dependency order during parallel validation.\n\tvalidationBarrier := routing.NewValidationBarrier(1000, d.quit)\n\n\tfor {\n\t\tselect {\n\t\t// A new policy update has arrived. We'll commit it to the\n\t\t// sub-systems below us, then craft, sign, and broadcast a new\n\t\t// ChannelUpdate for the set of affected clients.\n\t\tcase policyUpdate := <-d.chanPolicyUpdates:\n\t\t\tlog.Tracef(\"Received channel %d policy update requests\",\n\t\t\t\tlen(policyUpdate.edgesToUpdate))\n\n\t\t\t// First, we'll now create new fully signed updates for\n\t\t\t// the affected channels and also update the underlying\n\t\t\t// graph with the new state.\n\t\t\tnewChanUpdates, err := d.processChanPolicyUpdate(\n\t\t\t\tpolicyUpdate.edgesToUpdate,\n\t\t\t)\n\t\t\tpolicyUpdate.errChan <- err\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"Unable to craft policy updates: %v\",\n\t\t\t\t\terr)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Finally, with the updates committed, we'll now add\n\t\t\t// them to the announcement batch to be flushed at the\n\t\t\t// start of the next epoch.\n\t\t\tannouncements.AddMsgs(newChanUpdates...)\n\n\t\tcase announcement := <-d.networkMsgs:\n\t\t\tlog.Tracef(\"Received network message: \"+\n\t\t\t\t\"peer=%v, msg=%s, is_remote=%v\",\n\t\t\t\tannouncement.peer, announcement.msg.MsgType(),\n\t\t\t\tannouncement.isRemote)\n\n\t\t\tswitch announcement.msg.(type) {\n\t\t\t// Channel announcement signatures are amongst the only\n\t\t\t// messages that we'll process serially.\n\t\t\tcase *lnwire.AnnounceSignatures:\n\t\t\t\temittedAnnouncements, _ := d.processNetworkAnnouncement(\n\t\t\t\t\tannouncement,\n\t\t\t\t)\n\t\t\t\tlog.Debugf(\"Processed network message %s, \"+\n\t\t\t\t\t\"returned len(announcements)=%v\",\n\t\t\t\t\tannouncement.msg.MsgType(),\n\t\t\t\t\tlen(emittedAnnouncements))\n\n\t\t\t\tif emittedAnnouncements != nil {\n\t\t\t\t\tannouncements.AddMsgs(\n\t\t\t\t\t\temittedAnnouncements...,\n\t\t\t\t\t)\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// If this message was recently rejected, then we won't\n\t\t\t// attempt to re-process it.\n\t\t\tif announcement.isRemote && d.isRecentlyRejectedMsg(\n\t\t\t\tannouncement.msg,\n\t\t\t\tsourceToPub(announcement.source),\n\t\t\t) {\n\n\t\t\t\tannouncement.err <- fmt.Errorf(\"recently \" +\n\t\t\t\t\t\"rejected\")\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// We'll set up any dependent, and wait until a free\n\t\t\t// slot for this job opens up, this allow us to not\n\t\t\t// have thousands of goroutines active.\n\t\t\tvalidationBarrier.InitJobDependencies(announcement.msg)\n\n\t\t\td.wg.Add(1)\n\t\t\tgo d.handleNetworkMessages(\n\t\t\t\tannouncement, &announcements, validationBarrier,\n\t\t\t)\n\n\t\t// The trickle timer has ticked, which indicates we should\n\t\t// flush to the network the pending batch of new announcements\n\t\t// we've received since the last trickle tick.\n\t\tcase <-trickleTimer.C:\n\t\t\t// Emit the current batch of announcements from\n\t\t\t// deDupedAnnouncements.\n\t\t\tannouncementBatch := announcements.Emit()\n\n\t\t\t// If the current announcements batch is nil, then we\n\t\t\t// have no further work here.\n\t\t\tif announcementBatch.isEmpty() {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// At this point, we have the set of local and remote\n\t\t\t// announcements we want to send out. We'll do the\n\t\t\t// batching as normal for both, but for local\n\t\t\t// announcements, we'll blast them out w/o regard for\n\t\t\t// our peer's policies so we ensure they propagate\n\t\t\t// properly.\n\t\t\td.splitAndSendAnnBatch(announcementBatch)\n\n\t\t// The retransmission timer has ticked which indicates that we\n\t\t// should check if we need to prune or re-broadcast any of our\n\t\t// personal channels or node announcement. This addresses the\n\t\t// case of \"zombie\" channels and channel advertisements that\n\t\t// have been dropped, or not properly propagated through the\n\t\t// network.\n\t\tcase tick := <-d.cfg.RetransmitTicker.Ticks():\n\t\t\tif err := d.retransmitStaleAnns(tick); err != nil {\n\t\t\t\tlog.Errorf(\"unable to rebroadcast stale \"+\n\t\t\t\t\t\"announcements: %v\", err)\n\t\t\t}\n\n\t\t// The gossiper has been signalled to exit, to we exit our\n\t\t// main loop so the wait group can be decremented.\n\t\tcase <-d.quit:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// handleNetworkMessages is responsible for waiting for dependencies for a\n// given network message and processing the message. Once processed, it will\n// signal its dependants and add the new announcements to the announce batch.\n//\n// NOTE: must be run as a goroutine.",
      "length": 4614,
      "tokens": 595,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) handleNetworkMessages(nMsg *networkMsg,",
      "content": "func (d *AuthenticatedGossiper) handleNetworkMessages(nMsg *networkMsg,\n\tdeDuped *deDupedAnnouncements, vb *routing.ValidationBarrier) {\n\n\tdefer d.wg.Done()\n\tdefer vb.CompleteJob()\n\n\t// We should only broadcast this message forward if it originated from\n\t// us or it wasn't received as part of our initial historical sync.\n\tshouldBroadcast := !nMsg.isRemote || d.syncMgr.IsGraphSynced()\n\n\t// If this message has an existing dependency, then we'll wait until\n\t// that has been fully validated before we proceed.\n\terr := vb.WaitForDependants(nMsg.msg)\n\tif err != nil {\n\t\tlog.Debugf(\"Validating network message %s got err: %v\",\n\t\t\tnMsg.msg.MsgType(), err)\n\n\t\tif !routing.IsError(\n\t\t\terr,\n\t\t\trouting.ErrVBarrierShuttingDown,\n\t\t\trouting.ErrParentValidationFailed,\n\t\t) {\n\n\t\t\tlog.Warnf(\"unexpected error during validation \"+\n\t\t\t\t\"barrier shutdown: %v\", err)\n\t\t}\n\t\tnMsg.err <- err\n\n\t\treturn\n\t}\n\n\t// Process the network announcement to determine if this is either a\n\t// new announcement from our PoV or an edges to a prior vertex/edge we\n\t// previously proceeded.\n\tnewAnns, allow := d.processNetworkAnnouncement(nMsg)\n\n\tlog.Tracef(\"Processed network message %s, returned \"+\n\t\t\"len(announcements)=%v, allowDependents=%v\",\n\t\tnMsg.msg.MsgType(), len(newAnns), allow)\n\n\t// If this message had any dependencies, then we can now signal them to\n\t// continue.\n\tvb.SignalDependants(nMsg.msg, allow)\n\n\t// If the announcement was accepted, then add the emitted announcements\n\t// to our announce batch to be broadcast once the trickle timer ticks\n\t// gain.\n\tif newAnns != nil && shouldBroadcast {\n\t\t// TODO(roasbeef): exclude peer that sent.\n\t\tdeDuped.AddMsgs(newAnns...)\n\t} else if newAnns != nil {\n\t\tlog.Trace(\"Skipping broadcast of announcements received \" +\n\t\t\t\"during initial graph sync\")\n\t}\n}\n\n// TODO(roasbeef): d/c peers that send updates not on our chain\n\n// InitSyncState is called by outside sub-systems when a connection is\n// established to a new peer that understands how to perform channel range\n// queries. We'll allocate a new gossip syncer for it, and start any goroutines\n// needed to handle new queries.",
      "length": 1970,
      "tokens": 276,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) InitSyncState(syncPeer lnpeer.Peer) {",
      "content": "func (d *AuthenticatedGossiper) InitSyncState(syncPeer lnpeer.Peer) {\n\td.syncMgr.InitSyncState(syncPeer)\n}\n\n// PruneSyncState is called by outside sub-systems once a peer that we were\n// previously connected to has been disconnected. In this case we can stop the\n// existing GossipSyncer assigned to the peer and free up resources.",
      "length": 256,
      "tokens": 40,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) PruneSyncState(peer route.Vertex) {",
      "content": "func (d *AuthenticatedGossiper) PruneSyncState(peer route.Vertex) {\n\td.syncMgr.PruneSyncState(peer)\n}\n\n// isRecentlyRejectedMsg returns true if we recently rejected a message, and\n// false otherwise, This avoids expensive reprocessing of the message.",
      "length": 178,
      "tokens": 23,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) isRecentlyRejectedMsg(msg lnwire.Message,",
      "content": "func (d *AuthenticatedGossiper) isRecentlyRejectedMsg(msg lnwire.Message,\n\tpeerPub [33]byte) bool {\n\n\tvar scid uint64\n\tswitch m := msg.(type) {\n\tcase *lnwire.ChannelUpdate:\n\t\tscid = m.ShortChannelID.ToUint64()\n\n\tcase *lnwire.ChannelAnnouncement:\n\t\tscid = m.ShortChannelID.ToUint64()\n\n\tdefault:\n\t\treturn false\n\t}\n\n\t_, err := d.recentRejects.Get(newRejectCacheKey(scid, peerPub))\n\treturn err != cache.ErrElementNotFound\n}\n\n// retransmitStaleAnns examines all outgoing channels that the source node is\n// known to maintain to check to see if any of them are \"stale\". A channel is\n// stale iff, the last timestamp of its rebroadcast is older than the\n// RebroadcastInterval. We also check if a refreshed node announcement should\n// be resent.",
      "length": 642,
      "tokens": 91,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) retransmitStaleAnns(now time.Time) error {",
      "content": "func (d *AuthenticatedGossiper) retransmitStaleAnns(now time.Time) error {\n\t// Iterate over all of our channels and check if any of them fall\n\t// within the prune interval or re-broadcast interval.\n\ttype updateTuple struct {\n\t\tinfo *channeldb.ChannelEdgeInfo\n\t\tedge *channeldb.ChannelEdgePolicy\n\t}\n\n\tvar (\n\t\thavePublicChannels bool\n\t\tedgesToUpdate      []updateTuple\n\t)\n\terr := d.cfg.Router.ForAllOutgoingChannels(func(\n\t\t_ kvdb.RTx,\n\t\tinfo *channeldb.ChannelEdgeInfo,\n\t\tedge *channeldb.ChannelEdgePolicy) error {\n\n\t\t// If there's no auth proof attached to this edge, it means\n\t\t// that it is a private channel not meant to be announced to\n\t\t// the greater network, so avoid sending channel updates for\n\t\t// this channel to not leak its\n\t\t// existence.\n\t\tif info.AuthProof == nil {\n\t\t\tlog.Debugf(\"Skipping retransmission of channel \"+\n\t\t\t\t\"without AuthProof: %v\", info.ChannelID)\n\t\t\treturn nil\n\t\t}\n\n\t\t// We make a note that we have at least one public channel. We\n\t\t// use this to determine whether we should send a node\n\t\t// announcement below.\n\t\thavePublicChannels = true\n\n\t\t// If this edge has a ChannelUpdate that was created before the\n\t\t// introduction of the MaxHTLC field, then we'll update this\n\t\t// edge to propagate this information in the network.\n\t\tif !edge.MessageFlags.HasMaxHtlc() {\n\t\t\t// We'll make sure we support the new max_htlc field if\n\t\t\t// not already present.\n\t\t\tedge.MessageFlags |= lnwire.ChanUpdateRequiredMaxHtlc\n\t\t\tedge.MaxHTLC = lnwire.NewMSatFromSatoshis(info.Capacity)\n\n\t\t\tedgesToUpdate = append(edgesToUpdate, updateTuple{\n\t\t\t\tinfo: info,\n\t\t\t\tedge: edge,\n\t\t\t})\n\t\t\treturn nil\n\t\t}\n\n\t\ttimeElapsed := now.Sub(edge.LastUpdate)\n\n\t\t// If it's been longer than RebroadcastInterval since we've\n\t\t// re-broadcasted the channel, add the channel to the set of\n\t\t// edges we need to update.\n\t\tif timeElapsed >= d.cfg.RebroadcastInterval {\n\t\t\tedgesToUpdate = append(edgesToUpdate, updateTuple{\n\t\t\t\tinfo: info,\n\t\t\t\tedge: edge,\n\t\t\t})\n\t\t}\n\n\t\treturn nil\n\t})\n\tif err != nil && err != channeldb.ErrGraphNoEdgesFound {\n\t\treturn fmt.Errorf(\"unable to retrieve outgoing channels: %v\",\n\t\t\terr)\n\t}\n\n\tvar signedUpdates []lnwire.Message\n\tfor _, chanToUpdate := range edgesToUpdate {\n\t\t// Re-sign and update the channel on disk and retrieve our\n\t\t// ChannelUpdate to broadcast.\n\t\tchanAnn, chanUpdate, err := d.updateChannel(\n\t\t\tchanToUpdate.info, chanToUpdate.edge,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to update channel: %v\", err)\n\t\t}\n\n\t\t// If we have a valid announcement to transmit, then we'll send\n\t\t// that along with the update.\n\t\tif chanAnn != nil {\n\t\t\tsignedUpdates = append(signedUpdates, chanAnn)\n\t\t}\n\n\t\tsignedUpdates = append(signedUpdates, chanUpdate)\n\t}\n\n\t// If we don't have any public channels, we return as we don't want to\n\t// broadcast anything that would reveal our existence.\n\tif !havePublicChannels {\n\t\treturn nil\n\t}\n\n\t// We'll also check that our NodeAnnouncement is not too old.\n\tcurrentNodeAnn, err := d.cfg.SelfNodeAnnouncement(false)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to get current node announment: %v\",\n\t\t\terr)\n\t}\n\n\ttimestamp := time.Unix(int64(currentNodeAnn.Timestamp), 0)\n\ttimeElapsed := now.Sub(timestamp)\n\n\t// If it's been a full day since we've re-broadcasted the\n\t// node announcement, refresh it and resend it.\n\tnodeAnnStr := \"\"\n\tif timeElapsed >= d.cfg.RebroadcastInterval {\n\t\tnewNodeAnn, err := d.cfg.SelfNodeAnnouncement(true)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to get refreshed node \"+\n\t\t\t\t\"announcement: %v\", err)\n\t\t}\n\n\t\tsignedUpdates = append(signedUpdates, &newNodeAnn)\n\t\tnodeAnnStr = \" and our refreshed node announcement\"\n\n\t\t// Before broadcasting the refreshed node announcement, add it\n\t\t// to our own graph.\n\t\tif err := d.addNode(&newNodeAnn); err != nil {\n\t\t\tlog.Errorf(\"Unable to add refreshed node announcement \"+\n\t\t\t\t\"to graph: %v\", err)\n\t\t}\n\t}\n\n\t// If we don't have any updates to re-broadcast, then we'll exit\n\t// early.\n\tif len(signedUpdates) == 0 {\n\t\treturn nil\n\t}\n\n\tlog.Infof(\"Retransmitting %v outgoing channels%v\",\n\t\tlen(edgesToUpdate), nodeAnnStr)\n\n\t// With all the wire announcements properly crafted, we'll broadcast\n\t// our known outgoing channels to all our immediate peers.\n\tif err := d.cfg.Broadcast(nil, signedUpdates...); err != nil {\n\t\treturn fmt.Errorf(\"unable to re-broadcast channels: %v\", err)\n\t}\n\n\treturn nil\n}\n\n// processChanPolicyUpdate generates a new set of channel updates for the\n// provided list of edges and updates the backing ChannelGraphSource.",
      "length": 4255,
      "tokens": 601,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) processChanPolicyUpdate(",
      "content": "func (d *AuthenticatedGossiper) processChanPolicyUpdate(\n\tedgesToUpdate []EdgeWithInfo) ([]networkMsg, error) {\n\n\tvar chanUpdates []networkMsg\n\tfor _, edgeInfo := range edgesToUpdate {\n\t\t// Now that we've collected all the channels we need to update,\n\t\t// we'll re-sign and update the backing ChannelGraphSource, and\n\t\t// retrieve our ChannelUpdate to broadcast.\n\t\t_, chanUpdate, err := d.updateChannel(\n\t\t\tedgeInfo.Info, edgeInfo.Edge,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// We'll avoid broadcasting any updates for private channels to\n\t\t// avoid directly giving away their existence. Instead, we'll\n\t\t// send the update directly to the remote party.\n\t\tif edgeInfo.Info.AuthProof == nil {\n\t\t\t// If AuthProof is nil and an alias was found for this\n\t\t\t// ChannelID (meaning the option-scid-alias feature was\n\t\t\t// negotiated), we'll replace the ShortChannelID in the\n\t\t\t// update with the peer's alias. We do this after\n\t\t\t// updateChannel so that the alias isn't persisted to\n\t\t\t// the database.\n\t\t\top := &edgeInfo.Info.ChannelPoint\n\t\t\tchanID := lnwire.NewChanIDFromOutPoint(op)\n\n\t\t\tvar defaultAlias lnwire.ShortChannelID\n\t\t\tfoundAlias, _ := d.cfg.GetAlias(chanID)\n\t\t\tif foundAlias != defaultAlias {\n\t\t\t\tchanUpdate.ShortChannelID = foundAlias\n\n\t\t\t\tsig, err := d.cfg.SignAliasUpdate(chanUpdate)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Errorf(\"Unable to sign alias \"+\n\t\t\t\t\t\t\"update: %v\", err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tlnSig, err := lnwire.NewSigFromSignature(sig)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Errorf(\"Unable to create sig: %v\",\n\t\t\t\t\t\terr)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tchanUpdate.Signature = lnSig\n\t\t\t}\n\n\t\t\tremotePubKey := remotePubFromChanInfo(\n\t\t\t\tedgeInfo.Info, chanUpdate.ChannelFlags,\n\t\t\t)\n\t\t\terr := d.reliableSender.sendMessage(\n\t\t\t\tchanUpdate, remotePubKey,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"Unable to reliably send %v for \"+\n\t\t\t\t\t\"channel=%v to peer=%x: %v\",\n\t\t\t\t\tchanUpdate.MsgType(),\n\t\t\t\t\tchanUpdate.ShortChannelID,\n\t\t\t\t\tremotePubKey, err)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// We set ourselves as the source of this message to indicate\n\t\t// that we shouldn't skip any peers when sending this message.\n\t\tchanUpdates = append(chanUpdates, networkMsg{\n\t\t\tsource:   d.selfKey,\n\t\t\tisRemote: false,\n\t\t\tmsg:      chanUpdate,\n\t\t})\n\t}\n\n\treturn chanUpdates, nil\n}\n\n// remotePubFromChanInfo returns the public key of the remote peer given a\n// ChannelEdgeInfo that describe a channel we have with them.",
      "length": 2256,
      "tokens": 298,
      "embedding": []
    },
    {
      "slug": "func remotePubFromChanInfo(chanInfo *channeldb.ChannelEdgeInfo,",
      "content": "func remotePubFromChanInfo(chanInfo *channeldb.ChannelEdgeInfo,\n\tchanFlags lnwire.ChanUpdateChanFlags) [33]byte {\n\n\tvar remotePubKey [33]byte\n\tswitch {\n\tcase chanFlags&lnwire.ChanUpdateDirection == 0:\n\t\tremotePubKey = chanInfo.NodeKey2Bytes\n\tcase chanFlags&lnwire.ChanUpdateDirection == 1:\n\t\tremotePubKey = chanInfo.NodeKey1Bytes\n\t}\n\n\treturn remotePubKey\n}\n\n// processRejectedEdge examines a rejected edge to see if we can extract any\n// new announcements from it.  An edge will get rejected if we already added\n// the same edge without AuthProof to the graph. If the received announcement\n// contains a proof, we can add this proof to our edge.  We can end up in this\n// situation in the case where we create a channel, but for some reason fail\n// to receive the remote peer's proof, while the remote peer is able to fully\n// assemble the proof and craft the ChannelAnnouncement.",
      "length": 797,
      "tokens": 123,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) processRejectedEdge(",
      "content": "func (d *AuthenticatedGossiper) processRejectedEdge(\n\tchanAnnMsg *lnwire.ChannelAnnouncement,\n\tproof *channeldb.ChannelAuthProof) ([]networkMsg, error) {\n\n\t// First, we'll fetch the state of the channel as we know if from the\n\t// database.\n\tchanInfo, e1, e2, err := d.cfg.Router.GetChannelByID(\n\t\tchanAnnMsg.ShortChannelID,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// The edge is in the graph, and has a proof attached, then we'll just\n\t// reject it as normal.\n\tif chanInfo.AuthProof != nil {\n\t\treturn nil, nil\n\t}\n\n\t// Otherwise, this means that the edge is within the graph, but it\n\t// doesn't yet have a proper proof attached. If we did not receive\n\t// the proof such that we now can add it, there's nothing more we\n\t// can do.\n\tif proof == nil {\n\t\treturn nil, nil\n\t}\n\n\t// We'll then create then validate the new fully assembled\n\t// announcement.\n\tchanAnn, e1Ann, e2Ann, err := netann.CreateChanAnnouncement(\n\t\tproof, chanInfo, e1, e2,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\terr = routing.ValidateChannelAnn(chanAnn)\n\tif err != nil {\n\t\terr := fmt.Errorf(\"assembled channel announcement proof \"+\n\t\t\t\"for shortChanID=%v isn't valid: %v\",\n\t\t\tchanAnnMsg.ShortChannelID, err)\n\t\tlog.Error(err)\n\t\treturn nil, err\n\t}\n\n\t// If everything checks out, then we'll add the fully assembled proof\n\t// to the database.\n\terr = d.cfg.Router.AddProof(chanAnnMsg.ShortChannelID, proof)\n\tif err != nil {\n\t\terr := fmt.Errorf(\"unable add proof to shortChanID=%v: %v\",\n\t\t\tchanAnnMsg.ShortChannelID, err)\n\t\tlog.Error(err)\n\t\treturn nil, err\n\t}\n\n\t// As we now have a complete channel announcement for this channel,\n\t// we'll construct the announcement so they can be broadcast out to all\n\t// our peers.\n\tannouncements := make([]networkMsg, 0, 3)\n\tannouncements = append(announcements, networkMsg{\n\t\tsource: d.selfKey,\n\t\tmsg:    chanAnn,\n\t})\n\tif e1Ann != nil {\n\t\tannouncements = append(announcements, networkMsg{\n\t\t\tsource: d.selfKey,\n\t\t\tmsg:    e1Ann,\n\t\t})\n\t}\n\tif e2Ann != nil {\n\t\tannouncements = append(announcements, networkMsg{\n\t\t\tsource: d.selfKey,\n\t\t\tmsg:    e2Ann,\n\t\t})\n\n\t}\n\n\treturn announcements, nil\n}\n\n// addNode processes the given node announcement, and adds it to our channel\n// graph.",
      "length": 2044,
      "tokens": 312,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) addNode(msg *lnwire.NodeAnnouncement,",
      "content": "func (d *AuthenticatedGossiper) addNode(msg *lnwire.NodeAnnouncement,\n\top ...batch.SchedulerOption) error {\n\n\tif err := routing.ValidateNodeAnn(msg); err != nil {\n\t\treturn fmt.Errorf(\"unable to validate node announcement: %v\",\n\t\t\terr)\n\t}\n\n\ttimestamp := time.Unix(int64(msg.Timestamp), 0)\n\tfeatures := lnwire.NewFeatureVector(msg.Features, lnwire.Features)\n\tnode := &channeldb.LightningNode{\n\t\tHaveNodeAnnouncement: true,\n\t\tLastUpdate:           timestamp,\n\t\tAddresses:            msg.Addresses,\n\t\tPubKeyBytes:          msg.NodeID,\n\t\tAlias:                msg.Alias.String(),\n\t\tAuthSigBytes:         msg.Signature.ToSignatureBytes(),\n\t\tFeatures:             features,\n\t\tColor:                msg.RGBColor,\n\t\tExtraOpaqueData:      msg.ExtraOpaqueData,\n\t}\n\n\treturn d.cfg.Router.AddNode(node, op...)\n}\n\n// isPremature decides whether a given network message has a block height+delta\n// value specified in the future. If so, the message will be added to the\n// future message map and be processed when the block height as reached.\n//\n// NOTE: must be used inside a lock.",
      "length": 967,
      "tokens": 104,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) isPremature(chanID lnwire.ShortChannelID,",
      "content": "func (d *AuthenticatedGossiper) isPremature(chanID lnwire.ShortChannelID,\n\tdelta uint32, msg *networkMsg) bool {\n\t// TODO(roasbeef) make height delta 6\n\t//  * or configurable\n\n\tmsgHeight := chanID.BlockHeight + delta\n\n\t// The message height is smaller or equal to our best known height,\n\t// thus the message is mature.\n\tif msgHeight <= d.bestHeight {\n\t\treturn false\n\t}\n\n\t// Add the premature message to our future messages which will\n\t// be resent once the block height has reached.\n\t//\n\t// Init an empty cached message and overwrite it if there are cached\n\t// messages found.\n\tcachedMsgs := &cachedNetworkMsg{\n\t\tmsgs: make([]*processedNetworkMsg, 0),\n\t}\n\n\tresult, err := d.futureMsgs.Get(msgHeight)\n\t// No error returned means we have old messages cached.\n\tif err == nil {\n\t\tcachedMsgs = result\n\t}\n\n\t// Copy the networkMsgs since the old message's err chan will\n\t// be consumed.\n\tcopied := &networkMsg{\n\t\tpeer:              msg.peer,\n\t\tsource:            msg.source,\n\t\tmsg:               msg.msg,\n\t\toptionalMsgFields: msg.optionalMsgFields,\n\t\tisRemote:          msg.isRemote,\n\t\terr:               make(chan error, 1),\n\t}\n\n\t// The processed boolean is unused in the futureMsgs case.\n\tpMsg := &processedNetworkMsg{msg: copied}\n\n\t// Add the network message.\n\tmsgs := cachedMsgs.msgs\n\tmsgs = append(msgs, pMsg)\n\t_, err = d.futureMsgs.Put(msgHeight, &cachedNetworkMsg{\n\t\tmsgs: msgs,\n\t})\n\tif err != nil {\n\t\tlog.Errorf(\"Adding future message got error: %v\", err)\n\t}\n\n\tlog.Debugf(\"Network message: %v added to future messages for \"+\n\t\t\"msgHeight=%d, bestHeight=%d\", msg.msg.MsgType(),\n\t\tmsgHeight, d.bestHeight)\n\n\treturn true\n}\n\n// processNetworkAnnouncement processes a new network relate authenticated\n// channel or node announcement or announcements proofs. If the announcement\n// didn't affect the internal state due to either being out of date, invalid,\n// or redundant, then nil is returned. Otherwise, the set of announcements will\n// be returned which should be broadcasted to the rest of the network. The\n// boolean returned indicates whether any dependents of the announcement should\n// attempt to be processed as well.",
      "length": 1984,
      "tokens": 289,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) processNetworkAnnouncement(",
      "content": "func (d *AuthenticatedGossiper) processNetworkAnnouncement(\n\tnMsg *networkMsg) ([]networkMsg, bool) {\n\n\t// If this is a remote update, we set the scheduler option to lazily\n\t// add it to the graph.\n\tvar schedulerOp []batch.SchedulerOption\n\tif nMsg.isRemote {\n\t\tschedulerOp = append(schedulerOp, batch.LazyAdd())\n\t}\n\n\tswitch msg := nMsg.msg.(type) {\n\t// A new node announcement has arrived which either presents new\n\t// information about a node in one of the channels we know about, or a\n\t// updating previously advertised information.\n\tcase *lnwire.NodeAnnouncement:\n\t\treturn d.handleNodeAnnouncement(nMsg, msg, schedulerOp)\n\n\t// A new channel announcement has arrived, this indicates the\n\t// *creation* of a new channel within the network. This only advertises\n\t// the existence of a channel and not yet the routing policies in\n\t// either direction of the channel.\n\tcase *lnwire.ChannelAnnouncement:\n\t\treturn d.handleChanAnnouncement(nMsg, msg, schedulerOp)\n\n\t// A new authenticated channel edge update has arrived. This indicates\n\t// that the directional information for an already known channel has\n\t// been updated.\n\tcase *lnwire.ChannelUpdate:\n\t\treturn d.handleChanUpdate(nMsg, msg, schedulerOp)\n\n\t// A new signature announcement has been received. This indicates\n\t// willingness of nodes involved in the funding of a channel to\n\t// announce this new channel to the rest of the world.\n\tcase *lnwire.AnnounceSignatures:\n\t\treturn d.handleAnnSig(nMsg, msg)\n\n\tdefault:\n\t\terr := errors.New(\"wrong type of the announcement\")\n\t\tnMsg.err <- err\n\t\treturn nil, false\n\t}\n}\n\n// processZombieUpdate determines whether the provided channel update should\n// resurrect a given zombie edge.",
      "length": 1575,
      "tokens": 225,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) processZombieUpdate(",
      "content": "func (d *AuthenticatedGossiper) processZombieUpdate(\n\tchanInfo *channeldb.ChannelEdgeInfo, msg *lnwire.ChannelUpdate) error {\n\n\t// The least-significant bit in the flag on the channel update tells us\n\t// which edge is being updated.\n\tisNode1 := msg.ChannelFlags&lnwire.ChanUpdateDirection == 0\n\n\t// Since we've deemed the update as not stale above, before marking it\n\t// live, we'll make sure it has been signed by the correct party. If we\n\t// have both pubkeys, either party can resurect the channel. If we've\n\t// already marked this with the stricter, single-sided resurrection we\n\t// will only have the pubkey of the node with the oldest timestamp.\n\tvar pubKey *btcec.PublicKey\n\tswitch {\n\tcase isNode1 && chanInfo.NodeKey1Bytes != emptyPubkey:\n\t\tpubKey, _ = chanInfo.NodeKey1()\n\tcase !isNode1 && chanInfo.NodeKey2Bytes != emptyPubkey:\n\t\tpubKey, _ = chanInfo.NodeKey2()\n\t}\n\tif pubKey == nil {\n\t\treturn fmt.Errorf(\"incorrect pubkey to resurrect zombie \"+\n\t\t\t\"with chan_id=%v\", msg.ShortChannelID)\n\t}\n\n\terr := routing.VerifyChannelUpdateSignature(msg, pubKey)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to verify channel \"+\n\t\t\t\"update signature: %v\", err)\n\t}\n\n\t// With the signature valid, we'll proceed to mark the\n\t// edge as live and wait for the channel announcement to\n\t// come through again.\n\tbaseScid := lnwire.NewShortChanIDFromInt(chanInfo.ChannelID)\n\terr = d.cfg.Router.MarkEdgeLive(baseScid)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to remove edge with \"+\n\t\t\t\"chan_id=%v from zombie index: %v\",\n\t\t\tmsg.ShortChannelID, err)\n\t}\n\n\tlog.Debugf(\"Removed edge with chan_id=%v from zombie \"+\n\t\t\"index\", msg.ShortChannelID)\n\n\treturn nil\n}\n\n// fetchNodeAnn fetches the latest signed node announcement from our point of\n// view for the node with the given public key.",
      "length": 1673,
      "tokens": 240,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) fetchNodeAnn(",
      "content": "func (d *AuthenticatedGossiper) fetchNodeAnn(\n\tpubKey [33]byte) (*lnwire.NodeAnnouncement, error) {\n\n\tnode, err := d.cfg.Router.FetchLightningNode(pubKey)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn node.NodeAnnouncement(true)\n}\n\n// isMsgStale determines whether a message retrieved from the backing\n// MessageStore is seen as stale by the current graph.",
      "length": 300,
      "tokens": 41,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) isMsgStale(msg lnwire.Message) bool {",
      "content": "func (d *AuthenticatedGossiper) isMsgStale(msg lnwire.Message) bool {\n\tswitch msg := msg.(type) {\n\tcase *lnwire.AnnounceSignatures:\n\t\tchanInfo, _, _, err := d.cfg.Router.GetChannelByID(\n\t\t\tmsg.ShortChannelID,\n\t\t)\n\n\t\t// If the channel cannot be found, it is most likely a leftover\n\t\t// message for a channel that was closed, so we can consider it\n\t\t// stale.\n\t\tif err == channeldb.ErrEdgeNotFound {\n\t\t\treturn true\n\t\t}\n\t\tif err != nil {\n\t\t\tlog.Debugf(\"Unable to retrieve channel=%v from graph: \"+\n\t\t\t\t\"%v\", chanInfo.ChannelID, err)\n\t\t\treturn false\n\t\t}\n\n\t\t// If the proof exists in the graph, then we have successfully\n\t\t// received the remote proof and assembled the full proof, so we\n\t\t// can safely delete the local proof from the database.\n\t\treturn chanInfo.AuthProof != nil\n\n\tcase *lnwire.ChannelUpdate:\n\t\t_, p1, p2, err := d.cfg.Router.GetChannelByID(msg.ShortChannelID)\n\n\t\t// If the channel cannot be found, it is most likely a leftover\n\t\t// message for a channel that was closed, so we can consider it\n\t\t// stale.\n\t\tif err == channeldb.ErrEdgeNotFound {\n\t\t\treturn true\n\t\t}\n\t\tif err != nil {\n\t\t\tlog.Debugf(\"Unable to retrieve channel=%v from graph: \"+\n\t\t\t\t\"%v\", msg.ShortChannelID, err)\n\t\t\treturn false\n\t\t}\n\n\t\t// Otherwise, we'll retrieve the correct policy that we\n\t\t// currently have stored within our graph to check if this\n\t\t// message is stale by comparing its timestamp.\n\t\tvar p *channeldb.ChannelEdgePolicy\n\t\tif msg.ChannelFlags&lnwire.ChanUpdateDirection == 0 {\n\t\t\tp = p1\n\t\t} else {\n\t\t\tp = p2\n\t\t}\n\n\t\t// If the policy is still unknown, then we can consider this\n\t\t// policy fresh.\n\t\tif p == nil {\n\t\t\treturn false\n\t\t}\n\n\t\ttimestamp := time.Unix(int64(msg.Timestamp), 0)\n\t\treturn p.LastUpdate.After(timestamp)\n\n\tdefault:\n\t\t// We'll make sure to not mark any unsupported messages as stale\n\t\t// to ensure they are not removed.\n\t\treturn false\n\t}\n}\n\n// updateChannel creates a new fully signed update for the channel, and updates\n// the underlying graph with the new state.",
      "length": 1842,
      "tokens": 289,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) updateChannel(info *channeldb.ChannelEdgeInfo,",
      "content": "func (d *AuthenticatedGossiper) updateChannel(info *channeldb.ChannelEdgeInfo,\n\tedge *channeldb.ChannelEdgePolicy) (*lnwire.ChannelAnnouncement,\n\t*lnwire.ChannelUpdate, error) {\n\n\t// Parse the unsigned edge into a channel update.\n\tchanUpdate := netann.UnsignedChannelUpdateFromEdge(info, edge)\n\n\t// We'll generate a new signature over a digest of the channel\n\t// announcement itself and update the timestamp to ensure it propagate.\n\terr := netann.SignChannelUpdate(\n\t\td.cfg.AnnSigner, d.selfKeyLoc, chanUpdate,\n\t\tnetann.ChanUpdSetTimestamp,\n\t)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\t// Next, we'll set the new signature in place, and update the reference\n\t// in the backing slice.\n\tedge.LastUpdate = time.Unix(int64(chanUpdate.Timestamp), 0)\n\tedge.SigBytes = chanUpdate.Signature.ToSignatureBytes()\n\n\t// To ensure that our signature is valid, we'll verify it ourself\n\t// before committing it to the slice returned.\n\terr = routing.ValidateChannelUpdateAnn(d.selfKey, info.Capacity, chanUpdate)\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"generated invalid channel \"+\n\t\t\t\"update sig: %v\", err)\n\t}\n\n\t// Finally, we'll write the new edge policy to disk.\n\tif err := d.cfg.Router.UpdateEdge(edge); err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\t// We'll also create the original channel announcement so the two can\n\t// be broadcast along side each other (if necessary), but only if we\n\t// have a full channel announcement for this channel.\n\tvar chanAnn *lnwire.ChannelAnnouncement\n\tif info.AuthProof != nil {\n\t\tchanID := lnwire.NewShortChanIDFromInt(info.ChannelID)\n\t\tchanAnn = &lnwire.ChannelAnnouncement{\n\t\t\tShortChannelID:  chanID,\n\t\t\tNodeID1:         info.NodeKey1Bytes,\n\t\t\tNodeID2:         info.NodeKey2Bytes,\n\t\t\tChainHash:       info.ChainHash,\n\t\t\tBitcoinKey1:     info.BitcoinKey1Bytes,\n\t\t\tFeatures:        lnwire.NewRawFeatureVector(),\n\t\t\tBitcoinKey2:     info.BitcoinKey2Bytes,\n\t\t\tExtraOpaqueData: edge.ExtraOpaqueData,\n\t\t}\n\t\tchanAnn.NodeSig1, err = lnwire.NewSigFromRawSignature(\n\t\t\tinfo.AuthProof.NodeSig1Bytes,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\tchanAnn.NodeSig2, err = lnwire.NewSigFromRawSignature(\n\t\t\tinfo.AuthProof.NodeSig2Bytes,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\tchanAnn.BitcoinSig1, err = lnwire.NewSigFromRawSignature(\n\t\t\tinfo.AuthProof.BitcoinSig1Bytes,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\tchanAnn.BitcoinSig2, err = lnwire.NewSigFromRawSignature(\n\t\t\tinfo.AuthProof.BitcoinSig2Bytes,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t}\n\n\treturn chanAnn, chanUpdate, err\n}\n\n// SyncManager returns the gossiper's SyncManager instance.",
      "length": 2444,
      "tokens": 292,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) SyncManager() *SyncManager {",
      "content": "func (d *AuthenticatedGossiper) SyncManager() *SyncManager {\n\treturn d.syncMgr\n}\n\n// IsKeepAliveUpdate determines whether this channel update is considered a\n// keep-alive update based on the previous channel update processed for the same\n// direction.",
      "length": 186,
      "tokens": 28,
      "embedding": []
    },
    {
      "slug": "func IsKeepAliveUpdate(update *lnwire.ChannelUpdate,",
      "content": "func IsKeepAliveUpdate(update *lnwire.ChannelUpdate,\n\tprev *channeldb.ChannelEdgePolicy) bool {\n\n\t// Both updates should be from the same direction.\n\tif update.ChannelFlags&lnwire.ChanUpdateDirection !=\n\t\tprev.ChannelFlags&lnwire.ChanUpdateDirection {\n\n\t\treturn false\n\t}\n\n\t// The timestamp should always increase for a keep-alive update.\n\ttimestamp := time.Unix(int64(update.Timestamp), 0)\n\tif !timestamp.After(prev.LastUpdate) {\n\t\treturn false\n\t}\n\n\t// None of the remaining fields should change for a keep-alive update.\n\tif update.ChannelFlags.IsDisabled() != prev.ChannelFlags.IsDisabled() {\n\t\treturn false\n\t}\n\tif lnwire.MilliSatoshi(update.BaseFee) != prev.FeeBaseMSat {\n\t\treturn false\n\t}\n\tif lnwire.MilliSatoshi(update.FeeRate) != prev.FeeProportionalMillionths {\n\t\treturn false\n\t}\n\tif update.TimeLockDelta != prev.TimeLockDelta {\n\t\treturn false\n\t}\n\tif update.HtlcMinimumMsat != prev.MinHTLC {\n\t\treturn false\n\t}\n\tif update.MessageFlags.HasMaxHtlc() && !prev.MessageFlags.HasMaxHtlc() {\n\t\treturn false\n\t}\n\tif update.HtlcMaximumMsat != prev.MaxHTLC {\n\t\treturn false\n\t}\n\tif !bytes.Equal(update.ExtraOpaqueData, prev.ExtraOpaqueData) {\n\t\treturn false\n\t}\n\treturn true\n}\n\n// latestHeight returns the gossiper's latest height known of the chain.",
      "length": 1146,
      "tokens": 130,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) latestHeight() uint32 {",
      "content": "func (d *AuthenticatedGossiper) latestHeight() uint32 {\n\td.Lock()\n\tdefer d.Unlock()\n\treturn d.bestHeight\n}\n\n// handleNodeAnnouncement processes a new node announcement.",
      "length": 107,
      "tokens": 13,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) handleNodeAnnouncement(nMsg *networkMsg,",
      "content": "func (d *AuthenticatedGossiper) handleNodeAnnouncement(nMsg *networkMsg,\n\tnodeAnn *lnwire.NodeAnnouncement,\n\tops []batch.SchedulerOption) ([]networkMsg, bool) {\n\n\ttimestamp := time.Unix(int64(nodeAnn.Timestamp), 0)\n\n\tlog.Debugf(\"Processing NodeAnnouncement: peer=%v, timestamp=%v, \"+\n\t\t\"node=%x\", nMsg.peer, timestamp, nodeAnn.NodeID)\n\n\t// We'll quickly ask the router if it already has a newer update for\n\t// this node so we can skip validating signatures if not required.\n\tif d.cfg.Router.IsStaleNode(nodeAnn.NodeID, timestamp) {\n\t\tlog.Debugf(\"Skipped processing stale node: %x\", nodeAnn.NodeID)\n\t\tnMsg.err <- nil\n\t\treturn nil, true\n\t}\n\n\tif err := d.addNode(nodeAnn, ops...); err != nil {\n\t\tlog.Debugf(\"Adding node: %x got error: %v\", nodeAnn.NodeID,\n\t\t\terr)\n\n\t\tif !routing.IsError(\n\t\t\terr,\n\t\t\trouting.ErrOutdated,\n\t\t\trouting.ErrIgnored,\n\t\t\trouting.ErrVBarrierShuttingDown,\n\t\t) {\n\n\t\t\tlog.Error(err)\n\t\t}\n\n\t\tnMsg.err <- err\n\t\treturn nil, false\n\t}\n\n\t// In order to ensure we don't leak unadvertised nodes, we'll make a\n\t// quick check to ensure this node intends to publicly advertise itself\n\t// to the network.\n\tisPublic, err := d.cfg.Router.IsPublicNode(nodeAnn.NodeID)\n\tif err != nil {\n\t\tlog.Errorf(\"Unable to determine if node %x is advertised: %v\",\n\t\t\tnodeAnn.NodeID, err)\n\t\tnMsg.err <- err\n\t\treturn nil, false\n\t}\n\n\tvar announcements []networkMsg\n\n\t// If it does, we'll add their announcement to our batch so that it can\n\t// be broadcast to the rest of our peers.\n\tif isPublic {\n\t\tannouncements = append(announcements, networkMsg{\n\t\t\tpeer:     nMsg.peer,\n\t\t\tisRemote: nMsg.isRemote,\n\t\t\tsource:   nMsg.source,\n\t\t\tmsg:      nodeAnn,\n\t\t})\n\t} else {\n\t\tlog.Tracef(\"Skipping broadcasting node announcement for %x \"+\n\t\t\t\"due to being unadvertised\", nodeAnn.NodeID)\n\t}\n\n\tnMsg.err <- nil\n\t// TODO(roasbeef): get rid of the above\n\n\tlog.Debugf(\"Processed NodeAnnouncement: peer=%v, timestamp=%v, \"+\n\t\t\"node=%x\", nMsg.peer, timestamp, nodeAnn.NodeID)\n\n\treturn announcements, true\n}\n\n// handleChanAnnouncement processes a new channel announcement.",
      "length": 1895,
      "tokens": 242,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) handleChanAnnouncement(nMsg *networkMsg,",
      "content": "func (d *AuthenticatedGossiper) handleChanAnnouncement(nMsg *networkMsg,\n\tann *lnwire.ChannelAnnouncement,\n\tops []batch.SchedulerOption) ([]networkMsg, bool) {\n\n\tlog.Debugf(\"Processing ChannelAnnouncement: peer=%v, short_chan_id=%v\",\n\t\tnMsg.peer, ann.ShortChannelID.ToUint64())\n\n\t// We'll ignore any channel announcements that target any chain other\n\t// than the set of chains we know of.\n\tif !bytes.Equal(ann.ChainHash[:], d.cfg.ChainHash[:]) {\n\t\terr := fmt.Errorf(\"ignoring ChannelAnnouncement from chain=%v\"+\n\t\t\t\", gossiper on chain=%v\", ann.ChainHash,\n\t\t\td.cfg.ChainHash)\n\t\tlog.Errorf(err.Error())\n\n\t\tkey := newRejectCacheKey(\n\t\t\tann.ShortChannelID.ToUint64(),\n\t\t\tsourceToPub(nMsg.source),\n\t\t)\n\t\t_, _ = d.recentRejects.Put(key, &cachedReject{})\n\n\t\tnMsg.err <- err\n\t\treturn nil, false\n\t}\n\n\t// If this is a remote ChannelAnnouncement with an alias SCID, we'll\n\t// reject the announcement. Since the router accepts alias SCIDs,\n\t// not erroring out would be a DoS vector.\n\tif nMsg.isRemote && d.cfg.IsAlias(ann.ShortChannelID) {\n\t\terr := fmt.Errorf(\"ignoring remote alias channel=%v\",\n\t\t\tann.ShortChannelID)\n\t\tlog.Errorf(err.Error())\n\n\t\tkey := newRejectCacheKey(\n\t\t\tann.ShortChannelID.ToUint64(),\n\t\t\tsourceToPub(nMsg.source),\n\t\t)\n\t\t_, _ = d.recentRejects.Put(key, &cachedReject{})\n\n\t\tnMsg.err <- err\n\t\treturn nil, false\n\t}\n\n\t// If the advertised inclusionary block is beyond our knowledge of the\n\t// chain tip, then we'll ignore it for now.\n\td.Lock()\n\tif nMsg.isRemote && d.isPremature(ann.ShortChannelID, 0, nMsg) {\n\t\tlog.Warnf(\"Announcement for chan_id=(%v), is premature: \"+\n\t\t\t\"advertises height %v, only height %v is known\",\n\t\t\tann.ShortChannelID.ToUint64(),\n\t\t\tann.ShortChannelID.BlockHeight, d.bestHeight)\n\t\td.Unlock()\n\t\tnMsg.err <- nil\n\t\treturn nil, false\n\t}\n\td.Unlock()\n\n\t// At this point, we'll now ask the router if this is a zombie/known\n\t// edge. If so we can skip all the processing below.\n\tif d.cfg.Router.IsKnownEdge(ann.ShortChannelID) {\n\t\tnMsg.err <- nil\n\t\treturn nil, true\n\t}\n\n\t// If this is a remote channel announcement, then we'll validate all\n\t// the signatures within the proof as it should be well formed.\n\tvar proof *channeldb.ChannelAuthProof\n\tif nMsg.isRemote {\n\t\tif err := routing.ValidateChannelAnn(ann); err != nil {\n\t\t\terr := fmt.Errorf(\"unable to validate announcement: \"+\n\t\t\t\t\"%v\", err)\n\n\t\t\tkey := newRejectCacheKey(\n\t\t\t\tann.ShortChannelID.ToUint64(),\n\t\t\t\tsourceToPub(nMsg.source),\n\t\t\t)\n\t\t\t_, _ = d.recentRejects.Put(key, &cachedReject{})\n\n\t\t\tlog.Error(err)\n\t\t\tnMsg.err <- err\n\t\t\treturn nil, false\n\t\t}\n\n\t\t// If the proof checks out, then we'll save the proof itself to\n\t\t// the database so we can fetch it later when gossiping with\n\t\t// other nodes.\n\t\tproof = &channeldb.ChannelAuthProof{\n\t\t\tNodeSig1Bytes:    ann.NodeSig1.ToSignatureBytes(),\n\t\t\tNodeSig2Bytes:    ann.NodeSig2.ToSignatureBytes(),\n\t\t\tBitcoinSig1Bytes: ann.BitcoinSig1.ToSignatureBytes(),\n\t\t\tBitcoinSig2Bytes: ann.BitcoinSig2.ToSignatureBytes(),\n\t\t}\n\t}\n\n\t// With the proof validated (if necessary), we can now store it within\n\t// the database for our path finding and syncing needs.\n\tvar featureBuf bytes.Buffer\n\tif err := ann.Features.Encode(&featureBuf); err != nil {\n\t\tlog.Errorf(\"unable to encode features: %v\", err)\n\t\tnMsg.err <- err\n\t\treturn nil, false\n\t}\n\n\tedge := &channeldb.ChannelEdgeInfo{\n\t\tChannelID:        ann.ShortChannelID.ToUint64(),\n\t\tChainHash:        ann.ChainHash,\n\t\tNodeKey1Bytes:    ann.NodeID1,\n\t\tNodeKey2Bytes:    ann.NodeID2,\n\t\tBitcoinKey1Bytes: ann.BitcoinKey1,\n\t\tBitcoinKey2Bytes: ann.BitcoinKey2,\n\t\tAuthProof:        proof,\n\t\tFeatures:         featureBuf.Bytes(),\n\t\tExtraOpaqueData:  ann.ExtraOpaqueData,\n\t}\n\n\t// If there were any optional message fields provided, we'll include\n\t// them in its serialized disk representation now.\n\tif nMsg.optionalMsgFields != nil {\n\t\tif nMsg.optionalMsgFields.capacity != nil {\n\t\t\tedge.Capacity = *nMsg.optionalMsgFields.capacity\n\t\t}\n\t\tif nMsg.optionalMsgFields.channelPoint != nil {\n\t\t\tcp := *nMsg.optionalMsgFields.channelPoint\n\t\t\tedge.ChannelPoint = cp\n\t\t}\n\t}\n\n\tlog.Debugf(\"Adding edge for short_chan_id: %v\",\n\t\tann.ShortChannelID.ToUint64())\n\n\t// We will add the edge to the channel router. If the nodes present in\n\t// this channel are not present in the database, a partial node will be\n\t// added to represent each node while we wait for a node announcement.\n\t//\n\t// Before we add the edge to the database, we obtain the mutex for this\n\t// channel ID. We do this to ensure no other goroutine has read the\n\t// database and is now making decisions based on this DB state, before\n\t// it writes to the DB.\n\td.channelMtx.Lock(ann.ShortChannelID.ToUint64())\n\terr := d.cfg.Router.AddEdge(edge, ops...)\n\tif err != nil {\n\t\tlog.Debugf(\"Router rejected edge for short_chan_id(%v): %v\",\n\t\t\tann.ShortChannelID.ToUint64(), err)\n\n\t\tdefer d.channelMtx.Unlock(ann.ShortChannelID.ToUint64())\n\n\t\t// If the edge was rejected due to already being known, then it\n\t\t// may be the case that this new message has a fresh channel\n\t\t// proof, so we'll check.\n\t\tif routing.IsError(err, routing.ErrIgnored) {\n\t\t\t// Attempt to process the rejected message to see if we\n\t\t\t// get any new announcements.\n\t\t\tanns, rErr := d.processRejectedEdge(ann, proof)\n\t\t\tif rErr != nil {\n\t\t\t\tkey := newRejectCacheKey(\n\t\t\t\t\tann.ShortChannelID.ToUint64(),\n\t\t\t\t\tsourceToPub(nMsg.source),\n\t\t\t\t)\n\t\t\t\tcr := &cachedReject{}\n\t\t\t\t_, _ = d.recentRejects.Put(key, cr)\n\n\t\t\t\tnMsg.err <- rErr\n\t\t\t\treturn nil, false\n\t\t\t}\n\n\t\t\tlog.Debugf(\"Extracted %v announcements from rejected \"+\n\t\t\t\t\"msgs\", len(anns))\n\n\t\t\t// If while processing this rejected edge, we realized\n\t\t\t// there's a set of announcements we could extract,\n\t\t\t// then we'll return those directly.\n\t\t\t//\n\t\t\t// NOTE: since this is an ErrIgnored, we can return\n\t\t\t// true here to signal \"allow\" to its dependants.\n\t\t\tnMsg.err <- nil\n\n\t\t\treturn anns, true\n\t\t} else {\n\t\t\t// Otherwise, this is just a regular rejected edge.\n\t\t\tkey := newRejectCacheKey(\n\t\t\t\tann.ShortChannelID.ToUint64(),\n\t\t\t\tsourceToPub(nMsg.source),\n\t\t\t)\n\t\t\t_, _ = d.recentRejects.Put(key, &cachedReject{})\n\t\t}\n\n\t\tnMsg.err <- err\n\t\treturn nil, false\n\t}\n\n\t// If err is nil, release the lock immediately.\n\td.channelMtx.Unlock(ann.ShortChannelID.ToUint64())\n\n\tlog.Debugf(\"Finish adding edge for short_chan_id: %v\",\n\t\tann.ShortChannelID.ToUint64())\n\n\t// If we earlier received any ChannelUpdates for this channel, we can\n\t// now process them, as the channel is added to the graph.\n\tshortChanID := ann.ShortChannelID.ToUint64()\n\tvar channelUpdates []*processedNetworkMsg\n\n\tearlyChanUpdates, err := d.prematureChannelUpdates.Get(shortChanID)\n\tif err == nil {\n\t\t// There was actually an entry in the map, so we'll accumulate\n\t\t// it. We don't worry about deletion, since it'll eventually\n\t\t// fall out anyway.\n\t\tchanMsgs := earlyChanUpdates\n\t\tchannelUpdates = append(channelUpdates, chanMsgs.msgs...)\n\t}\n\n\t// Launch a new goroutine to handle each ChannelUpdate, this is to\n\t// ensure we don't block here, as we can handle only one announcement\n\t// at a time.\n\tfor _, cu := range channelUpdates {\n\t\t// Skip if already processed.\n\t\tif cu.processed {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Mark the ChannelUpdate as processed. This ensures that a\n\t\t// subsequent announcement in the option-scid-alias case does\n\t\t// not re-use an old ChannelUpdate.\n\t\tcu.processed = true\n\n\t\td.wg.Add(1)\n\t\tgo func(updMsg *networkMsg) {\n\t\t\tdefer d.wg.Done()\n\n\t\t\tswitch msg := updMsg.msg.(type) {\n\t\t\t// Reprocess the message, making sure we return an\n\t\t\t// error to the original caller in case the gossiper\n\t\t\t// shuts down.\n\t\t\tcase *lnwire.ChannelUpdate:\n\t\t\t\tlog.Debugf(\"Reprocessing ChannelUpdate for \"+\n\t\t\t\t\t\"shortChanID=%v\",\n\t\t\t\t\tmsg.ShortChannelID.ToUint64())\n\n\t\t\t\tselect {\n\t\t\t\tcase d.networkMsgs <- updMsg:\n\t\t\t\tcase <-d.quit:\n\t\t\t\t\tupdMsg.err <- ErrGossiperShuttingDown\n\t\t\t\t}\n\n\t\t\t// We don't expect any other message type than\n\t\t\t// ChannelUpdate to be in this cache.\n\t\t\tdefault:\n\t\t\t\tlog.Errorf(\"Unsupported message type found \"+\n\t\t\t\t\t\"among ChannelUpdates: %T\", msg)\n\t\t\t}\n\t\t}(cu.msg)\n\t}\n\n\t// Channel announcement was successfully processed and now it might be\n\t// broadcast to other connected nodes if it was an announcement with\n\t// proof (remote).\n\tvar announcements []networkMsg\n\n\tif proof != nil {\n\t\tannouncements = append(announcements, networkMsg{\n\t\t\tpeer:     nMsg.peer,\n\t\t\tisRemote: nMsg.isRemote,\n\t\t\tsource:   nMsg.source,\n\t\t\tmsg:      ann,\n\t\t})\n\t}\n\n\tnMsg.err <- nil\n\n\tlog.Debugf(\"Processed ChannelAnnouncement: peer=%v, short_chan_id=%v\",\n\t\tnMsg.peer, ann.ShortChannelID.ToUint64())\n\n\treturn announcements, true\n}\n\n// handleChanUpdate processes a new channel update.",
      "length": 8221,
      "tokens": 1044,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) handleChanUpdate(nMsg *networkMsg,",
      "content": "func (d *AuthenticatedGossiper) handleChanUpdate(nMsg *networkMsg,\n\tupd *lnwire.ChannelUpdate,\n\tops []batch.SchedulerOption) ([]networkMsg, bool) {\n\n\tlog.Debugf(\"Processing ChannelUpdate: peer=%v, short_chan_id=%v, \",\n\t\tnMsg.peer, upd.ShortChannelID.ToUint64())\n\n\t// We'll ignore any channel updates that target any chain other than\n\t// the set of chains we know of.\n\tif !bytes.Equal(upd.ChainHash[:], d.cfg.ChainHash[:]) {\n\t\terr := fmt.Errorf(\"ignoring ChannelUpdate from chain=%v, \"+\n\t\t\t\"gossiper on chain=%v\", upd.ChainHash, d.cfg.ChainHash)\n\t\tlog.Errorf(err.Error())\n\n\t\tkey := newRejectCacheKey(\n\t\t\tupd.ShortChannelID.ToUint64(),\n\t\t\tsourceToPub(nMsg.source),\n\t\t)\n\t\t_, _ = d.recentRejects.Put(key, &cachedReject{})\n\n\t\tnMsg.err <- err\n\t\treturn nil, false\n\t}\n\n\tblockHeight := upd.ShortChannelID.BlockHeight\n\tshortChanID := upd.ShortChannelID.ToUint64()\n\n\t// If the advertised inclusionary block is beyond our knowledge of the\n\t// chain tip, then we'll put the announcement in limbo to be fully\n\t// verified once we advance forward in the chain. If the update has an\n\t// alias SCID, we'll skip the isPremature check. This is necessary\n\t// since aliases start at block height 16_000_000.\n\td.Lock()\n\tif nMsg.isRemote && !d.cfg.IsAlias(upd.ShortChannelID) &&\n\t\td.isPremature(upd.ShortChannelID, 0, nMsg) {\n\n\t\tlog.Warnf(\"Update announcement for short_chan_id(%v), is \"+\n\t\t\t\"premature: advertises height %v, only height %v is \"+\n\t\t\t\"known\", shortChanID, blockHeight, d.bestHeight)\n\t\td.Unlock()\n\t\tnMsg.err <- nil\n\t\treturn nil, false\n\t}\n\td.Unlock()\n\n\t// Before we perform any of the expensive checks below, we'll check\n\t// whether this update is stale or is for a zombie channel in order to\n\t// quickly reject it.\n\ttimestamp := time.Unix(int64(upd.Timestamp), 0)\n\n\t// Fetch the SCID we should be using to lock the channelMtx and make\n\t// graph queries with.\n\tgraphScid, err := d.cfg.FindBaseByAlias(upd.ShortChannelID)\n\tif err != nil {\n\t\t// Fallback and set the graphScid to the peer-provided SCID.\n\t\t// This will occur for non-option-scid-alias channels and for\n\t\t// public option-scid-alias channels after 6 confirmations.\n\t\t// Once public option-scid-alias channels have 6 confs, we'll\n\t\t// ignore ChannelUpdates with one of their aliases.\n\t\tgraphScid = upd.ShortChannelID\n\t}\n\n\tif d.cfg.Router.IsStaleEdgePolicy(\n\t\tgraphScid, timestamp, upd.ChannelFlags,\n\t) {\n\n\t\tlog.Debugf(\"Ignored stale edge policy for short_chan_id(%v): \"+\n\t\t\t\"peer=%v, msg=%s, is_remote=%v\", shortChanID,\n\t\t\tnMsg.peer, nMsg.msg.MsgType(), nMsg.isRemote,\n\t\t)\n\n\t\tnMsg.err <- nil\n\t\treturn nil, true\n\t}\n\n\t// Get the node pub key as far since we don't have it in the channel\n\t// update announcement message. We'll need this to properly verify the\n\t// message's signature.\n\t//\n\t// We make sure to obtain the mutex for this channel ID before we\n\t// access the database. This ensures the state we read from the\n\t// database has not changed between this point and when we call\n\t// UpdateEdge() later.\n\td.channelMtx.Lock(graphScid.ToUint64())\n\tdefer d.channelMtx.Unlock(graphScid.ToUint64())\n\n\tchanInfo, e1, e2, err := d.cfg.Router.GetChannelByID(graphScid)\n\tswitch err {\n\t// No error, break.\n\tcase nil:\n\t\tbreak\n\n\tcase channeldb.ErrZombieEdge:\n\t\terr = d.processZombieUpdate(chanInfo, upd)\n\t\tif err != nil {\n\t\t\tlog.Debug(err)\n\t\t\tnMsg.err <- err\n\t\t\treturn nil, false\n\t\t}\n\n\t\t// We'll fallthrough to ensure we stash the update until we\n\t\t// receive its corresponding ChannelAnnouncement. This is\n\t\t// needed to ensure the edge exists in the graph before\n\t\t// applying the update.\n\t\tfallthrough\n\tcase channeldb.ErrGraphNotFound:\n\t\tfallthrough\n\tcase channeldb.ErrGraphNoEdgesFound:\n\t\tfallthrough\n\tcase channeldb.ErrEdgeNotFound:\n\t\t// If the edge corresponding to this ChannelUpdate was not\n\t\t// found in the graph, this might be a channel in the process\n\t\t// of being opened, and we haven't processed our own\n\t\t// ChannelAnnouncement yet, hence it is not not found in the\n\t\t// graph. This usually gets resolved after the channel proofs\n\t\t// are exchanged and the channel is broadcasted to the rest of\n\t\t// the network, but in case this is a private channel this\n\t\t// won't ever happen. This can also happen in the case of a\n\t\t// zombie channel with a fresh update for which we don't have a\n\t\t// ChannelAnnouncement for since we reject them. Because of\n\t\t// this, we temporarily add it to a map, and reprocess it after\n\t\t// our own ChannelAnnouncement has been processed.\n\t\t//\n\t\t// The shortChanID may be an alias, but it is fine to use here\n\t\t// since we don't have an edge in the graph and if the peer is\n\t\t// not buggy, we should be able to use it once the gossiper\n\t\t// receives the local announcement.\n\t\tpMsg := &processedNetworkMsg{msg: nMsg}\n\n\t\tearlyMsgs, err := d.prematureChannelUpdates.Get(shortChanID)\n\t\tswitch {\n\t\t// Nothing in the cache yet, we can just directly insert this\n\t\t// element.\n\t\tcase err == cache.ErrElementNotFound:\n\t\t\t_, _ = d.prematureChannelUpdates.Put(\n\t\t\t\tshortChanID, &cachedNetworkMsg{\n\t\t\t\t\tmsgs: []*processedNetworkMsg{pMsg},\n\t\t\t\t})\n\n\t\t// There's already something in the cache, so we'll combine the\n\t\t// set of messages into a single value.\n\t\tdefault:\n\t\t\tmsgs := earlyMsgs.msgs\n\t\t\tmsgs = append(msgs, pMsg)\n\t\t\t_, _ = d.prematureChannelUpdates.Put(\n\t\t\t\tshortChanID, &cachedNetworkMsg{\n\t\t\t\t\tmsgs: msgs,\n\t\t\t\t})\n\t\t}\n\n\t\tlog.Debugf(\"Got ChannelUpdate for edge not found in graph\"+\n\t\t\t\"(shortChanID=%v), saving for reprocessing later\",\n\t\t\tshortChanID)\n\n\t\t// NOTE: We don't return anything on the error channel for this\n\t\t// message, as we expect that will be done when this\n\t\t// ChannelUpdate is later reprocessed.\n\t\treturn nil, false\n\n\tdefault:\n\t\terr := fmt.Errorf(\"unable to validate channel update \"+\n\t\t\t\"short_chan_id=%v: %v\", shortChanID, err)\n\t\tlog.Error(err)\n\t\tnMsg.err <- err\n\n\t\tkey := newRejectCacheKey(\n\t\t\tupd.ShortChannelID.ToUint64(),\n\t\t\tsourceToPub(nMsg.source),\n\t\t)\n\t\t_, _ = d.recentRejects.Put(key, &cachedReject{})\n\n\t\treturn nil, false\n\t}\n\n\t// The least-significant bit in the flag on the channel update\n\t// announcement tells us \"which\" side of the channels directed edge is\n\t// being updated.\n\tvar (\n\t\tpubKey       *btcec.PublicKey\n\t\tedgeToUpdate *channeldb.ChannelEdgePolicy\n\t)\n\tdirection := upd.ChannelFlags & lnwire.ChanUpdateDirection\n\tswitch direction {\n\tcase 0:\n\t\tpubKey, _ = chanInfo.NodeKey1()\n\t\tedgeToUpdate = e1\n\tcase 1:\n\t\tpubKey, _ = chanInfo.NodeKey2()\n\t\tedgeToUpdate = e2\n\t}\n\n\tlog.Debugf(\"Validating ChannelUpdate: channel=%v, from node=%x, has \"+\n\t\t\"edge=%v\", chanInfo.ChannelID, pubKey.SerializeCompressed(),\n\t\tedgeToUpdate != nil)\n\n\t// Validate the channel announcement with the expected public key and\n\t// channel capacity. In the case of an invalid channel update, we'll\n\t// return an error to the caller and exit early.\n\terr = routing.ValidateChannelUpdateAnn(pubKey, chanInfo.Capacity, upd)\n\tif err != nil {\n\t\trErr := fmt.Errorf(\"unable to validate channel update \"+\n\t\t\t\"announcement for short_chan_id=%v: %v\",\n\t\t\tspew.Sdump(upd.ShortChannelID), err)\n\n\t\tlog.Error(rErr)\n\t\tnMsg.err <- rErr\n\t\treturn nil, false\n\t}\n\n\t// If we have a previous version of the edge being updated, we'll want\n\t// to rate limit its updates to prevent spam throughout the network.\n\tif nMsg.isRemote && edgeToUpdate != nil {\n\t\t// If it's a keep-alive update, we'll only propagate one if\n\t\t// it's been a day since the previous. This follows our own\n\t\t// heuristic of sending keep-alive updates after the same\n\t\t// duration (see retransmitStaleAnns).\n\t\ttimeSinceLastUpdate := timestamp.Sub(edgeToUpdate.LastUpdate)\n\t\tif IsKeepAliveUpdate(upd, edgeToUpdate) {\n\t\t\tif timeSinceLastUpdate < d.cfg.RebroadcastInterval {\n\t\t\t\tlog.Debugf(\"Ignoring keep alive update not \"+\n\t\t\t\t\t\"within %v period for channel %v\",\n\t\t\t\t\td.cfg.RebroadcastInterval, shortChanID)\n\t\t\t\tnMsg.err <- nil\n\t\t\t\treturn nil, false\n\t\t\t}\n\t\t} else {\n\t\t\t// If it's not, we'll allow an update per minute with a\n\t\t\t// maximum burst of 10. If we haven't seen an update\n\t\t\t// for this channel before, we'll need to initialize a\n\t\t\t// rate limiter for each direction.\n\t\t\t//\n\t\t\t// Since the edge exists in the graph, we'll create a\n\t\t\t// rate limiter for chanInfo.ChannelID rather then the\n\t\t\t// SCID the peer sent. This is because there may be\n\t\t\t// multiple aliases for a channel and we may otherwise\n\t\t\t// rate-limit only a single alias of the channel,\n\t\t\t// instead of the whole channel.\n\t\t\tbaseScid := chanInfo.ChannelID\n\t\t\td.Lock()\n\t\t\trls, ok := d.chanUpdateRateLimiter[baseScid]\n\t\t\tif !ok {\n\t\t\t\tr := rate.Every(d.cfg.ChannelUpdateInterval)\n\t\t\t\tb := d.cfg.MaxChannelUpdateBurst\n\t\t\t\trls = [2]*rate.Limiter{\n\t\t\t\t\trate.NewLimiter(r, b),\n\t\t\t\t\trate.NewLimiter(r, b),\n\t\t\t\t}\n\t\t\t\td.chanUpdateRateLimiter[baseScid] = rls\n\t\t\t}\n\t\t\td.Unlock()\n\n\t\t\tif !rls[direction].Allow() {\n\t\t\t\tlog.Debugf(\"Rate limiting update for channel \"+\n\t\t\t\t\t\"%v from direction %x\", shortChanID,\n\t\t\t\t\tpubKey.SerializeCompressed())\n\t\t\t\tnMsg.err <- nil\n\t\t\t\treturn nil, false\n\t\t\t}\n\t\t}\n\t}\n\n\t// We'll use chanInfo.ChannelID rather than the peer-supplied\n\t// ShortChannelID in the ChannelUpdate to avoid the router having to\n\t// lookup the stored SCID. If we're sending the update, we'll always\n\t// use the SCID stored in the database rather than a potentially\n\t// different alias. This might mean that SigBytes is incorrect as it\n\t// signs a different SCID than the database SCID, but since there will\n\t// only be a difference if AuthProof == nil, this is fine.\n\tupdate := &channeldb.ChannelEdgePolicy{\n\t\tSigBytes:                  upd.Signature.ToSignatureBytes(),\n\t\tChannelID:                 chanInfo.ChannelID,\n\t\tLastUpdate:                timestamp,\n\t\tMessageFlags:              upd.MessageFlags,\n\t\tChannelFlags:              upd.ChannelFlags,\n\t\tTimeLockDelta:             upd.TimeLockDelta,\n\t\tMinHTLC:                   upd.HtlcMinimumMsat,\n\t\tMaxHTLC:                   upd.HtlcMaximumMsat,\n\t\tFeeBaseMSat:               lnwire.MilliSatoshi(upd.BaseFee),\n\t\tFeeProportionalMillionths: lnwire.MilliSatoshi(upd.FeeRate),\n\t\tExtraOpaqueData:           upd.ExtraOpaqueData,\n\t}\n\n\tif err := d.cfg.Router.UpdateEdge(update, ops...); err != nil {\n\t\tif routing.IsError(\n\t\t\terr, routing.ErrOutdated,\n\t\t\trouting.ErrIgnored,\n\t\t\trouting.ErrVBarrierShuttingDown,\n\t\t) {\n\n\t\t\tlog.Debugf(\"Update edge for short_chan_id(%v) got: %v\",\n\t\t\t\tshortChanID, err)\n\t\t} else {\n\t\t\t// Since we know the stored SCID in the graph, we'll\n\t\t\t// cache that SCID.\n\t\t\tkey := newRejectCacheKey(\n\t\t\t\tchanInfo.ChannelID,\n\t\t\t\tsourceToPub(nMsg.source),\n\t\t\t)\n\t\t\t_, _ = d.recentRejects.Put(key, &cachedReject{})\n\n\t\t\tlog.Errorf(\"Update edge for short_chan_id(%v) got: %v\",\n\t\t\t\tshortChanID, err)\n\t\t}\n\n\t\tnMsg.err <- err\n\t\treturn nil, false\n\t}\n\n\t// If this is a local ChannelUpdate without an AuthProof, it means it\n\t// is an update to a channel that is not (yet) supposed to be announced\n\t// to the greater network. However, our channel counter party will need\n\t// to be given the update, so we'll try sending the update directly to\n\t// the remote peer.\n\tif !nMsg.isRemote && chanInfo.AuthProof == nil {\n\t\tif nMsg.optionalMsgFields != nil {\n\t\t\tremoteAlias := nMsg.optionalMsgFields.remoteAlias\n\t\t\tif remoteAlias != nil {\n\t\t\t\t// The remoteAlias field was specified, meaning\n\t\t\t\t// that we should replace the SCID in the\n\t\t\t\t// update with the remote's alias. We'll also\n\t\t\t\t// need to re-sign the channel update. This is\n\t\t\t\t// required for option-scid-alias feature-bit\n\t\t\t\t// negotiated channels.\n\t\t\t\tupd.ShortChannelID = *remoteAlias\n\n\t\t\t\tsig, err := d.cfg.SignAliasUpdate(upd)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Error(err)\n\t\t\t\t\tnMsg.err <- err\n\t\t\t\t\treturn nil, false\n\t\t\t\t}\n\n\t\t\t\tlnSig, err := lnwire.NewSigFromSignature(sig)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Error(err)\n\t\t\t\t\tnMsg.err <- err\n\t\t\t\t\treturn nil, false\n\t\t\t\t}\n\n\t\t\t\tupd.Signature = lnSig\n\t\t\t}\n\t\t}\n\n\t\t// Get our peer's public key.\n\t\tremotePubKey := remotePubFromChanInfo(\n\t\t\tchanInfo, upd.ChannelFlags,\n\t\t)\n\n\t\tlog.Debugf(\"The message %v has no AuthProof, sending the \"+\n\t\t\t\"update to remote peer %x\", upd.MsgType(), remotePubKey)\n\n\t\t// Now we'll attempt to send the channel update message\n\t\t// reliably to the remote peer in the background, so that we\n\t\t// don't block if the peer happens to be offline at the moment.\n\t\terr := d.reliableSender.sendMessage(upd, remotePubKey)\n\t\tif err != nil {\n\t\t\terr := fmt.Errorf(\"unable to reliably send %v for \"+\n\t\t\t\t\"channel=%v to peer=%x: %v\", upd.MsgType(),\n\t\t\t\tupd.ShortChannelID, remotePubKey, err)\n\t\t\tnMsg.err <- err\n\t\t\treturn nil, false\n\t\t}\n\t}\n\n\t// Channel update announcement was successfully processed and now it\n\t// can be broadcast to the rest of the network. However, we'll only\n\t// broadcast the channel update announcement if it has an attached\n\t// authentication proof. We also won't broadcast the update if it\n\t// contains an alias because the network would reject this.\n\tvar announcements []networkMsg\n\tif chanInfo.AuthProof != nil && !d.cfg.IsAlias(upd.ShortChannelID) {\n\t\tannouncements = append(announcements, networkMsg{\n\t\t\tpeer:     nMsg.peer,\n\t\t\tsource:   nMsg.source,\n\t\t\tisRemote: nMsg.isRemote,\n\t\t\tmsg:      upd,\n\t\t})\n\t}\n\n\tnMsg.err <- nil\n\n\tlog.Debugf(\"Processed ChannelUpdate: peer=%v, short_chan_id=%v, \"+\n\t\t\"timestamp=%v\", nMsg.peer, upd.ShortChannelID.ToUint64(),\n\t\ttimestamp)\n\treturn announcements, true\n}\n\n// handleAnnSig processes a new announcement signatures message.",
      "length": 12789,
      "tokens": 1722,
      "embedding": []
    },
    {
      "slug": "func (d *AuthenticatedGossiper) handleAnnSig(nMsg *networkMsg,",
      "content": "func (d *AuthenticatedGossiper) handleAnnSig(nMsg *networkMsg,\n\tann *lnwire.AnnounceSignatures) ([]networkMsg, bool) {\n\n\tneedBlockHeight := ann.ShortChannelID.BlockHeight +\n\t\td.cfg.ProofMatureDelta\n\tshortChanID := ann.ShortChannelID.ToUint64()\n\n\tprefix := \"local\"\n\tif nMsg.isRemote {\n\t\tprefix = \"remote\"\n\t}\n\n\tlog.Infof(\"Received new %v announcement signature for %v\", prefix,\n\t\tann.ShortChannelID)\n\n\t// By the specification, channel announcement proofs should be sent\n\t// after some number of confirmations after channel was registered in\n\t// bitcoin blockchain. Therefore, we check if the proof is mature.\n\td.Lock()\n\tpremature := d.isPremature(\n\t\tann.ShortChannelID, d.cfg.ProofMatureDelta, nMsg,\n\t)\n\tif premature {\n\t\tlog.Warnf(\"Premature proof announcement, current block height\"+\n\t\t\t\"lower than needed: %v < %v\", d.bestHeight,\n\t\t\tneedBlockHeight)\n\t\td.Unlock()\n\t\tnMsg.err <- nil\n\t\treturn nil, false\n\t}\n\td.Unlock()\n\n\t// Ensure that we know of a channel with the target channel ID before\n\t// proceeding further.\n\t//\n\t// We must acquire the mutex for this channel ID before getting the\n\t// channel from the database, to ensure what we read does not change\n\t// before we call AddProof() later.\n\td.channelMtx.Lock(ann.ShortChannelID.ToUint64())\n\tdefer d.channelMtx.Unlock(ann.ShortChannelID.ToUint64())\n\n\tchanInfo, e1, e2, err := d.cfg.Router.GetChannelByID(\n\t\tann.ShortChannelID,\n\t)\n\tif err != nil {\n\t\t_, err = d.cfg.FindChannel(nMsg.source, ann.ChannelID)\n\t\tif err != nil {\n\t\t\terr := fmt.Errorf(\"unable to store the proof for \"+\n\t\t\t\t\"short_chan_id=%v: %v\", shortChanID, err)\n\t\t\tlog.Error(err)\n\t\t\tnMsg.err <- err\n\n\t\t\treturn nil, false\n\t\t}\n\n\t\tproof := channeldb.NewWaitingProof(nMsg.isRemote, ann)\n\t\terr := d.cfg.WaitingProofStore.Add(proof)\n\t\tif err != nil {\n\t\t\terr := fmt.Errorf(\"unable to store the proof for \"+\n\t\t\t\t\"short_chan_id=%v: %v\", shortChanID, err)\n\t\t\tlog.Error(err)\n\t\t\tnMsg.err <- err\n\t\t\treturn nil, false\n\t\t}\n\n\t\tlog.Infof(\"Orphan %v proof announcement with short_chan_id=%v\"+\n\t\t\t\", adding to waiting batch\", prefix, shortChanID)\n\t\tnMsg.err <- nil\n\t\treturn nil, false\n\t}\n\n\tnodeID := nMsg.source.SerializeCompressed()\n\tisFirstNode := bytes.Equal(nodeID, chanInfo.NodeKey1Bytes[:])\n\tisSecondNode := bytes.Equal(nodeID, chanInfo.NodeKey2Bytes[:])\n\n\t// Ensure that channel that was retrieved belongs to the peer which\n\t// sent the proof announcement.\n\tif !(isFirstNode || isSecondNode) {\n\t\terr := fmt.Errorf(\"channel that was received doesn't belong \"+\n\t\t\t\"to the peer which sent the proof, short_chan_id=%v\",\n\t\t\tshortChanID)\n\t\tlog.Error(err)\n\t\tnMsg.err <- err\n\t\treturn nil, false\n\t}\n\n\t// If proof was sent by a local sub-system, then we'll send the\n\t// announcement signature to the remote node so they can also\n\t// reconstruct the full channel announcement.\n\tif !nMsg.isRemote {\n\t\tvar remotePubKey [33]byte\n\t\tif isFirstNode {\n\t\t\tremotePubKey = chanInfo.NodeKey2Bytes\n\t\t} else {\n\t\t\tremotePubKey = chanInfo.NodeKey1Bytes\n\t\t}\n\n\t\t// Since the remote peer might not be online we'll call a\n\t\t// method that will attempt to deliver the proof when it comes\n\t\t// online.\n\t\terr := d.reliableSender.sendMessage(ann, remotePubKey)\n\t\tif err != nil {\n\t\t\terr := fmt.Errorf(\"unable to reliably send %v for \"+\n\t\t\t\t\"channel=%v to peer=%x: %v\", ann.MsgType(),\n\t\t\t\tann.ShortChannelID, remotePubKey, err)\n\t\t\tnMsg.err <- err\n\t\t\treturn nil, false\n\t\t}\n\t}\n\n\t// Check if we already have the full proof for this channel.\n\tif chanInfo.AuthProof != nil {\n\t\t// If we already have the fully assembled proof, then the peer\n\t\t// sending us their proof has probably not received our local\n\t\t// proof yet. So be kind and send them the full proof.\n\t\tif nMsg.isRemote {\n\t\t\tpeerID := nMsg.source.SerializeCompressed()\n\t\t\tlog.Debugf(\"Got AnnounceSignatures for channel with \" +\n\t\t\t\t\"full proof.\")\n\n\t\t\td.wg.Add(1)\n\t\t\tgo func() {\n\t\t\t\tdefer d.wg.Done()\n\n\t\t\t\tlog.Debugf(\"Received half proof for channel \"+\n\t\t\t\t\t\"%v with existing full proof. Sending\"+\n\t\t\t\t\t\" full proof to peer=%x\",\n\t\t\t\t\tann.ChannelID, peerID)\n\n\t\t\t\tca, _, _, err := netann.CreateChanAnnouncement(\n\t\t\t\t\tchanInfo.AuthProof, chanInfo, e1, e2,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Errorf(\"unable to gen ann: %v\",\n\t\t\t\t\t\terr)\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\terr = nMsg.peer.SendMessage(false, ca)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Errorf(\"Failed sending full proof\"+\n\t\t\t\t\t\t\" to peer=%x: %v\", peerID, err)\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tlog.Debugf(\"Full proof sent to peer=%x for \"+\n\t\t\t\t\t\"chanID=%v\", peerID, ann.ChannelID)\n\t\t\t}()\n\t\t}\n\n\t\tlog.Debugf(\"Already have proof for channel with chanID=%v\",\n\t\t\tann.ChannelID)\n\t\tnMsg.err <- nil\n\t\treturn nil, true\n\t}\n\n\t// Check that we received the opposite proof. If so, then we're now\n\t// able to construct the full proof, and create the channel\n\t// announcement. If we didn't receive the opposite half of the proof\n\t// then we should store this one, and wait for the opposite to be\n\t// received.\n\tproof := channeldb.NewWaitingProof(nMsg.isRemote, ann)\n\toppProof, err := d.cfg.WaitingProofStore.Get(proof.OppositeKey())\n\tif err != nil && err != channeldb.ErrWaitingProofNotFound {\n\t\terr := fmt.Errorf(\"unable to get the opposite proof for \"+\n\t\t\t\"short_chan_id=%v: %v\", shortChanID, err)\n\t\tlog.Error(err)\n\t\tnMsg.err <- err\n\t\treturn nil, false\n\t}\n\n\tif err == channeldb.ErrWaitingProofNotFound {\n\t\terr := d.cfg.WaitingProofStore.Add(proof)\n\t\tif err != nil {\n\t\t\terr := fmt.Errorf(\"unable to store the proof for \"+\n\t\t\t\t\"short_chan_id=%v: %v\", shortChanID, err)\n\t\t\tlog.Error(err)\n\t\t\tnMsg.err <- err\n\t\t\treturn nil, false\n\t\t}\n\n\t\tlog.Infof(\"1/2 of channel ann proof received for \"+\n\t\t\t\"short_chan_id=%v, waiting for other half\",\n\t\t\tshortChanID)\n\n\t\tnMsg.err <- nil\n\t\treturn nil, false\n\t}\n\n\t// We now have both halves of the channel announcement proof, then\n\t// we'll reconstruct the initial announcement so we can validate it\n\t// shortly below.\n\tvar dbProof channeldb.ChannelAuthProof\n\tif isFirstNode {\n\t\tdbProof.NodeSig1Bytes = ann.NodeSignature.ToSignatureBytes()\n\t\tdbProof.NodeSig2Bytes = oppProof.NodeSignature.ToSignatureBytes()\n\t\tdbProof.BitcoinSig1Bytes = ann.BitcoinSignature.ToSignatureBytes()\n\t\tdbProof.BitcoinSig2Bytes = oppProof.BitcoinSignature.ToSignatureBytes()\n\t} else {\n\t\tdbProof.NodeSig1Bytes = oppProof.NodeSignature.ToSignatureBytes()\n\t\tdbProof.NodeSig2Bytes = ann.NodeSignature.ToSignatureBytes()\n\t\tdbProof.BitcoinSig1Bytes = oppProof.BitcoinSignature.ToSignatureBytes()\n\t\tdbProof.BitcoinSig2Bytes = ann.BitcoinSignature.ToSignatureBytes()\n\t}\n\n\tchanAnn, e1Ann, e2Ann, err := netann.CreateChanAnnouncement(\n\t\t&dbProof, chanInfo, e1, e2,\n\t)\n\tif err != nil {\n\t\tlog.Error(err)\n\t\tnMsg.err <- err\n\t\treturn nil, false\n\t}\n\n\t// With all the necessary components assembled validate the full\n\t// channel announcement proof.\n\tif err := routing.ValidateChannelAnn(chanAnn); err != nil {\n\t\terr := fmt.Errorf(\"channel announcement proof for \"+\n\t\t\t\"short_chan_id=%v isn't valid: %v\", shortChanID, err)\n\n\t\tlog.Error(err)\n\t\tnMsg.err <- err\n\t\treturn nil, false\n\t}\n\n\t// If the channel was returned by the router it means that existence of\n\t// funding point and inclusion of nodes bitcoin keys in it already\n\t// checked by the router. In this stage we should check that node keys\n\t// attest to the bitcoin keys by validating the signatures of\n\t// announcement. If proof is valid then we'll populate the channel edge\n\t// with it, so we can announce it on peer connect.\n\terr = d.cfg.Router.AddProof(ann.ShortChannelID, &dbProof)\n\tif err != nil {\n\t\terr := fmt.Errorf(\"unable add proof to the channel chanID=%v:\"+\n\t\t\t\" %v\", ann.ChannelID, err)\n\t\tlog.Error(err)\n\t\tnMsg.err <- err\n\t\treturn nil, false\n\t}\n\n\terr = d.cfg.WaitingProofStore.Remove(proof.OppositeKey())\n\tif err != nil {\n\t\terr := fmt.Errorf(\"unable to remove opposite proof for the \"+\n\t\t\t\"channel with chanID=%v: %v\", ann.ChannelID, err)\n\t\tlog.Error(err)\n\t\tnMsg.err <- err\n\t\treturn nil, false\n\t}\n\n\t// Proof was successfully created and now can announce the channel to\n\t// the remain network.\n\tlog.Infof(\"Fully valid channel proof for short_chan_id=%v constructed\"+\n\t\t\", adding to next ann batch\", shortChanID)\n\n\t// Assemble the necessary announcements to add to the next broadcasting\n\t// batch.\n\tvar announcements []networkMsg\n\tannouncements = append(announcements, networkMsg{\n\t\tpeer:   nMsg.peer,\n\t\tsource: nMsg.source,\n\t\tmsg:    chanAnn,\n\t})\n\tif src, err := chanInfo.NodeKey1(); err == nil && e1Ann != nil {\n\t\tannouncements = append(announcements, networkMsg{\n\t\t\tpeer:   nMsg.peer,\n\t\t\tsource: src,\n\t\t\tmsg:    e1Ann,\n\t\t})\n\t}\n\tif src, err := chanInfo.NodeKey2(); err == nil && e2Ann != nil {\n\t\tannouncements = append(announcements, networkMsg{\n\t\t\tpeer:   nMsg.peer,\n\t\t\tsource: src,\n\t\t\tmsg:    e2Ann,\n\t\t})\n\t}\n\n\t// We'll also send along the node announcements for each channel\n\t// participant if we know of them. To ensure our node announcement\n\t// propagates to our channel counterparty, we'll set the source for\n\t// each announcement to the node it belongs to, otherwise we won't send\n\t// it since the source gets skipped. This isn't necessary for channel\n\t// updates and announcement signatures since we send those directly to\n\t// our channel counterparty through the gossiper's reliable sender.\n\tnode1Ann, err := d.fetchNodeAnn(chanInfo.NodeKey1Bytes)\n\tif err != nil {\n\t\tlog.Debugf(\"Unable to fetch node announcement for %x: %v\",\n\t\t\tchanInfo.NodeKey1Bytes, err)\n\t} else {\n\t\tif nodeKey1, err := chanInfo.NodeKey1(); err == nil {\n\t\t\tannouncements = append(announcements, networkMsg{\n\t\t\t\tpeer:   nMsg.peer,\n\t\t\t\tsource: nodeKey1,\n\t\t\t\tmsg:    node1Ann,\n\t\t\t})\n\t\t}\n\t}\n\n\tnode2Ann, err := d.fetchNodeAnn(chanInfo.NodeKey2Bytes)\n\tif err != nil {\n\t\tlog.Debugf(\"Unable to fetch node announcement for %x: %v\",\n\t\t\tchanInfo.NodeKey2Bytes, err)\n\t} else {\n\t\tif nodeKey2, err := chanInfo.NodeKey2(); err == nil {\n\t\t\tannouncements = append(announcements, networkMsg{\n\t\t\t\tpeer:   nMsg.peer,\n\t\t\t\tsource: nodeKey2,\n\t\t\t\tmsg:    node2Ann,\n\t\t\t})\n\t\t}\n\t}\n\n\tnMsg.err <- nil\n\treturn announcements, true\n}\n",
      "length": 9487,
      "tokens": 1270,
      "embedding": []
    }
  ]
}