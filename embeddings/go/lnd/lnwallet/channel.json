{
  "filepath": "../implementations/go/lnd/lnwallet/channel.go",
  "package": "lnwallet",
  "sections": [
    {
      "slug": "type ErrCommitSyncLocalDataLoss struct {",
      "content": "type ErrCommitSyncLocalDataLoss struct {\n\t// ChannelPoint is the identifier for the channel that experienced data\n\t// loss.\n\tChannelPoint wire.OutPoint\n\n\t// CommitPoint is the last unrevoked commit point, sent to us by the\n\t// remote when we determined we had lost state.\n\tCommitPoint *btcec.PublicKey\n}\n\n// Error returns a string representation of the local data loss error.",
      "length": 325,
      "tokens": 52,
      "embedding": []
    },
    {
      "slug": "func (e *ErrCommitSyncLocalDataLoss) Error() string {",
      "content": "func (e *ErrCommitSyncLocalDataLoss) Error() string {\n\treturn fmt.Sprintf(\"ChannelPoint(%v) with CommitPoint(%x) had \"+\n\t\t\"possible local commitment state data loss\", e.ChannelPoint,\n\t\te.CommitPoint.SerializeCompressed())\n}\n\n// channelState is an enum like type which represents the current state of a\n// particular channel.\n// TODO(roasbeef): actually update state",
      "length": 304,
      "tokens": 37,
      "embedding": []
    },
    {
      "slug": "type channelState uint8",
      "content": "type channelState uint8\n\nconst (\n\t// channelPending indicates this channel is still going through the\n\t// funding workflow, and isn't yet open.\n\tchannelPending channelState = iota // nolint: unused\n\n\t// channelOpen represents an open, active channel capable of\n\t// sending/receiving HTLCs.\n\tchannelOpen\n\n\t// channelClosing represents a channel which is in the process of being\n\t// closed.\n\tchannelClosing\n\n\t// channelClosed represents a channel which has been fully closed. Note\n\t// that before a channel can be closed, ALL pending HTLCs must be\n\t// settled/removed.\n\tchannelClosed\n\n\t// channelDispute indicates that an un-cooperative closure has been\n\t// detected within the channel.\n\tchannelDispute\n\n\t// channelPendingPayment indicates that there a currently outstanding\n\t// HTLCs within the channel.\n\tchannelPendingPayment // nolint:unused\n)\n\n// PaymentHash represents the sha256 of a random value. This hash is used to\n// uniquely track incoming/outgoing payments within this channel, as well as\n// payments requested by the wallet/daemon.",
      "length": 989,
      "tokens": 144,
      "embedding": []
    },
    {
      "slug": "type PaymentHash [32]byte",
      "content": "type PaymentHash [32]byte\n\n// updateType is the exact type of an entry within the shared HTLC log.",
      "length": 71,
      "tokens": 14,
      "embedding": []
    },
    {
      "slug": "type updateType uint8",
      "content": "type updateType uint8\n\nconst (\n\t// Add is an update type that adds a new HTLC entry into the log.\n\t// Either side can add a new pending HTLC by adding a new Add entry\n\t// into their update log.\n\tAdd updateType = iota\n\n\t// Fail is an update type which removes a prior HTLC entry from the\n\t// log. Adding a Fail entry to ones log will modify the _remote_\n\t// parties update log once a new commitment view has been evaluated\n\t// which contains the Fail entry.\n\tFail\n\n\t// MalformedFail is an update type which removes a prior HTLC entry\n\t// from the log. Adding a MalformedFail entry to ones log will modify\n\t// the _remote_ parties update log once a new commitment view has been\n\t// evaluated which contains the MalformedFail entry. The difference\n\t// from Fail type lie in the different data we have to store.\n\tMalformedFail\n\n\t// Settle is an update type which settles a prior HTLC crediting the\n\t// balance of the receiving node. Adding a Settle entry to a log will\n\t// result in the settle entry being removed on the log as well as the\n\t// original add entry from the remote party's log after the next state\n\t// transition.\n\tSettle\n\n\t// FeeUpdate is an update type sent by the channel initiator that\n\t// updates the fee rate used when signing the commitment transaction.\n\tFeeUpdate\n)\n\n// String returns a human readable string that uniquely identifies the target\n// update type.",
      "length": 1323,
      "tokens": 246,
      "embedding": []
    },
    {
      "slug": "func (u updateType) String() string {",
      "content": "func (u updateType) String() string {\n\tswitch u {\n\tcase Add:\n\t\treturn \"Add\"\n\tcase Fail:\n\t\treturn \"Fail\"\n\tcase MalformedFail:\n\t\treturn \"MalformedFail\"\n\tcase Settle:\n\t\treturn \"Settle\"\n\tcase FeeUpdate:\n\t\treturn \"FeeUpdate\"\n\tdefault:\n\t\treturn \"<unknown type>\"\n\t}\n}\n\n// PaymentDescriptor represents a commitment state update which either adds,\n// settles, or removes an HTLC. PaymentDescriptors encapsulate all necessary\n// metadata w.r.t to an HTLC, and additional data pairing a settle message to\n// the original added HTLC.\n//\n// TODO(roasbeef): LogEntry interface??\n//   - need to separate attrs for cancel/add/settle/feeupdate",
      "length": 566,
      "tokens": 81,
      "embedding": []
    },
    {
      "slug": "type PaymentDescriptor struct {",
      "content": "type PaymentDescriptor struct {\n\t// RHash is the payment hash for this HTLC. The HTLC can be settled iff\n\t// the preimage to this hash is presented.\n\tRHash PaymentHash\n\n\t// RPreimage is the preimage that settles the HTLC pointed to within the\n\t// log by the ParentIndex.\n\tRPreimage PaymentHash\n\n\t// Timeout is the absolute timeout in blocks, after which this HTLC\n\t// expires.\n\tTimeout uint32\n\n\t// Amount is the HTLC amount in milli-satoshis.\n\tAmount lnwire.MilliSatoshi\n\n\t// LogIndex is the log entry number that his HTLC update has within the\n\t// log. Depending on if IsIncoming is true, this is either an entry the\n\t// remote party added, or one that we added locally.\n\tLogIndex uint64\n\n\t// HtlcIndex is the index within the main update log for this HTLC.\n\t// Entries within the log of type Add will have this field populated,\n\t// as other entries will point to the entry via this counter.\n\t//\n\t// NOTE: This field will only be populate if EntryType is Add.\n\tHtlcIndex uint64\n\n\t// ParentIndex is the HTLC index of the entry that this update settles or\n\t// times out.\n\t//\n\t// NOTE: This field will only be populate if EntryType is Fail or\n\t// Settle.\n\tParentIndex uint64\n\n\t// SourceRef points to an Add update in a forwarding package owned by\n\t// this channel.\n\t//\n\t// NOTE: This field will only be populated if EntryType is Fail or\n\t// Settle.\n\tSourceRef *channeldb.AddRef\n\n\t// DestRef points to a Fail/Settle update in another link's forwarding\n\t// package.\n\t//\n\t// NOTE: This field will only be populated if EntryType is Fail or\n\t// Settle, and the forwarded Add successfully included in an outgoing\n\t// link's commitment txn.\n\tDestRef *channeldb.SettleFailRef\n\n\t// OpenCircuitKey references the incoming Chan/HTLC ID of an Add HTLC\n\t// packet delivered by the switch.\n\t//\n\t// NOTE: This field is only populated for payment descriptors in the\n\t// *local* update log, and if the Add packet was delivered by the\n\t// switch.\n\tOpenCircuitKey *models.CircuitKey\n\n\t// ClosedCircuitKey references the incoming Chan/HTLC ID of the Add HTLC\n\t// that opened the circuit.\n\t//\n\t// NOTE: This field is only populated for payment descriptors in the\n\t// *local* update log, and if settle/fails have a committed circuit in\n\t// the circuit map.\n\tClosedCircuitKey *models.CircuitKey\n\n\t// localOutputIndex is the output index of this HTLc output in the\n\t// commitment transaction of the local node.\n\t//\n\t// NOTE: If the output is dust from the PoV of the local commitment\n\t// chain, then this value will be -1.\n\tlocalOutputIndex int32\n\n\t// remoteOutputIndex is the output index of this HTLC output in the\n\t// commitment transaction of the remote node.\n\t//\n\t// NOTE: If the output is dust from the PoV of the remote commitment\n\t// chain, then this value will be -1.\n\tremoteOutputIndex int32\n\n\t// sig is the signature for the second-level HTLC transaction that\n\t// spends the version of this HTLC on the commitment transaction of the\n\t// local node. This signature is generated by the remote node and\n\t// stored by the local node in the case that local node needs to\n\t// broadcast their commitment transaction.\n\tsig *ecdsa.Signature\n\n\t// addCommitHeight[Remote|Local] encodes the height of the commitment\n\t// which included this HTLC on either the remote or local commitment\n\t// chain. This value is used to determine when an HTLC is fully\n\t// \"locked-in\".\n\taddCommitHeightRemote uint64\n\taddCommitHeightLocal  uint64\n\n\t// removeCommitHeight[Remote|Local] encodes the height of the\n\t// commitment which removed the parent pointer of this\n\t// PaymentDescriptor either due to a timeout or a settle. Once both\n\t// these heights are below the tail of both chains, the log entries can\n\t// safely be removed.\n\tremoveCommitHeightRemote uint64\n\tremoveCommitHeightLocal  uint64\n\n\t// OnionBlob is an opaque blob which is used to complete multi-hop\n\t// routing.\n\t//\n\t// NOTE: Populated only on add payment descriptor entry types.\n\tOnionBlob []byte\n\n\t// ShaOnionBlob is a sha of the onion blob.\n\t//\n\t// NOTE: Populated only in payment descriptor with MalformedFail type.\n\tShaOnionBlob [sha256.Size]byte\n\n\t// FailReason stores the reason why a particular payment was canceled.\n\t//\n\t// NOTE: Populate only in fail payment descriptor entry types.\n\tFailReason []byte\n\n\t// FailCode stores the code why a particular payment was canceled.\n\t//\n\t// NOTE: Populated only in payment descriptor with MalformedFail type.\n\tFailCode lnwire.FailCode\n\n\t// [our|their|]PkScript are the raw public key scripts that encodes the\n\t// redemption rules for this particular HTLC. These fields will only be\n\t// populated iff the EntryType of this PaymentDescriptor is Add.\n\t// ourPkScript is the ourPkScript from the context of our local\n\t// commitment chain. theirPkScript is the latest pkScript from the\n\t// context of the remote commitment chain.\n\t//\n\t// NOTE: These values may change within the logs themselves, however,\n\t// they'll stay consistent within the commitment chain entries\n\t// themselves.\n\tourPkScript        []byte\n\tourWitnessScript   []byte\n\ttheirPkScript      []byte\n\ttheirWitnessScript []byte\n\n\t// EntryType denotes the exact type of the PaymentDescriptor. In the\n\t// case of a Timeout, or Settle type, then the Parent field will point\n\t// into the log to the HTLC being modified.\n\tEntryType updateType\n\n\t// isForwarded denotes if an incoming HTLC has been forwarded to any\n\t// possible upstream peers in the route.\n\tisForwarded bool\n}\n\n// PayDescsFromRemoteLogUpdates converts a slice of LogUpdates received from the\n// remote peer into PaymentDescriptors to inform a link's forwarding decisions.\n//\n// NOTE: The provided `logUpdates` MUST corresponding exactly to either the Adds\n// or SettleFails in this channel's forwarding package at `height`.",
      "length": 5531,
      "tokens": 896,
      "embedding": []
    },
    {
      "slug": "func PayDescsFromRemoteLogUpdates(chanID lnwire.ShortChannelID, height uint64,",
      "content": "func PayDescsFromRemoteLogUpdates(chanID lnwire.ShortChannelID, height uint64,\n\tlogUpdates []channeldb.LogUpdate) ([]*PaymentDescriptor, error) {\n\n\t// Allocate enough space to hold all of the payment descriptors we will\n\t// reconstruct, and also the list of pointers that will be returned to\n\t// the caller.\n\tpayDescs := make([]PaymentDescriptor, 0, len(logUpdates))\n\tpayDescPtrs := make([]*PaymentDescriptor, 0, len(logUpdates))\n\n\t// Iterate over the log updates we loaded from disk, and reconstruct the\n\t// payment descriptor corresponding to one of the four types of htlcs we\n\t// can receive from the remote peer. We only repopulate the information\n\t// necessary to process the packets and, if necessary, forward them to\n\t// the switch.\n\t//\n\t// For each log update, we include either an AddRef or a SettleFailRef\n\t// so that they can be ACK'd and garbage collected.\n\tfor i, logUpdate := range logUpdates {\n\t\tvar pd PaymentDescriptor\n\t\tswitch wireMsg := logUpdate.UpdateMsg.(type) {\n\n\t\tcase *lnwire.UpdateAddHTLC:\n\t\t\tpd = PaymentDescriptor{\n\t\t\t\tRHash:     wireMsg.PaymentHash,\n\t\t\t\tTimeout:   wireMsg.Expiry,\n\t\t\t\tAmount:    wireMsg.Amount,\n\t\t\t\tEntryType: Add,\n\t\t\t\tHtlcIndex: wireMsg.ID,\n\t\t\t\tLogIndex:  logUpdate.LogIndex,\n\t\t\t\tSourceRef: &channeldb.AddRef{\n\t\t\t\t\tHeight: height,\n\t\t\t\t\tIndex:  uint16(i),\n\t\t\t\t},\n\t\t\t}\n\t\t\tpd.OnionBlob = make([]byte, len(wireMsg.OnionBlob))\n\t\t\tcopy(pd.OnionBlob[:], wireMsg.OnionBlob[:])\n\n\t\tcase *lnwire.UpdateFulfillHTLC:\n\t\t\tpd = PaymentDescriptor{\n\t\t\t\tRPreimage:   wireMsg.PaymentPreimage,\n\t\t\t\tParentIndex: wireMsg.ID,\n\t\t\t\tEntryType:   Settle,\n\t\t\t\tDestRef: &channeldb.SettleFailRef{\n\t\t\t\t\tSource: chanID,\n\t\t\t\t\tHeight: height,\n\t\t\t\t\tIndex:  uint16(i),\n\t\t\t\t},\n\t\t\t}\n\n\t\tcase *lnwire.UpdateFailHTLC:\n\t\t\tpd = PaymentDescriptor{\n\t\t\t\tParentIndex: wireMsg.ID,\n\t\t\t\tEntryType:   Fail,\n\t\t\t\tFailReason:  wireMsg.Reason[:],\n\t\t\t\tDestRef: &channeldb.SettleFailRef{\n\t\t\t\t\tSource: chanID,\n\t\t\t\t\tHeight: height,\n\t\t\t\t\tIndex:  uint16(i),\n\t\t\t\t},\n\t\t\t}\n\n\t\tcase *lnwire.UpdateFailMalformedHTLC:\n\t\t\tpd = PaymentDescriptor{\n\t\t\t\tParentIndex:  wireMsg.ID,\n\t\t\t\tEntryType:    MalformedFail,\n\t\t\t\tFailCode:     wireMsg.FailureCode,\n\t\t\t\tShaOnionBlob: wireMsg.ShaOnionBlob,\n\t\t\t\tDestRef: &channeldb.SettleFailRef{\n\t\t\t\t\tSource: chanID,\n\t\t\t\t\tHeight: height,\n\t\t\t\t\tIndex:  uint16(i),\n\t\t\t\t},\n\t\t\t}\n\n\t\t// NOTE: UpdateFee is not expected since they are not forwarded.\n\t\tcase *lnwire.UpdateFee:\n\t\t\treturn nil, fmt.Errorf(\"unexpected update fee\")\n\n\t\t}\n\n\t\tpayDescs = append(payDescs, pd)\n\t\tpayDescPtrs = append(payDescPtrs, &payDescs[i])\n\t}\n\n\treturn payDescPtrs, nil\n}\n\n// commitment represents a commitment to a new state within an active channel.\n// New commitments can be initiated by either side. Commitments are ordered\n// into a commitment chain, with one existing for both parties. Each side can\n// independently extend the other side's commitment chain, up to a certain\n// \"revocation window\", which once reached, disallows new commitments until\n// the local nodes receives the revocation for the remote node's chain tail.",
      "length": 2839,
      "tokens": 338,
      "embedding": []
    },
    {
      "slug": "type commitment struct {",
      "content": "type commitment struct {\n\t// height represents the commitment height of this commitment, or the\n\t// update number of this commitment.\n\theight uint64\n\n\t// isOurs indicates whether this is the local or remote node's version\n\t// of the commitment.\n\tisOurs bool\n\n\t// [our|their]MessageIndex are indexes into the HTLC log, up to which\n\t// this commitment transaction includes. These indexes allow both sides\n\t// to independently, and concurrent send create new commitments. Each\n\t// new commitment sent to the remote party includes an index in the\n\t// shared log which details which of their updates we're including in\n\t// this new commitment.\n\tourMessageIndex   uint64\n\ttheirMessageIndex uint64\n\n\t// [our|their]HtlcIndex are the current running counters for the HTLC's\n\t// offered by either party. This value is incremented each time a party\n\t// offers a new HTLC. The log update methods that consume HTLC's will\n\t// reference these counters, rather than the running cumulative message\n\t// counters.\n\tourHtlcIndex   uint64\n\ttheirHtlcIndex uint64\n\n\t// txn is the commitment transaction generated by including any HTLC\n\t// updates whose index are below the two indexes listed above. If this\n\t// commitment is being added to the remote chain, then this txn is\n\t// their version of the commitment transactions. If the local commit\n\t// chain is being modified, the opposite is true.\n\ttxn *wire.MsgTx\n\n\t// sig is a signature for the above commitment transaction.\n\tsig []byte\n\n\t// [our|their]Balance represents the settled balances at this point\n\t// within the commitment chain. This balance is computed by properly\n\t// evaluating all the add/remove/settle log entries before the listed\n\t// indexes.\n\t//\n\t// NOTE: This is the balance *after* subtracting any commitment fee,\n\t// AND anchor output values.\n\tourBalance   lnwire.MilliSatoshi\n\ttheirBalance lnwire.MilliSatoshi\n\n\t// fee is the amount that will be paid as fees for this commitment\n\t// transaction. The fee is recorded here so that it can be added back\n\t// and recalculated for each new update to the channel state.\n\tfee btcutil.Amount\n\n\t// feePerKw is the fee per kw used to calculate this commitment\n\t// transaction's fee.\n\tfeePerKw chainfee.SatPerKWeight\n\n\t// dustLimit is the limit on the commitment transaction such that no\n\t// output values should be below this amount.\n\tdustLimit btcutil.Amount\n\n\t// outgoingHTLCs is a slice of all the outgoing HTLC's (from our PoV)\n\t// on this commitment transaction.\n\toutgoingHTLCs []PaymentDescriptor\n\n\t// incomingHTLCs is a slice of all the incoming HTLC's (from our PoV)\n\t// on this commitment transaction.\n\tincomingHTLCs []PaymentDescriptor\n\n\t// [outgoing|incoming]HTLCIndex is an index that maps an output index\n\t// on the commitment transaction to the payment descriptor that\n\t// represents the HTLC output.\n\t//\n\t// NOTE: that these fields are only populated if this commitment state\n\t// belongs to the local node. These maps are used when validating any\n\t// HTLC signatures which are part of the local commitment state. We use\n\t// this map in order to locate the details needed to validate an HTLC\n\t// signature while iterating of the outputs in the local commitment\n\t// view.\n\toutgoingHTLCIndex map[int32]*PaymentDescriptor\n\tincomingHTLCIndex map[int32]*PaymentDescriptor\n}\n\n// locateOutputIndex is a small helper function to locate the output index of a\n// particular HTLC within the current commitment transaction. The duplicate map\n// massed in is to be retained for each output within the commitment\n// transition.  This ensures that we don't assign multiple HTLC's to the same\n// index within the commitment transaction.",
      "length": 3515,
      "tokens": 550,
      "embedding": []
    },
    {
      "slug": "func locateOutputIndex(p *PaymentDescriptor, tx *wire.MsgTx, ourCommit bool,",
      "content": "func locateOutputIndex(p *PaymentDescriptor, tx *wire.MsgTx, ourCommit bool,\n\tdups map[PaymentHash][]int32, cltvs []uint32) (int32, error) {\n\n\t// Checks to see if element (e) exists in slice (s).\n\tcontains := func(s []int32, e int32) bool {\n\t\tfor _, a := range s {\n\t\t\tif a == e {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t\treturn false\n\t}\n\n\t// If this their commitment transaction, we'll be trying to locate\n\t// their pkScripts, otherwise we'll be looking for ours. This is\n\t// required as the commitment states are asymmetric in order to ascribe\n\t// blame in the case of a contract breach.\n\tpkScript := p.theirPkScript\n\tif ourCommit {\n\t\tpkScript = p.ourPkScript\n\t}\n\n\tfor i, txOut := range tx.TxOut {\n\t\tcltv := cltvs[i]\n\n\t\tif bytes.Equal(txOut.PkScript, pkScript) &&\n\t\t\ttxOut.Value == int64(p.Amount.ToSatoshis()) &&\n\t\t\tcltv == p.Timeout {\n\n\t\t\t// If this payment hash and index has already been\n\t\t\t// found, then we'll continue in order to avoid any\n\t\t\t// duplicate indexes.\n\t\t\tif contains(dups[p.RHash], int32(i)) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tidx := int32(i)\n\t\t\tdups[p.RHash] = append(dups[p.RHash], idx)\n\t\t\treturn idx, nil\n\t\t}\n\t}\n\n\treturn 0, fmt.Errorf(\"unable to find htlc: script=%x, value=%v, \"+\n\t\t\"cltv=%v\", pkScript, p.Amount, p.Timeout)\n}\n\n// populateHtlcIndexes modifies the set of HTLC's locked-into the target view\n// to have full indexing information populated. This information is required as\n// we need to keep track of the indexes of each HTLC in order to properly write\n// the current state to disk, and also to locate the PaymentDescriptor\n// corresponding to HTLC outputs in the commitment transaction.",
      "length": 1473,
      "tokens": 236,
      "embedding": []
    },
    {
      "slug": "func (c *commitment) populateHtlcIndexes(chanType channeldb.ChannelType,",
      "content": "func (c *commitment) populateHtlcIndexes(chanType channeldb.ChannelType,\n\tcltvs []uint32) error {\n\n\t// First, we'll set up some state to allow us to locate the output\n\t// index of the all the HTLC's within the commitment transaction. We\n\t// must keep this index so we can validate the HTLC signatures sent to\n\t// us.\n\tdups := make(map[PaymentHash][]int32)\n\tc.outgoingHTLCIndex = make(map[int32]*PaymentDescriptor)\n\tc.incomingHTLCIndex = make(map[int32]*PaymentDescriptor)\n\n\t// populateIndex is a helper function that populates the necessary\n\t// indexes within the commitment view for a particular HTLC.\n\tpopulateIndex := func(htlc *PaymentDescriptor, incoming bool) error {\n\t\tisDust := HtlcIsDust(\n\t\t\tchanType, incoming, c.isOurs, c.feePerKw,\n\t\t\thtlc.Amount.ToSatoshis(), c.dustLimit,\n\t\t)\n\n\t\tvar err error\n\t\tswitch {\n\n\t\t// If this is our commitment transaction, and this is a dust\n\t\t// output then we mark it as such using a -1 index.\n\t\tcase c.isOurs && isDust:\n\t\t\thtlc.localOutputIndex = -1\n\n\t\t// If this is the commitment transaction of the remote party,\n\t\t// and this is a dust output then we mark it as such using a -1\n\t\t// index.\n\t\tcase !c.isOurs && isDust:\n\t\t\thtlc.remoteOutputIndex = -1\n\n\t\t// If this is our commitment transaction, then we'll need to\n\t\t// locate the output and the index so we can verify an HTLC\n\t\t// signatures.\n\t\tcase c.isOurs:\n\t\t\thtlc.localOutputIndex, err = locateOutputIndex(\n\t\t\t\thtlc, c.txn, c.isOurs, dups, cltvs,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t// As this is our commitment transactions, we need to\n\t\t\t// keep track of the locations of each output on the\n\t\t\t// transaction so we can verify any HTLC signatures\n\t\t\t// sent to us after we construct the HTLC view.\n\t\t\tif incoming {\n\t\t\t\tc.incomingHTLCIndex[htlc.localOutputIndex] = htlc\n\t\t\t} else {\n\t\t\t\tc.outgoingHTLCIndex[htlc.localOutputIndex] = htlc\n\t\t\t}\n\n\t\t// Otherwise, this is there remote party's commitment\n\t\t// transaction and we only need to populate the remote output\n\t\t// index within the HTLC index.\n\t\tcase !c.isOurs:\n\t\t\thtlc.remoteOutputIndex, err = locateOutputIndex(\n\t\t\t\thtlc, c.txn, c.isOurs, dups, cltvs,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"invalid commitment configuration\")\n\t\t}\n\n\t\treturn nil\n\t}\n\n\t// Finally, we'll need to locate the index within the commitment\n\t// transaction of all the HTLC outputs. This index will be required\n\t// later when we write the commitment state to disk, and also when\n\t// generating signatures for each of the HTLC transactions.\n\tfor i := 0; i < len(c.outgoingHTLCs); i++ {\n\t\thtlc := &c.outgoingHTLCs[i]\n\t\tif err := populateIndex(htlc, false); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tfor i := 0; i < len(c.incomingHTLCs); i++ {\n\t\thtlc := &c.incomingHTLCs[i]\n\t\tif err := populateIndex(htlc, true); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// toDiskCommit converts the target commitment into a format suitable to be\n// written to disk after an accepted state transition.",
      "length": 2793,
      "tokens": 437,
      "embedding": []
    },
    {
      "slug": "func (c *commitment) toDiskCommit(ourCommit bool) *channeldb.ChannelCommitment {",
      "content": "func (c *commitment) toDiskCommit(ourCommit bool) *channeldb.ChannelCommitment {\n\tnumHtlcs := len(c.outgoingHTLCs) + len(c.incomingHTLCs)\n\n\tcommit := &channeldb.ChannelCommitment{\n\t\tCommitHeight:    c.height,\n\t\tLocalLogIndex:   c.ourMessageIndex,\n\t\tLocalHtlcIndex:  c.ourHtlcIndex,\n\t\tRemoteLogIndex:  c.theirMessageIndex,\n\t\tRemoteHtlcIndex: c.theirHtlcIndex,\n\t\tLocalBalance:    c.ourBalance,\n\t\tRemoteBalance:   c.theirBalance,\n\t\tCommitFee:       c.fee,\n\t\tFeePerKw:        btcutil.Amount(c.feePerKw),\n\t\tCommitTx:        c.txn,\n\t\tCommitSig:       c.sig,\n\t\tHtlcs:           make([]channeldb.HTLC, 0, numHtlcs),\n\t}\n\n\tfor _, htlc := range c.outgoingHTLCs {\n\t\toutputIndex := htlc.localOutputIndex\n\t\tif !ourCommit {\n\t\t\toutputIndex = htlc.remoteOutputIndex\n\t\t}\n\n\t\th := channeldb.HTLC{\n\t\t\tRHash:         htlc.RHash,\n\t\t\tAmt:           htlc.Amount,\n\t\t\tRefundTimeout: htlc.Timeout,\n\t\t\tOutputIndex:   outputIndex,\n\t\t\tHtlcIndex:     htlc.HtlcIndex,\n\t\t\tLogIndex:      htlc.LogIndex,\n\t\t\tIncoming:      false,\n\t\t}\n\t\th.OnionBlob = make([]byte, len(htlc.OnionBlob))\n\t\tcopy(h.OnionBlob[:], htlc.OnionBlob)\n\n\t\tif ourCommit && htlc.sig != nil {\n\t\t\th.Signature = htlc.sig.Serialize()\n\t\t}\n\n\t\tcommit.Htlcs = append(commit.Htlcs, h)\n\t}\n\n\tfor _, htlc := range c.incomingHTLCs {\n\t\toutputIndex := htlc.localOutputIndex\n\t\tif !ourCommit {\n\t\t\toutputIndex = htlc.remoteOutputIndex\n\t\t}\n\n\t\th := channeldb.HTLC{\n\t\t\tRHash:         htlc.RHash,\n\t\t\tAmt:           htlc.Amount,\n\t\t\tRefundTimeout: htlc.Timeout,\n\t\t\tOutputIndex:   outputIndex,\n\t\t\tHtlcIndex:     htlc.HtlcIndex,\n\t\t\tLogIndex:      htlc.LogIndex,\n\t\t\tIncoming:      true,\n\t\t}\n\t\th.OnionBlob = make([]byte, len(htlc.OnionBlob))\n\t\tcopy(h.OnionBlob[:], htlc.OnionBlob)\n\n\t\tif ourCommit && htlc.sig != nil {\n\t\t\th.Signature = htlc.sig.Serialize()\n\t\t}\n\n\t\tcommit.Htlcs = append(commit.Htlcs, h)\n\t}\n\n\treturn commit\n}\n\n// diskHtlcToPayDesc converts an HTLC previously written to disk within a\n// commitment state to the form required to manipulate in memory within the\n// commitment struct and updateLog. This function is used when we need to\n// restore commitment state written do disk back into memory once we need to\n// restart a channel session.",
      "length": 2002,
      "tokens": 208,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) diskHtlcToPayDesc(feeRate chainfee.SatPerKWeight,",
      "content": "func (lc *LightningChannel) diskHtlcToPayDesc(feeRate chainfee.SatPerKWeight,\n\tcommitHeight uint64, htlc *channeldb.HTLC, localCommitKeys,\n\tremoteCommitKeys *CommitmentKeyRing, isLocal bool) (PaymentDescriptor,\n\terror) {\n\n\t// The proper pkScripts for this PaymentDescriptor must be\n\t// generated so we can easily locate them within the commitment\n\t// transaction in the future.\n\tvar (\n\t\tourP2WSH, theirP2WSH                 []byte\n\t\tourWitnessScript, theirWitnessScript []byte\n\t\tpd                                   PaymentDescriptor\n\t\terr                                  error\n\t\tchanType                             = lc.channelState.ChanType\n\t)\n\n\t// If the either outputs is dust from the local or remote node's\n\t// perspective, then we don't need to generate the scripts as we only\n\t// generate them in order to locate the outputs within the commitment\n\t// transaction. As we'll mark dust with a special output index in the\n\t// on-disk state snapshot.\n\tisDustLocal := HtlcIsDust(\n\t\tchanType, htlc.Incoming, true, feeRate,\n\t\thtlc.Amt.ToSatoshis(), lc.channelState.LocalChanCfg.DustLimit,\n\t)\n\tif !isDustLocal && localCommitKeys != nil {\n\t\tourP2WSH, ourWitnessScript, err = genHtlcScript(\n\t\t\tchanType, htlc.Incoming, true, htlc.RefundTimeout,\n\t\t\thtlc.RHash, localCommitKeys,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn pd, err\n\t\t}\n\t}\n\tisDustRemote := HtlcIsDust(\n\t\tchanType, htlc.Incoming, false, feeRate,\n\t\thtlc.Amt.ToSatoshis(), lc.channelState.RemoteChanCfg.DustLimit,\n\t)\n\tif !isDustRemote && remoteCommitKeys != nil {\n\t\ttheirP2WSH, theirWitnessScript, err = genHtlcScript(\n\t\t\tchanType, htlc.Incoming, false, htlc.RefundTimeout,\n\t\t\thtlc.RHash, remoteCommitKeys,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn pd, err\n\t\t}\n\t}\n\n\t// Reconstruct the proper local/remote output indexes from the HTLC's\n\t// persisted output index depending on whose commitment we are\n\t// generating.\n\tvar (\n\t\tlocalOutputIndex  int32\n\t\tremoteOutputIndex int32\n\t)\n\tif isLocal {\n\t\tlocalOutputIndex = htlc.OutputIndex\n\t} else {\n\t\tremoteOutputIndex = htlc.OutputIndex\n\t}\n\n\t// With the scripts reconstructed (depending on if this is our commit\n\t// vs theirs or a pending commit for the remote party), we can now\n\t// re-create the original payment descriptor.\n\tpd = PaymentDescriptor{\n\t\tRHash:              htlc.RHash,\n\t\tTimeout:            htlc.RefundTimeout,\n\t\tAmount:             htlc.Amt,\n\t\tEntryType:          Add,\n\t\tHtlcIndex:          htlc.HtlcIndex,\n\t\tLogIndex:           htlc.LogIndex,\n\t\tOnionBlob:          htlc.OnionBlob,\n\t\tlocalOutputIndex:   localOutputIndex,\n\t\tremoteOutputIndex:  remoteOutputIndex,\n\t\tourPkScript:        ourP2WSH,\n\t\tourWitnessScript:   ourWitnessScript,\n\t\ttheirPkScript:      theirP2WSH,\n\t\ttheirWitnessScript: theirWitnessScript,\n\t}\n\n\treturn pd, nil\n}\n\n// extractPayDescs will convert all HTLC's present within a disk commit state\n// to a set of incoming and outgoing payment descriptors. Once reconstructed,\n// these payment descriptors can be re-inserted into the in-memory updateLog\n// for each side.",
      "length": 2818,
      "tokens": 333,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) extractPayDescs(commitHeight uint64,",
      "content": "func (lc *LightningChannel) extractPayDescs(commitHeight uint64,\n\tfeeRate chainfee.SatPerKWeight, htlcs []channeldb.HTLC, localCommitKeys,\n\tremoteCommitKeys *CommitmentKeyRing, isLocal bool) ([]PaymentDescriptor,\n\t[]PaymentDescriptor, error) {\n\n\tvar (\n\t\tincomingHtlcs []PaymentDescriptor\n\t\toutgoingHtlcs []PaymentDescriptor\n\t)\n\n\t// For each included HTLC within this commitment state, we'll convert\n\t// the disk format into our in memory PaymentDescriptor format,\n\t// partitioning based on if we offered or received the HTLC.\n\tfor _, htlc := range htlcs {\n\t\t// TODO(roasbeef): set isForwarded to false for all? need to\n\t\t// persist state w.r.t to if forwarded or not, or can\n\t\t// inadvertently trigger replays\n\n\t\tpayDesc, err := lc.diskHtlcToPayDesc(\n\t\t\tfeeRate, commitHeight, &htlc,\n\t\t\tlocalCommitKeys, remoteCommitKeys,\n\t\t\tisLocal,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn incomingHtlcs, outgoingHtlcs, err\n\t\t}\n\n\t\tif htlc.Incoming {\n\t\t\tincomingHtlcs = append(incomingHtlcs, payDesc)\n\t\t} else {\n\t\t\toutgoingHtlcs = append(outgoingHtlcs, payDesc)\n\t\t}\n\t}\n\n\treturn incomingHtlcs, outgoingHtlcs, nil\n}\n\n// diskCommitToMemCommit converts the on-disk commitment format to our\n// in-memory commitment format which is needed in order to properly resume\n// channel operations after a restart.",
      "length": 1171,
      "tokens": 153,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) diskCommitToMemCommit(isLocal bool,",
      "content": "func (lc *LightningChannel) diskCommitToMemCommit(isLocal bool,\n\tdiskCommit *channeldb.ChannelCommitment, localCommitPoint,\n\tremoteCommitPoint *btcec.PublicKey) (*commitment, error) {\n\n\t// First, we'll need to re-derive the commitment key ring for each\n\t// party used within this particular state. If this is a pending commit\n\t// (we extended but weren't able to complete the commitment dance\n\t// before shutdown), then the localCommitPoint won't be set as we\n\t// haven't yet received a responding commitment from the remote party.\n\tvar localCommitKeys, remoteCommitKeys *CommitmentKeyRing\n\tif localCommitPoint != nil {\n\t\tlocalCommitKeys = DeriveCommitmentKeys(\n\t\t\tlocalCommitPoint, true, lc.channelState.ChanType,\n\t\t\t&lc.channelState.LocalChanCfg,\n\t\t\t&lc.channelState.RemoteChanCfg,\n\t\t)\n\t}\n\tif remoteCommitPoint != nil {\n\t\tremoteCommitKeys = DeriveCommitmentKeys(\n\t\t\tremoteCommitPoint, false, lc.channelState.ChanType,\n\t\t\t&lc.channelState.LocalChanCfg,\n\t\t\t&lc.channelState.RemoteChanCfg,\n\t\t)\n\t}\n\n\t// With the key rings re-created, we'll now convert all the on-disk\n\t// HTLC\"s into PaymentDescriptor's so we can re-insert them into our\n\t// update log.\n\tincomingHtlcs, outgoingHtlcs, err := lc.extractPayDescs(\n\t\tdiskCommit.CommitHeight,\n\t\tchainfee.SatPerKWeight(diskCommit.FeePerKw),\n\t\tdiskCommit.Htlcs, localCommitKeys, remoteCommitKeys,\n\t\tisLocal,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// With the necessary items generated, we'll now re-construct the\n\t// commitment state as it was originally present in memory.\n\tcommit := &commitment{\n\t\theight:            diskCommit.CommitHeight,\n\t\tisOurs:            isLocal,\n\t\tourBalance:        diskCommit.LocalBalance,\n\t\ttheirBalance:      diskCommit.RemoteBalance,\n\t\tourMessageIndex:   diskCommit.LocalLogIndex,\n\t\tourHtlcIndex:      diskCommit.LocalHtlcIndex,\n\t\ttheirMessageIndex: diskCommit.RemoteLogIndex,\n\t\ttheirHtlcIndex:    diskCommit.RemoteHtlcIndex,\n\t\ttxn:               diskCommit.CommitTx,\n\t\tsig:               diskCommit.CommitSig,\n\t\tfee:               diskCommit.CommitFee,\n\t\tfeePerKw:          chainfee.SatPerKWeight(diskCommit.FeePerKw),\n\t\tincomingHTLCs:     incomingHtlcs,\n\t\toutgoingHTLCs:     outgoingHtlcs,\n\t}\n\tif isLocal {\n\t\tcommit.dustLimit = lc.channelState.LocalChanCfg.DustLimit\n\t} else {\n\t\tcommit.dustLimit = lc.channelState.RemoteChanCfg.DustLimit\n\t}\n\n\treturn commit, nil\n}\n\n// commitmentChain represents a chain of unrevoked commitments. The tail of the\n// chain is the latest fully signed, yet unrevoked commitment. Two chains are\n// tracked, one for the local node, and another for the remote node. New\n// commitments we create locally extend the remote node's chain, and vice\n// versa. Commitment chains are allowed to grow to a bounded length, after\n// which the tail needs to be \"dropped\" before new commitments can be received.\n// The tail is \"dropped\" when the owner of the chain sends a revocation for the\n// previous tail.",
      "length": 2768,
      "tokens": 313,
      "embedding": []
    },
    {
      "slug": "type commitmentChain struct {",
      "content": "type commitmentChain struct {\n\t// commitments is a linked list of commitments to new states. New\n\t// commitments are added to the end of the chain with increase height.\n\t// Once a commitment transaction is revoked, the tail is incremented,\n\t// freeing up the revocation window for new commitments.\n\tcommitments *list.List\n}\n\n// newCommitmentChain creates a new commitment chain.",
      "length": 341,
      "tokens": 55,
      "embedding": []
    },
    {
      "slug": "func newCommitmentChain() *commitmentChain {",
      "content": "func newCommitmentChain() *commitmentChain {\n\treturn &commitmentChain{\n\t\tcommitments: list.New(),\n\t}\n}\n\n// addCommitment extends the commitment chain by a single commitment. This\n// added commitment represents a state update proposed by either party. Once\n// the commitment prior to this commitment is revoked, the commitment becomes\n// the new defacto state within the channel.",
      "length": 325,
      "tokens": 49,
      "embedding": []
    },
    {
      "slug": "func (s *commitmentChain) addCommitment(c *commitment) {",
      "content": "func (s *commitmentChain) addCommitment(c *commitment) {\n\ts.commitments.PushBack(c)\n}\n\n// advanceTail reduces the length of the commitment chain by one. The tail of\n// the chain should be advanced once a revocation for the lowest unrevoked\n// commitment in the chain is received.",
      "length": 217,
      "tokens": 36,
      "embedding": []
    },
    {
      "slug": "func (s *commitmentChain) advanceTail() {",
      "content": "func (s *commitmentChain) advanceTail() {\n\ts.commitments.Remove(s.commitments.Front())\n}\n\n// tip returns the latest commitment added to the chain.",
      "length": 101,
      "tokens": 12,
      "embedding": []
    },
    {
      "slug": "func (s *commitmentChain) tip() *commitment {",
      "content": "func (s *commitmentChain) tip() *commitment {\n\treturn s.commitments.Back().Value.(*commitment)\n}\n\n// tail returns the lowest unrevoked commitment transaction in the chain.",
      "length": 122,
      "tokens": 14,
      "embedding": []
    },
    {
      "slug": "func (s *commitmentChain) tail() *commitment {",
      "content": "func (s *commitmentChain) tail() *commitment {\n\treturn s.commitments.Front().Value.(*commitment)\n}\n\n// hasUnackedCommitment returns true if the commitment chain has more than one\n// entry. The tail of the commitment chain has been ACKed by revoking all prior\n// commitments, but any subsequent commitments have not yet been ACKed.",
      "length": 278,
      "tokens": 41,
      "embedding": []
    },
    {
      "slug": "func (s *commitmentChain) hasUnackedCommitment() bool {",
      "content": "func (s *commitmentChain) hasUnackedCommitment() bool {\n\treturn s.commitments.Front() != s.commitments.Back()\n}\n\n// updateLog is an append-only log that stores updates to a node's commitment\n// chain. This structure can be seen as the \"mempool\" within Lightning where\n// changes are stored before they're committed to the chain. Once an entry has\n// been committed in both the local and remote commitment chain, then it can be\n// removed from this log.\n//\n// TODO(roasbeef): create lightning package, move commitment and update to\n// package?\n//   - also move state machine, separate from lnwallet package\n//   - possible embed updateLog within commitmentChain.",
      "length": 593,
      "tokens": 95,
      "embedding": []
    },
    {
      "slug": "type updateLog struct {",
      "content": "type updateLog struct {\n\t// logIndex is a monotonically increasing integer that tracks the total\n\t// number of update entries ever applied to the log. When sending new\n\t// commitment states, we include all updates up to this index.\n\tlogIndex uint64\n\n\t// htlcCounter is a monotonically increasing integer that tracks the\n\t// total number of offered HTLC's by the owner of this update log,\n\t// hence the `Add` update type. We use a distinct index for this\n\t// purpose, as update's that remove entries from the log will be\n\t// indexed using this counter.\n\thtlcCounter uint64\n\n\t// List is the updatelog itself, we embed this value so updateLog has\n\t// access to all the method of a list.List.\n\t*list.List\n\n\t// updateIndex maps a `logIndex` to a particular update entry. It\n\t// deals with the four update types:\n\t//   `Fail|MalformedFail|Settle|FeeUpdate`\n\tupdateIndex map[uint64]*list.Element\n\n\t// htlcIndex maps a `htlcCounter` to an offered HTLC entry, hence the\n\t// `Add` update.\n\thtlcIndex map[uint64]*list.Element\n\n\t// modifiedHtlcs is a set that keeps track of all the current modified\n\t// htlcs, hence update types `Fail|MalformedFail|Settle`. A modified\n\t// HTLC is one that's present in the log, and has as a pending fail or\n\t// settle that's attempting to consume it.\n\tmodifiedHtlcs map[uint64]struct{}\n}\n\n// newUpdateLog creates a new updateLog instance.",
      "length": 1305,
      "tokens": 208,
      "embedding": []
    },
    {
      "slug": "func newUpdateLog(logIndex, htlcCounter uint64) *updateLog {",
      "content": "func newUpdateLog(logIndex, htlcCounter uint64) *updateLog {\n\treturn &updateLog{\n\t\tList:          list.New(),\n\t\tupdateIndex:   make(map[uint64]*list.Element),\n\t\thtlcIndex:     make(map[uint64]*list.Element),\n\t\tlogIndex:      logIndex,\n\t\thtlcCounter:   htlcCounter,\n\t\tmodifiedHtlcs: make(map[uint64]struct{}),\n\t}\n}\n\n// restoreHtlc will \"restore\" a prior HTLC to the updateLog. We say restore as\n// this method is intended to be used when re-covering a prior commitment\n// state. This function differs from appendHtlc in that it won't increment\n// either of log's counters. If the HTLC is already present, then it is\n// ignored.",
      "length": 551,
      "tokens": 71,
      "embedding": []
    },
    {
      "slug": "func (u *updateLog) restoreHtlc(pd *PaymentDescriptor) {",
      "content": "func (u *updateLog) restoreHtlc(pd *PaymentDescriptor) {\n\tif _, ok := u.htlcIndex[pd.HtlcIndex]; ok {\n\t\treturn\n\t}\n\n\tu.htlcIndex[pd.HtlcIndex] = u.PushBack(pd)\n}\n\n// appendUpdate appends a new update to the tip of the updateLog. The entry is\n// also added to index accordingly.",
      "length": 211,
      "tokens": 34,
      "embedding": []
    },
    {
      "slug": "func (u *updateLog) appendUpdate(pd *PaymentDescriptor) {",
      "content": "func (u *updateLog) appendUpdate(pd *PaymentDescriptor) {\n\tu.updateIndex[u.logIndex] = u.PushBack(pd)\n\tu.logIndex++\n}\n\n// restoreUpdate appends a new update to the tip of the updateLog. The entry is\n// also added to index accordingly. This function differs from appendUpdate in\n// that it won't increment the log index counter.",
      "length": 263,
      "tokens": 41,
      "embedding": []
    },
    {
      "slug": "func (u *updateLog) restoreUpdate(pd *PaymentDescriptor) {",
      "content": "func (u *updateLog) restoreUpdate(pd *PaymentDescriptor) {\n\tu.updateIndex[pd.LogIndex] = u.PushBack(pd)\n}\n\n// appendHtlc appends a new HTLC offer to the tip of the update log. The entry\n// is also added to the offer index accordingly.",
      "length": 171,
      "tokens": 29,
      "embedding": []
    },
    {
      "slug": "func (u *updateLog) appendHtlc(pd *PaymentDescriptor) {",
      "content": "func (u *updateLog) appendHtlc(pd *PaymentDescriptor) {\n\tu.htlcIndex[u.htlcCounter] = u.PushBack(pd)\n\tu.htlcCounter++\n\n\tu.logIndex++\n}\n\n// lookupHtlc attempts to look up an offered HTLC according to its offer\n// index. If the entry isn't found, then a nil pointer is returned.",
      "length": 213,
      "tokens": 32,
      "embedding": []
    },
    {
      "slug": "func (u *updateLog) lookupHtlc(i uint64) *PaymentDescriptor {",
      "content": "func (u *updateLog) lookupHtlc(i uint64) *PaymentDescriptor {\n\thtlc, ok := u.htlcIndex[i]\n\tif !ok {\n\t\treturn nil\n\t}\n\n\treturn htlc.Value.(*PaymentDescriptor)\n}\n\n// remove attempts to remove an entry from the update log. If the entry is\n// found, then the entry will be removed from the update log and index.",
      "length": 235,
      "tokens": 42,
      "embedding": []
    },
    {
      "slug": "func (u *updateLog) removeUpdate(i uint64) {",
      "content": "func (u *updateLog) removeUpdate(i uint64) {\n\tentry := u.updateIndex[i]\n\tu.Remove(entry)\n\tdelete(u.updateIndex, i)\n}\n\n// removeHtlc attempts to remove an HTLC offer form the update log. If the\n// entry is found, then the entry will be removed from both the main log and\n// the offer index.",
      "length": 237,
      "tokens": 41,
      "embedding": []
    },
    {
      "slug": "func (u *updateLog) removeHtlc(i uint64) {",
      "content": "func (u *updateLog) removeHtlc(i uint64) {\n\tentry := u.htlcIndex[i]\n\tu.Remove(entry)\n\tdelete(u.htlcIndex, i)\n\n\tdelete(u.modifiedHtlcs, i)\n}\n\n// htlcHasModification returns true if the HTLC identified by the passed index\n// has a pending modification within the log.",
      "length": 214,
      "tokens": 29,
      "embedding": []
    },
    {
      "slug": "func (u *updateLog) htlcHasModification(i uint64) bool {",
      "content": "func (u *updateLog) htlcHasModification(i uint64) bool {\n\t_, o := u.modifiedHtlcs[i]\n\treturn o\n}\n\n// markHtlcModified marks an HTLC as modified based on its HTLC index. After a\n// call to this method, htlcHasModification will return true until the HTLC is\n// removed.",
      "length": 204,
      "tokens": 36,
      "embedding": []
    },
    {
      "slug": "func (u *updateLog) markHtlcModified(i uint64) {",
      "content": "func (u *updateLog) markHtlcModified(i uint64) {\n\tu.modifiedHtlcs[i] = struct{}{}\n}\n\n// compactLogs performs garbage collection within the log removing HTLCs which\n// have been removed from the point-of-view of the tail of both chains. The\n// entries which timeout/settle HTLCs are also removed.",
      "length": 241,
      "tokens": 37,
      "embedding": []
    },
    {
      "slug": "func compactLogs(ourLog, theirLog *updateLog,",
      "content": "func compactLogs(ourLog, theirLog *updateLog,\n\tlocalChainTail, remoteChainTail uint64) {\n\n\tcompactLog := func(logA, logB *updateLog) {\n\t\tvar nextA *list.Element\n\t\tfor e := logA.Front(); e != nil; e = nextA {\n\t\t\t// Assign next iteration element at top of loop because\n\t\t\t// we may remove the current element from the list,\n\t\t\t// which can change the iterated sequence.\n\t\t\tnextA = e.Next()\n\n\t\t\thtlc := e.Value.(*PaymentDescriptor)\n\n\t\t\t// We skip Adds, as they will be removed along with the\n\t\t\t// fail/settles below.\n\t\t\tif htlc.EntryType == Add {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// If the HTLC hasn't yet been removed from either\n\t\t\t// chain, the skip it.\n\t\t\tif htlc.removeCommitHeightRemote == 0 ||\n\t\t\t\thtlc.removeCommitHeightLocal == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Otherwise if the height of the tail of both chains\n\t\t\t// is at least the height in which the HTLC was\n\t\t\t// removed, then evict the settle/timeout entry along\n\t\t\t// with the original add entry.\n\t\t\tif remoteChainTail >= htlc.removeCommitHeightRemote &&\n\t\t\t\tlocalChainTail >= htlc.removeCommitHeightLocal {\n\n\t\t\t\t// Fee updates have no parent htlcs, so we only\n\t\t\t\t// remove the update itself.\n\t\t\t\tif htlc.EntryType == FeeUpdate {\n\t\t\t\t\tlogA.removeUpdate(htlc.LogIndex)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\t// The other types (fail/settle) do have a\n\t\t\t\t// parent HTLC, so we'll remove that HTLC from\n\t\t\t\t// the other log.\n\t\t\t\tlogA.removeUpdate(htlc.LogIndex)\n\t\t\t\tlogB.removeHtlc(htlc.ParentIndex)\n\t\t\t}\n\n\t\t}\n\t}\n\n\tcompactLog(ourLog, theirLog)\n\tcompactLog(theirLog, ourLog)\n}\n\n// LightningChannel implements the state machine which corresponds to the\n// current commitment protocol wire spec. The state machine implemented allows\n// for asynchronous fully desynchronized, batched+pipelined updates to\n// commitment transactions allowing for a high degree of non-blocking\n// bi-directional payment throughput.\n//\n// In order to allow updates to be fully non-blocking, either side is able to\n// create multiple new commitment states up to a pre-determined window size.\n// This window size is encoded within InitialRevocationWindow. Before the start\n// of a session, both side should send out revocation messages with nil\n// preimages in order to populate their revocation window for the remote party.\n//\n// The state machine has for main methods:\n//   - .SignNextCommitment()\n//   - Called one one wishes to sign the next commitment, either initiating a\n//     new state update, or responding to a received commitment.\n//   - .ReceiveNewCommitment()\n//   - Called upon receipt of a new commitment from the remote party. If the\n//     new commitment is valid, then a revocation should immediately be\n//     generated and sent.\n//   - .RevokeCurrentCommitment()\n//   - Revokes the current commitment. Should be called directly after\n//     receiving a new commitment.\n//   - .ReceiveRevocation()\n//   - Processes a revocation from the remote party. If successful creates a\n//     new defacto broadcastable state.\n//\n// See the individual comments within the above methods for further details.",
      "length": 2897,
      "tokens": 434,
      "embedding": []
    },
    {
      "slug": "type LightningChannel struct {",
      "content": "type LightningChannel struct {\n\t// Signer is the main signer instances that will be responsible for\n\t// signing any HTLC and commitment transaction generated by the state\n\t// machine.\n\tSigner input.Signer\n\n\t// signDesc is the primary sign descriptor that is capable of signing\n\t// the commitment transaction that spends the multi-sig output.\n\tsignDesc *input.SignDescriptor\n\n\tstatus channelState\n\n\t// ChanPoint is the funding outpoint of this channel.\n\tChanPoint *wire.OutPoint\n\n\t// sigPool is a pool of workers that are capable of signing and\n\t// validating signatures in parallel. This is utilized as an\n\t// optimization to void serially signing or validating the HTLC\n\t// signatures, of which there may be hundreds.\n\tsigPool *SigPool\n\n\t// Capacity is the total capacity of this channel.\n\tCapacity btcutil.Amount\n\n\t// currentHeight is the current height of our local commitment chain.\n\t// This is also the same as the number of updates to the channel we've\n\t// accepted.\n\tcurrentHeight uint64\n\n\t// remoteCommitChain is the remote node's commitment chain. Any new\n\t// commitments we initiate are added to the tip of this chain.\n\tremoteCommitChain *commitmentChain\n\n\t// localCommitChain is our local commitment chain. Any new commitments\n\t// received are added to the tip of this chain. The tail (or lowest\n\t// height) in this chain is our current accepted state, which we are\n\t// able to broadcast safely.\n\tlocalCommitChain *commitmentChain\n\n\tchannelState *channeldb.OpenChannel\n\n\tcommitBuilder *CommitmentBuilder\n\n\t// [local|remote]Log is a (mostly) append-only log storing all the HTLC\n\t// updates to this channel. The log is walked backwards as HTLC updates\n\t// are applied in order to re-construct a commitment transaction from a\n\t// commitment. The log is compacted once a revocation is received.\n\tlocalUpdateLog  *updateLog\n\tremoteUpdateLog *updateLog\n\n\t// LocalFundingKey is the public key under control by the wallet that\n\t// was used for the 2-of-2 funding output which created this channel.\n\tLocalFundingKey *btcec.PublicKey\n\n\t// RemoteFundingKey is the public key for the remote channel counter\n\t// party  which used for the 2-of-2 funding output which created this\n\t// channel.\n\tRemoteFundingKey *btcec.PublicKey\n\n\t// log is a channel-specific logging instance.\n\tlog btclog.Logger\n\n\tsync.RWMutex\n}\n\n// NewLightningChannel creates a new, active payment channel given an\n// implementation of the chain notifier, channel database, and the current\n// settled channel state. Throughout state transitions, then channel will\n// automatically persist pertinent state to the database in an efficient\n// manner.",
      "length": 2515,
      "tokens": 378,
      "embedding": []
    },
    {
      "slug": "func NewLightningChannel(signer input.Signer,",
      "content": "func NewLightningChannel(signer input.Signer,\n\tstate *channeldb.OpenChannel,\n\tsigPool *SigPool) (*LightningChannel, error) {\n\n\tlocalCommit := state.LocalCommitment\n\tremoteCommit := state.RemoteCommitment\n\n\t// First, initialize the update logs with their current counter values\n\t// from the local and remote commitments.\n\tlocalUpdateLog := newUpdateLog(\n\t\tremoteCommit.LocalLogIndex, remoteCommit.LocalHtlcIndex,\n\t)\n\tremoteUpdateLog := newUpdateLog(\n\t\tlocalCommit.RemoteLogIndex, localCommit.RemoteHtlcIndex,\n\t)\n\n\tlogPrefix := fmt.Sprintf(\"ChannelPoint(%v):\", state.FundingOutpoint)\n\n\tlc := &LightningChannel{\n\t\tSigner:            signer,\n\t\tsigPool:           sigPool,\n\t\tcurrentHeight:     localCommit.CommitHeight,\n\t\tremoteCommitChain: newCommitmentChain(),\n\t\tlocalCommitChain:  newCommitmentChain(),\n\t\tchannelState:      state,\n\t\tcommitBuilder:     NewCommitmentBuilder(state),\n\t\tlocalUpdateLog:    localUpdateLog,\n\t\tremoteUpdateLog:   remoteUpdateLog,\n\t\tChanPoint:         &state.FundingOutpoint,\n\t\tCapacity:          state.Capacity,\n\t\tLocalFundingKey:   state.LocalChanCfg.MultiSigKey.PubKey,\n\t\tRemoteFundingKey:  state.RemoteChanCfg.MultiSigKey.PubKey,\n\t\tlog:               build.NewPrefixLog(logPrefix, walletLog),\n\t}\n\n\t// With the main channel struct reconstructed, we'll now restore the\n\t// commitment state in memory and also the update logs themselves.\n\terr := lc.restoreCommitState(&localCommit, &remoteCommit)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Create the sign descriptor which we'll be using very frequently to\n\t// request a signature for the 2-of-2 multi-sig from the signer in\n\t// order to complete channel state transitions.\n\tif err := lc.createSignDesc(); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn lc, nil\n}\n\n// createSignDesc derives the SignDescriptor for commitment transactions from\n// other fields on the LightningChannel.",
      "length": 1754,
      "tokens": 177,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) createSignDesc() error {",
      "content": "func (lc *LightningChannel) createSignDesc() error {\n\tlocalKey := lc.channelState.LocalChanCfg.MultiSigKey.PubKey.\n\t\tSerializeCompressed()\n\tremoteKey := lc.channelState.RemoteChanCfg.MultiSigKey.PubKey.\n\t\tSerializeCompressed()\n\n\tmultiSigScript, err := input.GenMultiSigScript(localKey, remoteKey)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tfundingPkScript, err := input.WitnessScriptHash(multiSigScript)\n\tif err != nil {\n\t\treturn err\n\t}\n\tlc.signDesc = &input.SignDescriptor{\n\t\tKeyDesc:       lc.channelState.LocalChanCfg.MultiSigKey,\n\t\tWitnessScript: multiSigScript,\n\t\tOutput: &wire.TxOut{\n\t\t\tPkScript: fundingPkScript,\n\t\t\tValue:    int64(lc.channelState.Capacity),\n\t\t},\n\t\tHashType:   txscript.SigHashAll,\n\t\tInputIndex: 0,\n\t}\n\n\treturn nil\n}\n\n// ResetState resets the state of the channel back to the default state. This\n// ensures that any active goroutines which need to act based on on-chain\n// events do so properly.",
      "length": 828,
      "tokens": 87,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) ResetState() {",
      "content": "func (lc *LightningChannel) ResetState() {\n\tlc.Lock()\n\tlc.status = channelOpen\n\tlc.Unlock()\n}\n\n// logUpdateToPayDesc converts a LogUpdate into a matching PaymentDescriptor\n// entry that can be re-inserted into the update log. This method is used when\n// we extended a state to the remote party, but the connection was obstructed\n// before we could finish the commitment dance. In this case, we need to\n// re-insert the original entries back into the update log so we can resume as\n// if nothing happened.",
      "length": 451,
      "tokens": 77,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) logUpdateToPayDesc(logUpdate *channeldb.LogUpdate,",
      "content": "func (lc *LightningChannel) logUpdateToPayDesc(logUpdate *channeldb.LogUpdate,\n\tremoteUpdateLog *updateLog, commitHeight uint64,\n\tfeeRate chainfee.SatPerKWeight, remoteCommitKeys *CommitmentKeyRing,\n\tremoteDustLimit btcutil.Amount) (*PaymentDescriptor, error) {\n\n\t// Depending on the type of update message we'll map that to a distinct\n\t// PaymentDescriptor instance.\n\tvar pd *PaymentDescriptor\n\n\tswitch wireMsg := logUpdate.UpdateMsg.(type) {\n\n\t// For offered HTLC's, we'll map that to a PaymentDescriptor with the\n\t// type Add, ensuring we restore the necessary fields. From the PoV of\n\t// the commitment chain, this HTLC was included in the remote chain,\n\t// but not the local chain.\n\tcase *lnwire.UpdateAddHTLC:\n\t\t// First, we'll map all the relevant fields in the\n\t\t// UpdateAddHTLC message to their corresponding fields in the\n\t\t// PaymentDescriptor struct. We also set addCommitHeightRemote\n\t\t// as we've included this HTLC in our local commitment chain\n\t\t// for the remote party.\n\t\tpd = &PaymentDescriptor{\n\t\t\tRHash:                 wireMsg.PaymentHash,\n\t\t\tTimeout:               wireMsg.Expiry,\n\t\t\tAmount:                wireMsg.Amount,\n\t\t\tEntryType:             Add,\n\t\t\tHtlcIndex:             wireMsg.ID,\n\t\t\tLogIndex:              logUpdate.LogIndex,\n\t\t\taddCommitHeightRemote: commitHeight,\n\t\t}\n\t\tpd.OnionBlob = make([]byte, len(wireMsg.OnionBlob))\n\t\tcopy(pd.OnionBlob[:], wireMsg.OnionBlob[:])\n\n\t\tisDustRemote := HtlcIsDust(\n\t\t\tlc.channelState.ChanType, false, false, feeRate,\n\t\t\twireMsg.Amount.ToSatoshis(), remoteDustLimit,\n\t\t)\n\t\tif !isDustRemote {\n\t\t\ttheirP2WSH, theirWitnessScript, err := genHtlcScript(\n\t\t\t\tlc.channelState.ChanType, false, false,\n\t\t\t\twireMsg.Expiry, wireMsg.PaymentHash,\n\t\t\t\tremoteCommitKeys,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tpd.theirPkScript = theirP2WSH\n\t\t\tpd.theirWitnessScript = theirWitnessScript\n\t\t}\n\n\t// For HTLC's we're offered we'll fetch the original offered HTLC\n\t// from the remote party's update log so we can retrieve the same\n\t// PaymentDescriptor that SettleHTLC would produce.\n\tcase *lnwire.UpdateFulfillHTLC:\n\t\togHTLC := remoteUpdateLog.lookupHtlc(wireMsg.ID)\n\n\t\tpd = &PaymentDescriptor{\n\t\t\tAmount:                   ogHTLC.Amount,\n\t\t\tRHash:                    ogHTLC.RHash,\n\t\t\tRPreimage:                wireMsg.PaymentPreimage,\n\t\t\tLogIndex:                 logUpdate.LogIndex,\n\t\t\tParentIndex:              ogHTLC.HtlcIndex,\n\t\t\tEntryType:                Settle,\n\t\t\tremoveCommitHeightRemote: commitHeight,\n\t\t}\n\n\t// If we sent a failure for a prior incoming HTLC, then we'll consult\n\t// the update log of the remote party so we can retrieve the\n\t// information of the original HTLC we're failing. We also set the\n\t// removal height for the remote commitment.\n\tcase *lnwire.UpdateFailHTLC:\n\t\togHTLC := remoteUpdateLog.lookupHtlc(wireMsg.ID)\n\n\t\tpd = &PaymentDescriptor{\n\t\t\tAmount:                   ogHTLC.Amount,\n\t\t\tRHash:                    ogHTLC.RHash,\n\t\t\tParentIndex:              ogHTLC.HtlcIndex,\n\t\t\tLogIndex:                 logUpdate.LogIndex,\n\t\t\tEntryType:                Fail,\n\t\t\tFailReason:               wireMsg.Reason[:],\n\t\t\tremoveCommitHeightRemote: commitHeight,\n\t\t}\n\n\t// HTLC fails due to malformed onion blobs are treated the exact same\n\t// way as regular HTLC fails.\n\tcase *lnwire.UpdateFailMalformedHTLC:\n\t\togHTLC := remoteUpdateLog.lookupHtlc(wireMsg.ID)\n\t\t// TODO(roasbeef): err if nil?\n\n\t\tpd = &PaymentDescriptor{\n\t\t\tAmount:                   ogHTLC.Amount,\n\t\t\tRHash:                    ogHTLC.RHash,\n\t\t\tParentIndex:              ogHTLC.HtlcIndex,\n\t\t\tLogIndex:                 logUpdate.LogIndex,\n\t\t\tEntryType:                MalformedFail,\n\t\t\tFailCode:                 wireMsg.FailureCode,\n\t\t\tShaOnionBlob:             wireMsg.ShaOnionBlob,\n\t\t\tremoveCommitHeightRemote: commitHeight,\n\t\t}\n\n\t// For fee updates we'll create a FeeUpdate type to add to the log. We\n\t// reuse the amount field to hold the fee rate. Since the amount field\n\t// is denominated in msat we won't lose precision when storing the\n\t// sat/kw denominated feerate. Note that we set both the add and remove\n\t// height to the same value, as we consider the fee update locked in by\n\t// adding and removing it at the same height.\n\tcase *lnwire.UpdateFee:\n\t\tpd = &PaymentDescriptor{\n\t\t\tLogIndex: logUpdate.LogIndex,\n\t\t\tAmount: lnwire.NewMSatFromSatoshis(\n\t\t\t\tbtcutil.Amount(wireMsg.FeePerKw),\n\t\t\t),\n\t\t\tEntryType:                FeeUpdate,\n\t\t\taddCommitHeightRemote:    commitHeight,\n\t\t\tremoveCommitHeightRemote: commitHeight,\n\t\t}\n\t}\n\n\treturn pd, nil\n}\n\n// localLogUpdateToPayDesc converts a LogUpdate into a matching PaymentDescriptor\n// entry that can be re-inserted into the local update log. This method is used\n// when we sent an update+sig, receive a revocation, but drop right before the\n// counterparty can sign for the update we just sent. In this case, we need to\n// re-insert the original entries back into the update log so we'll be expecting\n// the peer to sign them. The height of the remote commitment is expected to be\n// provided and we restore all log update entries with this height, even though\n// the real height may be lower. In the way these fields are used elsewhere, this\n// doesn't change anything.",
      "length": 4968,
      "tokens": 580,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) localLogUpdateToPayDesc(logUpdate *channeldb.LogUpdate,",
      "content": "func (lc *LightningChannel) localLogUpdateToPayDesc(logUpdate *channeldb.LogUpdate,\n\tremoteUpdateLog *updateLog, commitHeight uint64) (*PaymentDescriptor,\n\terror) {\n\n\t// Since Add updates aren't saved to disk under this key, the update will\n\t// never be an Add.\n\tswitch wireMsg := logUpdate.UpdateMsg.(type) {\n\t// For HTLCs that we settled, we'll fetch the original offered HTLC from\n\t// the remote update log so we can retrieve the same PaymentDescriptor that\n\t// ReceiveHTLCSettle would produce.\n\tcase *lnwire.UpdateFulfillHTLC:\n\t\togHTLC := remoteUpdateLog.lookupHtlc(wireMsg.ID)\n\n\t\treturn &PaymentDescriptor{\n\t\t\tAmount:                   ogHTLC.Amount,\n\t\t\tRHash:                    ogHTLC.RHash,\n\t\t\tRPreimage:                wireMsg.PaymentPreimage,\n\t\t\tLogIndex:                 logUpdate.LogIndex,\n\t\t\tParentIndex:              ogHTLC.HtlcIndex,\n\t\t\tEntryType:                Settle,\n\t\t\tremoveCommitHeightRemote: commitHeight,\n\t\t}, nil\n\n\t// If we sent a failure for a prior incoming HTLC, then we'll consult the\n\t// remote update log so we can retrieve the information of the original\n\t// HTLC we're failing.\n\tcase *lnwire.UpdateFailHTLC:\n\t\togHTLC := remoteUpdateLog.lookupHtlc(wireMsg.ID)\n\n\t\treturn &PaymentDescriptor{\n\t\t\tAmount:                   ogHTLC.Amount,\n\t\t\tRHash:                    ogHTLC.RHash,\n\t\t\tParentIndex:              ogHTLC.HtlcIndex,\n\t\t\tLogIndex:                 logUpdate.LogIndex,\n\t\t\tEntryType:                Fail,\n\t\t\tFailReason:               wireMsg.Reason[:],\n\t\t\tremoveCommitHeightRemote: commitHeight,\n\t\t}, nil\n\n\t// HTLC fails due to malformed onion blocks are treated the exact same\n\t// way as regular HTLC fails.\n\tcase *lnwire.UpdateFailMalformedHTLC:\n\t\togHTLC := remoteUpdateLog.lookupHtlc(wireMsg.ID)\n\n\t\treturn &PaymentDescriptor{\n\t\t\tAmount:                   ogHTLC.Amount,\n\t\t\tRHash:                    ogHTLC.RHash,\n\t\t\tParentIndex:              ogHTLC.HtlcIndex,\n\t\t\tLogIndex:                 logUpdate.LogIndex,\n\t\t\tEntryType:                MalformedFail,\n\t\t\tFailCode:                 wireMsg.FailureCode,\n\t\t\tShaOnionBlob:             wireMsg.ShaOnionBlob,\n\t\t\tremoveCommitHeightRemote: commitHeight,\n\t\t}, nil\n\n\tcase *lnwire.UpdateFee:\n\t\treturn &PaymentDescriptor{\n\t\t\tLogIndex: logUpdate.LogIndex,\n\t\t\tAmount: lnwire.NewMSatFromSatoshis(\n\t\t\t\tbtcutil.Amount(wireMsg.FeePerKw),\n\t\t\t),\n\t\t\tEntryType:                FeeUpdate,\n\t\t\taddCommitHeightRemote:    commitHeight,\n\t\t\tremoveCommitHeightRemote: commitHeight,\n\t\t}, nil\n\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown message type: %T\", wireMsg)\n\t}\n}\n\n// remoteLogUpdateToPayDesc converts a LogUpdate into a matching\n// PaymentDescriptor entry that can be re-inserted into the update log. This\n// method is used when we revoked a local commitment, but the connection was\n// obstructed before we could sign a remote commitment that contains these\n// updates. In this case, we need to re-insert the original entries back into\n// the update log so we can resume as if nothing happened. The height of the\n// latest local commitment is also expected to be provided. We are restoring all\n// log update entries with this height, even though the real commitment height\n// may be lower. In the way these fields are used elsewhere, this doesn't change\n// anything.",
      "length": 3068,
      "tokens": 331,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) remoteLogUpdateToPayDesc(logUpdate *channeldb.LogUpdate,",
      "content": "func (lc *LightningChannel) remoteLogUpdateToPayDesc(logUpdate *channeldb.LogUpdate,\n\tlocalUpdateLog *updateLog, commitHeight uint64) (*PaymentDescriptor,\n\terror) {\n\n\tswitch wireMsg := logUpdate.UpdateMsg.(type) {\n\tcase *lnwire.UpdateAddHTLC:\n\t\tpd := &PaymentDescriptor{\n\t\t\tRHash:                wireMsg.PaymentHash,\n\t\t\tTimeout:              wireMsg.Expiry,\n\t\t\tAmount:               wireMsg.Amount,\n\t\t\tEntryType:            Add,\n\t\t\tHtlcIndex:            wireMsg.ID,\n\t\t\tLogIndex:             logUpdate.LogIndex,\n\t\t\taddCommitHeightLocal: commitHeight,\n\t\t}\n\t\tpd.OnionBlob = make([]byte, len(wireMsg.OnionBlob))\n\t\tcopy(pd.OnionBlob, wireMsg.OnionBlob[:])\n\n\t\t// We don't need to generate an htlc script yet. This will be\n\t\t// done once we sign our remote commitment.\n\n\t\treturn pd, nil\n\n\t// For HTLCs that the remote party settled, we'll fetch the original\n\t// offered HTLC from the local update log so we can retrieve the same\n\t// PaymentDescriptor that ReceiveHTLCSettle would produce.\n\tcase *lnwire.UpdateFulfillHTLC:\n\t\togHTLC := localUpdateLog.lookupHtlc(wireMsg.ID)\n\n\t\treturn &PaymentDescriptor{\n\t\t\tAmount:                  ogHTLC.Amount,\n\t\t\tRHash:                   ogHTLC.RHash,\n\t\t\tRPreimage:               wireMsg.PaymentPreimage,\n\t\t\tLogIndex:                logUpdate.LogIndex,\n\t\t\tParentIndex:             ogHTLC.HtlcIndex,\n\t\t\tEntryType:               Settle,\n\t\t\tremoveCommitHeightLocal: commitHeight,\n\t\t}, nil\n\n\t// If we received a failure for a prior outgoing HTLC, then we'll\n\t// consult the local update log so we can retrieve the information of\n\t// the original HTLC we're failing.\n\tcase *lnwire.UpdateFailHTLC:\n\t\togHTLC := localUpdateLog.lookupHtlc(wireMsg.ID)\n\n\t\treturn &PaymentDescriptor{\n\t\t\tAmount:                  ogHTLC.Amount,\n\t\t\tRHash:                   ogHTLC.RHash,\n\t\t\tParentIndex:             ogHTLC.HtlcIndex,\n\t\t\tLogIndex:                logUpdate.LogIndex,\n\t\t\tEntryType:               Fail,\n\t\t\tFailReason:              wireMsg.Reason[:],\n\t\t\tremoveCommitHeightLocal: commitHeight,\n\t\t}, nil\n\n\t// HTLC fails due to malformed onion blobs are treated the exact same\n\t// way as regular HTLC fails.\n\tcase *lnwire.UpdateFailMalformedHTLC:\n\t\togHTLC := localUpdateLog.lookupHtlc(wireMsg.ID)\n\n\t\treturn &PaymentDescriptor{\n\t\t\tAmount:                  ogHTLC.Amount,\n\t\t\tRHash:                   ogHTLC.RHash,\n\t\t\tParentIndex:             ogHTLC.HtlcIndex,\n\t\t\tLogIndex:                logUpdate.LogIndex,\n\t\t\tEntryType:               MalformedFail,\n\t\t\tFailCode:                wireMsg.FailureCode,\n\t\t\tShaOnionBlob:            wireMsg.ShaOnionBlob,\n\t\t\tremoveCommitHeightLocal: commitHeight,\n\t\t}, nil\n\n\t// For fee updates we'll create a FeeUpdate type to add to the log. We\n\t// reuse the amount field to hold the fee rate. Since the amount field\n\t// is denominated in msat we won't lose precision when storing the\n\t// sat/kw denominated feerate. Note that we set both the add and remove\n\t// height to the same value, as we consider the fee update locked in by\n\t// adding and removing it at the same height.\n\tcase *lnwire.UpdateFee:\n\t\treturn &PaymentDescriptor{\n\t\t\tLogIndex: logUpdate.LogIndex,\n\t\t\tAmount: lnwire.NewMSatFromSatoshis(\n\t\t\t\tbtcutil.Amount(wireMsg.FeePerKw),\n\t\t\t),\n\t\t\tEntryType:               FeeUpdate,\n\t\t\taddCommitHeightLocal:    commitHeight,\n\t\t\tremoveCommitHeightLocal: commitHeight,\n\t\t}, nil\n\n\tdefault:\n\t\treturn nil, errors.New(\"unknown message type\")\n\t}\n}\n\n// restoreCommitState will restore the local commitment chain and updateLog\n// state to a consistent in-memory representation of the passed disk commitment.\n// This method is to be used upon reconnection to our channel counter party.\n// Once the connection has been established, we'll prepare our in memory state\n// to re-sync states with the remote party, and also verify/extend new proposed\n// commitment states.",
      "length": 3611,
      "tokens": 385,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) restoreCommitState(",
      "content": "func (lc *LightningChannel) restoreCommitState(\n\tlocalCommitState, remoteCommitState *channeldb.ChannelCommitment) error {\n\n\t// In order to reconstruct the pkScripts on each of the pending HTLC\n\t// outputs (if any) we'll need to regenerate the current revocation for\n\t// this current un-revoked state as well as retrieve the current\n\t// revocation for the remote party.\n\tourRevPreImage, err := lc.channelState.RevocationProducer.AtIndex(\n\t\tlc.currentHeight,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\tlocalCommitPoint := input.ComputeCommitmentPoint(ourRevPreImage[:])\n\tremoteCommitPoint := lc.channelState.RemoteCurrentRevocation\n\n\t// With the revocation state reconstructed, we can now convert the disk\n\t// commitment into our in-memory commitment format, inserting it into\n\t// the local commitment chain.\n\tlocalCommit, err := lc.diskCommitToMemCommit(\n\t\ttrue, localCommitState, localCommitPoint,\n\t\tremoteCommitPoint,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\tlc.localCommitChain.addCommitment(localCommit)\n\n\tlc.log.Tracef(\"starting local commitment: %v\",\n\t\tnewLogClosure(func() string {\n\t\t\treturn spew.Sdump(lc.localCommitChain.tail())\n\t\t}),\n\t)\n\n\t// We'll also do the same for the remote commitment chain.\n\tremoteCommit, err := lc.diskCommitToMemCommit(\n\t\tfalse, remoteCommitState, localCommitPoint,\n\t\tremoteCommitPoint,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\tlc.remoteCommitChain.addCommitment(remoteCommit)\n\n\tlc.log.Tracef(\"starting remote commitment: %v\",\n\t\tnewLogClosure(func() string {\n\t\t\treturn spew.Sdump(lc.remoteCommitChain.tail())\n\t\t}),\n\t)\n\n\tvar (\n\t\tpendingRemoteCommit     *commitment\n\t\tpendingRemoteCommitDiff *channeldb.CommitDiff\n\t\tpendingRemoteKeyChain   *CommitmentKeyRing\n\t)\n\n\t// Next, we'll check to see if we have an un-acked commitment state we\n\t// extended to the remote party but which was never ACK'd.\n\tpendingRemoteCommitDiff, err = lc.channelState.RemoteCommitChainTip()\n\tif err != nil && err != channeldb.ErrNoPendingCommit {\n\t\treturn err\n\t}\n\n\tif pendingRemoteCommitDiff != nil {\n\t\t// If we have a pending remote commitment, then we'll also\n\t\t// reconstruct the original commitment for that state,\n\t\t// inserting it into the remote party's commitment chain. We\n\t\t// don't pass our commit point as we don't have the\n\t\t// corresponding state for the local commitment chain.\n\t\tpendingCommitPoint := lc.channelState.RemoteNextRevocation\n\t\tpendingRemoteCommit, err = lc.diskCommitToMemCommit(\n\t\t\tfalse, &pendingRemoteCommitDiff.Commitment,\n\t\t\tnil, pendingCommitPoint,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlc.remoteCommitChain.addCommitment(pendingRemoteCommit)\n\n\t\tlc.log.Debugf(\"pending remote commitment: %v\",\n\t\t\tnewLogClosure(func() string {\n\t\t\t\treturn spew.Sdump(lc.remoteCommitChain.tip())\n\t\t\t}),\n\t\t)\n\n\t\t// We'll also re-create the set of commitment keys needed to\n\t\t// fully re-derive the state.\n\t\tpendingRemoteKeyChain = DeriveCommitmentKeys(\n\t\t\tpendingCommitPoint, false, lc.channelState.ChanType,\n\t\t\t&lc.channelState.LocalChanCfg, &lc.channelState.RemoteChanCfg,\n\t\t)\n\t}\n\n\t// Fetch remote updates that we have acked but not yet signed for.\n\tunsignedAckedUpdates, err := lc.channelState.UnsignedAckedUpdates()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Fetch the local updates the peer still needs to sign for.\n\tremoteUnsignedLocalUpdates, err := lc.channelState.RemoteUnsignedLocalUpdates()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Finally, with the commitment states restored, we'll now restore the\n\t// state logs based on the current local+remote commit, and any pending\n\t// remote commit that exists.\n\terr = lc.restoreStateLogs(\n\t\tlocalCommit, remoteCommit, pendingRemoteCommit,\n\t\tpendingRemoteCommitDiff, pendingRemoteKeyChain,\n\t\tunsignedAckedUpdates, remoteUnsignedLocalUpdates,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// restoreStateLogs runs through the current locked-in HTLCs from the point of\n// view of the channel and insert corresponding log entries (both local and\n// remote) for each HTLC read from disk. This method is required to sync the\n// in-memory state of the state machine with that read from persistent storage.",
      "length": 3892,
      "tokens": 476,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) restoreStateLogs(",
      "content": "func (lc *LightningChannel) restoreStateLogs(\n\tlocalCommitment, remoteCommitment, pendingRemoteCommit *commitment,\n\tpendingRemoteCommitDiff *channeldb.CommitDiff,\n\tpendingRemoteKeys *CommitmentKeyRing,\n\tunsignedAckedUpdates,\n\tremoteUnsignedLocalUpdates []channeldb.LogUpdate) error {\n\n\t// We make a map of incoming HTLCs to the height of the remote\n\t// commitment they were first added, and outgoing HTLCs to the height\n\t// of the local commit they were first added. This will be used when we\n\t// restore the update logs below.\n\tincomingRemoteAddHeights := make(map[uint64]uint64)\n\toutgoingLocalAddHeights := make(map[uint64]uint64)\n\n\t// We start by setting the height of the incoming HTLCs on the pending\n\t// remote commitment. We set these heights first since if there are\n\t// duplicates, these will be overwritten by the lower height of the\n\t// remoteCommitment below.\n\tif pendingRemoteCommit != nil {\n\t\tfor _, r := range pendingRemoteCommit.incomingHTLCs {\n\t\t\tincomingRemoteAddHeights[r.HtlcIndex] =\n\t\t\t\tpendingRemoteCommit.height\n\t\t}\n\t}\n\n\t// Now set the remote commit height of all incoming HTLCs found on the\n\t// remote commitment.\n\tfor _, r := range remoteCommitment.incomingHTLCs {\n\t\tincomingRemoteAddHeights[r.HtlcIndex] = remoteCommitment.height\n\t}\n\n\t// And finally we can do the same for the outgoing HTLCs.\n\tfor _, l := range localCommitment.outgoingHTLCs {\n\t\toutgoingLocalAddHeights[l.HtlcIndex] = localCommitment.height\n\t}\n\n\t// If we have any unsigned acked updates to sign for, then the add is no\n\t// longer on our local commitment, but is still on the remote's commitment.\n\t// <---fail---\n\t// <---sig----\n\t// ----rev--->\n\t// To ensure proper channel operation, we restore the add's addCommitHeightLocal\n\t// field to the height of our local commitment.\n\tfor _, logUpdate := range unsignedAckedUpdates {\n\t\tvar htlcIdx uint64\n\t\tswitch wireMsg := logUpdate.UpdateMsg.(type) {\n\t\tcase *lnwire.UpdateFulfillHTLC:\n\t\t\thtlcIdx = wireMsg.ID\n\t\tcase *lnwire.UpdateFailHTLC:\n\t\t\thtlcIdx = wireMsg.ID\n\t\tcase *lnwire.UpdateFailMalformedHTLC:\n\t\t\thtlcIdx = wireMsg.ID\n\t\tdefault:\n\t\t\tcontinue\n\t\t}\n\n\t\t// The htlcIdx is stored in the map with the local commitment\n\t\t// height so the related add's addCommitHeightLocal field can be\n\t\t// restored.\n\t\toutgoingLocalAddHeights[htlcIdx] = localCommitment.height\n\t}\n\n\t// If there are local updates that the peer needs to sign for, then the\n\t// corresponding add is no longer on the remote commitment, but is still on\n\t// our local commitment.\n\t// ----fail--->\n\t// ----sig---->\n\t// <---rev-----\n\t// To ensure proper channel operation, we restore the add's addCommitHeightRemote\n\t// field to the height of the remote commitment.\n\tfor _, logUpdate := range remoteUnsignedLocalUpdates {\n\t\tvar htlcIdx uint64\n\t\tswitch wireMsg := logUpdate.UpdateMsg.(type) {\n\t\tcase *lnwire.UpdateFulfillHTLC:\n\t\t\thtlcIdx = wireMsg.ID\n\t\tcase *lnwire.UpdateFailHTLC:\n\t\t\thtlcIdx = wireMsg.ID\n\t\tcase *lnwire.UpdateFailMalformedHTLC:\n\t\t\thtlcIdx = wireMsg.ID\n\t\tdefault:\n\t\t\tcontinue\n\t\t}\n\n\t\t// The htlcIdx is stored in the map with the remote commitment\n\t\t// height so the related add's addCommitHeightRemote field can be\n\t\t// restored.\n\t\tincomingRemoteAddHeights[htlcIdx] = remoteCommitment.height\n\t}\n\n\t// For each incoming HTLC within the local commitment, we add it to the\n\t// remote update log. Since HTLCs are added first to the receiver's\n\t// commitment, we don't have to restore outgoing HTLCs, as they will be\n\t// restored from the remote commitment below.\n\tfor i := range localCommitment.incomingHTLCs {\n\t\thtlc := localCommitment.incomingHTLCs[i]\n\n\t\t// We'll need to set the add height of the HTLC. Since it is on\n\t\t// this local commit, we can use its height as local add\n\t\t// height. As remote add height we consult the incoming HTLC\n\t\t// map we created earlier. Note that if this HTLC is not in\n\t\t// incomingRemoteAddHeights, the remote add height will be set\n\t\t// to zero, which indicates that it is not added yet.\n\t\thtlc.addCommitHeightLocal = localCommitment.height\n\t\thtlc.addCommitHeightRemote = incomingRemoteAddHeights[htlc.HtlcIndex]\n\n\t\t// Restore the htlc back to the remote log.\n\t\tlc.remoteUpdateLog.restoreHtlc(&htlc)\n\t}\n\n\t// Similarly, we'll do the same for the outgoing HTLCs within the\n\t// remote commitment, adding them to the local update log.\n\tfor i := range remoteCommitment.outgoingHTLCs {\n\t\thtlc := remoteCommitment.outgoingHTLCs[i]\n\n\t\t// As for the incoming HTLCs, we'll use the current remote\n\t\t// commit height as remote add height, and consult the map\n\t\t// created above for the local add height.\n\t\thtlc.addCommitHeightRemote = remoteCommitment.height\n\t\thtlc.addCommitHeightLocal = outgoingLocalAddHeights[htlc.HtlcIndex]\n\n\t\t// Restore the htlc back to the local log.\n\t\tlc.localUpdateLog.restoreHtlc(&htlc)\n\t}\n\n\t// If we have a dangling (un-acked) commit for the remote party, then we\n\t// restore the updates leading up to this commit.\n\tif pendingRemoteCommit != nil {\n\t\terr := lc.restorePendingLocalUpdates(\n\t\t\tpendingRemoteCommitDiff, pendingRemoteKeys,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Restore unsigned acked remote log updates so that we can include them\n\t// in our next signature.\n\terr := lc.restorePendingRemoteUpdates(\n\t\tunsignedAckedUpdates, localCommitment.height,\n\t\tpendingRemoteCommit,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Restore unsigned acked local log updates so we expect the peer to\n\t// sign for them.\n\treturn lc.restorePeerLocalUpdates(\n\t\tremoteUnsignedLocalUpdates, remoteCommitment.height,\n\t)\n}\n\n// restorePendingRemoteUpdates restores the acked remote log updates that we\n// haven't yet signed for.",
      "length": 5380,
      "tokens": 746,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) restorePendingRemoteUpdates(",
      "content": "func (lc *LightningChannel) restorePendingRemoteUpdates(\n\tunsignedAckedUpdates []channeldb.LogUpdate,\n\tlocalCommitmentHeight uint64,\n\tpendingRemoteCommit *commitment) error {\n\n\tlc.log.Debugf(\"Restoring %v dangling remote updates\",\n\t\tlen(unsignedAckedUpdates))\n\n\tfor _, logUpdate := range unsignedAckedUpdates {\n\t\tlogUpdate := logUpdate\n\n\t\tpayDesc, err := lc.remoteLogUpdateToPayDesc(\n\t\t\t&logUpdate, lc.localUpdateLog, localCommitmentHeight,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tlogIdx := payDesc.LogIndex\n\n\t\t// Sanity check that we are not restoring a remote log update\n\t\t// that we haven't received a sig for.\n\t\tif logIdx >= lc.remoteUpdateLog.logIndex {\n\t\t\treturn fmt.Errorf(\"attempted to restore an \"+\n\t\t\t\t\"unsigned remote update: log_index=%v\",\n\t\t\t\tlogIdx)\n\t\t}\n\n\t\t// We previously restored Adds along with all the other updates,\n\t\t// but this Add restoration was a no-op as every single one of\n\t\t// these Adds was already restored since they're all incoming\n\t\t// htlcs on the local commitment.\n\t\tif payDesc.EntryType == Add {\n\t\t\tcontinue\n\t\t}\n\n\t\tvar (\n\t\t\theight    uint64\n\t\t\theightSet bool\n\t\t)\n\n\t\t// If we have a pending commitment for them, and this update\n\t\t// is included in that commit, then we'll use this commitment\n\t\t// height as this commitment will include these updates for\n\t\t// their new remote commitment.\n\t\tif pendingRemoteCommit != nil {\n\t\t\tif logIdx < pendingRemoteCommit.theirMessageIndex {\n\t\t\t\theight = pendingRemoteCommit.height\n\t\t\t\theightSet = true\n\t\t\t}\n\t\t}\n\n\t\t// Insert the update into the log. The log update index doesn't\n\t\t// need to be incremented (hence the restore calls), because its\n\t\t// final value was properly persisted with the last local\n\t\t// commitment update.\n\t\tswitch payDesc.EntryType {\n\t\tcase FeeUpdate:\n\t\t\tif heightSet {\n\t\t\t\tpayDesc.addCommitHeightRemote = height\n\t\t\t\tpayDesc.removeCommitHeightRemote = height\n\t\t\t}\n\n\t\t\tlc.remoteUpdateLog.restoreUpdate(payDesc)\n\n\t\tdefault:\n\t\t\tif heightSet {\n\t\t\t\tpayDesc.removeCommitHeightRemote = height\n\t\t\t}\n\n\t\t\tlc.remoteUpdateLog.restoreUpdate(payDesc)\n\t\t\tlc.localUpdateLog.markHtlcModified(payDesc.ParentIndex)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// restorePeerLocalUpdates restores the acked local log updates the peer still\n// needs to sign for.",
      "length": 2083,
      "tokens": 273,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) restorePeerLocalUpdates(updates []channeldb.LogUpdate,",
      "content": "func (lc *LightningChannel) restorePeerLocalUpdates(updates []channeldb.LogUpdate,\n\tremoteCommitmentHeight uint64) error {\n\n\tlc.log.Debugf(\"Restoring %v local updates that the peer should sign\",\n\t\tlen(updates))\n\n\tfor _, logUpdate := range updates {\n\t\tlogUpdate := logUpdate\n\n\t\tpayDesc, err := lc.localLogUpdateToPayDesc(\n\t\t\t&logUpdate, lc.remoteUpdateLog, remoteCommitmentHeight,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tlc.localUpdateLog.restoreUpdate(payDesc)\n\n\t\t// Since Add updates are not stored and FeeUpdates don't have a\n\t\t// corresponding entry in the remote update log, we only need to\n\t\t// mark the htlc as modified if the update was Settle, Fail, or\n\t\t// MalformedFail.\n\t\tif payDesc.EntryType != FeeUpdate {\n\t\t\tlc.remoteUpdateLog.markHtlcModified(payDesc.ParentIndex)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// restorePendingLocalUpdates restores the local log updates leading up to the\n// given pending remote commitment.",
      "length": 806,
      "tokens": 107,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) restorePendingLocalUpdates(",
      "content": "func (lc *LightningChannel) restorePendingLocalUpdates(\n\tpendingRemoteCommitDiff *channeldb.CommitDiff,\n\tpendingRemoteKeys *CommitmentKeyRing) error {\n\n\tpendingCommit := pendingRemoteCommitDiff.Commitment\n\tpendingHeight := pendingCommit.CommitHeight\n\n\t// If we did have a dangling commit, then we'll examine which updates\n\t// we included in that state and re-insert them into our update log.\n\tfor _, logUpdate := range pendingRemoteCommitDiff.LogUpdates {\n\t\tlogUpdate := logUpdate\n\n\t\tpayDesc, err := lc.logUpdateToPayDesc(\n\t\t\t&logUpdate, lc.remoteUpdateLog, pendingHeight,\n\t\t\tchainfee.SatPerKWeight(pendingCommit.FeePerKw),\n\t\t\tpendingRemoteKeys,\n\t\t\tlc.channelState.RemoteChanCfg.DustLimit,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Earlier versions did not write the log index to disk for fee\n\t\t// updates, so they will be unset. To account for this we set\n\t\t// them to to current update log index.\n\t\tif payDesc.EntryType == FeeUpdate && payDesc.LogIndex == 0 &&\n\t\t\tlc.localUpdateLog.logIndex > 0 {\n\n\t\t\tpayDesc.LogIndex = lc.localUpdateLog.logIndex\n\t\t\tlc.log.Debugf(\"Found FeeUpdate on \"+\n\t\t\t\t\"pendingRemoteCommitDiff without logIndex, \"+\n\t\t\t\t\"using %v\", payDesc.LogIndex)\n\t\t}\n\n\t\t// At this point the restored update's logIndex must be equal\n\t\t// to the update log, otherwise something is horribly wrong.\n\t\tif payDesc.LogIndex != lc.localUpdateLog.logIndex {\n\t\t\tpanic(fmt.Sprintf(\"log index mismatch: \"+\n\t\t\t\t\"%v vs %v\", payDesc.LogIndex,\n\t\t\t\tlc.localUpdateLog.logIndex))\n\t\t}\n\n\t\tswitch payDesc.EntryType {\n\t\tcase Add:\n\t\t\t// The HtlcIndex of the added HTLC _must_ be equal to\n\t\t\t// the log's htlcCounter at this point. If it is not we\n\t\t\t// panic to catch this.\n\t\t\t// TODO(halseth): remove when cause of htlc entry bug\n\t\t\t// is found.\n\t\t\tif payDesc.HtlcIndex != lc.localUpdateLog.htlcCounter {\n\t\t\t\tpanic(fmt.Sprintf(\"htlc index mismatch: \"+\n\t\t\t\t\t\"%v vs %v\", payDesc.HtlcIndex,\n\t\t\t\t\tlc.localUpdateLog.htlcCounter))\n\t\t\t}\n\n\t\t\tlc.localUpdateLog.appendHtlc(payDesc)\n\n\t\tcase FeeUpdate:\n\t\t\tlc.localUpdateLog.appendUpdate(payDesc)\n\n\t\tdefault:\n\t\t\tlc.localUpdateLog.appendUpdate(payDesc)\n\n\t\t\tlc.remoteUpdateLog.markHtlcModified(payDesc.ParentIndex)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// HtlcRetribution contains all the items necessary to seep a revoked HTLC\n// transaction from a revoked commitment transaction broadcast by the remote\n// party.",
      "length": 2198,
      "tokens": 262,
      "embedding": []
    },
    {
      "slug": "type HtlcRetribution struct {",
      "content": "type HtlcRetribution struct {\n\t// SignDesc is a design descriptor capable of generating the necessary\n\t// signatures to satisfy the revocation clause of the HTLC's public key\n\t// script.\n\tSignDesc input.SignDescriptor\n\n\t// OutPoint is the target outpoint of this HTLC pointing to the\n\t// breached commitment transaction.\n\tOutPoint wire.OutPoint\n\n\t// SecondLevelWitnessScript is the witness script that will be created\n\t// if the second level HTLC transaction for this output is\n\t// broadcast/confirmed. We provide this as if the remote party attempts\n\t// to go to the second level to claim the HTLC then we'll need to\n\t// update the SignDesc above accordingly to sweep properly.\n\tSecondLevelWitnessScript []byte\n\n\t// IsIncoming is a boolean flag that indicates whether or not this\n\t// HTLC was accepted from the counterparty. A false value indicates that\n\t// this HTLC was offered by us. This flag is used determine the exact\n\t// witness type should be used to sweep the output.\n\tIsIncoming bool\n}\n\n// BreachRetribution contains all the data necessary to bring a channel\n// counterparty to justice claiming ALL lingering funds within the channel in\n// the scenario that they broadcast a revoked commitment transaction. A\n// BreachRetribution is created by the closeObserver if it detects an\n// uncooperative close of the channel which uses a revoked commitment\n// transaction. The BreachRetribution is then sent over the ContractBreach\n// channel in order to allow the subscriber of the channel to dispatch justice.",
      "length": 1456,
      "tokens": 234,
      "embedding": []
    },
    {
      "slug": "type BreachRetribution struct {",
      "content": "type BreachRetribution struct {\n\t// BreachTxHash is the transaction hash which breached the channel\n\t// contract by spending from the funding multi-sig with a revoked\n\t// commitment transaction.\n\tBreachTxHash chainhash.Hash\n\n\t// BreachHeight records the block height confirming the breach\n\t// transaction, used as a height hint when registering for\n\t// confirmations.\n\tBreachHeight uint32\n\n\t// ChainHash is the chain that the contract beach was identified\n\t// within. This is also the resident chain of the contract (the chain\n\t// the contract was created on).\n\tChainHash chainhash.Hash\n\n\t// RevokedStateNum is the revoked state number which was broadcast.\n\tRevokedStateNum uint64\n\n\t// LocalOutputSignDesc is a SignDescriptor which is capable of\n\t// generating the signature necessary to sweep the output within the\n\t// breach transaction that pays directly us.\n\t//\n\t// NOTE: A nil value indicates that the local output is considered dust\n\t// according to the remote party's dust limit.\n\tLocalOutputSignDesc *input.SignDescriptor\n\n\t// LocalOutpoint is the outpoint of the output paying to us (the local\n\t// party) within the breach transaction.\n\tLocalOutpoint wire.OutPoint\n\n\t// LocalDelay is the CSV delay for the to_remote script on the breached\n\t// commitment.\n\tLocalDelay uint32\n\n\t// RemoteOutputSignDesc is a SignDescriptor which is capable of\n\t// generating the signature required to claim the funds as described\n\t// within the revocation clause of the remote party's commitment\n\t// output.\n\t//\n\t// NOTE: A nil value indicates that the local output is considered dust\n\t// according to the remote party's dust limit.\n\tRemoteOutputSignDesc *input.SignDescriptor\n\n\t// RemoteOutpoint is the outpoint of the output paying to the remote\n\t// party within the breach transaction.\n\tRemoteOutpoint wire.OutPoint\n\n\t// RemoteDelay specifies the CSV delay applied to to-local scripts on\n\t// the breaching commitment transaction.\n\tRemoteDelay uint32\n\n\t// HtlcRetributions is a slice of HTLC retributions for each output\n\t// active HTLC output within the breached commitment transaction.\n\tHtlcRetributions []HtlcRetribution\n\n\t// KeyRing contains the derived public keys used to construct the\n\t// breaching commitment transaction. This allows downstream clients to\n\t// have access to the public keys used in the scripts.\n\tKeyRing *CommitmentKeyRing\n}\n\n// NewBreachRetribution creates a new fully populated BreachRetribution for the\n// passed channel, at a particular revoked state number. If the spend\n// transaction that the breach retribution should target is known, then it can\n// be provided via the spendTx parameter. Otherwise, if the spendTx parameter is\n// nil, then the revocation log will be checked to see if it contains the info\n// required to construct the BreachRetribution. If the revocation log is missing\n// the required fields then ErrRevLogDataMissing will be returned.",
      "length": 2779,
      "tokens": 417,
      "embedding": []
    },
    {
      "slug": "func NewBreachRetribution(chanState *channeldb.OpenChannel, stateNum uint64,",
      "content": "func NewBreachRetribution(chanState *channeldb.OpenChannel, stateNum uint64,\n\tbreachHeight uint32, spendTx *wire.MsgTx) (*BreachRetribution, error) {\n\n\t// Query the on-disk revocation log for the snapshot which was recorded\n\t// at this particular state num. Based on whether a legacy revocation\n\t// log is returned or not, we will process them differently.\n\trevokedLog, revokedLogLegacy, err := chanState.FindPreviousState(\n\t\tstateNum,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Sanity check that at least one of the logs is returned.\n\tif revokedLog == nil && revokedLogLegacy == nil {\n\t\treturn nil, ErrNoRevocationLogFound\n\t}\n\n\t// With the state number broadcast known, we can now derive/restore the\n\t// proper revocation preimage necessary to sweep the remote party's\n\t// output.\n\trevocationPreimage, err := chanState.RevocationStore.LookUp(stateNum)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tcommitmentSecret, commitmentPoint := btcec.PrivKeyFromBytes(\n\t\trevocationPreimage[:],\n\t)\n\n\t// With the commitment point generated, we can now generate the four\n\t// keys we'll need to reconstruct the commitment state,\n\tkeyRing := DeriveCommitmentKeys(\n\t\tcommitmentPoint, false, chanState.ChanType,\n\t\t&chanState.LocalChanCfg, &chanState.RemoteChanCfg,\n\t)\n\n\t// Next, reconstruct the scripts as they were present at this state\n\t// number so we can have the proper witness script to sign and include\n\t// within the final witness.\n\tvar leaseExpiry uint32\n\tif chanState.ChanType.HasLeaseExpiration() {\n\t\tleaseExpiry = chanState.ThawHeight\n\t}\n\n\t// Since it is the remote breach we are reconstructing, the output\n\t// going to us will be a to-remote script with our local params.\n\tisRemoteInitiator := !chanState.IsInitiator\n\tourScript, ourDelay, err := CommitScriptToRemote(\n\t\tchanState.ChanType, isRemoteInitiator, keyRing.ToRemoteKey,\n\t\tleaseExpiry,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ttheirDelay := uint32(chanState.RemoteChanCfg.CsvDelay)\n\ttheirScript, err := CommitScriptToSelf(\n\t\tchanState.ChanType, isRemoteInitiator, keyRing.ToLocalKey,\n\t\tkeyRing.RevocationKey, theirDelay, leaseExpiry,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Define an empty breach retribution that will be overwritten based on\n\t// different version of the revocation log found.\n\tvar br *BreachRetribution\n\n\t// Define our and their amounts, that will be overwritten below.\n\tvar ourAmt, theirAmt int64\n\n\t// If the returned *RevocationLog is non-nil, use it to derive the info\n\t// we need.\n\tif revokedLog != nil {\n\t\tbr, ourAmt, theirAmt, err = createBreachRetribution(\n\t\t\trevokedLog, spendTx, chanState, keyRing,\n\t\t\tcommitmentSecret, leaseExpiry,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else {\n\t\t// The returned revocation log is in legacy format, which is a\n\t\t// *ChannelCommitment.\n\t\t//\n\t\t// NOTE: this branch is kept for compatibility such that for\n\t\t// old nodes which refuse to migrate the legacy revocation log\n\t\t// data can still function. This branch can be deleted once we\n\t\t// are confident that no legacy format is in use.\n\t\tbr, ourAmt, theirAmt, err = createBreachRetributionLegacy(\n\t\t\trevokedLogLegacy, chanState, keyRing, commitmentSecret,\n\t\t\tourScript, theirScript, leaseExpiry,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// Conditionally instantiate a sign descriptor for each of the\n\t// commitment outputs. If either is considered dust using the remote\n\t// party's dust limit, the respective sign descriptor will be nil.\n\t//\n\t// If our balance exceeds the remote party's dust limit, instantiate\n\t// the sign descriptor for our output.\n\tif ourAmt >= int64(chanState.RemoteChanCfg.DustLimit) {\n\t\tbr.LocalOutputSignDesc = &input.SignDescriptor{\n\t\t\tSingleTweak:   keyRing.LocalCommitKeyTweak,\n\t\t\tKeyDesc:       chanState.LocalChanCfg.PaymentBasePoint,\n\t\t\tWitnessScript: ourScript.WitnessScript,\n\t\t\tOutput: &wire.TxOut{\n\t\t\t\tPkScript: ourScript.PkScript,\n\t\t\t\tValue:    ourAmt,\n\t\t\t},\n\t\t\tHashType: txscript.SigHashAll,\n\t\t}\n\t}\n\n\t// Similarly, if their balance exceeds the remote party's dust limit,\n\t// assemble the sign descriptor for their output, which we can sweep.\n\tif theirAmt >= int64(chanState.RemoteChanCfg.DustLimit) {\n\t\tbr.RemoteOutputSignDesc = &input.SignDescriptor{\n\t\t\tKeyDesc: chanState.LocalChanCfg.\n\t\t\t\tRevocationBasePoint,\n\t\t\tDoubleTweak:   commitmentSecret,\n\t\t\tWitnessScript: theirScript.WitnessScript,\n\t\t\tOutput: &wire.TxOut{\n\t\t\t\tPkScript: theirScript.PkScript,\n\t\t\t\tValue:    theirAmt,\n\t\t\t},\n\t\t\tHashType: txscript.SigHashAll,\n\t\t}\n\t}\n\n\t// Finally, with all the necessary data constructed, we can pad the\n\t// BreachRetribution struct which houses all the data necessary to\n\t// swiftly bring justice to the cheating remote party.\n\tbr.BreachHeight = breachHeight\n\tbr.RevokedStateNum = stateNum\n\tbr.LocalDelay = ourDelay\n\tbr.RemoteDelay = theirDelay\n\n\treturn br, nil\n}\n\n// createHtlcRetribution is a helper function to construct an HtlcRetribution\n// based on the passed params.",
      "length": 4668,
      "tokens": 622,
      "embedding": []
    },
    {
      "slug": "func createHtlcRetribution(chanState *channeldb.OpenChannel,",
      "content": "func createHtlcRetribution(chanState *channeldb.OpenChannel,\n\tkeyRing *CommitmentKeyRing, commitHash chainhash.Hash,\n\tcommitmentSecret *btcec.PrivateKey, leaseExpiry uint32,\n\thtlc *channeldb.HTLCEntry) (HtlcRetribution, error) {\n\n\tvar emptyRetribution HtlcRetribution\n\n\ttheirDelay := uint32(chanState.RemoteChanCfg.CsvDelay)\n\tisRemoteInitiator := !chanState.IsInitiator\n\n\t// We'll generate the original second level witness script now, as\n\t// we'll need it if we're revoking an HTLC output on the remote\n\t// commitment transaction, and *they* go to the second level.\n\tsecondLevelScript, err := SecondLevelHtlcScript(\n\t\tchanState.ChanType, isRemoteInitiator,\n\t\tkeyRing.RevocationKey, keyRing.ToLocalKey, theirDelay,\n\t\tleaseExpiry,\n\t)\n\tif err != nil {\n\t\treturn emptyRetribution, err\n\t}\n\n\t// If this is an incoming HTLC, then this means that they were the\n\t// sender of the HTLC (relative to us). So we'll re-generate the sender\n\t// HTLC script. Otherwise, is this was an outgoing HTLC that we sent,\n\t// then from the PoV of the remote commitment state, they're the\n\t// receiver of this HTLC.\n\thtlcPkScript, htlcWitnessScript, err := genHtlcScript(\n\t\tchanState.ChanType, htlc.Incoming, false,\n\t\thtlc.RefundTimeout, htlc.RHash, keyRing,\n\t)\n\tif err != nil {\n\t\treturn emptyRetribution, err\n\t}\n\n\treturn HtlcRetribution{\n\t\tSignDesc: input.SignDescriptor{\n\t\t\tKeyDesc: chanState.LocalChanCfg.\n\t\t\t\tRevocationBasePoint,\n\t\t\tDoubleTweak:   commitmentSecret,\n\t\t\tWitnessScript: htlcWitnessScript,\n\t\t\tOutput: &wire.TxOut{\n\t\t\t\tPkScript: htlcPkScript,\n\t\t\t\tValue:    int64(htlc.Amt),\n\t\t\t},\n\t\t\tHashType: txscript.SigHashAll,\n\t\t},\n\t\tOutPoint: wire.OutPoint{\n\t\t\tHash:  commitHash,\n\t\t\tIndex: uint32(htlc.OutputIndex),\n\t\t},\n\t\tSecondLevelWitnessScript: secondLevelScript.WitnessScript,\n\t\tIsIncoming:               htlc.Incoming,\n\t}, nil\n}\n\n// createBreachRetribution creates a partially initiated BreachRetribution\n// using a RevocationLog. Returns the constructed retribution, our amount,\n// their amount, and a possible non-nil error. If the spendTx parameter is\n// non-nil, then it will be used to glean the breach transaction's to-local and\n// to-remote output amounts. Otherwise, the RevocationLog will be checked to\n// see if these fields are present there. If they are not, then\n// ErrRevLogDataMissing is returned.",
      "length": 2174,
      "tokens": 261,
      "embedding": []
    },
    {
      "slug": "func createBreachRetribution(revokedLog *channeldb.RevocationLog,",
      "content": "func createBreachRetribution(revokedLog *channeldb.RevocationLog,\n\tspendTx *wire.MsgTx, chanState *channeldb.OpenChannel,\n\tkeyRing *CommitmentKeyRing, commitmentSecret *btcec.PrivateKey,\n\tleaseExpiry uint32) (*BreachRetribution, int64, int64, error) {\n\n\tcommitHash := revokedLog.CommitTxHash\n\n\t// Create the htlc retributions.\n\thtlcRetributions := make([]HtlcRetribution, len(revokedLog.HTLCEntries))\n\tfor i, htlc := range revokedLog.HTLCEntries {\n\t\thr, err := createHtlcRetribution(\n\t\t\tchanState, keyRing, commitHash,\n\t\t\tcommitmentSecret, leaseExpiry, htlc,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, 0, 0, err\n\t\t}\n\t\thtlcRetributions[i] = hr\n\t}\n\n\tvar ourAmt, theirAmt int64\n\n\t// Construct the our outpoint.\n\tourOutpoint := wire.OutPoint{\n\t\tHash: commitHash,\n\t}\n\tif revokedLog.OurOutputIndex != channeldb.OutputIndexEmpty {\n\t\tourOutpoint.Index = uint32(revokedLog.OurOutputIndex)\n\n\t\t// If the spend transaction is provided, then we use it to get\n\t\t// the value of our output.\n\t\tif spendTx != nil {\n\t\t\t// Sanity check that OurOutputIndex is within range.\n\t\t\tif int(ourOutpoint.Index) >= len(spendTx.TxOut) {\n\t\t\t\treturn nil, 0, 0, fmt.Errorf(\"%w: ours=%v, \"+\n\t\t\t\t\t\"len(TxOut)=%v\",\n\t\t\t\t\tErrOutputIndexOutOfRange,\n\t\t\t\t\tourOutpoint.Index, len(spendTx.TxOut),\n\t\t\t\t)\n\t\t\t}\n\t\t\t// Read the amounts from the breach transaction.\n\t\t\t//\n\t\t\t// NOTE: ourAmt here includes commit fee and anchor\n\t\t\t// amount (if enabled).\n\t\t\tourAmt = spendTx.TxOut[ourOutpoint.Index].Value\n\t\t} else {\n\t\t\t// Otherwise, we check to see if the revocation log\n\t\t\t// contains our output amount. Due to a previous\n\t\t\t// migration, this field may be empty in which case an\n\t\t\t// error will be returned.\n\t\t\tif revokedLog.OurBalance == nil {\n\t\t\t\treturn nil, 0, 0, ErrRevLogDataMissing\n\t\t\t}\n\n\t\t\tourAmt = int64(revokedLog.OurBalance.ToSatoshis())\n\t\t}\n\t}\n\n\t// Construct the their outpoint.\n\ttheirOutpoint := wire.OutPoint{\n\t\tHash: commitHash,\n\t}\n\tif revokedLog.TheirOutputIndex != channeldb.OutputIndexEmpty {\n\t\ttheirOutpoint.Index = uint32(revokedLog.TheirOutputIndex)\n\n\t\t// If the spend transaction is provided, then we use it to get\n\t\t// the value of the remote parties' output.\n\t\tif spendTx != nil {\n\t\t\t// Sanity check that TheirOutputIndex is within range.\n\t\t\tif int(revokedLog.TheirOutputIndex) >=\n\t\t\t\tlen(spendTx.TxOut) {\n\n\t\t\t\treturn nil, 0, 0, fmt.Errorf(\"%w: theirs=%v, \"+\n\t\t\t\t\t\"len(TxOut)=%v\",\n\t\t\t\t\tErrOutputIndexOutOfRange,\n\t\t\t\t\trevokedLog.TheirOutputIndex,\n\t\t\t\t\tlen(spendTx.TxOut),\n\t\t\t\t)\n\t\t\t}\n\n\t\t\t// Read the amounts from the breach transaction.\n\t\t\ttheirAmt = spendTx.TxOut[theirOutpoint.Index].Value\n\n\t\t} else {\n\t\t\t// Otherwise, we check to see if the revocation log\n\t\t\t// contains remote parties' output amount. Due to a\n\t\t\t// previous migration, this field may be empty in which\n\t\t\t// case an error will be returned.\n\t\t\tif revokedLog.TheirBalance == nil {\n\t\t\t\treturn nil, 0, 0, ErrRevLogDataMissing\n\t\t\t}\n\n\t\t\ttheirAmt = int64(revokedLog.TheirBalance.ToSatoshis())\n\t\t}\n\t}\n\n\treturn &BreachRetribution{\n\t\tBreachTxHash:     commitHash,\n\t\tChainHash:        chanState.ChainHash,\n\t\tLocalOutpoint:    ourOutpoint,\n\t\tRemoteOutpoint:   theirOutpoint,\n\t\tHtlcRetributions: htlcRetributions,\n\t\tKeyRing:          keyRing,\n\t}, ourAmt, theirAmt, nil\n}\n\n// createBreachRetributionLegacy creates a partially initiated\n// BreachRetribution using a ChannelCommitment. Returns the constructed\n// retribution, our amount, their amount, and a possible non-nil error.",
      "length": 3232,
      "tokens": 393,
      "embedding": []
    },
    {
      "slug": "func createBreachRetributionLegacy(revokedLog *channeldb.ChannelCommitment,",
      "content": "func createBreachRetributionLegacy(revokedLog *channeldb.ChannelCommitment,\n\tchanState *channeldb.OpenChannel, keyRing *CommitmentKeyRing,\n\tcommitmentSecret *btcec.PrivateKey,\n\tourScript, theirScript *ScriptInfo,\n\tleaseExpiry uint32) (*BreachRetribution, int64, int64, error) {\n\n\tcommitHash := revokedLog.CommitTx.TxHash()\n\tourOutpoint := wire.OutPoint{\n\t\tHash: commitHash,\n\t}\n\ttheirOutpoint := wire.OutPoint{\n\t\tHash: commitHash,\n\t}\n\n\t// In order to fully populate the breach retribution struct, we'll need\n\t// to find the exact index of the commitment outputs.\n\tfor i, txOut := range revokedLog.CommitTx.TxOut {\n\t\tswitch {\n\t\tcase bytes.Equal(txOut.PkScript, ourScript.PkScript):\n\t\t\tourOutpoint.Index = uint32(i)\n\t\tcase bytes.Equal(txOut.PkScript, theirScript.PkScript):\n\t\t\ttheirOutpoint.Index = uint32(i)\n\t\t}\n\t}\n\n\t// With the commitment outputs located, we'll now generate all the\n\t// retribution structs for each of the HTLC transactions active on the\n\t// remote commitment transaction.\n\thtlcRetributions := make([]HtlcRetribution, len(revokedLog.Htlcs))\n\tfor i, htlc := range revokedLog.Htlcs {\n\t\t// If the HTLC is dust, then we'll skip it as it doesn't have\n\t\t// an output on the commitment transaction.\n\t\tif HtlcIsDust(\n\t\t\tchanState.ChanType, htlc.Incoming, false,\n\t\t\tchainfee.SatPerKWeight(revokedLog.FeePerKw),\n\t\t\thtlc.Amt.ToSatoshis(),\n\t\t\tchanState.RemoteChanCfg.DustLimit,\n\t\t) {\n\n\t\t\tcontinue\n\t\t}\n\n\t\tentry := &channeldb.HTLCEntry{\n\t\t\tRHash:         htlc.RHash,\n\t\t\tRefundTimeout: htlc.RefundTimeout,\n\t\t\tOutputIndex:   uint16(htlc.OutputIndex),\n\t\t\tIncoming:      htlc.Incoming,\n\t\t\tAmt:           htlc.Amt.ToSatoshis(),\n\t\t}\n\t\thr, err := createHtlcRetribution(\n\t\t\tchanState, keyRing, commitHash,\n\t\t\tcommitmentSecret, leaseExpiry, entry,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, 0, 0, err\n\t\t}\n\t\thtlcRetributions[i] = hr\n\t}\n\n\t// Compute the balances in satoshis.\n\tourAmt := int64(revokedLog.LocalBalance.ToSatoshis())\n\ttheirAmt := int64(revokedLog.RemoteBalance.ToSatoshis())\n\n\treturn &BreachRetribution{\n\t\tBreachTxHash:     commitHash,\n\t\tChainHash:        chanState.ChainHash,\n\t\tLocalOutpoint:    ourOutpoint,\n\t\tRemoteOutpoint:   theirOutpoint,\n\t\tHtlcRetributions: htlcRetributions,\n\t\tKeyRing:          keyRing,\n\t}, ourAmt, theirAmt, nil\n}\n\n// HtlcIsDust determines if an HTLC output is dust or not depending on two\n// bits: if the HTLC is incoming and if the HTLC will be placed on our\n// commitment transaction, or theirs. These two pieces of information are\n// require as we currently used second-level HTLC transactions as off-chain\n// covenants. Depending on the two bits, we'll either be using a timeout or\n// success transaction which have different weights.",
      "length": 2511,
      "tokens": 291,
      "embedding": []
    },
    {
      "slug": "func HtlcIsDust(chanType channeldb.ChannelType,",
      "content": "func HtlcIsDust(chanType channeldb.ChannelType,\n\tincoming, ourCommit bool, feePerKw chainfee.SatPerKWeight,\n\thtlcAmt, dustLimit btcutil.Amount) bool {\n\n\t// First we'll determine the fee required for this HTLC based on if this is\n\t// an incoming HTLC or not, and also on whose commitment transaction it\n\t// will be placed on.\n\tvar htlcFee btcutil.Amount\n\tswitch {\n\n\t// If this is an incoming HTLC on our commitment transaction, then the\n\t// second-level transaction will be a success transaction.\n\tcase incoming && ourCommit:\n\t\thtlcFee = HtlcSuccessFee(chanType, feePerKw)\n\n\t// If this is an incoming HTLC on their commitment transaction, then\n\t// we'll be using a second-level timeout transaction as they've added\n\t// this HTLC.\n\tcase incoming && !ourCommit:\n\t\thtlcFee = HtlcTimeoutFee(chanType, feePerKw)\n\n\t// If this is an outgoing HTLC on our commitment transaction, then\n\t// we'll be using a timeout transaction as we're the sender of the\n\t// HTLC.\n\tcase !incoming && ourCommit:\n\t\thtlcFee = HtlcTimeoutFee(chanType, feePerKw)\n\n\t// If this is an outgoing HTLC on their commitment transaction, then\n\t// we'll be using an HTLC success transaction as they're the receiver\n\t// of this HTLC.\n\tcase !incoming && !ourCommit:\n\t\thtlcFee = HtlcSuccessFee(chanType, feePerKw)\n\t}\n\n\treturn (htlcAmt - htlcFee) < dustLimit\n}\n\n// htlcView represents the \"active\" HTLCs at a particular point within the\n// history of the HTLC update log.",
      "length": 1339,
      "tokens": 209,
      "embedding": []
    },
    {
      "slug": "type htlcView struct {",
      "content": "type htlcView struct {\n\tourUpdates   []*PaymentDescriptor\n\ttheirUpdates []*PaymentDescriptor\n\tfeePerKw     chainfee.SatPerKWeight\n}\n\n// fetchHTLCView returns all the candidate HTLC updates which should be\n// considered for inclusion within a commitment based on the passed HTLC log\n// indexes.",
      "length": 263,
      "tokens": 33,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) fetchHTLCView(theirLogIndex, ourLogIndex uint64) *htlcView {",
      "content": "func (lc *LightningChannel) fetchHTLCView(theirLogIndex, ourLogIndex uint64) *htlcView {\n\tvar ourHTLCs []*PaymentDescriptor\n\tfor e := lc.localUpdateLog.Front(); e != nil; e = e.Next() {\n\t\thtlc := e.Value.(*PaymentDescriptor)\n\n\t\t// This HTLC is active from this point-of-view iff the log\n\t\t// index of the state update is below the specified index in\n\t\t// our update log.\n\t\tif htlc.LogIndex < ourLogIndex {\n\t\t\tourHTLCs = append(ourHTLCs, htlc)\n\t\t}\n\t}\n\n\tvar theirHTLCs []*PaymentDescriptor\n\tfor e := lc.remoteUpdateLog.Front(); e != nil; e = e.Next() {\n\t\thtlc := e.Value.(*PaymentDescriptor)\n\n\t\t// If this is an incoming HTLC, then it is only active from\n\t\t// this point-of-view if the index of the HTLC addition in\n\t\t// their log is below the specified view index.\n\t\tif htlc.LogIndex < theirLogIndex {\n\t\t\ttheirHTLCs = append(theirHTLCs, htlc)\n\t\t}\n\t}\n\n\treturn &htlcView{\n\t\tourUpdates:   ourHTLCs,\n\t\ttheirUpdates: theirHTLCs,\n\t}\n}\n\n// fetchCommitmentView returns a populated commitment which expresses the state\n// of the channel from the point of view of a local or remote chain, evaluating\n// the HTLC log up to the passed indexes. This function is used to construct\n// both local and remote commitment transactions in order to sign or verify new\n// commitment updates. A fully populated commitment is returned which reflects\n// the proper balances for both sides at this point in the commitment chain.",
      "length": 1277,
      "tokens": 204,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) fetchCommitmentView(remoteChain bool,",
      "content": "func (lc *LightningChannel) fetchCommitmentView(remoteChain bool,\n\tourLogIndex, ourHtlcIndex, theirLogIndex, theirHtlcIndex uint64,\n\tkeyRing *CommitmentKeyRing) (*commitment, error) {\n\n\tcommitChain := lc.localCommitChain\n\tdustLimit := lc.channelState.LocalChanCfg.DustLimit\n\tif remoteChain {\n\t\tcommitChain = lc.remoteCommitChain\n\t\tdustLimit = lc.channelState.RemoteChanCfg.DustLimit\n\t}\n\n\tnextHeight := commitChain.tip().height + 1\n\n\t// Run through all the HTLCs that will be covered by this transaction\n\t// in order to update their commitment addition height, and to adjust\n\t// the balances on the commitment transaction accordingly. Note that\n\t// these balances will be *before* taking a commitment fee from the\n\t// initiator.\n\thtlcView := lc.fetchHTLCView(theirLogIndex, ourLogIndex)\n\tourBalance, theirBalance, _, filteredHTLCView, err := lc.computeView(\n\t\thtlcView, remoteChain, true,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfeePerKw := filteredHTLCView.feePerKw\n\n\t// Actually generate unsigned commitment transaction for this view.\n\tcommitTx, err := lc.commitBuilder.createUnsignedCommitmentTx(\n\t\tourBalance, theirBalance, !remoteChain, feePerKw, nextHeight,\n\t\tfilteredHTLCView, keyRing,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// We'll assert that there hasn't been a mistake during fee calculation\n\t// leading to a fee too low.\n\tvar totalOut btcutil.Amount\n\tfor _, txOut := range commitTx.txn.TxOut {\n\t\ttotalOut += btcutil.Amount(txOut.Value)\n\t}\n\tfee := lc.channelState.Capacity - totalOut\n\n\t// Since the transaction is not signed yet, we use the witness weight\n\t// used for weight calculation.\n\tuTx := btcutil.NewTx(commitTx.txn)\n\tweight := blockchain.GetTransactionWeight(uTx) +\n\t\tinput.WitnessCommitmentTxWeight\n\n\teffFeeRate := chainfee.SatPerKWeight(fee) * 1000 /\n\t\tchainfee.SatPerKWeight(weight)\n\tif effFeeRate < chainfee.AbsoluteFeePerKwFloor {\n\t\treturn nil, fmt.Errorf(\"height=%v, for ChannelPoint(%v) \"+\n\t\t\t\"attempts to create commitment with feerate %v: %v\",\n\t\t\tnextHeight, lc.channelState.FundingOutpoint,\n\t\t\teffFeeRate, spew.Sdump(commitTx))\n\t}\n\n\t// With the commitment view created, store the resulting balances and\n\t// transaction with the other parameters for this height.\n\tc := &commitment{\n\t\tourBalance:        commitTx.ourBalance,\n\t\ttheirBalance:      commitTx.theirBalance,\n\t\ttxn:               commitTx.txn,\n\t\tfee:               commitTx.fee,\n\t\tourMessageIndex:   ourLogIndex,\n\t\tourHtlcIndex:      ourHtlcIndex,\n\t\ttheirMessageIndex: theirLogIndex,\n\t\ttheirHtlcIndex:    theirHtlcIndex,\n\t\theight:            nextHeight,\n\t\tfeePerKw:          feePerKw,\n\t\tdustLimit:         dustLimit,\n\t\tisOurs:            !remoteChain,\n\t}\n\n\t// In order to ensure _none_ of the HTLC's associated with this new\n\t// commitment are mutated, we'll manually copy over each HTLC to its\n\t// respective slice.\n\tc.outgoingHTLCs = make([]PaymentDescriptor, len(filteredHTLCView.ourUpdates))\n\tfor i, htlc := range filteredHTLCView.ourUpdates {\n\t\tc.outgoingHTLCs[i] = *htlc\n\t}\n\tc.incomingHTLCs = make([]PaymentDescriptor, len(filteredHTLCView.theirUpdates))\n\tfor i, htlc := range filteredHTLCView.theirUpdates {\n\t\tc.incomingHTLCs[i] = *htlc\n\t}\n\n\t// Finally, we'll populate all the HTLC indexes so we can track the\n\t// locations of each HTLC in the commitment state. We pass in the sorted\n\t// slice of CLTV deltas in order to properly locate HTLCs that otherwise\n\t// have the same payment hash and amount.\n\terr = c.populateHtlcIndexes(lc.channelState.ChanType, commitTx.cltvs)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn c, nil\n}\n",
      "length": 3365,
      "tokens": 403,
      "embedding": []
    },
    {
      "slug": "func fundingTxIn(chanState *channeldb.OpenChannel) wire.TxIn {",
      "content": "func fundingTxIn(chanState *channeldb.OpenChannel) wire.TxIn {\n\treturn *wire.NewTxIn(&chanState.FundingOutpoint, nil, nil)\n}\n\n// evaluateHTLCView processes all update entries in both HTLC update logs,\n// producing a final view which is the result of properly applying all adds,\n// settles, timeouts and fee updates found in both logs. The resulting view\n// returned reflects the current state of HTLCs within the remote or local\n// commitment chain, and the current commitment fee rate.\n//\n// If mutateState is set to true, then the add height of all added HTLCs\n// will be set to nextHeight, and the remove height of all removed HTLCs\n// will be set to nextHeight. This should therefore only be set to true\n// once for each height, and only in concert with signing a new commitment.\n// TODO(halseth): return htlcs to mutate instead of mutating inside\n// method.",
      "length": 785,
      "tokens": 135,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) evaluateHTLCView(view *htlcView, ourBalance,",
      "content": "func (lc *LightningChannel) evaluateHTLCView(view *htlcView, ourBalance,\n\ttheirBalance *lnwire.MilliSatoshi, nextHeight uint64,\n\tremoteChain, mutateState bool) (*htlcView, error) {\n\n\t// We initialize the view's fee rate to the fee rate of the unfiltered\n\t// view. If any fee updates are found when evaluating the view, it will\n\t// be updated.\n\tnewView := &htlcView{\n\t\tfeePerKw: view.feePerKw,\n\t}\n\n\t// We use two maps, one for the local log and one for the remote log to\n\t// keep track of which entries we need to skip when creating the final\n\t// htlc view. We skip an entry whenever we find a settle or a timeout\n\t// modifying an entry.\n\tskipUs := make(map[uint64]struct{})\n\tskipThem := make(map[uint64]struct{})\n\n\t// First we run through non-add entries in both logs, populating the\n\t// skip sets and mutating the current chain state (crediting balances,\n\t// etc) to reflect the settle/timeout entry encountered.\n\tfor _, entry := range view.ourUpdates {\n\t\tswitch entry.EntryType {\n\t\t// Skip adds for now. They will be processed below.\n\t\tcase Add:\n\t\t\tcontinue\n\n\t\t// Process fee updates, updating the current feePerKw.\n\t\tcase FeeUpdate:\n\t\t\tprocessFeeUpdate(\n\t\t\t\tentry, nextHeight, remoteChain, mutateState,\n\t\t\t\tnewView,\n\t\t\t)\n\t\t\tcontinue\n\t\t}\n\n\t\t// If we're settling an inbound HTLC, and it hasn't been\n\t\t// processed yet, then increment our state tracking the total\n\t\t// number of satoshis we've received within the channel.\n\t\tif mutateState && entry.EntryType == Settle && !remoteChain &&\n\t\t\tentry.removeCommitHeightLocal == 0 {\n\t\t\tlc.channelState.TotalMSatReceived += entry.Amount\n\t\t}\n\n\t\taddEntry, err := lc.fetchParent(entry, remoteChain, true)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tskipThem[addEntry.HtlcIndex] = struct{}{}\n\t\tprocessRemoveEntry(entry, ourBalance, theirBalance,\n\t\t\tnextHeight, remoteChain, true, mutateState)\n\t}\n\tfor _, entry := range view.theirUpdates {\n\t\tswitch entry.EntryType {\n\t\t// Skip adds for now. They will be processed below.\n\t\tcase Add:\n\t\t\tcontinue\n\n\t\t// Process fee updates, updating the current feePerKw.\n\t\tcase FeeUpdate:\n\t\t\tprocessFeeUpdate(\n\t\t\t\tentry, nextHeight, remoteChain, mutateState,\n\t\t\t\tnewView,\n\t\t\t)\n\t\t\tcontinue\n\t\t}\n\n\t\t// If the remote party is settling one of our outbound HTLC's,\n\t\t// and it hasn't been processed, yet, the increment our state\n\t\t// tracking the total number of satoshis we've sent within the\n\t\t// channel.\n\t\tif mutateState && entry.EntryType == Settle && !remoteChain &&\n\t\t\tentry.removeCommitHeightLocal == 0 {\n\t\t\tlc.channelState.TotalMSatSent += entry.Amount\n\t\t}\n\n\t\taddEntry, err := lc.fetchParent(entry, remoteChain, false)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tskipUs[addEntry.HtlcIndex] = struct{}{}\n\t\tprocessRemoveEntry(entry, ourBalance, theirBalance,\n\t\t\tnextHeight, remoteChain, false, mutateState)\n\t}\n\n\t// Next we take a second pass through all the log entries, skipping any\n\t// settled HTLCs, and debiting the chain state balance due to any newly\n\t// added HTLCs.\n\tfor _, entry := range view.ourUpdates {\n\t\tisAdd := entry.EntryType == Add\n\t\tif _, ok := skipUs[entry.HtlcIndex]; !isAdd || ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tprocessAddEntry(entry, ourBalance, theirBalance, nextHeight,\n\t\t\tremoteChain, false, mutateState)\n\t\tnewView.ourUpdates = append(newView.ourUpdates, entry)\n\t}\n\tfor _, entry := range view.theirUpdates {\n\t\tisAdd := entry.EntryType == Add\n\t\tif _, ok := skipThem[entry.HtlcIndex]; !isAdd || ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tprocessAddEntry(entry, ourBalance, theirBalance, nextHeight,\n\t\t\tremoteChain, true, mutateState)\n\t\tnewView.theirUpdates = append(newView.theirUpdates, entry)\n\t}\n\n\treturn newView, nil\n}\n\n// fetchParent is a helper that looks up update log parent entries in the\n// appropriate log.",
      "length": 3494,
      "tokens": 491,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) fetchParent(entry *PaymentDescriptor,",
      "content": "func (lc *LightningChannel) fetchParent(entry *PaymentDescriptor,\n\tremoteChain, remoteLog bool) (*PaymentDescriptor, error) {\n\n\tvar (\n\t\tupdateLog *updateLog\n\t\tlogName   string\n\t)\n\n\tif remoteLog {\n\t\tupdateLog = lc.remoteUpdateLog\n\t\tlogName = \"remote\"\n\t} else {\n\t\tupdateLog = lc.localUpdateLog\n\t\tlogName = \"local\"\n\t}\n\n\taddEntry := updateLog.lookupHtlc(entry.ParentIndex)\n\n\tswitch {\n\t// We check if the parent entry is not found at this point.\n\t// This could happen for old versions of lnd, and we return an\n\t// error to gracefully shut down the state machine if such an\n\t// entry is still in the logs.\n\tcase addEntry == nil:\n\t\treturn nil, fmt.Errorf(\"unable to find parent entry \"+\n\t\t\t\"%d in %v update log: %v\\nUpdatelog: %v\",\n\t\t\tentry.ParentIndex, logName,\n\t\t\tnewLogClosure(func() string {\n\t\t\t\treturn spew.Sdump(entry)\n\t\t\t}), newLogClosure(func() string {\n\t\t\t\treturn spew.Sdump(updateLog)\n\t\t\t}),\n\t\t)\n\n\t// The parent add height should never be zero at this point. If\n\t// that's the case we probably forgot to send a new commitment.\n\tcase remoteChain && addEntry.addCommitHeightRemote == 0:\n\t\treturn nil, fmt.Errorf(\"parent entry %d for update %d \"+\n\t\t\t\"had zero remote add height\", entry.ParentIndex,\n\t\t\tentry.LogIndex)\n\tcase !remoteChain && addEntry.addCommitHeightLocal == 0:\n\t\treturn nil, fmt.Errorf(\"parent entry %d for update %d \"+\n\t\t\t\"had zero local add height\", entry.ParentIndex,\n\t\t\tentry.LogIndex)\n\t}\n\n\treturn addEntry, nil\n}\n\n// processAddEntry evaluates the effect of an add entry within the HTLC log.\n// If the HTLC hasn't yet been committed in either chain, then the height it\n// was committed is updated. Keeping track of this inclusion height allows us to\n// later compact the log once the change is fully committed in both chains.",
      "length": 1627,
      "tokens": 246,
      "embedding": []
    },
    {
      "slug": "func processAddEntry(htlc *PaymentDescriptor, ourBalance, theirBalance *lnwire.MilliSatoshi,",
      "content": "func processAddEntry(htlc *PaymentDescriptor, ourBalance, theirBalance *lnwire.MilliSatoshi,\n\tnextHeight uint64, remoteChain bool, isIncoming, mutateState bool) {\n\n\t// If we're evaluating this entry for the remote chain (to create/view\n\t// a new commitment), then we'll may be updating the height this entry\n\t// was added to the chain. Otherwise, we may be updating the entry's\n\t// height w.r.t the local chain.\n\tvar addHeight *uint64\n\tif remoteChain {\n\t\taddHeight = &htlc.addCommitHeightRemote\n\t} else {\n\t\taddHeight = &htlc.addCommitHeightLocal\n\t}\n\n\tif *addHeight != 0 {\n\t\treturn\n\t}\n\n\tif isIncoming {\n\t\t// If this is a new incoming (un-committed) HTLC, then we need\n\t\t// to update their balance accordingly by subtracting the\n\t\t// amount of the HTLC that are funds pending.\n\t\t*theirBalance -= htlc.Amount\n\t} else {\n\t\t// Similarly, we need to debit our balance if this is an out\n\t\t// going HTLC to reflect the pending balance.\n\t\t*ourBalance -= htlc.Amount\n\t}\n\n\tif mutateState {\n\t\t*addHeight = nextHeight\n\t}\n}\n\n// processRemoveEntry processes a log entry which settles or times out a\n// previously added HTLC. If the removal entry has already been processed, it\n// is skipped.",
      "length": 1047,
      "tokens": 175,
      "embedding": []
    },
    {
      "slug": "func processRemoveEntry(htlc *PaymentDescriptor, ourBalance,",
      "content": "func processRemoveEntry(htlc *PaymentDescriptor, ourBalance,\n\ttheirBalance *lnwire.MilliSatoshi, nextHeight uint64,\n\tremoteChain bool, isIncoming, mutateState bool) {\n\n\tvar removeHeight *uint64\n\tif remoteChain {\n\t\tremoveHeight = &htlc.removeCommitHeightRemote\n\t} else {\n\t\tremoveHeight = &htlc.removeCommitHeightLocal\n\t}\n\n\t// Ignore any removal entries which have already been processed.\n\tif *removeHeight != 0 {\n\t\treturn\n\t}\n\n\tswitch {\n\t// If an incoming HTLC is being settled, then this means that we've\n\t// received the preimage either from another subsystem, or the\n\t// upstream peer in the route. Therefore, we increase our balance by\n\t// the HTLC amount.\n\tcase isIncoming && htlc.EntryType == Settle:\n\t\t*ourBalance += htlc.Amount\n\n\t// Otherwise, this HTLC is being failed out, therefore the value of the\n\t// HTLC should return to the remote party.\n\tcase isIncoming && (htlc.EntryType == Fail || htlc.EntryType == MalformedFail):\n\t\t*theirBalance += htlc.Amount\n\n\t// If an outgoing HTLC is being settled, then this means that the\n\t// downstream party resented the preimage or learned of it via a\n\t// downstream peer. In either case, we credit their settled value with\n\t// the value of the HTLC.\n\tcase !isIncoming && htlc.EntryType == Settle:\n\t\t*theirBalance += htlc.Amount\n\n\t// Otherwise, one of our outgoing HTLC's has timed out, so the value of\n\t// the HTLC should be returned to our settled balance.\n\tcase !isIncoming && (htlc.EntryType == Fail || htlc.EntryType == MalformedFail):\n\t\t*ourBalance += htlc.Amount\n\t}\n\n\tif mutateState {\n\t\t*removeHeight = nextHeight\n\t}\n}\n\n// processFeeUpdate processes a log update that updates the current commitment\n// fee.",
      "length": 1551,
      "tokens": 238,
      "embedding": []
    },
    {
      "slug": "func processFeeUpdate(feeUpdate *PaymentDescriptor, nextHeight uint64,",
      "content": "func processFeeUpdate(feeUpdate *PaymentDescriptor, nextHeight uint64,\n\tremoteChain bool, mutateState bool, view *htlcView) {\n\n\t// Fee updates are applied for all commitments after they are\n\t// sent/received, so we consider them being added and removed at the\n\t// same height.\n\tvar addHeight *uint64\n\tvar removeHeight *uint64\n\tif remoteChain {\n\t\taddHeight = &feeUpdate.addCommitHeightRemote\n\t\tremoveHeight = &feeUpdate.removeCommitHeightRemote\n\t} else {\n\t\taddHeight = &feeUpdate.addCommitHeightLocal\n\t\tremoveHeight = &feeUpdate.removeCommitHeightLocal\n\t}\n\n\tif *addHeight != 0 {\n\t\treturn\n\t}\n\n\t// If the update wasn't already locked in, update the current fee rate\n\t// to reflect this update.\n\tview.feePerKw = chainfee.SatPerKWeight(feeUpdate.Amount.ToSatoshis())\n\n\tif mutateState {\n\t\t*addHeight = nextHeight\n\t\t*removeHeight = nextHeight\n\t}\n}\n\n// generateRemoteHtlcSigJobs generates a series of HTLC signature jobs for the\n// sig pool, along with a channel that if closed, will cancel any jobs after\n// they have been submitted to the sigPool. This method is to be used when\n// generating a new commitment for the remote party. The jobs generated by the\n// signature can be submitted to the sigPool to generate all the signatures\n// asynchronously and in parallel.",
      "length": 1157,
      "tokens": 170,
      "embedding": []
    },
    {
      "slug": "func genRemoteHtlcSigJobs(keyRing *CommitmentKeyRing,",
      "content": "func genRemoteHtlcSigJobs(keyRing *CommitmentKeyRing,\n\tchanType channeldb.ChannelType, isRemoteInitiator bool,\n\tleaseExpiry uint32, localChanCfg, remoteChanCfg *channeldb.ChannelConfig,\n\tremoteCommitView *commitment) ([]SignJob, chan struct{}, error) {\n\n\ttxHash := remoteCommitView.txn.TxHash()\n\tdustLimit := remoteChanCfg.DustLimit\n\tfeePerKw := remoteCommitView.feePerKw\n\tsigHashType := HtlcSigHashType(chanType)\n\n\t// With the keys generated, we'll make a slice with enough capacity to\n\t// hold potentially all the HTLCs. The actual slice may be a bit\n\t// smaller (than its total capacity) and some HTLCs may be dust.\n\tnumSigs := (len(remoteCommitView.incomingHTLCs) +\n\t\tlen(remoteCommitView.outgoingHTLCs))\n\tsigBatch := make([]SignJob, 0, numSigs)\n\n\tvar err error\n\tcancelChan := make(chan struct{})\n\n\t// For each outgoing and incoming HTLC, if the HTLC isn't considered a\n\t// dust output after taking into account second-level HTLC fees, then a\n\t// sigJob will be generated and appended to the current batch.\n\tfor _, htlc := range remoteCommitView.incomingHTLCs {\n\t\tif HtlcIsDust(\n\t\t\tchanType, true, false, feePerKw,\n\t\t\thtlc.Amount.ToSatoshis(), dustLimit,\n\t\t) {\n\n\t\t\tcontinue\n\t\t}\n\n\t\t// If the HTLC isn't dust, then we'll create an empty sign job\n\t\t// to add to the batch momentarily.\n\t\tsigJob := SignJob{}\n\t\tsigJob.Cancel = cancelChan\n\t\tsigJob.Resp = make(chan SignJobResp, 1)\n\n\t\t// As this is an incoming HTLC and we're sinning the commitment\n\t\t// transaction of the remote node, we'll need to generate an\n\t\t// HTLC timeout transaction for them. The output of the timeout\n\t\t// transaction needs to account for fees, so we'll compute the\n\t\t// required fee and output now.\n\t\thtlcFee := HtlcTimeoutFee(chanType, feePerKw)\n\t\toutputAmt := htlc.Amount.ToSatoshis() - htlcFee\n\n\t\t// With the fee calculate, we can properly create the HTLC\n\t\t// timeout transaction using the HTLC amount minus the fee.\n\t\top := wire.OutPoint{\n\t\t\tHash:  txHash,\n\t\t\tIndex: uint32(htlc.remoteOutputIndex),\n\t\t}\n\t\tsigJob.Tx, err = CreateHtlcTimeoutTx(\n\t\t\tchanType, isRemoteInitiator, op, outputAmt,\n\t\t\thtlc.Timeout, uint32(remoteChanCfg.CsvDelay),\n\t\t\tleaseExpiry, keyRing.RevocationKey, keyRing.ToLocalKey,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n\t\t// Finally, we'll generate a sign descriptor to generate a\n\t\t// signature to give to the remote party for this commitment\n\t\t// transaction. Note we use the raw HTLC amount.\n\t\ttxOut := remoteCommitView.txn.TxOut[htlc.remoteOutputIndex]\n\t\tsigJob.SignDesc = input.SignDescriptor{\n\t\t\tKeyDesc:       localChanCfg.HtlcBasePoint,\n\t\t\tSingleTweak:   keyRing.LocalHtlcKeyTweak,\n\t\t\tWitnessScript: htlc.theirWitnessScript,\n\t\t\tOutput:        txOut,\n\t\t\tHashType:      sigHashType,\n\t\t\tSigHashes:     input.NewTxSigHashesV0Only(sigJob.Tx),\n\t\t\tInputIndex:    0,\n\t\t}\n\t\tsigJob.OutputIndex = htlc.remoteOutputIndex\n\n\t\tsigBatch = append(sigBatch, sigJob)\n\t}\n\tfor _, htlc := range remoteCommitView.outgoingHTLCs {\n\t\tif HtlcIsDust(\n\t\t\tchanType, false, false, feePerKw,\n\t\t\thtlc.Amount.ToSatoshis(), dustLimit,\n\t\t) {\n\n\t\t\tcontinue\n\t\t}\n\n\t\tsigJob := SignJob{}\n\t\tsigJob.Cancel = cancelChan\n\t\tsigJob.Resp = make(chan SignJobResp, 1)\n\n\t\t// As this is an outgoing HTLC and we're signing the commitment\n\t\t// transaction of the remote node, we'll need to generate an\n\t\t// HTLC success transaction for them. The output of the timeout\n\t\t// transaction needs to account for fees, so we'll compute the\n\t\t// required fee and output now.\n\t\thtlcFee := HtlcSuccessFee(chanType, feePerKw)\n\t\toutputAmt := htlc.Amount.ToSatoshis() - htlcFee\n\n\t\t// With the proper output amount calculated, we can now\n\t\t// generate the success transaction using the remote party's\n\t\t// CSV delay.\n\t\top := wire.OutPoint{\n\t\t\tHash:  txHash,\n\t\t\tIndex: uint32(htlc.remoteOutputIndex),\n\t\t}\n\t\tsigJob.Tx, err = CreateHtlcSuccessTx(\n\t\t\tchanType, isRemoteInitiator, op, outputAmt,\n\t\t\tuint32(remoteChanCfg.CsvDelay), leaseExpiry,\n\t\t\tkeyRing.RevocationKey, keyRing.ToLocalKey,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n\t\t// Finally, we'll generate a sign descriptor to generate a\n\t\t// signature to give to the remote party for this commitment\n\t\t// transaction. Note we use the raw HTLC amount.\n\t\ttxOut := remoteCommitView.txn.TxOut[htlc.remoteOutputIndex]\n\t\tsigJob.SignDesc = input.SignDescriptor{\n\t\t\tKeyDesc:       localChanCfg.HtlcBasePoint,\n\t\t\tSingleTweak:   keyRing.LocalHtlcKeyTweak,\n\t\t\tWitnessScript: htlc.theirWitnessScript,\n\t\t\tOutput:        txOut,\n\t\t\tHashType:      sigHashType,\n\t\t\tSigHashes:     input.NewTxSigHashesV0Only(sigJob.Tx),\n\t\t\tInputIndex:    0,\n\t\t}\n\t\tsigJob.OutputIndex = htlc.remoteOutputIndex\n\n\t\tsigBatch = append(sigBatch, sigJob)\n\t}\n\n\treturn sigBatch, cancelChan, nil\n}\n\n// createCommitDiff will create a commit diff given a new pending commitment\n// for the remote party and the necessary signatures for the remote party to\n// validate this new state. This function is called right before sending the\n// new commitment to the remote party. The commit diff returned contains all\n// information necessary for retransmission.",
      "length": 4820,
      "tokens": 605,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) createCommitDiff(",
      "content": "func (lc *LightningChannel) createCommitDiff(\n\tnewCommit *commitment, commitSig lnwire.Sig,\n\thtlcSigs []lnwire.Sig) (*channeldb.CommitDiff, error) {\n\n\t// First, we need to convert the funding outpoint into the ID that's\n\t// used on the wire to identify this channel. We'll use this shortly\n\t// when recording the exact CommitSig message that we'll be sending\n\t// out.\n\tchanID := lnwire.NewChanIDFromOutPoint(&lc.channelState.FundingOutpoint)\n\n\tvar (\n\t\tlogUpdates        []channeldb.LogUpdate\n\t\tackAddRefs        []channeldb.AddRef\n\t\tsettleFailRefs    []channeldb.SettleFailRef\n\t\topenCircuitKeys   []models.CircuitKey\n\t\tclosedCircuitKeys []models.CircuitKey\n\t)\n\n\t// We'll now run through our local update log to locate the items which\n\t// were only just committed within this pending state. This will be the\n\t// set of items we need to retransmit if we reconnect and find that\n\t// they didn't process this new state fully.\n\tfor e := lc.localUpdateLog.Front(); e != nil; e = e.Next() {\n\t\tpd := e.Value.(*PaymentDescriptor)\n\n\t\t// If this entry wasn't committed at the exact height of this\n\t\t// remote commitment, then we'll skip it as it was already\n\t\t// lingering in the log.\n\t\tif pd.addCommitHeightRemote != newCommit.height &&\n\t\t\tpd.removeCommitHeightRemote != newCommit.height {\n\n\t\t\tcontinue\n\t\t}\n\n\t\t// Knowing that this update is a part of this new commitment,\n\t\t// we'll create a log update and not its index in the log so\n\t\t// we can later restore it properly if a restart occurs.\n\t\tlogUpdate := channeldb.LogUpdate{\n\t\t\tLogIndex: pd.LogIndex,\n\t\t}\n\n\t\t// We'll map the type of the PaymentDescriptor to one of the\n\t\t// four messages that it corresponds to. With this set of\n\t\t// messages obtained, we can simply read from disk and re-send\n\t\t// them in the case of a needed channel sync.\n\t\tswitch pd.EntryType {\n\t\tcase Add:\n\t\t\thtlc := &lnwire.UpdateAddHTLC{\n\t\t\t\tChanID:      chanID,\n\t\t\t\tID:          pd.HtlcIndex,\n\t\t\t\tAmount:      pd.Amount,\n\t\t\t\tExpiry:      pd.Timeout,\n\t\t\t\tPaymentHash: pd.RHash,\n\t\t\t}\n\t\t\tcopy(htlc.OnionBlob[:], pd.OnionBlob)\n\t\t\tlogUpdate.UpdateMsg = htlc\n\n\t\t\t// Gather any references for circuits opened by this Add\n\t\t\t// HTLC.\n\t\t\tif pd.OpenCircuitKey != nil {\n\t\t\t\topenCircuitKeys = append(openCircuitKeys,\n\t\t\t\t\t*pd.OpenCircuitKey)\n\t\t\t}\n\n\t\t\tlogUpdates = append(logUpdates, logUpdate)\n\n\t\t\t// Short circuit here since an add should not have any\n\t\t\t// of the references gathered in the case of settles,\n\t\t\t// fails or malformed fails.\n\t\t\tcontinue\n\n\t\tcase Settle:\n\t\t\tlogUpdate.UpdateMsg = &lnwire.UpdateFulfillHTLC{\n\t\t\t\tChanID:          chanID,\n\t\t\t\tID:              pd.ParentIndex,\n\t\t\t\tPaymentPreimage: pd.RPreimage,\n\t\t\t}\n\n\t\tcase Fail:\n\t\t\tlogUpdate.UpdateMsg = &lnwire.UpdateFailHTLC{\n\t\t\t\tChanID: chanID,\n\t\t\t\tID:     pd.ParentIndex,\n\t\t\t\tReason: pd.FailReason,\n\t\t\t}\n\n\t\tcase MalformedFail:\n\t\t\tlogUpdate.UpdateMsg = &lnwire.UpdateFailMalformedHTLC{\n\t\t\t\tChanID:       chanID,\n\t\t\t\tID:           pd.ParentIndex,\n\t\t\t\tShaOnionBlob: pd.ShaOnionBlob,\n\t\t\t\tFailureCode:  pd.FailCode,\n\t\t\t}\n\n\t\tcase FeeUpdate:\n\t\t\t// The Amount field holds the feerate denominated in\n\t\t\t// msat. Since feerates are only denominated in sat/kw,\n\t\t\t// we can convert it without loss of precision.\n\t\t\tlogUpdate.UpdateMsg = &lnwire.UpdateFee{\n\t\t\t\tChanID:   chanID,\n\t\t\t\tFeePerKw: uint32(pd.Amount.ToSatoshis()),\n\t\t\t}\n\t\t}\n\n\t\t// Gather the fwd pkg references from any settle or fail\n\t\t// packets, if they exist.\n\t\tif pd.SourceRef != nil {\n\t\t\tackAddRefs = append(ackAddRefs, *pd.SourceRef)\n\t\t}\n\t\tif pd.DestRef != nil {\n\t\t\tsettleFailRefs = append(settleFailRefs, *pd.DestRef)\n\t\t}\n\t\tif pd.ClosedCircuitKey != nil {\n\t\t\tclosedCircuitKeys = append(closedCircuitKeys,\n\t\t\t\t*pd.ClosedCircuitKey)\n\t\t}\n\n\t\tlogUpdates = append(logUpdates, logUpdate)\n\t}\n\n\t// With the set of log updates mapped into wire messages, we'll now\n\t// convert the in-memory commit into a format suitable for writing to\n\t// disk.\n\tdiskCommit := newCommit.toDiskCommit(false)\n\n\treturn &channeldb.CommitDiff{\n\t\tCommitment: *diskCommit,\n\t\tCommitSig: &lnwire.CommitSig{\n\t\t\tChanID: lnwire.NewChanIDFromOutPoint(\n\t\t\t\t&lc.channelState.FundingOutpoint,\n\t\t\t),\n\t\t\tCommitSig: commitSig,\n\t\t\tHtlcSigs:  htlcSigs,\n\t\t},\n\t\tLogUpdates:        logUpdates,\n\t\tOpenedCircuitKeys: openCircuitKeys,\n\t\tClosedCircuitKeys: closedCircuitKeys,\n\t\tAddAcks:           ackAddRefs,\n\t\tSettleFailAcks:    settleFailRefs,\n\t}, nil\n}\n\n// getUnsignedAckedUpdates returns all remote log updates that we haven't\n// signed for yet ourselves.",
      "length": 4247,
      "tokens": 530,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) getUnsignedAckedUpdates() []channeldb.LogUpdate {",
      "content": "func (lc *LightningChannel) getUnsignedAckedUpdates() []channeldb.LogUpdate {\n\t// First, we need to convert the funding outpoint into the ID that's\n\t// used on the wire to identify this channel.\n\tchanID := lnwire.NewChanIDFromOutPoint(&lc.channelState.FundingOutpoint)\n\n\t// Fetch the last remote update that we have signed for.\n\tlastRemoteCommitted := lc.remoteCommitChain.tail().theirMessageIndex\n\n\t// Fetch the last remote update that we have acked.\n\tlastLocalCommitted := lc.localCommitChain.tail().theirMessageIndex\n\n\t// We'll now run through the remote update log to locate the items that\n\t// we haven't signed for yet. This will be the set of items we need to\n\t// restore if we reconnect in order to produce the signature that the\n\t// remote party expects.\n\tvar logUpdates []channeldb.LogUpdate\n\tfor e := lc.remoteUpdateLog.Front(); e != nil; e = e.Next() {\n\t\tpd := e.Value.(*PaymentDescriptor)\n\n\t\t// Skip all remote updates that we have already included in our\n\t\t// commit chain.\n\t\tif pd.LogIndex < lastRemoteCommitted {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Skip all remote updates that we haven't acked yet. At the\n\t\t// moment this function is called, there shouldn't be any, but\n\t\t// we check it anyway to make this function more generally\n\t\t// usable.\n\t\tif pd.LogIndex >= lastLocalCommitted {\n\t\t\tcontinue\n\t\t}\n\n\t\tlogUpdate := channeldb.LogUpdate{\n\t\t\tLogIndex: pd.LogIndex,\n\t\t}\n\n\t\t// We'll map the type of the PaymentDescriptor to one of the\n\t\t// four messages that it corresponds to.\n\t\tswitch pd.EntryType {\n\t\tcase Add:\n\t\t\thtlc := &lnwire.UpdateAddHTLC{\n\t\t\t\tChanID:      chanID,\n\t\t\t\tID:          pd.HtlcIndex,\n\t\t\t\tAmount:      pd.Amount,\n\t\t\t\tExpiry:      pd.Timeout,\n\t\t\t\tPaymentHash: pd.RHash,\n\t\t\t}\n\t\t\tcopy(htlc.OnionBlob[:], pd.OnionBlob)\n\t\t\tlogUpdate.UpdateMsg = htlc\n\n\t\tcase Settle:\n\t\t\tlogUpdate.UpdateMsg = &lnwire.UpdateFulfillHTLC{\n\t\t\t\tChanID:          chanID,\n\t\t\t\tID:              pd.ParentIndex,\n\t\t\t\tPaymentPreimage: pd.RPreimage,\n\t\t\t}\n\n\t\tcase Fail:\n\t\t\tlogUpdate.UpdateMsg = &lnwire.UpdateFailHTLC{\n\t\t\t\tChanID: chanID,\n\t\t\t\tID:     pd.ParentIndex,\n\t\t\t\tReason: pd.FailReason,\n\t\t\t}\n\n\t\tcase MalformedFail:\n\t\t\tlogUpdate.UpdateMsg = &lnwire.UpdateFailMalformedHTLC{\n\t\t\t\tChanID:       chanID,\n\t\t\t\tID:           pd.ParentIndex,\n\t\t\t\tShaOnionBlob: pd.ShaOnionBlob,\n\t\t\t\tFailureCode:  pd.FailCode,\n\t\t\t}\n\n\t\tcase FeeUpdate:\n\t\t\t// The Amount field holds the feerate denominated in\n\t\t\t// msat. Since feerates are only denominated in sat/kw,\n\t\t\t// we can convert it without loss of precision.\n\t\t\tlogUpdate.UpdateMsg = &lnwire.UpdateFee{\n\t\t\t\tChanID:   chanID,\n\t\t\t\tFeePerKw: uint32(pd.Amount.ToSatoshis()),\n\t\t\t}\n\t\t}\n\n\t\tlogUpdates = append(logUpdates, logUpdate)\n\t}\n\treturn logUpdates\n}\n\n// validateCommitmentSanity is used to validate the current state of the\n// commitment transaction in terms of the ChannelConstraints that we and our\n// remote peer agreed upon during the funding workflow. The\n// predict[Our|Their]Add should parameters should be set to a valid\n// PaymentDescriptor if we are validating in the state when adding a new HTLC,\n// or nil otherwise.",
      "length": 2877,
      "tokens": 375,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) validateCommitmentSanity(theirLogCounter,",
      "content": "func (lc *LightningChannel) validateCommitmentSanity(theirLogCounter,\n\tourLogCounter uint64, remoteChain bool,\n\tpredictOurAdd, predictTheirAdd *PaymentDescriptor) error {\n\n\t// Fetch all updates not committed.\n\tview := lc.fetchHTLCView(theirLogCounter, ourLogCounter)\n\n\t// If we are checking if we can add a new HTLC, we add this to the\n\t// appropriate update log, in order to validate the sanity of the\n\t// commitment resulting from _actually adding_ this HTLC to the state.\n\tif predictOurAdd != nil {\n\t\tview.ourUpdates = append(view.ourUpdates, predictOurAdd)\n\t}\n\tif predictTheirAdd != nil {\n\t\tview.theirUpdates = append(view.theirUpdates, predictTheirAdd)\n\t}\n\n\tcommitChain := lc.localCommitChain\n\tif remoteChain {\n\t\tcommitChain = lc.remoteCommitChain\n\t}\n\tourInitialBalance := commitChain.tip().ourBalance\n\ttheirInitialBalance := commitChain.tip().theirBalance\n\n\tourBalance, theirBalance, commitWeight, filteredView, err := lc.computeView(\n\t\tview, remoteChain, false,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\tfeePerKw := filteredView.feePerKw\n\n\t// Calculate the commitment fee, and subtract it from the initiator's\n\t// balance.\n\tcommitFee := feePerKw.FeeForWeight(commitWeight)\n\tcommitFeeMsat := lnwire.NewMSatFromSatoshis(commitFee)\n\tif lc.channelState.IsInitiator {\n\t\tourBalance -= commitFeeMsat\n\t} else {\n\t\ttheirBalance -= commitFeeMsat\n\t}\n\n\t// As a quick sanity check, we'll ensure that if we interpret the\n\t// balances as signed integers, they haven't dipped down below zero. If\n\t// they have, then this indicates that a party doesn't have sufficient\n\t// balance to satisfy the final evaluated HTLC's.\n\tswitch {\n\tcase int64(ourBalance) < 0:\n\t\treturn fmt.Errorf(\"%w: negative local balance\",\n\t\t\tErrBelowChanReserve)\n\n\tcase int64(theirBalance) < 0:\n\t\treturn fmt.Errorf(\"%w: negative remote balance\",\n\t\t\tErrBelowChanReserve)\n\t}\n\n\t// Ensure that the fee being applied is enough to be relayed across the\n\t// network in a reasonable time frame.\n\tif feePerKw < chainfee.FeePerKwFloor {\n\t\treturn fmt.Errorf(\"commitment fee per kw %v below fee floor %v\",\n\t\t\tfeePerKw, chainfee.FeePerKwFloor)\n\t}\n\n\t// If the added HTLCs will decrease the balance, make sure they won't\n\t// dip the local and remote balances below the channel reserves.\n\tourReserve := lnwire.NewMSatFromSatoshis(\n\t\tlc.channelState.LocalChanCfg.ChanReserve,\n\t)\n\ttheirReserve := lnwire.NewMSatFromSatoshis(\n\t\tlc.channelState.RemoteChanCfg.ChanReserve,\n\t)\n\n\tswitch {\n\tcase ourBalance < ourInitialBalance && ourBalance < ourReserve:\n\t\tlc.log.Debugf(\"Funds below chan reserve: ourBalance=%v, \"+\n\t\t\t\"ourReserve=%v\", ourBalance, ourReserve)\n\t\treturn fmt.Errorf(\"%w: our balance below chan reserve\",\n\t\t\tErrBelowChanReserve)\n\n\tcase theirBalance < theirInitialBalance && theirBalance < theirReserve:\n\t\tlc.log.Debugf(\"Funds below chan reserve: theirBalance=%v, \"+\n\t\t\t\"theirReserve=%v\", theirBalance, theirReserve)\n\t\treturn fmt.Errorf(\"%w: their balance below chan reserve\",\n\t\t\tErrBelowChanReserve)\n\t}\n\n\t// validateUpdates take a set of updates, and validates them against\n\t// the passed channel constraints.\n\tvalidateUpdates := func(updates []*PaymentDescriptor,\n\t\tconstraints *channeldb.ChannelConfig) error {\n\n\t\t// We keep track of the number of HTLCs in flight for the\n\t\t// commitment, and the amount in flight.\n\t\tvar numInFlight uint16\n\t\tvar amtInFlight lnwire.MilliSatoshi\n\n\t\t// Go through all updates, checking that they don't violate the\n\t\t// channel constraints.\n\t\tfor _, entry := range updates {\n\t\t\tif entry.EntryType == Add {\n\t\t\t\t// An HTLC is being added, this will add to the\n\t\t\t\t// number and amount in flight.\n\t\t\t\tamtInFlight += entry.Amount\n\t\t\t\tnumInFlight++\n\n\t\t\t\t// Check that the HTLC amount is positive.\n\t\t\t\tif entry.Amount == 0 {\n\t\t\t\t\treturn ErrInvalidHTLCAmt\n\t\t\t\t}\n\n\t\t\t\t// Check that the value of the HTLC they added\n\t\t\t\t// is above our minimum.\n\t\t\t\tif entry.Amount < constraints.MinHTLC {\n\t\t\t\t\treturn ErrBelowMinHTLC\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Now that we know the total value of added HTLCs, we check\n\t\t// that this satisfy the MaxPendingAmont constraint.\n\t\tif amtInFlight > constraints.MaxPendingAmount {\n\t\t\treturn ErrMaxPendingAmount\n\t\t}\n\n\t\t// In this step, we verify that the total number of active\n\t\t// HTLCs does not exceed the constraint of the maximum number\n\t\t// of HTLCs in flight.\n\t\tif numInFlight > constraints.MaxAcceptedHtlcs {\n\t\t\treturn ErrMaxHTLCNumber\n\t\t}\n\n\t\treturn nil\n\t}\n\n\t// First check that the remote updates won't violate it's channel\n\t// constraints.\n\terr = validateUpdates(\n\t\tfilteredView.theirUpdates, &lc.channelState.RemoteChanCfg,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Secondly check that our updates won't violate our channel\n\t// constraints.\n\terr = validateUpdates(\n\t\tfilteredView.ourUpdates, &lc.channelState.LocalChanCfg,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// SignNextCommitment signs a new commitment which includes any previous\n// unsettled HTLCs, any new HTLCs, and any modifications to prior HTLCs\n// committed in previous commitment updates. Signing a new commitment\n// decrements the available revocation window by 1. After a successful method\n// call, the remote party's commitment chain is extended by a new commitment\n// which includes all updates to the HTLC log prior to this method invocation.\n// The first return parameter is the signature for the commitment transaction\n// itself, while the second parameter is a slice of all HTLC signatures (if\n// any). The HTLC signatures are sorted according to the BIP 69 order of the\n// HTLC's on the commitment transaction. Finally, the new set of pending HTLCs\n// for the remote party's commitment are also returned.",
      "length": 5350,
      "tokens": 738,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) SignNextCommitment() (lnwire.Sig, []lnwire.Sig,",
      "content": "func (lc *LightningChannel) SignNextCommitment() (lnwire.Sig, []lnwire.Sig,\n\t[]channeldb.HTLC, error) {\n\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\t// Check for empty commit sig. This should never happen, but we don't\n\t// dare to fail hard here. We assume peers can deal with the empty sig\n\t// and continue channel operation. We log an error so that the bug\n\t// causing this can be tracked down.\n\tif !lc.oweCommitment(true) {\n\t\tlc.log.Errorf(\"sending empty commit sig\")\n\t}\n\n\tvar (\n\t\tsig      lnwire.Sig\n\t\thtlcSigs []lnwire.Sig\n\t)\n\n\t// If we're awaiting for an ACK to a commitment signature, or if we\n\t// don't yet have the initial next revocation point of the remote\n\t// party, then we're unable to create new states. Each time we create a\n\t// new state, we consume a prior revocation point.\n\tcommitPoint := lc.channelState.RemoteNextRevocation\n\tunacked := lc.remoteCommitChain.hasUnackedCommitment()\n\tif unacked || commitPoint == nil {\n\t\tlc.log.Tracef(\"waiting for remote ack=%v, nil \"+\n\t\t\t\"RemoteNextRevocation: %v\", unacked, commitPoint == nil)\n\t\treturn sig, htlcSigs, nil, ErrNoWindow\n\t}\n\n\t// Determine the last update on the remote log that has been locked in.\n\tremoteACKedIndex := lc.localCommitChain.tail().theirMessageIndex\n\tremoteHtlcIndex := lc.localCommitChain.tail().theirHtlcIndex\n\n\t// Before we extend this new commitment to the remote commitment chain,\n\t// ensure that we aren't violating any of the constraints the remote\n\t// party set up when we initially set up the channel. If we are, then\n\t// we'll abort this state transition.\n\terr := lc.validateCommitmentSanity(\n\t\tremoteACKedIndex, lc.localUpdateLog.logIndex, true, nil, nil,\n\t)\n\tif err != nil {\n\t\treturn sig, htlcSigs, nil, err\n\t}\n\n\t// Grab the next commitment point for the remote party. This will be\n\t// used within fetchCommitmentView to derive all the keys necessary to\n\t// construct the commitment state.\n\tkeyRing := DeriveCommitmentKeys(\n\t\tcommitPoint, false, lc.channelState.ChanType,\n\t\t&lc.channelState.LocalChanCfg, &lc.channelState.RemoteChanCfg,\n\t)\n\n\t// Create a new commitment view which will calculate the evaluated\n\t// state of the remote node's new commitment including our latest added\n\t// HTLCs. The view includes the latest balances for both sides on the\n\t// remote node's chain, and also update the addition height of any new\n\t// HTLC log entries. When we creating a new remote view, we include\n\t// _all_ of our changes (pending or committed) but only the remote\n\t// node's changes up to the last change we've ACK'd.\n\tnewCommitView, err := lc.fetchCommitmentView(\n\t\ttrue, lc.localUpdateLog.logIndex, lc.localUpdateLog.htlcCounter,\n\t\tremoteACKedIndex, remoteHtlcIndex, keyRing,\n\t)\n\tif err != nil {\n\t\treturn sig, htlcSigs, nil, err\n\t}\n\n\tlc.log.Tracef(\"extending remote chain to height %v, \"+\n\t\t\"local_log=%v, remote_log=%v\",\n\t\tnewCommitView.height,\n\t\tlc.localUpdateLog.logIndex, remoteACKedIndex)\n\n\tlc.log.Tracef(\"remote chain: our_balance=%v, \"+\n\t\t\"their_balance=%v, commit_tx: %v\",\n\t\tnewCommitView.ourBalance,\n\t\tnewCommitView.theirBalance,\n\t\tnewLogClosure(func() string {\n\t\t\treturn spew.Sdump(newCommitView.txn)\n\t\t}),\n\t)\n\n\t// With the commitment view constructed, if there are any HTLC's, we'll\n\t// need to generate signatures of each of them for the remote party's\n\t// commitment state. We do so in two phases: first we generate and\n\t// submit the set of signature jobs to the worker pool.\n\tvar leaseExpiry uint32\n\tif lc.channelState.ChanType.HasLeaseExpiration() {\n\t\tleaseExpiry = lc.channelState.ThawHeight\n\t}\n\tsigBatch, cancelChan, err := genRemoteHtlcSigJobs(\n\t\tkeyRing, lc.channelState.ChanType, !lc.channelState.IsInitiator,\n\t\tleaseExpiry, &lc.channelState.LocalChanCfg,\n\t\t&lc.channelState.RemoteChanCfg, newCommitView,\n\t)\n\tif err != nil {\n\t\treturn sig, htlcSigs, nil, err\n\t}\n\tlc.sigPool.SubmitSignBatch(sigBatch)\n\n\t// While the jobs are being carried out, we'll Sign their version of\n\t// the new commitment transaction while we're waiting for the rest of\n\t// the HTLC signatures to be processed.\n\tlc.signDesc.SigHashes = input.NewTxSigHashesV0Only(newCommitView.txn)\n\trawSig, err := lc.Signer.SignOutputRaw(newCommitView.txn, lc.signDesc)\n\tif err != nil {\n\t\tclose(cancelChan)\n\t\treturn sig, htlcSigs, nil, err\n\t}\n\tsig, err = lnwire.NewSigFromSignature(rawSig)\n\tif err != nil {\n\t\tclose(cancelChan)\n\t\treturn sig, htlcSigs, nil, err\n\t}\n\n\t// We'll need to send over the signatures to the remote party in the\n\t// order as they appear on the commitment transaction after BIP 69\n\t// sorting.\n\tsort.Slice(sigBatch, func(i, j int) bool {\n\t\treturn sigBatch[i].OutputIndex < sigBatch[j].OutputIndex\n\t})\n\n\t// With the jobs sorted, we'll now iterate through all the responses to\n\t// gather each of the signatures in order.\n\thtlcSigs = make([]lnwire.Sig, 0, len(sigBatch))\n\tfor _, htlcSigJob := range sigBatch {\n\t\tjobResp := <-htlcSigJob.Resp\n\n\t\t// If an error occurred, then we'll cancel any other active\n\t\t// jobs.\n\t\tif jobResp.Err != nil {\n\t\t\tclose(cancelChan)\n\t\t\treturn sig, htlcSigs, nil, jobResp.Err\n\t\t}\n\n\t\thtlcSigs = append(htlcSigs, jobResp.Sig)\n\t}\n\n\t// As we're about to proposer a new commitment state for the remote\n\t// party, we'll write this pending state to disk before we exit, so we\n\t// can retransmit it if necessary.\n\tcommitDiff, err := lc.createCommitDiff(newCommitView, sig, htlcSigs)\n\tif err != nil {\n\t\treturn sig, htlcSigs, nil, err\n\t}\n\terr = lc.channelState.AppendRemoteCommitChain(commitDiff)\n\tif err != nil {\n\t\treturn sig, htlcSigs, nil, err\n\t}\n\n\t// TODO(roasbeef): check that one eclair bug\n\t//  * need to retransmit on first state still?\n\t//  * after initial reconnect\n\n\t// Extend the remote commitment chain by one with the addition of our\n\t// latest commitment update.\n\tlc.remoteCommitChain.addCommitment(newCommitView)\n\n\treturn sig, htlcSigs, commitDiff.Commitment.Htlcs, nil\n}\n\n// ProcessChanSyncMsg processes a ChannelReestablish message sent by the remote\n// connection upon re establishment of our connection with them. This method\n// will return a single message if we are currently out of sync, otherwise a\n// nil lnwire.Message will be returned. If it is decided that our level of\n// de-synchronization is irreconcilable, then an error indicating the issue\n// will be returned. In this case that an error is returned, the channel should\n// be force closed, as we cannot continue updates.\n//\n// One of two message sets will be returned:\n//\n//   - CommitSig+Updates: if we have a pending remote commit which they claim to\n//     have not received\n//   - RevokeAndAck: if we sent a revocation message that they claim to have\n//     not received\n//\n// If we detect a scenario where we need to send a CommitSig+Updates, this\n// method also returns two sets models.CircuitKeys identifying the circuits\n// that were opened and closed, respectively, as a result of signing the\n// previous commitment txn. This allows the link to clear its mailbox of those\n// circuits in case they are still in memory, and ensure the switch's circuit\n// map has been updated by deleting the closed circuits.",
      "length": 6737,
      "tokens": 979,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) ProcessChanSyncMsg(",
      "content": "func (lc *LightningChannel) ProcessChanSyncMsg(\n\tmsg *lnwire.ChannelReestablish) ([]lnwire.Message, []models.CircuitKey,\n\t[]models.CircuitKey, error) {\n\n\t// Now we'll examine the state we have, vs what was contained in the\n\t// chain sync message. If we're de-synchronized, then we'll send a\n\t// batch of messages which when applied will kick start the chain\n\t// resync.\n\tvar (\n\t\tupdates        []lnwire.Message\n\t\topenedCircuits []models.CircuitKey\n\t\tclosedCircuits []models.CircuitKey\n\t)\n\n\t// If the remote party included the optional fields, then we'll verify\n\t// their correctness first, as it will influence our decisions below.\n\thasRecoveryOptions := msg.LocalUnrevokedCommitPoint != nil\n\tif hasRecoveryOptions && msg.RemoteCommitTailHeight != 0 {\n\t\t// We'll check that they've really sent a valid commit\n\t\t// secret from our shachain for our prior height, but only if\n\t\t// this isn't the first state.\n\t\theightSecret, err := lc.channelState.RevocationProducer.AtIndex(\n\t\t\tmsg.RemoteCommitTailHeight - 1,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, nil, nil, err\n\t\t}\n\t\tcommitSecretCorrect := bytes.Equal(\n\t\t\theightSecret[:], msg.LastRemoteCommitSecret[:],\n\t\t)\n\n\t\t// If the commit secret they sent is incorrect then we'll fail\n\t\t// the channel as the remote node has an inconsistent state.\n\t\tif !commitSecretCorrect {\n\t\t\t// In this case, we'll return an error to indicate the\n\t\t\t// remote node sent us the wrong values. This will let\n\t\t\t// the caller act accordingly.\n\t\t\tlc.log.Errorf(\"sync failed: remote provided invalid \" +\n\t\t\t\t\"commit secret!\")\n\t\t\treturn nil, nil, nil, ErrInvalidLastCommitSecret\n\t\t}\n\t}\n\n\t// If we detect that this is is a restored channel, then we can skip a\n\t// portion of the verification, as we already know that we're unable to\n\t// proceed with any updates.\n\tisRestoredChan := lc.channelState.HasChanStatus(\n\t\tchanneldb.ChanStatusRestored,\n\t)\n\n\t// Take note of our current commit chain heights before we begin adding\n\t// more to them.\n\tvar (\n\t\tlocalTailHeight  = lc.localCommitChain.tail().height\n\t\tremoteTailHeight = lc.remoteCommitChain.tail().height\n\t\tremoteTipHeight  = lc.remoteCommitChain.tip().height\n\t)\n\n\t// We'll now check that their view of our local chain is up-to-date.\n\t// This means checking that what their view of our local chain tail\n\t// height is what they believe. Note that the tail and tip height will\n\t// always be the same for the local chain at this stage, as we won't\n\t// store any received commitment to disk before it is ACKed.\n\tswitch {\n\n\t// If their reported height for our local chain tail is ahead of our\n\t// view, then we're behind!\n\tcase msg.RemoteCommitTailHeight > localTailHeight || isRestoredChan:\n\t\tlc.log.Errorf(\"sync failed with local data loss: remote \"+\n\t\t\t\"believes our tail height is %v, while we have %v!\",\n\t\t\tmsg.RemoteCommitTailHeight, localTailHeight)\n\n\t\tif isRestoredChan {\n\t\t\tlc.log.Warnf(\"detected restored triggering DLP\")\n\t\t}\n\n\t\t// We must check that we had recovery options to ensure the\n\t\t// commitment secret matched up, and the remote is just not\n\t\t// lying about its height.\n\t\tif !hasRecoveryOptions {\n\t\t\t// At this point we the remote is either lying about\n\t\t\t// its height, or we are actually behind but the remote\n\t\t\t// doesn't support data loss protection. In either case\n\t\t\t// it is not safe for us to keep using the channel, so\n\t\t\t// we mark it borked and fail the channel.\n\t\t\tlc.log.Errorf(\"sync failed: local data loss, but no \" +\n\t\t\t\t\"recovery option.\")\n\n\t\t\treturn nil, nil, nil, ErrCannotSyncCommitChains\n\t\t}\n\n\t\t// In this case, we've likely lost data and shouldn't proceed\n\t\t// with channel updates.\n\t\treturn nil, nil, nil, &ErrCommitSyncLocalDataLoss{\n\t\t\tChannelPoint: lc.channelState.FundingOutpoint,\n\t\t\tCommitPoint:  msg.LocalUnrevokedCommitPoint,\n\t\t}\n\n\t// If the height of our commitment chain reported by the remote party\n\t// is behind our view of the chain, then they probably lost some state,\n\t// and we'll force close the channel.\n\tcase msg.RemoteCommitTailHeight+1 < localTailHeight:\n\t\tlc.log.Errorf(\"sync failed: remote believes our tail height is \"+\n\t\t\t\"%v, while we have %v!\",\n\t\t\tmsg.RemoteCommitTailHeight, localTailHeight)\n\t\treturn nil, nil, nil, ErrCommitSyncRemoteDataLoss\n\n\t// Their view of our commit chain is consistent with our view.\n\tcase msg.RemoteCommitTailHeight == localTailHeight:\n\t\t// In sync, don't have to do anything.\n\n\t// We owe them a revocation if the tail of our current commitment chain\n\t// is one greater than what they _think_ our commitment tail is. In\n\t// this case we'll re-send the last revocation message that we sent.\n\t// This will be the revocation message for our prior chain tail.\n\tcase msg.RemoteCommitTailHeight+1 == localTailHeight:\n\t\tlc.log.Debugf(\"sync: remote believes our tail height is %v, \"+\n\t\t\t\"while we have %v, we owe them a revocation\",\n\t\t\tmsg.RemoteCommitTailHeight, localTailHeight)\n\n\t\trevocationMsg, err := lc.generateRevocation(\n\t\t\tlocalTailHeight - 1,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, nil, nil, err\n\t\t}\n\t\tupdates = append(updates, revocationMsg)\n\n\t\t// Next, as a precaution, we'll check a special edge case. If\n\t\t// they initiated a state transition, we sent the revocation,\n\t\t// but died before the signature was sent. We re-transmit our\n\t\t// revocation, but also initiate a state transition to re-sync\n\t\t// them.\n\t\tif lc.OweCommitment() {\n\t\t\tcommitSig, htlcSigs, _, err := lc.SignNextCommitment()\n\t\t\tswitch {\n\n\t\t\t// If we signed this state, then we'll accumulate\n\t\t\t// another update to send over.\n\t\t\tcase err == nil:\n\t\t\t\tupdates = append(updates, &lnwire.CommitSig{\n\t\t\t\t\tChanID: lnwire.NewChanIDFromOutPoint(\n\t\t\t\t\t\t&lc.channelState.FundingOutpoint,\n\t\t\t\t\t),\n\t\t\t\t\tCommitSig: commitSig,\n\t\t\t\t\tHtlcSigs:  htlcSigs,\n\t\t\t\t})\n\n\t\t\t// If we get a failure due to not knowing their next\n\t\t\t// point, then this is fine as they'll either send\n\t\t\t// FundingLocked, or revoke their next state to allow\n\t\t\t// us to continue forwards.\n\t\t\tcase err == ErrNoWindow:\n\n\t\t\t// Otherwise, this is an error and we'll treat it as\n\t\t\t// such.\n\t\t\tdefault:\n\t\t\t\treturn nil, nil, nil, err\n\t\t\t}\n\t\t}\n\n\t// There should be no other possible states.\n\tdefault:\n\t\tlc.log.Errorf(\"sync failed: remote believes our tail height is \"+\n\t\t\t\"%v, while we have %v!\",\n\t\t\tmsg.RemoteCommitTailHeight, localTailHeight)\n\t\treturn nil, nil, nil, ErrCannotSyncCommitChains\n\t}\n\n\t// Now check if our view of the remote chain is consistent with what\n\t// they tell us.\n\tswitch {\n\n\t// The remote's view of what their next commit height is 2+ states\n\t// ahead of us, we most likely lost data, or the remote is trying to\n\t// trick us. Since we have no way of verifying whether they are lying\n\t// or not, we will fail the channel, but should not force close it\n\t// automatically.\n\tcase msg.NextLocalCommitHeight > remoteTipHeight+1:\n\t\tlc.log.Errorf(\"sync failed: remote's next commit height is %v, \"+\n\t\t\t\"while we believe it is %v!\",\n\t\t\tmsg.NextLocalCommitHeight, remoteTipHeight)\n\n\t\treturn nil, nil, nil, ErrCannotSyncCommitChains\n\n\t// They are waiting for a state they have already ACKed.\n\tcase msg.NextLocalCommitHeight <= remoteTailHeight:\n\t\tlc.log.Errorf(\"sync failed: remote's next commit height is %v, \"+\n\t\t\t\"while we believe it is %v!\",\n\t\t\tmsg.NextLocalCommitHeight, remoteTipHeight)\n\n\t\t// They previously ACKed our current tail, and now they are\n\t\t// waiting for it. They probably lost state.\n\t\treturn nil, nil, nil, ErrCommitSyncRemoteDataLoss\n\n\t// They have received our latest commitment, life is good.\n\tcase msg.NextLocalCommitHeight == remoteTipHeight+1:\n\n\t// We owe them a commitment if the tip of their chain (from our Pov) is\n\t// equal to what they think their next commit height should be. We'll\n\t// re-send all the updates necessary to recreate this state, along\n\t// with the commit sig.\n\tcase msg.NextLocalCommitHeight == remoteTipHeight:\n\t\tlc.log.Debugf(\"sync: remote's next commit height is %v, while \"+\n\t\t\t\"we believe it is %v, we owe them a commitment\",\n\t\t\tmsg.NextLocalCommitHeight, remoteTipHeight)\n\n\t\t// Grab the current remote chain tip from the database.  This\n\t\t// commit diff contains all the information required to re-sync\n\t\t// our states.\n\t\tcommitDiff, err := lc.channelState.RemoteCommitChainTip()\n\t\tif err != nil {\n\t\t\treturn nil, nil, nil, err\n\t\t}\n\n\t\tvar commitUpdates []lnwire.Message\n\n\t\t// Next, we'll need to send over any updates we sent as part of\n\t\t// this new proposed commitment state.\n\t\tfor _, logUpdate := range commitDiff.LogUpdates {\n\t\t\tcommitUpdates = append(commitUpdates, logUpdate.UpdateMsg)\n\t\t}\n\n\t\t// With the batch of updates accumulated, we'll now re-send the\n\t\t// original CommitSig message required to re-sync their remote\n\t\t// commitment chain with our local version of their chain.\n\t\tcommitUpdates = append(commitUpdates, commitDiff.CommitSig)\n\n\t\t// NOTE: If a revocation is not owed, then updates is empty.\n\t\tif lc.channelState.LastWasRevoke {\n\t\t\t// If lastWasRevoke is set to true, a revocation was last and we\n\t\t\t// need to reorder the updates so that the revocation stored in\n\t\t\t// updates comes after the LogUpdates+CommitSig.\n\t\t\t//\n\t\t\t// ---logupdates--->\n\t\t\t// ---commitsig---->\n\t\t\t// ---revocation--->\n\t\t\tupdates = append(commitUpdates, updates...)\n\t\t} else {\n\t\t\t// Otherwise, the revocation should come before LogUpdates\n\t\t\t// + CommitSig.\n\t\t\t//\n\t\t\t// ---revocation--->\n\t\t\t// ---logupdates--->\n\t\t\t// ---commitsig---->\n\t\t\tupdates = append(updates, commitUpdates...)\n\t\t}\n\n\t\topenedCircuits = commitDiff.OpenedCircuitKeys\n\t\tclosedCircuits = commitDiff.ClosedCircuitKeys\n\n\t// There should be no other possible states as long as the commit chain\n\t// can have at most two elements. If that's the case, something is\n\t// wrong.\n\tdefault:\n\t\tlc.log.Errorf(\"sync failed: remote's next commit height is %v, \"+\n\t\t\t\"while we believe it is %v!\",\n\t\t\tmsg.NextLocalCommitHeight, remoteTipHeight)\n\t\treturn nil, nil, nil, ErrCannotSyncCommitChains\n\t}\n\n\t// If we didn't have recovery options, then the final check cannot be\n\t// performed, and we'll return early.\n\tif !hasRecoveryOptions {\n\t\treturn updates, openedCircuits, closedCircuits, nil\n\t}\n\n\t// At this point we have determined that either the commit heights are\n\t// in sync, or that we are in a state we can recover from. As a final\n\t// check, we ensure that the commitment point sent to us by the remote\n\t// is valid.\n\tvar commitPoint *btcec.PublicKey\n\tswitch {\n\t// If their height is one beyond what we know their current height to\n\t// be, then we need to compare their current unrevoked commitment point\n\t// as that's what they should send.\n\tcase msg.NextLocalCommitHeight == remoteTailHeight+1:\n\t\tcommitPoint = lc.channelState.RemoteCurrentRevocation\n\n\t// Alternatively, if their height is two beyond what we know their best\n\t// height to be, then they're holding onto two commitments, and the\n\t// highest unrevoked point is their next revocation.\n\t//\n\t// TODO(roasbeef): verify this in the spec...\n\tcase msg.NextLocalCommitHeight == remoteTailHeight+2:\n\t\tcommitPoint = lc.channelState.RemoteNextRevocation\n\t}\n\n\t// Only if this is a tweakless channel will we attempt to verify the\n\t// commitment point, as otherwise it has no validity requirements.\n\ttweakless := lc.channelState.ChanType.IsTweakless()\n\tif !tweakless && commitPoint != nil &&\n\t\t!commitPoint.IsEqual(msg.LocalUnrevokedCommitPoint) {\n\n\t\tlc.log.Errorf(\"sync failed: remote sent invalid commit point \"+\n\t\t\t\"for height %v!\",\n\t\t\tmsg.NextLocalCommitHeight)\n\t\treturn nil, nil, nil, ErrInvalidLocalUnrevokedCommitPoint\n\t}\n\n\treturn updates, openedCircuits, closedCircuits, nil\n}\n\n// computeView takes the given htlcView, and calculates the balances, filtered\n// view (settling unsettled HTLCs), commitment weight and feePerKw, after\n// applying the HTLCs to the latest commitment. The returned balances are the\n// balances *before* subtracting the commitment fee from the initiator's\n// balance.\n//\n// If the updateState boolean is set true, the add and remove heights of the\n// HTLCs will be set to the next commitment height.",
      "length": 11528,
      "tokens": 1683,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) computeView(view *htlcView, remoteChain bool,",
      "content": "func (lc *LightningChannel) computeView(view *htlcView, remoteChain bool,\n\tupdateState bool) (lnwire.MilliSatoshi, lnwire.MilliSatoshi, int64,\n\t*htlcView, error) {\n\n\tcommitChain := lc.localCommitChain\n\tdustLimit := lc.channelState.LocalChanCfg.DustLimit\n\tif remoteChain {\n\t\tcommitChain = lc.remoteCommitChain\n\t\tdustLimit = lc.channelState.RemoteChanCfg.DustLimit\n\t}\n\n\t// Since the fetched htlc view will include all updates added after the\n\t// last committed state, we start with the balances reflecting that\n\t// state.\n\tourBalance := commitChain.tip().ourBalance\n\ttheirBalance := commitChain.tip().theirBalance\n\n\t// Add the fee from the previous commitment state back to the\n\t// initiator's balance, so that the fee can be recalculated and\n\t// re-applied in case fee estimation parameters have changed or the\n\t// number of outstanding HTLCs has changed.\n\tif lc.channelState.IsInitiator {\n\t\tourBalance += lnwire.NewMSatFromSatoshis(\n\t\t\tcommitChain.tip().fee)\n\t} else if !lc.channelState.IsInitiator {\n\t\ttheirBalance += lnwire.NewMSatFromSatoshis(\n\t\t\tcommitChain.tip().fee)\n\t}\n\tnextHeight := commitChain.tip().height + 1\n\n\t// Initiate feePerKw to the last committed fee for this chain as we'll\n\t// need this to determine which HTLCs are dust, and also the final fee\n\t// rate.\n\tview.feePerKw = commitChain.tip().feePerKw\n\n\t// We evaluate the view at this stage, meaning settled and failed HTLCs\n\t// will remove their corresponding added HTLCs.  The resulting filtered\n\t// view will only have Add entries left, making it easy to compare the\n\t// channel constraints to the final commitment state. If any fee\n\t// updates are found in the logs, the commitment fee rate should be\n\t// changed, so we'll also set the feePerKw to this new value.\n\tfilteredHTLCView, err := lc.evaluateHTLCView(view, &ourBalance,\n\t\t&theirBalance, nextHeight, remoteChain, updateState)\n\tif err != nil {\n\t\treturn 0, 0, 0, nil, err\n\t}\n\tfeePerKw := filteredHTLCView.feePerKw\n\n\t// Now go through all HTLCs at this stage, to calculate the total\n\t// weight, needed to calculate the transaction fee.\n\tvar totalHtlcWeight int64\n\tfor _, htlc := range filteredHTLCView.ourUpdates {\n\t\tif HtlcIsDust(\n\t\t\tlc.channelState.ChanType, false, !remoteChain,\n\t\t\tfeePerKw, htlc.Amount.ToSatoshis(), dustLimit,\n\t\t) {\n\n\t\t\tcontinue\n\t\t}\n\n\t\ttotalHtlcWeight += input.HTLCWeight\n\t}\n\tfor _, htlc := range filteredHTLCView.theirUpdates {\n\t\tif HtlcIsDust(\n\t\t\tlc.channelState.ChanType, true, !remoteChain,\n\t\t\tfeePerKw, htlc.Amount.ToSatoshis(), dustLimit,\n\t\t) {\n\n\t\t\tcontinue\n\t\t}\n\n\t\ttotalHtlcWeight += input.HTLCWeight\n\t}\n\n\ttotalCommitWeight := CommitWeight(lc.channelState.ChanType) +\n\t\ttotalHtlcWeight\n\treturn ourBalance, theirBalance, totalCommitWeight, filteredHTLCView, nil\n}\n\n// genHtlcSigValidationJobs generates a series of signatures verification jobs\n// meant to verify all the signatures for HTLC's attached to a newly created\n// commitment state. The jobs generated are fully populated, and can be sent\n// directly into the pool of workers.",
      "length": 2834,
      "tokens": 373,
      "embedding": []
    },
    {
      "slug": "func genHtlcSigValidationJobs(localCommitmentView *commitment,",
      "content": "func genHtlcSigValidationJobs(localCommitmentView *commitment,\n\tkeyRing *CommitmentKeyRing, htlcSigs []lnwire.Sig,\n\tchanType channeldb.ChannelType, isLocalInitiator bool, leaseExpiry uint32,\n\tlocalChanCfg, remoteChanCfg *channeldb.ChannelConfig) ([]VerifyJob, error) {\n\n\ttxHash := localCommitmentView.txn.TxHash()\n\tfeePerKw := localCommitmentView.feePerKw\n\tsigHashType := HtlcSigHashType(chanType)\n\n\t// With the required state generated, we'll create a slice with large\n\t// enough capacity to hold verification jobs for all HTLC's in this\n\t// view. In the case that we have some dust outputs, then the actual\n\t// length will be smaller than the total capacity.\n\tnumHtlcs := (len(localCommitmentView.incomingHTLCs) +\n\t\tlen(localCommitmentView.outgoingHTLCs))\n\tverifyJobs := make([]VerifyJob, 0, numHtlcs)\n\n\t// We'll iterate through each output in the commitment transaction,\n\t// populating the sigHash closure function if it's detected to be an\n\t// HLTC output. Given the sighash, and the signing key, we'll be able\n\t// to validate each signature within the worker pool.\n\ti := 0\n\tfor index := range localCommitmentView.txn.TxOut {\n\t\tvar (\n\t\t\thtlcIndex uint64\n\t\t\tsigHash   func() ([]byte, error)\n\t\t\tsig       *ecdsa.Signature\n\t\t\terr       error\n\t\t)\n\n\t\toutputIndex := int32(index)\n\t\tswitch {\n\n\t\t// If this output index is found within the incoming HTLC\n\t\t// index, then this means that we need to generate an HTLC\n\t\t// success transaction in order to validate the signature.\n\t\tcase localCommitmentView.incomingHTLCIndex[outputIndex] != nil:\n\t\t\thtlc := localCommitmentView.incomingHTLCIndex[outputIndex]\n\n\t\t\thtlcIndex = htlc.HtlcIndex\n\n\t\t\tsigHash = func() ([]byte, error) {\n\t\t\t\top := wire.OutPoint{\n\t\t\t\t\tHash:  txHash,\n\t\t\t\t\tIndex: uint32(htlc.localOutputIndex),\n\t\t\t\t}\n\n\t\t\t\thtlcFee := HtlcSuccessFee(chanType, feePerKw)\n\t\t\t\toutputAmt := htlc.Amount.ToSatoshis() - htlcFee\n\n\t\t\t\tsuccessTx, err := CreateHtlcSuccessTx(\n\t\t\t\t\tchanType, isLocalInitiator, op,\n\t\t\t\t\toutputAmt, uint32(localChanCfg.CsvDelay),\n\t\t\t\t\tleaseExpiry, keyRing.RevocationKey,\n\t\t\t\t\tkeyRing.ToLocalKey,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\n\t\t\t\thashCache := input.NewTxSigHashesV0Only(successTx)\n\t\t\t\tsigHash, err := txscript.CalcWitnessSigHash(\n\t\t\t\t\thtlc.ourWitnessScript, hashCache,\n\t\t\t\t\tsigHashType, successTx, 0,\n\t\t\t\t\tint64(htlc.Amount.ToSatoshis()),\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\n\t\t\t\treturn sigHash, nil\n\t\t\t}\n\n\t\t\t// Make sure there are more signatures left.\n\t\t\tif i >= len(htlcSigs) {\n\t\t\t\treturn nil, fmt.Errorf(\"not enough HTLC \" +\n\t\t\t\t\t\"signatures\")\n\t\t\t}\n\n\t\t\t// With the sighash generated, we'll also store the\n\t\t\t// signature so it can be written to disk if this state\n\t\t\t// is valid.\n\t\t\tsig, err = htlcSigs[i].ToSignature()\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\thtlc.sig = sig\n\n\t\t// Otherwise, if this is an outgoing HTLC, then we'll need to\n\t\t// generate a timeout transaction so we can verify the\n\t\t// signature presented.\n\t\tcase localCommitmentView.outgoingHTLCIndex[outputIndex] != nil:\n\t\t\thtlc := localCommitmentView.outgoingHTLCIndex[outputIndex]\n\n\t\t\thtlcIndex = htlc.HtlcIndex\n\n\t\t\tsigHash = func() ([]byte, error) {\n\t\t\t\top := wire.OutPoint{\n\t\t\t\t\tHash:  txHash,\n\t\t\t\t\tIndex: uint32(htlc.localOutputIndex),\n\t\t\t\t}\n\n\t\t\t\thtlcFee := HtlcTimeoutFee(chanType, feePerKw)\n\t\t\t\toutputAmt := htlc.Amount.ToSatoshis() - htlcFee\n\n\t\t\t\ttimeoutTx, err := CreateHtlcTimeoutTx(\n\t\t\t\t\tchanType, isLocalInitiator, op,\n\t\t\t\t\toutputAmt, htlc.Timeout,\n\t\t\t\t\tuint32(localChanCfg.CsvDelay), leaseExpiry,\n\t\t\t\t\tkeyRing.RevocationKey, keyRing.ToLocalKey,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\n\t\t\t\thashCache := input.NewTxSigHashesV0Only(timeoutTx)\n\t\t\t\tsigHash, err := txscript.CalcWitnessSigHash(\n\t\t\t\t\thtlc.ourWitnessScript, hashCache,\n\t\t\t\t\tsigHashType, timeoutTx, 0,\n\t\t\t\t\tint64(htlc.Amount.ToSatoshis()),\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\n\t\t\t\treturn sigHash, nil\n\t\t\t}\n\n\t\t\t// Make sure there are more signatures left.\n\t\t\tif i >= len(htlcSigs) {\n\t\t\t\treturn nil, fmt.Errorf(\"not enough HTLC \" +\n\t\t\t\t\t\"signatures\")\n\t\t\t}\n\n\t\t\t// With the sighash generated, we'll also store the\n\t\t\t// signature so it can be written to disk if this state\n\t\t\t// is valid.\n\t\t\tsig, err = htlcSigs[i].ToSignature()\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\thtlc.sig = sig\n\n\t\tdefault:\n\t\t\tcontinue\n\t\t}\n\n\t\tverifyJobs = append(verifyJobs, VerifyJob{\n\t\t\tHtlcIndex: htlcIndex,\n\t\t\tPubKey:    keyRing.RemoteHtlcKey,\n\t\t\tSig:       sig,\n\t\t\tSigHash:   sigHash,\n\t\t})\n\n\t\ti++\n\t}\n\n\t// If we received a number of HTLC signatures that doesn't match our\n\t// commitment, we'll return an error now.\n\tif len(htlcSigs) != i {\n\t\treturn nil, fmt.Errorf(\"number of htlc sig mismatch. \"+\n\t\t\t\"Expected %v sigs, got %v\", i, len(htlcSigs))\n\t}\n\n\treturn verifyJobs, nil\n}\n\n// InvalidCommitSigError is a struct that implements the error interface to\n// report a failure to validate a commitment signature for a remote peer.\n// We'll use the items in this struct to generate a rich error message for the\n// remote peer when we receive an invalid signature from it. Doing so can\n// greatly aide in debugging cross implementation issues.",
      "length": 4871,
      "tokens": 624,
      "embedding": []
    },
    {
      "slug": "type InvalidCommitSigError struct {",
      "content": "type InvalidCommitSigError struct {\n\tcommitHeight uint64\n\n\tcommitSig []byte\n\n\tsigHash []byte\n\n\tcommitTx []byte\n}\n\n// Error returns a detailed error string including the exact transaction that\n// caused an invalid commitment signature.",
      "length": 188,
      "tokens": 27,
      "embedding": []
    },
    {
      "slug": "func (i *InvalidCommitSigError) Error() string {",
      "content": "func (i *InvalidCommitSigError) Error() string {\n\treturn fmt.Sprintf(\"rejected commitment: commit_height=%v, \"+\n\t\t\"invalid_commit_sig=%x, commit_tx=%x, sig_hash=%x\", i.commitHeight,\n\t\ti.commitSig[:], i.commitTx, i.sigHash[:])\n}\n\n// A compile time flag to ensure that InvalidCommitSigError implements the\n// error interface.\nvar _ error = (*InvalidCommitSigError)(nil)\n\n// InvalidHtlcSigError is a struct that implements the error interface to\n// report a failure to validate an htlc signature from a remote peer. We'll use\n// the items in this struct to generate a rich error message for the remote\n// peer when we receive an invalid signature from it. Doing so can greatly aide\n// in debugging across implementation issues.",
      "length": 662,
      "tokens": 94,
      "embedding": []
    },
    {
      "slug": "type InvalidHtlcSigError struct {",
      "content": "type InvalidHtlcSigError struct {\n\tcommitHeight uint64\n\n\thtlcSig []byte\n\n\thtlcIndex uint64\n\n\tsigHash []byte\n\n\tcommitTx []byte\n}\n\n// Error returns a detailed error string including the exact transaction that\n// caused an invalid htlc signature.",
      "length": 197,
      "tokens": 29,
      "embedding": []
    },
    {
      "slug": "func (i *InvalidHtlcSigError) Error() string {",
      "content": "func (i *InvalidHtlcSigError) Error() string {\n\treturn fmt.Sprintf(\"rejected commitment: commit_height=%v, \"+\n\t\t\"invalid_htlc_sig=%x, commit_tx=%x, sig_hash=%x\", i.commitHeight,\n\t\ti.htlcSig, i.commitTx, i.sigHash[:])\n}\n\n// A compile time flag to ensure that InvalidCommitSigError implements the\n// error interface.\nvar _ error = (*InvalidCommitSigError)(nil)\n\n// ReceiveNewCommitment process a signature for a new commitment state sent by\n// the remote party. This method should be called in response to the\n// remote party initiating a new change, or when the remote party sends a\n// signature fully accepting a new state we've initiated. If we are able to\n// successfully validate the signature, then the generated commitment is added\n// to our local commitment chain. Once we send a revocation for our prior\n// state, then this newly added commitment becomes our current accepted channel\n// state.",
      "length": 837,
      "tokens": 124,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) ReceiveNewCommitment(commitSig lnwire.Sig,",
      "content": "func (lc *LightningChannel) ReceiveNewCommitment(commitSig lnwire.Sig,\n\thtlcSigs []lnwire.Sig) error {\n\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\t// Check for empty commit sig. Because of a previously existing bug, it\n\t// is possible that we receive an empty commit sig from nodes running an\n\t// older version. This is a relaxation of the spec, but it is still\n\t// possible to handle it. To not break any channels with those older\n\t// nodes, we just log the event. This check is also not totally\n\t// reliable, because it could be that we've sent out a new sig, but the\n\t// remote hasn't received it yet. We could then falsely assume that they\n\t// should add our updates to their remote commitment tx.\n\tif !lc.oweCommitment(false) {\n\t\tlc.log.Warnf(\"empty commit sig message received\")\n\t}\n\n\t// Determine the last update on the local log that has been locked in.\n\tlocalACKedIndex := lc.remoteCommitChain.tail().ourMessageIndex\n\tlocalHtlcIndex := lc.remoteCommitChain.tail().ourHtlcIndex\n\n\t// Ensure that this new local update from the remote node respects all\n\t// the constraints we specified during initial channel setup. If not,\n\t// then we'll abort the channel as they've violated our constraints.\n\terr := lc.validateCommitmentSanity(\n\t\tlc.remoteUpdateLog.logIndex, localACKedIndex, false, nil, nil,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// We're receiving a new commitment which attempts to extend our local\n\t// commitment chain height by one, so fetch the proper commitment point\n\t// as this will be needed to derive the keys required to construct the\n\t// commitment.\n\tnextHeight := lc.currentHeight + 1\n\tcommitSecret, err := lc.channelState.RevocationProducer.AtIndex(nextHeight)\n\tif err != nil {\n\t\treturn err\n\t}\n\tcommitPoint := input.ComputeCommitmentPoint(commitSecret[:])\n\tkeyRing := DeriveCommitmentKeys(\n\t\tcommitPoint, true, lc.channelState.ChanType,\n\t\t&lc.channelState.LocalChanCfg, &lc.channelState.RemoteChanCfg,\n\t)\n\n\t// With the current commitment point re-calculated, construct the new\n\t// commitment view which includes all the entries (pending or committed)\n\t// we know of in the remote node's HTLC log, but only our local changes\n\t// up to the last change the remote node has ACK'd.\n\tlocalCommitmentView, err := lc.fetchCommitmentView(\n\t\tfalse, localACKedIndex, localHtlcIndex,\n\t\tlc.remoteUpdateLog.logIndex, lc.remoteUpdateLog.htlcCounter,\n\t\tkeyRing,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tlc.log.Tracef(\"extending local chain to height %v, \"+\n\t\t\"local_log=%v, remote_log=%v\",\n\t\tlocalCommitmentView.height,\n\t\tlocalACKedIndex, lc.remoteUpdateLog.logIndex)\n\n\tlc.log.Tracef(\"local chain: our_balance=%v, \"+\n\t\t\"their_balance=%v, commit_tx: %v\",\n\t\tlocalCommitmentView.ourBalance, localCommitmentView.theirBalance,\n\t\tnewLogClosure(func() string {\n\t\t\treturn spew.Sdump(localCommitmentView.txn)\n\t\t}),\n\t)\n\n\t// Construct the sighash of the commitment transaction corresponding to\n\t// this newly proposed state update.\n\tlocalCommitTx := localCommitmentView.txn\n\tmultiSigScript := lc.signDesc.WitnessScript\n\thashCache := input.NewTxSigHashesV0Only(localCommitTx)\n\tsigHash, err := txscript.CalcWitnessSigHash(\n\t\tmultiSigScript, hashCache, txscript.SigHashAll,\n\t\tlocalCommitTx, 0, int64(lc.channelState.Capacity),\n\t)\n\tif err != nil {\n\t\t// TODO(roasbeef): fetchview has already mutated the HTLCs...\n\t\t//  * need to either roll-back, or make pure\n\t\treturn err\n\t}\n\n\t// As an optimization, we'll generate a series of jobs for the worker\n\t// pool to verify each of the HTLc signatures presented. Once\n\t// generated, we'll submit these jobs to the worker pool.\n\tvar leaseExpiry uint32\n\tif lc.channelState.ChanType.HasLeaseExpiration() {\n\t\tleaseExpiry = lc.channelState.ThawHeight\n\t}\n\tverifyJobs, err := genHtlcSigValidationJobs(\n\t\tlocalCommitmentView, keyRing, htlcSigs,\n\t\tlc.channelState.ChanType, lc.channelState.IsInitiator,\n\t\tleaseExpiry, &lc.channelState.LocalChanCfg,\n\t\t&lc.channelState.RemoteChanCfg,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tcancelChan := make(chan struct{})\n\tverifyResps := lc.sigPool.SubmitVerifyBatch(verifyJobs, cancelChan)\n\n\t// While the HTLC verification jobs are proceeding asynchronously,\n\t// we'll ensure that the newly constructed commitment state has a valid\n\t// signature.\n\tverifyKey := lc.channelState.RemoteChanCfg.MultiSigKey.PubKey\n\n\tcSig, err := commitSig.ToSignature()\n\tif err != nil {\n\t\treturn err\n\t}\n\tif !cSig.Verify(sigHash, verifyKey) {\n\t\tclose(cancelChan)\n\n\t\t// If we fail to validate their commitment signature, we'll\n\t\t// generate a special error to send over the protocol. We'll\n\t\t// include the exact signature and commitment we failed to\n\t\t// verify against in order to aide debugging.\n\t\tvar txBytes bytes.Buffer\n\t\tlocalCommitTx.Serialize(&txBytes)\n\t\treturn &InvalidCommitSigError{\n\t\t\tcommitHeight: nextHeight,\n\t\t\tcommitSig:    commitSig.ToSignatureBytes(),\n\t\t\tsigHash:      sigHash,\n\t\t\tcommitTx:     txBytes.Bytes(),\n\t\t}\n\t}\n\n\t// With the primary commitment transaction validated, we'll check each\n\t// of the HTLC validation jobs.\n\tfor i := 0; i < len(verifyJobs); i++ {\n\t\t// In the case that a single signature is invalid, we'll exit\n\t\t// early and cancel all the outstanding verification jobs.\n\t\thtlcErr := <-verifyResps\n\t\tif htlcErr != nil {\n\t\t\tclose(cancelChan)\n\n\t\t\tsig, err := lnwire.NewSigFromSignature(\n\t\t\t\thtlcErr.Sig,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tsigHash, err := htlcErr.SigHash()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tvar txBytes bytes.Buffer\n\t\t\tlocalCommitTx.Serialize(&txBytes)\n\t\t\treturn &InvalidHtlcSigError{\n\t\t\t\tcommitHeight: nextHeight,\n\t\t\t\thtlcSig:      sig.ToSignatureBytes(),\n\t\t\t\thtlcIndex:    htlcErr.HtlcIndex,\n\t\t\t\tsigHash:      sigHash,\n\t\t\t\tcommitTx:     txBytes.Bytes(),\n\t\t\t}\n\t\t}\n\t}\n\n\t// The signature checks out, so we can now add the new commitment to\n\t// our local commitment chain.\n\tlocalCommitmentView.sig = commitSig.ToSignatureBytes()\n\tlc.localCommitChain.addCommitment(localCommitmentView)\n\n\treturn nil\n}\n\n// IsChannelClean returns true if neither side has pending commitments, neither\n// side has HTLC's, and all updates are locked in irrevocably. Internally, it\n// utilizes the oweCommitment function by calling it for local and remote\n// evaluation. We check if we have a pending commitment for our local state\n// since this function may be called by sub-systems that are not the link (e.g.\n// the rpcserver), and the ReceiveNewCommitment & RevokeCurrentCommitment calls\n// are not atomic, even though link processing ensures no updates can happen in\n// between.",
      "length": 6202,
      "tokens": 804,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) IsChannelClean() bool {",
      "content": "func (lc *LightningChannel) IsChannelClean() bool {\n\tlc.RLock()\n\tdefer lc.RUnlock()\n\n\t// Check whether we have a pending commitment for our local state.\n\tif lc.localCommitChain.hasUnackedCommitment() {\n\t\treturn false\n\t}\n\n\t// Check whether our counterparty has a pending commitment for their\n\t// state.\n\tif lc.remoteCommitChain.hasUnackedCommitment() {\n\t\treturn false\n\t}\n\n\t// We call ActiveHtlcs to ensure there are no HTLCs on either\n\t// commitment.\n\tif len(lc.channelState.ActiveHtlcs()) != 0 {\n\t\treturn false\n\t}\n\n\t// Now check that both local and remote commitments are signing the\n\t// same updates.\n\tif lc.oweCommitment(true) {\n\t\treturn false\n\t}\n\n\tif lc.oweCommitment(false) {\n\t\treturn false\n\t}\n\n\t// If we reached this point, the channel has no HTLCs and both\n\t// commitments sign the same updates.\n\treturn true\n}\n\n// OweCommitment returns a boolean value reflecting whether we need to send\n// out a commitment signature because there are outstanding local updates and/or\n// updates in the local commit tx that aren't reflected in the remote commit tx\n// yet.",
      "length": 972,
      "tokens": 152,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) OweCommitment() bool {",
      "content": "func (lc *LightningChannel) OweCommitment() bool {\n\tlc.RLock()\n\tdefer lc.RUnlock()\n\n\treturn lc.oweCommitment(true)\n}\n\n// oweCommitment is the internal version of OweCommitment. This function expects\n// to be executed with a lock held.",
      "length": 176,
      "tokens": 25,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) oweCommitment(local bool) bool {",
      "content": "func (lc *LightningChannel) oweCommitment(local bool) bool {\n\tvar (\n\t\tremoteUpdatesPending, localUpdatesPending bool\n\n\t\tlastLocalCommit  = lc.localCommitChain.tip()\n\t\tlastRemoteCommit = lc.remoteCommitChain.tip()\n\n\t\tperspective string\n\t)\n\n\tif local {\n\t\tperspective = \"local\"\n\n\t\t// There are local updates pending if our local update log is\n\t\t// not in sync with our remote commitment tx.\n\t\tlocalUpdatesPending = lc.localUpdateLog.logIndex !=\n\t\t\tlastRemoteCommit.ourMessageIndex\n\n\t\t// There are remote updates pending if their remote commitment\n\t\t// tx (our local commitment tx) contains updates that we don't\n\t\t// have added to our remote commitment tx yet.\n\t\tremoteUpdatesPending = lastLocalCommit.theirMessageIndex !=\n\t\t\tlastRemoteCommit.theirMessageIndex\n\t} else {\n\t\tperspective = \"remote\"\n\n\t\t// There are local updates pending (local updates from the\n\t\t// perspective of the remote party) if the remote party has\n\t\t// updates to their remote tx pending for which they haven't\n\t\t// signed yet.\n\t\tlocalUpdatesPending = lc.remoteUpdateLog.logIndex !=\n\t\t\tlastLocalCommit.theirMessageIndex\n\n\t\t// There are remote updates pending (remote updates from the\n\t\t// perspective of the remote party) if we have updates on our\n\t\t// remote commitment tx that they haven't added to theirs yet.\n\t\tremoteUpdatesPending = lastRemoteCommit.ourMessageIndex !=\n\t\t\tlastLocalCommit.ourMessageIndex\n\t}\n\n\t// If any of the conditions above is true, we owe a commitment\n\t// signature.\n\toweCommitment := localUpdatesPending || remoteUpdatesPending\n\n\tlc.log.Tracef(\"%v owes commit: %v (local updates: %v, \"+\n\t\t\"remote updates %v)\", perspective, oweCommitment,\n\t\tlocalUpdatesPending, remoteUpdatesPending)\n\n\treturn oweCommitment\n}\n\n// PendingLocalUpdateCount returns the number of local updates that still need\n// to be applied to the remote commitment tx.",
      "length": 1717,
      "tokens": 224,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) PendingLocalUpdateCount() uint64 {",
      "content": "func (lc *LightningChannel) PendingLocalUpdateCount() uint64 {\n\tlc.RLock()\n\tdefer lc.RUnlock()\n\n\tlastRemoteCommit := lc.remoteCommitChain.tip()\n\n\treturn lc.localUpdateLog.logIndex - lastRemoteCommit.ourMessageIndex\n}\n\n// RevokeCurrentCommitment revokes the next lowest unrevoked commitment\n// transaction in the local commitment chain. As a result the edge of our\n// revocation window is extended by one, and the tail of our local commitment\n// chain is advanced by a single commitment. This now lowest unrevoked\n// commitment becomes our currently accepted state within the channel. This\n// method also returns the set of HTLC's currently active within the commitment\n// transaction and the htlcs the were resolved. This return value allows callers\n// to act once an HTLC has been locked into our commitment transaction.",
      "length": 743,
      "tokens": 109,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) RevokeCurrentCommitment() (*lnwire.RevokeAndAck,",
      "content": "func (lc *LightningChannel) RevokeCurrentCommitment() (*lnwire.RevokeAndAck,\n\t[]channeldb.HTLC, map[uint64]bool, error) {\n\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\trevocationMsg, err := lc.generateRevocation(lc.currentHeight)\n\tif err != nil {\n\t\treturn nil, nil, nil, err\n\t}\n\n\tlc.log.Tracef(\"revoking height=%v, now at height=%v\",\n\t\tlc.localCommitChain.tail().height,\n\t\tlc.currentHeight+1)\n\n\t// Advance our tail, as we've revoked our previous state.\n\tlc.localCommitChain.advanceTail()\n\tlc.currentHeight++\n\n\t// Additionally, generate a channel delta for this state transition for\n\t// persistent storage.\n\tchainTail := lc.localCommitChain.tail()\n\tnewCommitment := chainTail.toDiskCommit(true)\n\n\t// Get the unsigned acked remotes updates that are currently in memory.\n\t// We need them after a restart to sync our remote commitment with what\n\t// is committed locally.\n\tunsignedAckedUpdates := lc.getUnsignedAckedUpdates()\n\n\tfinalHtlcs, err := lc.channelState.UpdateCommitment(\n\t\tnewCommitment, unsignedAckedUpdates,\n\t)\n\tif err != nil {\n\t\treturn nil, nil, nil, err\n\t}\n\n\tlc.log.Tracef(\"state transition accepted: \"+\n\t\t\"our_balance=%v, their_balance=%v, unsigned_acked_updates=%v\",\n\t\tchainTail.ourBalance,\n\t\tchainTail.theirBalance,\n\t\tlen(unsignedAckedUpdates))\n\n\trevocationMsg.ChanID = lnwire.NewChanIDFromOutPoint(\n\t\t&lc.channelState.FundingOutpoint,\n\t)\n\n\treturn revocationMsg, newCommitment.Htlcs, finalHtlcs, nil\n}\n\n// ReceiveRevocation processes a revocation sent by the remote party for the\n// lowest unrevoked commitment within their commitment chain. We receive a\n// revocation either during the initial session negotiation wherein revocation\n// windows are extended, or in response to a state update that we initiate. If\n// successful, then the remote commitment chain is advanced by a single\n// commitment, and a log compaction is attempted.\n//\n// The returned values correspond to:\n//  1. The forwarding package corresponding to the remote commitment height\n//     that was revoked.\n//  2. The PaymentDescriptor of any Add HTLCs that were locked in by this\n//     revocation.\n//  3. The PaymentDescriptor of any Settle/Fail HTLCs that were locked in by\n//     this revocation.\n//  4. The set of HTLCs present on the current valid commitment transaction\n//     for the remote party.",
      "length": 2134,
      "tokens": 273,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) ReceiveRevocation(revMsg *lnwire.RevokeAndAck) (",
      "content": "func (lc *LightningChannel) ReceiveRevocation(revMsg *lnwire.RevokeAndAck) (\n\t*channeldb.FwdPkg, []*PaymentDescriptor, []*PaymentDescriptor,\n\t[]channeldb.HTLC, error) {\n\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\t// Ensure that the new pre-image can be placed in preimage store.\n\tstore := lc.channelState.RevocationStore\n\trevocation, err := chainhash.NewHash(revMsg.Revocation[:])\n\tif err != nil {\n\t\treturn nil, nil, nil, nil, err\n\t}\n\tif err := store.AddNextEntry(revocation); err != nil {\n\t\treturn nil, nil, nil, nil, err\n\t}\n\n\t// Verify that if we use the commitment point computed based off of the\n\t// revealed secret to derive a revocation key with our revocation base\n\t// point, then it matches the current revocation of the remote party.\n\tcurrentCommitPoint := lc.channelState.RemoteCurrentRevocation\n\tderivedCommitPoint := input.ComputeCommitmentPoint(revMsg.Revocation[:])\n\tif !derivedCommitPoint.IsEqual(currentCommitPoint) {\n\t\treturn nil, nil, nil, nil, fmt.Errorf(\"revocation key mismatch\")\n\t}\n\n\t// Now that we've verified that the prior commitment has been properly\n\t// revoked, we'll advance the revocation state we track for the remote\n\t// party: the new current revocation is what was previously the next\n\t// revocation, and the new next revocation is set to the key included\n\t// in the message.\n\tlc.channelState.RemoteCurrentRevocation = lc.channelState.RemoteNextRevocation\n\tlc.channelState.RemoteNextRevocation = revMsg.NextRevocationKey\n\n\tlc.log.Tracef(\"remote party accepted state transition, revoked height \"+\n\t\t\"%v, now at %v\",\n\t\tlc.remoteCommitChain.tail().height,\n\t\tlc.remoteCommitChain.tail().height+1)\n\n\t// Add one to the remote tail since this will be height *after* we write\n\t// the revocation to disk, the local height will remain unchanged.\n\tremoteChainTail := lc.remoteCommitChain.tail().height + 1\n\tlocalChainTail := lc.localCommitChain.tail().height\n\n\tsource := lc.ShortChanID()\n\tchanID := lnwire.NewChanIDFromOutPoint(&lc.channelState.FundingOutpoint)\n\n\t// Determine the set of htlcs that can be forwarded as a result of\n\t// having received the revocation. We will simultaneously construct the\n\t// log updates and payment descriptors, allowing us to persist the log\n\t// updates to disk and optimistically buffer the forwarding package in\n\t// memory.\n\tvar (\n\t\taddsToForward        []*PaymentDescriptor\n\t\taddUpdates           []channeldb.LogUpdate\n\t\tsettleFailsToForward []*PaymentDescriptor\n\t\tsettleFailUpdates    []channeldb.LogUpdate\n\t)\n\n\tvar addIndex, settleFailIndex uint16\n\tfor e := lc.remoteUpdateLog.Front(); e != nil; e = e.Next() {\n\t\tpd := e.Value.(*PaymentDescriptor)\n\n\t\t// Fee updates are local to this particular channel, and should\n\t\t// never be forwarded.\n\t\tif pd.EntryType == FeeUpdate {\n\t\t\tcontinue\n\t\t}\n\n\t\tif pd.isForwarded {\n\t\t\tcontinue\n\t\t}\n\n\t\t// For each type of HTLC, we will only consider forwarding it if\n\t\t// both of the remote and local heights are non-zero. If either\n\t\t// of these values is zero, it has yet to be committed in both\n\t\t// the local and remote chains.\n\t\tcommittedAdd := pd.addCommitHeightRemote > 0 &&\n\t\t\tpd.addCommitHeightLocal > 0\n\t\tcommittedRmv := pd.removeCommitHeightRemote > 0 &&\n\t\t\tpd.removeCommitHeightLocal > 0\n\n\t\t// Using the height of the remote and local commitments,\n\t\t// preemptively compute whether or not to forward this HTLC for\n\t\t// the case in which this in an Add HTLC, or if this is a\n\t\t// Settle, Fail, or MalformedFail.\n\t\tshouldFwdAdd := remoteChainTail == pd.addCommitHeightRemote &&\n\t\t\tlocalChainTail >= pd.addCommitHeightLocal\n\t\tshouldFwdRmv := remoteChainTail == pd.removeCommitHeightRemote &&\n\t\t\tlocalChainTail >= pd.removeCommitHeightLocal\n\n\t\t// We'll only forward any new HTLC additions iff, it's \"freshly\n\t\t// locked in\". Meaning that the HTLC was only *just* considered\n\t\t// locked-in at this new state. By doing this we ensure that we\n\t\t// don't re-forward any already processed HTLC's after a\n\t\t// restart.\n\t\tswitch {\n\t\tcase pd.EntryType == Add && committedAdd && shouldFwdAdd:\n\t\t\t// Construct a reference specifying the location that\n\t\t\t// this forwarded Add will be written in the forwarding\n\t\t\t// package constructed at this remote height.\n\t\t\tpd.SourceRef = &channeldb.AddRef{\n\t\t\t\tHeight: remoteChainTail,\n\t\t\t\tIndex:  addIndex,\n\t\t\t}\n\t\t\taddIndex++\n\n\t\t\tpd.isForwarded = true\n\t\t\taddsToForward = append(addsToForward, pd)\n\n\t\tcase pd.EntryType != Add && committedRmv && shouldFwdRmv:\n\t\t\t// Construct a reference specifying the location that\n\t\t\t// this forwarded Settle/Fail will be written in the\n\t\t\t// forwarding package constructed at this remote height.\n\t\t\tpd.DestRef = &channeldb.SettleFailRef{\n\t\t\t\tSource: source,\n\t\t\t\tHeight: remoteChainTail,\n\t\t\t\tIndex:  settleFailIndex,\n\t\t\t}\n\t\t\tsettleFailIndex++\n\n\t\t\tpd.isForwarded = true\n\t\t\tsettleFailsToForward = append(settleFailsToForward, pd)\n\n\t\tdefault:\n\t\t\tcontinue\n\t\t}\n\n\t\t// If we've reached this point, this HTLC will be added to the\n\t\t// forwarding package at the height of the remote commitment.\n\t\t// All types of HTLCs will record their assigned log index.\n\t\tlogUpdate := channeldb.LogUpdate{\n\t\t\tLogIndex: pd.LogIndex,\n\t\t}\n\n\t\t// Next, we'll map the type of the PaymentDescriptor to one of\n\t\t// the four messages that it corresponds to and separate the\n\t\t// updates into Adds and Settle/Fail/MalformedFail such that\n\t\t// they can be written in the forwarding package. Adds are\n\t\t// aggregated separately from the other types of HTLCs.\n\t\tswitch pd.EntryType {\n\t\tcase Add:\n\t\t\thtlc := &lnwire.UpdateAddHTLC{\n\t\t\t\tChanID:      chanID,\n\t\t\t\tID:          pd.HtlcIndex,\n\t\t\t\tAmount:      pd.Amount,\n\t\t\t\tExpiry:      pd.Timeout,\n\t\t\t\tPaymentHash: pd.RHash,\n\t\t\t}\n\t\t\tcopy(htlc.OnionBlob[:], pd.OnionBlob)\n\t\t\tlogUpdate.UpdateMsg = htlc\n\t\t\taddUpdates = append(addUpdates, logUpdate)\n\n\t\tcase Settle:\n\t\t\tlogUpdate.UpdateMsg = &lnwire.UpdateFulfillHTLC{\n\t\t\t\tChanID:          chanID,\n\t\t\t\tID:              pd.ParentIndex,\n\t\t\t\tPaymentPreimage: pd.RPreimage,\n\t\t\t}\n\t\t\tsettleFailUpdates = append(settleFailUpdates, logUpdate)\n\n\t\tcase Fail:\n\t\t\tlogUpdate.UpdateMsg = &lnwire.UpdateFailHTLC{\n\t\t\t\tChanID: chanID,\n\t\t\t\tID:     pd.ParentIndex,\n\t\t\t\tReason: pd.FailReason,\n\t\t\t}\n\t\t\tsettleFailUpdates = append(settleFailUpdates, logUpdate)\n\n\t\tcase MalformedFail:\n\t\t\tlogUpdate.UpdateMsg = &lnwire.UpdateFailMalformedHTLC{\n\t\t\t\tChanID:       chanID,\n\t\t\t\tID:           pd.ParentIndex,\n\t\t\t\tShaOnionBlob: pd.ShaOnionBlob,\n\t\t\t\tFailureCode:  pd.FailCode,\n\t\t\t}\n\t\t\tsettleFailUpdates = append(settleFailUpdates, logUpdate)\n\t\t}\n\t}\n\n\t// We use the remote commitment chain's tip as it will soon become the tail\n\t// once advanceTail is called.\n\tremoteMessageIndex := lc.remoteCommitChain.tip().ourMessageIndex\n\tlocalMessageIndex := lc.localCommitChain.tail().ourMessageIndex\n\n\tlocalPeerUpdates := lc.unsignedLocalUpdates(\n\t\tremoteMessageIndex, localMessageIndex, chanID,\n\t)\n\n\t// Now that we have gathered the set of HTLCs to forward, separated by\n\t// type, construct a forwarding package using the height that the remote\n\t// commitment chain will be extended after persisting the revocation.\n\tfwdPkg := channeldb.NewFwdPkg(\n\t\tsource, remoteChainTail, addUpdates, settleFailUpdates,\n\t)\n\n\t// We will soon be saving the current remote commitment to revocation\n\t// log bucket, which is `lc.channelState.RemoteCommitment`. After that,\n\t// the `RemoteCommitment` will be replaced with a newer version found\n\t// in `CommitDiff`. Thus we need to compute the output indexes here\n\t// before the change since the indexes are meant for the current,\n\t// revoked remote commitment.\n\tourOutputIndex, theirOutputIndex, err := findOutputIndexesFromRemote(\n\t\trevocation, lc.channelState,\n\t)\n\tif err != nil {\n\t\treturn nil, nil, nil, nil, err\n\t}\n\n\t// At this point, the revocation has been accepted, and we've rotated\n\t// the current revocation key+hash for the remote party. Therefore we\n\t// sync now to ensure the revocation producer state is consistent with\n\t// the current commitment height and also to advance the on-disk\n\t// commitment chain.\n\terr = lc.channelState.AdvanceCommitChainTail(\n\t\tfwdPkg, localPeerUpdates,\n\t\tourOutputIndex, theirOutputIndex,\n\t)\n\tif err != nil {\n\t\treturn nil, nil, nil, nil, err\n\t}\n\n\t// Since they revoked the current lowest height in their commitment\n\t// chain, we can advance their chain by a single commitment.\n\tlc.remoteCommitChain.advanceTail()\n\n\t// As we've just completed a new state transition, attempt to see if we\n\t// can remove any entries from the update log which have been removed\n\t// from the PoV of both commitment chains.\n\tcompactLogs(\n\t\tlc.localUpdateLog, lc.remoteUpdateLog, localChainTail,\n\t\tremoteChainTail,\n\t)\n\n\tremoteHTLCs := lc.channelState.RemoteCommitment.Htlcs\n\n\treturn fwdPkg, addsToForward, settleFailsToForward, remoteHTLCs, nil\n}\n\n// LoadFwdPkgs loads any pending log updates from disk and returns the payment\n// descriptors to be processed by the link.",
      "length": 8473,
      "tokens": 1093,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) LoadFwdPkgs() ([]*channeldb.FwdPkg, error) {",
      "content": "func (lc *LightningChannel) LoadFwdPkgs() ([]*channeldb.FwdPkg, error) {\n\treturn lc.channelState.LoadFwdPkgs()\n}\n\n// AckAddHtlcs sets a bit in the FwdFilter of a forwarding package belonging to\n// this channel, that corresponds to the given AddRef. This method also succeeds\n// if no forwarding package is found.",
      "length": 234,
      "tokens": 37,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) AckAddHtlcs(addRef channeldb.AddRef) error {",
      "content": "func (lc *LightningChannel) AckAddHtlcs(addRef channeldb.AddRef) error {\n\treturn lc.channelState.AckAddHtlcs(addRef)\n}\n\n// AckSettleFails sets a bit in the SettleFailFilter of a forwarding package\n// belonging to this channel, that corresponds to the given SettleFailRef. This\n// method also succeeds if no forwarding package is found.",
      "length": 257,
      "tokens": 37,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) AckSettleFails(",
      "content": "func (lc *LightningChannel) AckSettleFails(\n\tsettleFailRefs ...channeldb.SettleFailRef) error {\n\n\treturn lc.channelState.AckSettleFails(settleFailRefs...)\n}\n\n// SetFwdFilter writes the forwarding decision for a given remote commitment\n// height.",
      "length": 195,
      "tokens": 20,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) SetFwdFilter(height uint64,",
      "content": "func (lc *LightningChannel) SetFwdFilter(height uint64,\n\tfwdFilter *channeldb.PkgFilter) error {\n\n\treturn lc.channelState.SetFwdFilter(height, fwdFilter)\n}\n\n// RemoveFwdPkgs permanently deletes the forwarding package at the given heights.",
      "length": 177,
      "tokens": 19,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) RemoveFwdPkgs(heights ...uint64) error {",
      "content": "func (lc *LightningChannel) RemoveFwdPkgs(heights ...uint64) error {\n\treturn lc.channelState.RemoveFwdPkgs(heights...)\n}\n\n// NextRevocationKey returns the commitment point for the _next_ commitment\n// height. The pubkey returned by this function is required by the remote party\n// along with their revocation base to extend our commitment chain with a\n// new commitment.",
      "length": 295,
      "tokens": 43,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) NextRevocationKey() (*btcec.PublicKey, error) {",
      "content": "func (lc *LightningChannel) NextRevocationKey() (*btcec.PublicKey, error) {\n\tlc.RLock()\n\tdefer lc.RUnlock()\n\n\tnextHeight := lc.currentHeight + 1\n\trevocation, err := lc.channelState.RevocationProducer.AtIndex(nextHeight)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn input.ComputeCommitmentPoint(revocation[:]), nil\n}\n\n// InitNextRevocation inserts the passed commitment point as the _next_\n// revocation to be used when creating a new commitment state for the remote\n// party. This function MUST be called before the channel can accept or propose\n// any new states.",
      "length": 475,
      "tokens": 67,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) InitNextRevocation(revKey *btcec.PublicKey) error {",
      "content": "func (lc *LightningChannel) InitNextRevocation(revKey *btcec.PublicKey) error {\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\treturn lc.channelState.InsertNextRevocation(revKey)\n}\n\n// AddHTLC adds an HTLC to the state machine's local update log. This method\n// should be called when preparing to send an outgoing HTLC.\n//\n// The additional openKey argument corresponds to the incoming CircuitKey of the\n// committed circuit for this HTLC. This value should never be nil.\n//\n// Note that AddHTLC doesn't reserve the HTLC fee for future payment (like\n// AvailableBalance does), so one could get into the \"stuck channel\" state by\n// sending dust HTLCs.\n// TODO(halseth): fix this either by using additional reserve, or better commit\n// format. See https://github.com/lightningnetwork/lightning-rfc/issues/728\n//\n// NOTE: It is okay for sourceRef to be nil when unit testing the wallet.",
      "length": 770,
      "tokens": 119,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) AddHTLC(htlc *lnwire.UpdateAddHTLC,",
      "content": "func (lc *LightningChannel) AddHTLC(htlc *lnwire.UpdateAddHTLC,\n\topenKey *models.CircuitKey) (uint64, error) {\n\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\tpd := lc.htlcAddDescriptor(htlc, openKey)\n\tif err := lc.validateAddHtlc(pd); err != nil {\n\t\treturn 0, err\n\t}\n\n\tlc.localUpdateLog.appendHtlc(pd)\n\n\treturn pd.HtlcIndex, nil\n}\n\n// GetDustSum takes in a boolean that determines which commitment to evaluate\n// the dust sum on. The return value is the sum of dust on the desired\n// commitment tx.\n//\n// NOTE: This over-estimates the dust exposure.",
      "length": 452,
      "tokens": 68,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) GetDustSum(remote bool) lnwire.MilliSatoshi {",
      "content": "func (lc *LightningChannel) GetDustSum(remote bool) lnwire.MilliSatoshi {\n\tlc.RLock()\n\tdefer lc.RUnlock()\n\n\tvar dustSum lnwire.MilliSatoshi\n\n\tdustLimit := lc.channelState.LocalChanCfg.DustLimit\n\tcommit := lc.channelState.LocalCommitment\n\tif remote {\n\t\t// Calculate dust sum on the remote's commitment.\n\t\tdustLimit = lc.channelState.RemoteChanCfg.DustLimit\n\t\tcommit = lc.channelState.RemoteCommitment\n\t}\n\n\tchanType := lc.channelState.ChanType\n\tfeeRate := chainfee.SatPerKWeight(commit.FeePerKw)\n\n\t// Grab all of our HTLCs and evaluate against the dust limit.\n\tfor e := lc.localUpdateLog.Front(); e != nil; e = e.Next() {\n\t\tpd := e.Value.(*PaymentDescriptor)\n\t\tif pd.EntryType != Add {\n\t\t\tcontinue\n\t\t}\n\n\t\tamt := pd.Amount.ToSatoshis()\n\n\t\t// If the satoshi amount is under the dust limit, add the msat\n\t\t// amount to the dust sum.\n\t\tif HtlcIsDust(\n\t\t\tchanType, false, !remote, feeRate, amt, dustLimit,\n\t\t) {\n\n\t\t\tdustSum += pd.Amount\n\t\t}\n\t}\n\n\t// Grab all of their HTLCs and evaluate against the dust limit.\n\tfor e := lc.remoteUpdateLog.Front(); e != nil; e = e.Next() {\n\t\tpd := e.Value.(*PaymentDescriptor)\n\t\tif pd.EntryType != Add {\n\t\t\tcontinue\n\t\t}\n\n\t\tamt := pd.Amount.ToSatoshis()\n\n\t\t// If the satoshi amount is under the dust limit, add the msat\n\t\t// amount to the dust sum.\n\t\tif HtlcIsDust(\n\t\t\tchanType, true, !remote, feeRate, amt, dustLimit,\n\t\t) {\n\n\t\t\tdustSum += pd.Amount\n\t\t}\n\t}\n\n\treturn dustSum\n}\n\n// MayAddOutgoingHtlc validates whether we can add an outgoing htlc to this\n// channel. We don't have a circuit for this htlc, because we just want to test\n// that we have slots for a potential htlc so we use a \"mock\" htlc to validate\n// a potential commitment state with one more outgoing htlc. If a zero htlc\n// amount is provided, we'll attempt to add the smallest possible htlc to the\n// channel (either the minimum htlc, or 1 sat).",
      "length": 1702,
      "tokens": 261,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) MayAddOutgoingHtlc(amt lnwire.MilliSatoshi) error {",
      "content": "func (lc *LightningChannel) MayAddOutgoingHtlc(amt lnwire.MilliSatoshi) error {\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\tvar mockHtlcAmt lnwire.MilliSatoshi\n\tswitch {\n\t// If the caller specifically set an amount, we use it.\n\tcase amt != 0:\n\t\tmockHtlcAmt = amt\n\n\t// In absence of a specific amount, we want to use minimum htlc value\n\t// for the channel. However certain implementations may set this value\n\t// to zero, so we only use this value if it is non-zero.\n\tcase lc.channelState.LocalChanCfg.MinHTLC != 0:\n\t\tmockHtlcAmt = lc.channelState.LocalChanCfg.MinHTLC\n\n\t// As a last resort, we just add a non-zero amount.\n\tdefault:\n\t\tmockHtlcAmt++\n\t}\n\n\t// Create a \"mock\" outgoing htlc, using the smallest amount we can add\n\t// to the commitment so that we validate commitment slots rather than\n\t// available balance, since our actual htlc amount is unknown at this\n\t// stage.\n\tpd := lc.htlcAddDescriptor(\n\t\t&lnwire.UpdateAddHTLC{\n\t\t\tAmount: mockHtlcAmt,\n\t\t},\n\t\t&models.CircuitKey{},\n\t)\n\n\tif err := lc.validateAddHtlc(pd); err != nil {\n\t\tlc.log.Debugf(\"May add outgoing htlc rejected: %v\", err)\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// htlcAddDescriptor returns a payment descriptor for the htlc and open key\n// provided to add to our local update log.",
      "length": 1113,
      "tokens": 175,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) htlcAddDescriptor(htlc *lnwire.UpdateAddHTLC,",
      "content": "func (lc *LightningChannel) htlcAddDescriptor(htlc *lnwire.UpdateAddHTLC,\n\topenKey *models.CircuitKey) *PaymentDescriptor {\n\n\treturn &PaymentDescriptor{\n\t\tEntryType:      Add,\n\t\tRHash:          PaymentHash(htlc.PaymentHash),\n\t\tTimeout:        htlc.Expiry,\n\t\tAmount:         htlc.Amount,\n\t\tLogIndex:       lc.localUpdateLog.logIndex,\n\t\tHtlcIndex:      lc.localUpdateLog.htlcCounter,\n\t\tOnionBlob:      htlc.OnionBlob[:],\n\t\tOpenCircuitKey: openKey,\n\t}\n}\n\n// validateAddHtlc validates the addition of an outgoing htlc to our local and\n// remote commitments.",
      "length": 464,
      "tokens": 40,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) validateAddHtlc(pd *PaymentDescriptor) error {",
      "content": "func (lc *LightningChannel) validateAddHtlc(pd *PaymentDescriptor) error {\n\t// Make sure adding this HTLC won't violate any of the constraints we\n\t// must keep on the commitment transactions.\n\tremoteACKedIndex := lc.localCommitChain.tail().theirMessageIndex\n\n\t// First we'll check whether this HTLC can be added to the remote\n\t// commitment transaction without violation any of the constraints.\n\terr := lc.validateCommitmentSanity(\n\t\tremoteACKedIndex, lc.localUpdateLog.logIndex, true, pd, nil,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// We must also check whether it can be added to our own commitment\n\t// transaction, or the remote node will refuse to sign. This is not\n\t// totally bullet proof, as the remote might be adding updates\n\t// concurrently, but if we fail this check there is for sure not\n\t// possible for us to add the HTLC.\n\terr = lc.validateCommitmentSanity(\n\t\tlc.remoteUpdateLog.logIndex, lc.localUpdateLog.logIndex,\n\t\tfalse, pd, nil,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// ReceiveHTLC adds an HTLC to the state machine's remote update log. This\n// method should be called in response to receiving a new HTLC from the remote\n// party.",
      "length": 1059,
      "tokens": 171,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) ReceiveHTLC(htlc *lnwire.UpdateAddHTLC) (uint64, error) {",
      "content": "func (lc *LightningChannel) ReceiveHTLC(htlc *lnwire.UpdateAddHTLC) (uint64, error) {\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\tif htlc.ID != lc.remoteUpdateLog.htlcCounter {\n\t\treturn 0, fmt.Errorf(\"ID %d on HTLC add does not match expected next \"+\n\t\t\t\"ID %d\", htlc.ID, lc.remoteUpdateLog.htlcCounter)\n\t}\n\n\tpd := &PaymentDescriptor{\n\t\tEntryType: Add,\n\t\tRHash:     PaymentHash(htlc.PaymentHash),\n\t\tTimeout:   htlc.Expiry,\n\t\tAmount:    htlc.Amount,\n\t\tLogIndex:  lc.remoteUpdateLog.logIndex,\n\t\tHtlcIndex: lc.remoteUpdateLog.htlcCounter,\n\t\tOnionBlob: htlc.OnionBlob[:],\n\t}\n\n\tlocalACKedIndex := lc.remoteCommitChain.tail().ourMessageIndex\n\n\t// Clamp down on the number of HTLC's we can receive by checking the\n\t// commitment sanity.\n\terr := lc.validateCommitmentSanity(\n\t\tlc.remoteUpdateLog.logIndex, localACKedIndex, false, nil, pd,\n\t)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tlc.remoteUpdateLog.appendHtlc(pd)\n\n\treturn pd.HtlcIndex, nil\n}\n\n// SettleHTLC attempts to settle an existing outstanding received HTLC. The\n// remote log index of the HTLC settled is returned in order to facilitate\n// creating the corresponding wire message. In the case the supplied preimage\n// is invalid, an error is returned.\n//\n// The additional arguments correspond to:\n//\n//   - sourceRef: specifies the location of the Add HTLC within a forwarding\n//     package that this HTLC is settling. Every Settle fails exactly one Add,\n//     so this should never be empty in practice.\n//\n//   - destRef: specifies the location of the Settle HTLC within another\n//     channel's forwarding package. This value can be nil if the corresponding\n//     Add HTLC was never locked into an outgoing commitment txn, or this\n//     HTLC does not originate as a response from the peer on the outgoing\n//     link, e.g. on-chain resolutions.\n//\n//   - closeKey: identifies the circuit that should be deleted after this Settle\n//     HTLC is included in a commitment txn. This value should only be nil if\n//     the HTLC was settled locally before committing a circuit to the circuit\n//     map.\n//\n// NOTE: It is okay for sourceRef, destRef, and closeKey to be nil when unit\n// testing the wallet.",
      "length": 2002,
      "tokens": 295,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) SettleHTLC(preimage [32]byte,",
      "content": "func (lc *LightningChannel) SettleHTLC(preimage [32]byte,\n\thtlcIndex uint64, sourceRef *channeldb.AddRef,\n\tdestRef *channeldb.SettleFailRef, closeKey *models.CircuitKey) error {\n\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\thtlc := lc.remoteUpdateLog.lookupHtlc(htlcIndex)\n\tif htlc == nil {\n\t\treturn ErrUnknownHtlcIndex{lc.ShortChanID(), htlcIndex}\n\t}\n\n\t// Now that we know the HTLC exists, before checking to see if the\n\t// preimage matches, we'll ensure that we haven't already attempted to\n\t// modify the HTLC.\n\tif lc.remoteUpdateLog.htlcHasModification(htlcIndex) {\n\t\treturn ErrHtlcIndexAlreadySettled(htlcIndex)\n\t}\n\n\tif htlc.RHash != sha256.Sum256(preimage[:]) {\n\t\treturn ErrInvalidSettlePreimage{preimage[:], htlc.RHash[:]}\n\t}\n\n\tpd := &PaymentDescriptor{\n\t\tAmount:           htlc.Amount,\n\t\tRPreimage:        preimage,\n\t\tLogIndex:         lc.localUpdateLog.logIndex,\n\t\tParentIndex:      htlcIndex,\n\t\tEntryType:        Settle,\n\t\tSourceRef:        sourceRef,\n\t\tDestRef:          destRef,\n\t\tClosedCircuitKey: closeKey,\n\t}\n\n\tlc.localUpdateLog.appendUpdate(pd)\n\n\t// With the settle added to our local log, we'll now mark the HTLC as\n\t// modified to prevent ourselves from accidentally attempting a\n\t// duplicate settle.\n\tlc.remoteUpdateLog.markHtlcModified(htlcIndex)\n\n\treturn nil\n}\n\n// ReceiveHTLCSettle attempts to settle an existing outgoing HTLC indexed by an\n// index into the local log. If the specified index doesn't exist within the\n// log, and error is returned. Similarly if the preimage is invalid w.r.t to\n// the referenced of then a distinct error is returned.",
      "length": 1456,
      "tokens": 171,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) ReceiveHTLCSettle(preimage [32]byte, htlcIndex uint64) error {",
      "content": "func (lc *LightningChannel) ReceiveHTLCSettle(preimage [32]byte, htlcIndex uint64) error {\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\thtlc := lc.localUpdateLog.lookupHtlc(htlcIndex)\n\tif htlc == nil {\n\t\treturn ErrUnknownHtlcIndex{lc.ShortChanID(), htlcIndex}\n\t}\n\n\t// Now that we know the HTLC exists, before checking to see if the\n\t// preimage matches, we'll ensure that they haven't already attempted\n\t// to modify the HTLC.\n\tif lc.localUpdateLog.htlcHasModification(htlcIndex) {\n\t\treturn ErrHtlcIndexAlreadySettled(htlcIndex)\n\t}\n\n\tif htlc.RHash != sha256.Sum256(preimage[:]) {\n\t\treturn ErrInvalidSettlePreimage{preimage[:], htlc.RHash[:]}\n\t}\n\n\tpd := &PaymentDescriptor{\n\t\tAmount:      htlc.Amount,\n\t\tRPreimage:   preimage,\n\t\tParentIndex: htlc.HtlcIndex,\n\t\tRHash:       htlc.RHash,\n\t\tLogIndex:    lc.remoteUpdateLog.logIndex,\n\t\tEntryType:   Settle,\n\t}\n\n\tlc.remoteUpdateLog.appendUpdate(pd)\n\n\t// With the settle added to the remote log, we'll now mark the HTLC as\n\t// modified to prevent the remote party from accidentally attempting a\n\t// duplicate settle.\n\tlc.localUpdateLog.markHtlcModified(htlcIndex)\n\n\treturn nil\n}\n\n// FailHTLC attempts to fail a targeted HTLC by its payment hash, inserting an\n// entry which will remove the target log entry within the next commitment\n// update. This method is intended to be called in order to cancel in\n// _incoming_ HTLC.\n//\n// The additional arguments correspond to:\n//\n//   - sourceRef: specifies the location of the Add HTLC within a forwarding\n//     package that this HTLC is failing. Every Fail fails exactly one Add, so\n//     this should never be empty in practice.\n//\n//   - destRef: specifies the location of the Fail HTLC within another channel's\n//     forwarding package. This value can be nil if the corresponding Add HTLC\n//     was never locked into an outgoing commitment txn, or this HTLC does not\n//     originate as a response from the peer on the outgoing link, e.g.\n//     on-chain resolutions.\n//\n//   - closeKey: identifies the circuit that should be deleted after this Fail\n//     HTLC is included in a commitment txn. This value should only be nil if\n//     the HTLC was failed locally before committing a circuit to the circuit\n//     map.\n//\n// NOTE: It is okay for sourceRef, destRef, and closeKey to be nil when unit\n// testing the wallet.",
      "length": 2148,
      "tokens": 317,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) FailHTLC(htlcIndex uint64, reason []byte,",
      "content": "func (lc *LightningChannel) FailHTLC(htlcIndex uint64, reason []byte,\n\tsourceRef *channeldb.AddRef, destRef *channeldb.SettleFailRef,\n\tcloseKey *models.CircuitKey) error {\n\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\thtlc := lc.remoteUpdateLog.lookupHtlc(htlcIndex)\n\tif htlc == nil {\n\t\treturn ErrUnknownHtlcIndex{lc.ShortChanID(), htlcIndex}\n\t}\n\n\t// Now that we know the HTLC exists, we'll ensure that we haven't\n\t// already attempted to fail the HTLC.\n\tif lc.remoteUpdateLog.htlcHasModification(htlcIndex) {\n\t\treturn ErrHtlcIndexAlreadyFailed(htlcIndex)\n\t}\n\n\tpd := &PaymentDescriptor{\n\t\tAmount:           htlc.Amount,\n\t\tRHash:            htlc.RHash,\n\t\tParentIndex:      htlcIndex,\n\t\tLogIndex:         lc.localUpdateLog.logIndex,\n\t\tEntryType:        Fail,\n\t\tFailReason:       reason,\n\t\tSourceRef:        sourceRef,\n\t\tDestRef:          destRef,\n\t\tClosedCircuitKey: closeKey,\n\t}\n\n\tlc.localUpdateLog.appendUpdate(pd)\n\n\t// With the fail added to the remote log, we'll now mark the HTLC as\n\t// modified to prevent ourselves from accidentally attempting a\n\t// duplicate fail.\n\tlc.remoteUpdateLog.markHtlcModified(htlcIndex)\n\n\treturn nil\n}\n\n// MalformedFailHTLC attempts to fail a targeted HTLC by its payment hash,\n// inserting an entry which will remove the target log entry within the next\n// commitment update. This method is intended to be called in order to cancel\n// in _incoming_ HTLC.\n//\n// The additional sourceRef specifies the location of the Add HTLC within a\n// forwarding package that this HTLC is failing. This value should never be\n// empty.\n//\n// NOTE: It is okay for sourceRef to be nil when unit testing the wallet.",
      "length": 1498,
      "tokens": 192,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) MalformedFailHTLC(htlcIndex uint64,",
      "content": "func (lc *LightningChannel) MalformedFailHTLC(htlcIndex uint64,\n\tfailCode lnwire.FailCode, shaOnionBlob [sha256.Size]byte,\n\tsourceRef *channeldb.AddRef) error {\n\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\thtlc := lc.remoteUpdateLog.lookupHtlc(htlcIndex)\n\tif htlc == nil {\n\t\treturn ErrUnknownHtlcIndex{lc.ShortChanID(), htlcIndex}\n\t}\n\n\t// Now that we know the HTLC exists, we'll ensure that we haven't\n\t// already attempted to fail the HTLC.\n\tif lc.remoteUpdateLog.htlcHasModification(htlcIndex) {\n\t\treturn ErrHtlcIndexAlreadyFailed(htlcIndex)\n\t}\n\n\tpd := &PaymentDescriptor{\n\t\tAmount:       htlc.Amount,\n\t\tRHash:        htlc.RHash,\n\t\tParentIndex:  htlcIndex,\n\t\tLogIndex:     lc.localUpdateLog.logIndex,\n\t\tEntryType:    MalformedFail,\n\t\tFailCode:     failCode,\n\t\tShaOnionBlob: shaOnionBlob,\n\t\tSourceRef:    sourceRef,\n\t}\n\n\tlc.localUpdateLog.appendUpdate(pd)\n\n\t// With the fail added to the remote log, we'll now mark the HTLC as\n\t// modified to prevent ourselves from accidentally attempting a\n\t// duplicate fail.\n\tlc.remoteUpdateLog.markHtlcModified(htlcIndex)\n\n\treturn nil\n}\n\n// ReceiveFailHTLC attempts to cancel a targeted HTLC by its log index,\n// inserting an entry which will remove the target log entry within the next\n// commitment update. This method should be called in response to the upstream\n// party cancelling an outgoing HTLC.",
      "length": 1225,
      "tokens": 146,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) ReceiveFailHTLC(htlcIndex uint64, reason []byte,",
      "content": "func (lc *LightningChannel) ReceiveFailHTLC(htlcIndex uint64, reason []byte,\n) error {\n\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\thtlc := lc.localUpdateLog.lookupHtlc(htlcIndex)\n\tif htlc == nil {\n\t\treturn ErrUnknownHtlcIndex{lc.ShortChanID(), htlcIndex}\n\t}\n\n\t// Now that we know the HTLC exists, we'll ensure that they haven't\n\t// already attempted to fail the HTLC.\n\tif lc.localUpdateLog.htlcHasModification(htlcIndex) {\n\t\treturn ErrHtlcIndexAlreadyFailed(htlcIndex)\n\t}\n\n\tpd := &PaymentDescriptor{\n\t\tAmount:      htlc.Amount,\n\t\tRHash:       htlc.RHash,\n\t\tParentIndex: htlc.HtlcIndex,\n\t\tLogIndex:    lc.remoteUpdateLog.logIndex,\n\t\tEntryType:   Fail,\n\t\tFailReason:  reason,\n\t}\n\n\tlc.remoteUpdateLog.appendUpdate(pd)\n\n\t// With the fail added to the remote log, we'll now mark the HTLC as\n\t// modified to prevent ourselves from accidentally attempting a\n\t// duplicate fail.\n\tlc.localUpdateLog.markHtlcModified(htlcIndex)\n\n\treturn nil\n}\n\n// ChannelPoint returns the outpoint of the original funding transaction which\n// created this active channel. This outpoint is used throughout various\n// subsystems to uniquely identify an open channel.",
      "length": 1012,
      "tokens": 122,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) ChannelPoint() *wire.OutPoint {",
      "content": "func (lc *LightningChannel) ChannelPoint() *wire.OutPoint {\n\treturn &lc.channelState.FundingOutpoint\n}\n\n// ShortChanID returns the short channel ID for the channel. The short channel\n// ID encodes the exact location in the main chain that the original\n// funding output can be found.",
      "length": 218,
      "tokens": 35,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) ShortChanID() lnwire.ShortChannelID {",
      "content": "func (lc *LightningChannel) ShortChanID() lnwire.ShortChannelID {\n\treturn lc.channelState.ShortChanID()\n}\n\n// LocalUpfrontShutdownScript returns the local upfront shutdown script for the\n// channel. If it was not set, an empty byte array is returned.",
      "length": 180,
      "tokens": 26,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) LocalUpfrontShutdownScript() lnwire.DeliveryAddress {",
      "content": "func (lc *LightningChannel) LocalUpfrontShutdownScript() lnwire.DeliveryAddress {\n\treturn lc.channelState.LocalShutdownScript\n}\n\n// RemoteUpfrontShutdownScript returns the remote upfront shutdown script for the\n// channel. If it was not set, an empty byte array is returned.",
      "length": 188,
      "tokens": 26,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) RemoteUpfrontShutdownScript() lnwire.DeliveryAddress {",
      "content": "func (lc *LightningChannel) RemoteUpfrontShutdownScript() lnwire.DeliveryAddress {\n\treturn lc.channelState.RemoteShutdownScript\n}\n\n// AbsoluteThawHeight determines a frozen channel's absolute thaw height. If\n// the channel is not frozen, then 0 is returned.\n//\n// An error is returned if the channel is penidng, or is an unconfirmed zero\n// conf channel.",
      "length": 264,
      "tokens": 42,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) AbsoluteThawHeight() (uint32, error) {",
      "content": "func (lc *LightningChannel) AbsoluteThawHeight() (uint32, error) {\n\treturn lc.channelState.AbsoluteThawHeight()\n}\n\n// getSignedCommitTx function take the latest commitment transaction and\n// populate it with witness data.",
      "length": 150,
      "tokens": 18,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) getSignedCommitTx() (*wire.MsgTx, error) {",
      "content": "func (lc *LightningChannel) getSignedCommitTx() (*wire.MsgTx, error) {\n\t// Fetch the current commitment transaction, along with their signature\n\t// for the transaction.\n\tlocalCommit := lc.channelState.LocalCommitment\n\tcommitTx := localCommit.CommitTx.Copy()\n\n\ttheirSig, err := ecdsa.ParseDERSignature(localCommit.CommitSig)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// With this, we then generate the full witness so the caller can\n\t// broadcast a fully signed transaction.\n\tlc.signDesc.SigHashes = input.NewTxSigHashesV0Only(commitTx)\n\tourSig, err := lc.Signer.SignOutputRaw(commitTx, lc.signDesc)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// With the final signature generated, create the witness stack\n\t// required to spend from the multi-sig output.\n\tourKey := lc.channelState.LocalChanCfg.MultiSigKey.PubKey.\n\t\tSerializeCompressed()\n\ttheirKey := lc.channelState.RemoteChanCfg.MultiSigKey.PubKey.\n\t\tSerializeCompressed()\n\n\tcommitTx.TxIn[0].Witness = input.SpendMultiSig(\n\t\tlc.signDesc.WitnessScript, ourKey,\n\t\tourSig, theirKey, theirSig,\n\t)\n\n\treturn commitTx, nil\n}\n\n// CommitOutputResolution carries the necessary information required to allow\n// us to sweep our commitment output in the case that either party goes to\n// chain.",
      "length": 1123,
      "tokens": 134,
      "embedding": []
    },
    {
      "slug": "type CommitOutputResolution struct {",
      "content": "type CommitOutputResolution struct {\n\t// SelfOutPoint is the full outpoint that points to out pay-to-self\n\t// output within the closing commitment transaction.\n\tSelfOutPoint wire.OutPoint\n\n\t// SelfOutputSignDesc is a fully populated sign descriptor capable of\n\t// generating a valid signature to sweep the output paying to us.\n\tSelfOutputSignDesc input.SignDescriptor\n\n\t// MaturityDelay is the relative time-lock, in blocks for all outputs\n\t// that pay to the local party within the broadcast commitment\n\t// transaction.\n\tMaturityDelay uint32\n}\n\n// UnilateralCloseSummary describes the details of a detected unilateral\n// channel closure. This includes the information about with which\n// transactions, and block the channel was unilaterally closed, as well as\n// summarization details concerning the _state_ of the channel at the point of\n// channel closure. Additionally, if we had a commitment output above dust on\n// the remote party's commitment transaction, the necessary a SignDescriptor\n// with the material necessary to seep the output are returned. Finally, if we\n// had any outgoing HTLC's within the commitment transaction, then an\n// OutgoingHtlcResolution for each output will included.",
      "length": 1141,
      "tokens": 170,
      "embedding": []
    },
    {
      "slug": "type UnilateralCloseSummary struct {",
      "content": "type UnilateralCloseSummary struct {\n\t// SpendDetail is a struct that describes how and when the funding\n\t// output was spent.\n\t*chainntnfs.SpendDetail\n\n\t// ChannelCloseSummary is a struct describing the final state of the\n\t// channel and in which state is was closed.\n\tchanneldb.ChannelCloseSummary\n\n\t// CommitResolution contains all the data required to sweep the output\n\t// to ourselves. If this is our commitment transaction, then we'll need\n\t// to wait a time delay before we can sweep the output.\n\t//\n\t// NOTE: If our commitment delivery output is below the dust limit,\n\t// then this will be nil.\n\tCommitResolution *CommitOutputResolution\n\n\t// HtlcResolutions contains a fully populated HtlcResolutions struct\n\t// which contains all the data required to sweep any outgoing HTLC's,\n\t// and also any incoming HTLC's that we know the pre-image to.\n\tHtlcResolutions *HtlcResolutions\n\n\t// RemoteCommit is the exact commitment state that the remote party\n\t// broadcast.\n\tRemoteCommit channeldb.ChannelCommitment\n\n\t// AnchorResolution contains the data required to sweep our anchor\n\t// output. If the channel type doesn't include anchors, the value of\n\t// this field will be nil.\n\tAnchorResolution *AnchorResolution\n}\n\n// NewUnilateralCloseSummary creates a new summary that provides the caller\n// with all the information required to claim all funds on chain in the event\n// that the remote party broadcasts their commitment. The commitPoint argument\n// should be set to the per_commitment_point corresponding to the spending\n// commitment.\n//\n// NOTE: The remoteCommit argument should be set to the stored commitment for\n// this particular state. If we don't have the commitment stored (should only\n// happen in case we have lost state) it should be set to an empty struct, in\n// which case we will attempt to sweep the non-HTLC output using the passed\n// commitPoint.",
      "length": 1791,
      "tokens": 283,
      "embedding": []
    },
    {
      "slug": "func NewUnilateralCloseSummary(chanState *channeldb.OpenChannel, signer input.Signer,",
      "content": "func NewUnilateralCloseSummary(chanState *channeldb.OpenChannel, signer input.Signer,\n\tcommitSpend *chainntnfs.SpendDetail,\n\tremoteCommit channeldb.ChannelCommitment,\n\tcommitPoint *btcec.PublicKey) (*UnilateralCloseSummary, error) {\n\n\t// First, we'll generate the commitment point and the revocation point\n\t// so we can re-construct the HTLC state and also our payment key.\n\tisOurCommit := false\n\tkeyRing := DeriveCommitmentKeys(\n\t\tcommitPoint, isOurCommit, chanState.ChanType,\n\t\t&chanState.LocalChanCfg, &chanState.RemoteChanCfg,\n\t)\n\n\t// Next, we'll obtain HTLC resolutions for all the outgoing HTLC's we\n\t// had on their commitment transaction.\n\tvar leaseExpiry uint32\n\tif chanState.ChanType.HasLeaseExpiration() {\n\t\tleaseExpiry = chanState.ThawHeight\n\t}\n\tisRemoteInitiator := !chanState.IsInitiator\n\thtlcResolutions, err := extractHtlcResolutions(\n\t\tchainfee.SatPerKWeight(remoteCommit.FeePerKw), isOurCommit,\n\t\tsigner, remoteCommit.Htlcs, keyRing, &chanState.LocalChanCfg,\n\t\t&chanState.RemoteChanCfg, commitSpend.SpendingTx,\n\t\tchanState.ChanType, isRemoteInitiator, leaseExpiry,\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to create htlc \"+\n\t\t\t\"resolutions: %v\", err)\n\t}\n\n\tcommitTxBroadcast := commitSpend.SpendingTx\n\n\t// Before we can generate the proper sign descriptor, we'll need to\n\t// locate the output index of our non-delayed output on the commitment\n\t// transaction.\n\tselfScript, maturityDelay, err := CommitScriptToRemote(\n\t\tchanState.ChanType, isRemoteInitiator, keyRing.ToRemoteKey,\n\t\tleaseExpiry,\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to create self commit \"+\n\t\t\t\"script: %v\", err)\n\t}\n\n\tvar (\n\t\tselfPoint    *wire.OutPoint\n\t\tlocalBalance int64\n\t)\n\n\tfor outputIndex, txOut := range commitTxBroadcast.TxOut {\n\t\tif bytes.Equal(txOut.PkScript, selfScript.PkScript) {\n\t\t\tselfPoint = &wire.OutPoint{\n\t\t\t\tHash:  *commitSpend.SpenderTxHash,\n\t\t\t\tIndex: uint32(outputIndex),\n\t\t\t}\n\t\t\tlocalBalance = txOut.Value\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// With the HTLC's taken care of, we'll generate the sign descriptor\n\t// necessary to sweep our commitment output, but only if we had a\n\t// non-trimmed balance.\n\tvar commitResolution *CommitOutputResolution\n\tif selfPoint != nil {\n\t\tlocalPayBase := chanState.LocalChanCfg.PaymentBasePoint\n\t\tcommitResolution = &CommitOutputResolution{\n\t\t\tSelfOutPoint: *selfPoint,\n\t\t\tSelfOutputSignDesc: input.SignDescriptor{\n\t\t\t\tKeyDesc:       localPayBase,\n\t\t\t\tSingleTweak:   keyRing.LocalCommitKeyTweak,\n\t\t\t\tWitnessScript: selfScript.WitnessScript,\n\t\t\t\tOutput: &wire.TxOut{\n\t\t\t\t\tValue:    localBalance,\n\t\t\t\t\tPkScript: selfScript.PkScript,\n\t\t\t\t},\n\t\t\t\tHashType: txscript.SigHashAll,\n\t\t\t},\n\t\t\tMaturityDelay: maturityDelay,\n\t\t}\n\t}\n\n\tcloseSummary := channeldb.ChannelCloseSummary{\n\t\tChanPoint:               chanState.FundingOutpoint,\n\t\tChainHash:               chanState.ChainHash,\n\t\tClosingTXID:             *commitSpend.SpenderTxHash,\n\t\tCloseHeight:             uint32(commitSpend.SpendingHeight),\n\t\tRemotePub:               chanState.IdentityPub,\n\t\tCapacity:                chanState.Capacity,\n\t\tSettledBalance:          btcutil.Amount(localBalance),\n\t\tCloseType:               channeldb.RemoteForceClose,\n\t\tIsPending:               true,\n\t\tRemoteCurrentRevocation: chanState.RemoteCurrentRevocation,\n\t\tRemoteNextRevocation:    chanState.RemoteNextRevocation,\n\t\tShortChanID:             chanState.ShortChanID(),\n\t\tLocalChanConfig:         chanState.LocalChanCfg,\n\t}\n\n\t// Attempt to add a channel sync message to the close summary.\n\tchanSync, err := chanState.ChanSyncMsg()\n\tif err != nil {\n\t\twalletLog.Errorf(\"ChannelPoint(%v): unable to create channel sync \"+\n\t\t\t\"message: %v\", chanState.FundingOutpoint, err)\n\t} else {\n\t\tcloseSummary.LastChanSyncMsg = chanSync\n\t}\n\n\tanchorResolution, err := NewAnchorResolution(\n\t\tchanState, commitTxBroadcast,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &UnilateralCloseSummary{\n\t\tSpendDetail:         commitSpend,\n\t\tChannelCloseSummary: closeSummary,\n\t\tCommitResolution:    commitResolution,\n\t\tHtlcResolutions:     htlcResolutions,\n\t\tRemoteCommit:        remoteCommit,\n\t\tAnchorResolution:    anchorResolution,\n\t}, nil\n}\n\n// IncomingHtlcResolution houses the information required to sweep any incoming\n// HTLC's that we know the preimage to. We'll need to sweep an HTLC manually\n// using this struct if we need to go on-chain for any reason, or if we detect\n// that the remote party broadcasts their commitment transaction.",
      "length": 4195,
      "tokens": 415,
      "embedding": []
    },
    {
      "slug": "type IncomingHtlcResolution struct {",
      "content": "type IncomingHtlcResolution struct {\n\t// Preimage is the preimage that will be used to satisfy the contract of\n\t// the HTLC.\n\t//\n\t// NOTE: This field will only be populated in the incoming contest\n\t// resolver.\n\tPreimage [32]byte\n\n\t// SignedSuccessTx is the fully signed HTLC success transaction. This\n\t// transaction (if non-nil) can be broadcast immediately. After a csv\n\t// delay (included below), then the output created by this transactions\n\t// can be swept on-chain.\n\t//\n\t// NOTE: If this field is nil, then this indicates that we don't need\n\t// to go to the second level to claim this HTLC. Instead, it can be\n\t// claimed directly from the outpoint listed below.\n\tSignedSuccessTx *wire.MsgTx\n\n\t// SignDetails is non-nil if SignedSuccessTx is non-nil, and the\n\t// channel is of the anchor type. As the above HTLC transaction will be\n\t// signed by the channel peer using SINGLE|ANYONECANPAY for such\n\t// channels, we can use the sign details to add the input-output pair\n\t// of the HTLC transaction to another transaction, thereby aggregating\n\t// multiple HTLC transactions together, and adding fees as needed.\n\tSignDetails *input.SignDetails\n\n\t// CsvDelay is the relative time lock (expressed in blocks) that must\n\t// pass after the SignedSuccessTx is confirmed in the chain before the\n\t// output can be swept.\n\t//\n\t// NOTE: If SignedTimeoutTx is nil, then this field denotes the CSV\n\t// delay needed to spend from the commitment transaction.\n\tCsvDelay uint32\n\n\t// ClaimOutpoint is the final outpoint that needs to be spent in order\n\t// to fully sweep the HTLC. The SignDescriptor below should be used to\n\t// spend this outpoint. In the case of a second-level HTLC (non-nil\n\t// SignedTimeoutTx), then we'll be spending a new transaction.\n\t// Otherwise, it'll be an output in the commitment transaction.\n\tClaimOutpoint wire.OutPoint\n\n\t// SweepSignDesc is a sign descriptor that has been populated with the\n\t// necessary items required to spend the sole output of the above\n\t// transaction.\n\tSweepSignDesc input.SignDescriptor\n}\n\n// OutgoingHtlcResolution houses the information necessary to sweep any\n// outgoing HTLC's after their contract has expired. This struct will be needed\n// in one of two cases: the local party force closes the commitment transaction\n// or the remote party unilaterally closes with their version of the commitment\n// transaction.",
      "length": 2274,
      "tokens": 372,
      "embedding": []
    },
    {
      "slug": "type OutgoingHtlcResolution struct {",
      "content": "type OutgoingHtlcResolution struct {\n\t// Expiry the absolute timeout of the HTLC. This value is expressed in\n\t// block height, meaning after this height the HLTC can be swept.\n\tExpiry uint32\n\n\t// SignedTimeoutTx is the fully signed HTLC timeout transaction. This\n\t// must be broadcast immediately after timeout has passed. Once this\n\t// has been confirmed, the HTLC output will transition into the\n\t// delay+claim state.\n\t//\n\t// NOTE: If this field is nil, then this indicates that we don't need\n\t// to go to the second level to claim this HTLC. Instead, it can be\n\t// claimed directly from the outpoint listed below.\n\tSignedTimeoutTx *wire.MsgTx\n\n\t// SignDetails is non-nil if SignedTimeoutTx is non-nil, and the\n\t// channel is of the anchor type. As the above HTLC transaction will be\n\t// signed by the channel peer using SINGLE|ANYONECANPAY for such\n\t// channels, we can use the sign details to add the input-output pair\n\t// of the HTLC transaction to another transaction, thereby aggregating\n\t// multiple HTLC transactions together, and adding fees as needed.\n\tSignDetails *input.SignDetails\n\n\t// CsvDelay is the relative time lock (expressed in blocks) that must\n\t// pass after the SignedTimeoutTx is confirmed in the chain before the\n\t// output can be swept.\n\t//\n\t// NOTE: If SignedTimeoutTx is nil, then this field denotes the CSV\n\t// delay needed to spend from the commitment transaction.\n\tCsvDelay uint32\n\n\t// ClaimOutpoint is the final outpoint that needs to be spent in order\n\t// to fully sweep the HTLC. The SignDescriptor below should be used to\n\t// spend this outpoint. In the case of a second-level HTLC (non-nil\n\t// SignedTimeoutTx), then we'll be spending a new transaction.\n\t// Otherwise, it'll be an output in the commitment transaction.\n\tClaimOutpoint wire.OutPoint\n\n\t// SweepSignDesc is a sign descriptor that has been populated with the\n\t// necessary items required to spend the sole output of the above\n\t// transaction.\n\tSweepSignDesc input.SignDescriptor\n}\n\n// HtlcResolutions contains the items necessary to sweep HTLC's on chain\n// directly from a commitment transaction. We'll use this in case either party\n// goes broadcasts a commitment transaction with live HTLC's.",
      "length": 2113,
      "tokens": 345,
      "embedding": []
    },
    {
      "slug": "type HtlcResolutions struct {",
      "content": "type HtlcResolutions struct {\n\t// IncomingHTLCs contains a set of structs that can be used to sweep\n\t// all the incoming HTL'C that we know the preimage to.\n\tIncomingHTLCs []IncomingHtlcResolution\n\n\t// OutgoingHTLCs contains a set of structs that contains all the info\n\t// needed to sweep an outgoing HTLC we've sent to the remote party\n\t// after an absolute delay has expired.\n\tOutgoingHTLCs []OutgoingHtlcResolution\n}\n\n// newOutgoingHtlcResolution generates a new HTLC resolution capable of\n// allowing the caller to sweep an outgoing HTLC present on either their, or\n// the remote party's commitment transaction.",
      "length": 573,
      "tokens": 90,
      "embedding": []
    },
    {
      "slug": "func newOutgoingHtlcResolution(signer input.Signer,",
      "content": "func newOutgoingHtlcResolution(signer input.Signer,\n\tlocalChanCfg *channeldb.ChannelConfig, commitTx *wire.MsgTx,\n\thtlc *channeldb.HTLC, keyRing *CommitmentKeyRing,\n\tfeePerKw chainfee.SatPerKWeight, csvDelay, leaseExpiry uint32,\n\tlocalCommit, isCommitFromInitiator bool,\n\tchanType channeldb.ChannelType) (*OutgoingHtlcResolution, error) {\n\n\top := wire.OutPoint{\n\t\tHash:  commitTx.TxHash(),\n\t\tIndex: uint32(htlc.OutputIndex),\n\t}\n\n\t// First, we'll re-generate the script used to send the HTLC to\n\t// the remote party within their commitment transaction.\n\thtlcScriptHash, htlcScript, err := genHtlcScript(\n\t\tchanType, false, localCommit, htlc.RefundTimeout, htlc.RHash,\n\t\tkeyRing,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// If we're spending this HTLC output from the remote node's\n\t// commitment, then we won't need to go to the second level as our\n\t// outputs don't have a CSV delay.\n\tif !localCommit {\n\t\t// With the script generated, we can completely populated the\n\t\t// SignDescriptor needed to sweep the output.\n\t\treturn &OutgoingHtlcResolution{\n\t\t\tExpiry:        htlc.RefundTimeout,\n\t\t\tClaimOutpoint: op,\n\t\t\tSweepSignDesc: input.SignDescriptor{\n\t\t\t\tKeyDesc:       localChanCfg.HtlcBasePoint,\n\t\t\t\tSingleTweak:   keyRing.LocalHtlcKeyTweak,\n\t\t\t\tWitnessScript: htlcScript,\n\t\t\t\tOutput: &wire.TxOut{\n\t\t\t\t\tPkScript: htlcScriptHash,\n\t\t\t\t\tValue:    int64(htlc.Amt.ToSatoshis()),\n\t\t\t\t},\n\t\t\t\tHashType: txscript.SigHashAll,\n\t\t\t},\n\t\t\tCsvDelay: HtlcSecondLevelInputSequence(chanType),\n\t\t}, nil\n\t}\n\n\t// Otherwise, we'll need to craft a second level HTLC transaction, as\n\t// well as a sign desc to sweep after the CSV delay.\n\n\t// In order to properly reconstruct the HTLC transaction, we'll need to\n\t// re-calculate the fee required at this state, so we can add the\n\t// correct output value amount to the transaction.\n\thtlcFee := HtlcTimeoutFee(chanType, feePerKw)\n\tsecondLevelOutputAmt := htlc.Amt.ToSatoshis() - htlcFee\n\n\t// With the fee calculated, re-construct the second level timeout\n\t// transaction.\n\ttimeoutTx, err := CreateHtlcTimeoutTx(\n\t\tchanType, isCommitFromInitiator, op, secondLevelOutputAmt,\n\t\thtlc.RefundTimeout, csvDelay, leaseExpiry, keyRing.RevocationKey,\n\t\tkeyRing.ToLocalKey,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// With the transaction created, we can generate a sign descriptor\n\t// that's capable of generating the signature required to spend the\n\t// HTLC output using the timeout transaction.\n\ttxOut := commitTx.TxOut[htlc.OutputIndex]\n\ttimeoutSignDesc := input.SignDescriptor{\n\t\tKeyDesc:       localChanCfg.HtlcBasePoint,\n\t\tSingleTweak:   keyRing.LocalHtlcKeyTweak,\n\t\tWitnessScript: htlcScript,\n\t\tOutput:        txOut,\n\t\tHashType:      txscript.SigHashAll,\n\t\tSigHashes:     input.NewTxSigHashesV0Only(timeoutTx),\n\t\tInputIndex:    0,\n\t}\n\n\thtlcSig, err := ecdsa.ParseDERSignature(htlc.Signature)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// With the sign desc created, we can now construct the full witness\n\t// for the timeout transaction, and populate it as well.\n\tsigHashType := HtlcSigHashType(chanType)\n\ttimeoutWitness, err := input.SenderHtlcSpendTimeout(\n\t\thtlcSig, sigHashType, signer, &timeoutSignDesc, timeoutTx,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ttimeoutTx.TxIn[0].Witness = timeoutWitness\n\n\t// If this is an anchor type channel, the sign details will let us\n\t// re-sign an aggregated tx later.\n\ttxSignDetails := HtlcSignDetails(\n\t\tchanType, timeoutSignDesc, sigHashType, htlcSig,\n\t)\n\n\t// Finally, we'll generate the script output that the timeout\n\t// transaction creates so we can generate the signDesc required to\n\t// complete the claim process after a delay period.\n\thtlcSweepScript, err := SecondLevelHtlcScript(\n\t\tchanType, isCommitFromInitiator, keyRing.RevocationKey,\n\t\tkeyRing.ToLocalKey, csvDelay, leaseExpiry,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlocalDelayTweak := input.SingleTweakBytes(\n\t\tkeyRing.CommitPoint, localChanCfg.DelayBasePoint.PubKey,\n\t)\n\treturn &OutgoingHtlcResolution{\n\t\tExpiry:          htlc.RefundTimeout,\n\t\tSignedTimeoutTx: timeoutTx,\n\t\tSignDetails:     txSignDetails,\n\t\tCsvDelay:        csvDelay,\n\t\tClaimOutpoint: wire.OutPoint{\n\t\t\tHash:  timeoutTx.TxHash(),\n\t\t\tIndex: 0,\n\t\t},\n\t\tSweepSignDesc: input.SignDescriptor{\n\t\t\tKeyDesc:       localChanCfg.DelayBasePoint,\n\t\t\tSingleTweak:   localDelayTweak,\n\t\t\tWitnessScript: htlcSweepScript.WitnessScript,\n\t\t\tOutput: &wire.TxOut{\n\t\t\t\tPkScript: htlcSweepScript.PkScript,\n\t\t\t\tValue:    int64(secondLevelOutputAmt),\n\t\t\t},\n\t\t\tHashType: txscript.SigHashAll,\n\t\t},\n\t}, nil\n}\n\n// newIncomingHtlcResolution creates a new HTLC resolution capable of allowing\n// the caller to sweep an incoming HTLC. If the HTLC is on the caller's\n// commitment transaction, then they'll need to broadcast a second-level\n// transaction before sweeping the output (and incur a CSV delay). Otherwise,\n// they can just sweep the output immediately with knowledge of the pre-image.\n//\n// TODO(roasbeef) consolidate code with above func",
      "length": 4721,
      "tokens": 553,
      "embedding": []
    },
    {
      "slug": "func newIncomingHtlcResolution(signer input.Signer,",
      "content": "func newIncomingHtlcResolution(signer input.Signer,\n\tlocalChanCfg *channeldb.ChannelConfig, commitTx *wire.MsgTx,\n\thtlc *channeldb.HTLC, keyRing *CommitmentKeyRing,\n\tfeePerKw chainfee.SatPerKWeight, csvDelay, leaseExpiry uint32,\n\tlocalCommit, isCommitFromInitiator bool, chanType channeldb.ChannelType) (\n\t*IncomingHtlcResolution, error) {\n\n\top := wire.OutPoint{\n\t\tHash:  commitTx.TxHash(),\n\t\tIndex: uint32(htlc.OutputIndex),\n\t}\n\n\t// First, we'll re-generate the script the remote party used to\n\t// send the HTLC to us in their commitment transaction.\n\thtlcScriptHash, htlcScript, err := genHtlcScript(\n\t\tchanType, true, localCommit, htlc.RefundTimeout, htlc.RHash,\n\t\tkeyRing,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// If we're spending this output from the remote node's commitment,\n\t// then we can skip the second layer and spend the output directly.\n\tif !localCommit {\n\t\t// With the script generated, we can completely populated the\n\t\t// SignDescriptor needed to sweep the output.\n\t\treturn &IncomingHtlcResolution{\n\t\t\tClaimOutpoint: op,\n\t\t\tSweepSignDesc: input.SignDescriptor{\n\t\t\t\tKeyDesc:       localChanCfg.HtlcBasePoint,\n\t\t\t\tSingleTweak:   keyRing.LocalHtlcKeyTweak,\n\t\t\t\tWitnessScript: htlcScript,\n\t\t\t\tOutput: &wire.TxOut{\n\t\t\t\t\tPkScript: htlcScriptHash,\n\t\t\t\t\tValue:    int64(htlc.Amt.ToSatoshis()),\n\t\t\t\t},\n\t\t\t\tHashType: txscript.SigHashAll,\n\t\t\t},\n\t\t\tCsvDelay: HtlcSecondLevelInputSequence(chanType),\n\t\t}, nil\n\t}\n\n\t// Otherwise, we'll need to go to the second level to sweep this HTLC.\n\n\t// First, we'll reconstruct the original HTLC success transaction,\n\t// taking into account the fee rate used.\n\thtlcFee := HtlcSuccessFee(chanType, feePerKw)\n\tsecondLevelOutputAmt := htlc.Amt.ToSatoshis() - htlcFee\n\tsuccessTx, err := CreateHtlcSuccessTx(\n\t\tchanType, isCommitFromInitiator, op, secondLevelOutputAmt,\n\t\tcsvDelay, leaseExpiry, keyRing.RevocationKey, keyRing.ToLocalKey,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Once we've created the second-level transaction, we'll generate the\n\t// SignDesc needed spend the HTLC output using the success transaction.\n\ttxOut := commitTx.TxOut[htlc.OutputIndex]\n\tsuccessSignDesc := input.SignDescriptor{\n\t\tKeyDesc:       localChanCfg.HtlcBasePoint,\n\t\tSingleTweak:   keyRing.LocalHtlcKeyTweak,\n\t\tWitnessScript: htlcScript,\n\t\tOutput:        txOut,\n\t\tHashType:      txscript.SigHashAll,\n\t\tSigHashes:     input.NewTxSigHashesV0Only(successTx),\n\t\tInputIndex:    0,\n\t}\n\n\thtlcSig, err := ecdsa.ParseDERSignature(htlc.Signature)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Next, we'll construct the full witness needed to satisfy the input of\n\t// the success transaction. Don't specify the preimage yet. The preimage\n\t// will be supplied by the contract resolver, either directly or when it\n\t// becomes known.\n\tsigHashType := HtlcSigHashType(chanType)\n\tsuccessWitness, err := input.ReceiverHtlcSpendRedeem(\n\t\thtlcSig, sigHashType, nil, signer, &successSignDesc, successTx,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsuccessTx.TxIn[0].Witness = successWitness\n\n\t// If this is an anchor type channel, the sign details will let us\n\t// re-sign an aggregated tx later.\n\ttxSignDetails := HtlcSignDetails(\n\t\tchanType, successSignDesc, sigHashType, htlcSig,\n\t)\n\n\t// Finally, we'll generate the script that the second-level transaction\n\t// creates so we can generate the proper signDesc to sweep it after the\n\t// CSV delay has passed.\n\thtlcSweepScript, err := SecondLevelHtlcScript(\n\t\tchanType, isCommitFromInitiator, keyRing.RevocationKey,\n\t\tkeyRing.ToLocalKey, csvDelay, leaseExpiry,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlocalDelayTweak := input.SingleTweakBytes(\n\t\tkeyRing.CommitPoint, localChanCfg.DelayBasePoint.PubKey,\n\t)\n\treturn &IncomingHtlcResolution{\n\t\tSignedSuccessTx: successTx,\n\t\tSignDetails:     txSignDetails,\n\t\tCsvDelay:        csvDelay,\n\t\tClaimOutpoint: wire.OutPoint{\n\t\t\tHash:  successTx.TxHash(),\n\t\t\tIndex: 0,\n\t\t},\n\t\tSweepSignDesc: input.SignDescriptor{\n\t\t\tKeyDesc:       localChanCfg.DelayBasePoint,\n\t\t\tSingleTweak:   localDelayTweak,\n\t\t\tWitnessScript: htlcSweepScript.WitnessScript,\n\t\t\tOutput: &wire.TxOut{\n\t\t\t\tPkScript: htlcSweepScript.PkScript,\n\t\t\t\tValue:    int64(secondLevelOutputAmt),\n\t\t\t},\n\t\t\tHashType: txscript.SigHashAll,\n\t\t},\n\t}, nil\n}\n\n// HtlcPoint returns the htlc's outpoint on the commitment tx.",
      "length": 4084,
      "tokens": 455,
      "embedding": []
    },
    {
      "slug": "func (r *IncomingHtlcResolution) HtlcPoint() wire.OutPoint {",
      "content": "func (r *IncomingHtlcResolution) HtlcPoint() wire.OutPoint {\n\t// If we have a success transaction, then the htlc's outpoint\n\t// is the transaction's only input. Otherwise, it's the claim\n\t// point.\n\tif r.SignedSuccessTx != nil {\n\t\treturn r.SignedSuccessTx.TxIn[0].PreviousOutPoint\n\t}\n\n\treturn r.ClaimOutpoint\n}\n\n// HtlcPoint returns the htlc's outpoint on the commitment tx.",
      "length": 303,
      "tokens": 44,
      "embedding": []
    },
    {
      "slug": "func (r *OutgoingHtlcResolution) HtlcPoint() wire.OutPoint {",
      "content": "func (r *OutgoingHtlcResolution) HtlcPoint() wire.OutPoint {\n\t// If we have a timeout transaction, then the htlc's outpoint\n\t// is the transaction's only input. Otherwise, it's the claim\n\t// point.\n\tif r.SignedTimeoutTx != nil {\n\t\treturn r.SignedTimeoutTx.TxIn[0].PreviousOutPoint\n\t}\n\n\treturn r.ClaimOutpoint\n}\n\n// extractHtlcResolutions creates a series of outgoing HTLC resolutions, and\n// the local key used when generating the HTLC scrips. This function is to be\n// used in two cases: force close, or a unilateral close.",
      "length": 451,
      "tokens": 70,
      "embedding": []
    },
    {
      "slug": "func extractHtlcResolutions(feePerKw chainfee.SatPerKWeight, ourCommit bool,",
      "content": "func extractHtlcResolutions(feePerKw chainfee.SatPerKWeight, ourCommit bool,\n\tsigner input.Signer, htlcs []channeldb.HTLC, keyRing *CommitmentKeyRing,\n\tlocalChanCfg, remoteChanCfg *channeldb.ChannelConfig,\n\tcommitTx *wire.MsgTx, chanType channeldb.ChannelType,\n\tisCommitFromInitiator bool, leaseExpiry uint32) (*HtlcResolutions, error) {\n\n\t// TODO(roasbeef): don't need to swap csv delay?\n\tdustLimit := remoteChanCfg.DustLimit\n\tcsvDelay := remoteChanCfg.CsvDelay\n\tif ourCommit {\n\t\tdustLimit = localChanCfg.DustLimit\n\t\tcsvDelay = localChanCfg.CsvDelay\n\t}\n\n\tincomingResolutions := make([]IncomingHtlcResolution, 0, len(htlcs))\n\toutgoingResolutions := make([]OutgoingHtlcResolution, 0, len(htlcs))\n\tfor _, htlc := range htlcs {\n\t\thtlc := htlc\n\n\t\t// We'll skip any HTLC's which were dust on the commitment\n\t\t// transaction, as these don't have a corresponding output\n\t\t// within the commitment transaction.\n\t\tif HtlcIsDust(\n\t\t\tchanType, htlc.Incoming, ourCommit, feePerKw,\n\t\t\thtlc.Amt.ToSatoshis(), dustLimit,\n\t\t) {\n\n\t\t\tcontinue\n\t\t}\n\n\t\t// If the HTLC is incoming, then we'll attempt to see if we\n\t\t// know the pre-image to the HTLC.\n\t\tif htlc.Incoming {\n\t\t\t// Otherwise, we'll create an incoming HTLC resolution\n\t\t\t// as we can satisfy the contract.\n\t\t\tihr, err := newIncomingHtlcResolution(\n\t\t\t\tsigner, localChanCfg, commitTx, &htlc,\n\t\t\t\tkeyRing, feePerKw, uint32(csvDelay), leaseExpiry,\n\t\t\t\tourCommit, isCommitFromInitiator, chanType,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tincomingResolutions = append(incomingResolutions, *ihr)\n\t\t\tcontinue\n\t\t}\n\n\t\tohr, err := newOutgoingHtlcResolution(\n\t\t\tsigner, localChanCfg, commitTx, &htlc, keyRing,\n\t\t\tfeePerKw, uint32(csvDelay), leaseExpiry, ourCommit,\n\t\t\tisCommitFromInitiator, chanType,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\toutgoingResolutions = append(outgoingResolutions, *ohr)\n\t}\n\n\treturn &HtlcResolutions{\n\t\tIncomingHTLCs: incomingResolutions,\n\t\tOutgoingHTLCs: outgoingResolutions,\n\t}, nil\n}\n\n// AnchorResolution holds the information necessary to spend our commitment tx\n// anchor.",
      "length": 1909,
      "tokens": 222,
      "embedding": []
    },
    {
      "slug": "type AnchorResolution struct {",
      "content": "type AnchorResolution struct {\n\t// AnchorSignDescriptor is the sign descriptor for our anchor.\n\tAnchorSignDescriptor input.SignDescriptor\n\n\t// CommitAnchor is the anchor outpoint on the commit tx.\n\tCommitAnchor wire.OutPoint\n\n\t// CommitFee is the fee of the commit tx.\n\tCommitFee btcutil.Amount\n\n\t// CommitWeight is the weight of the commit tx.\n\tCommitWeight int64\n}\n\n// LocalForceCloseSummary describes the final commitment state before the\n// channel is locked-down to initiate a force closure by broadcasting the\n// latest state on-chain. If we intend to broadcast this this state, the\n// channel should not be used after generating this close summary.  The summary\n// includes all the information required to claim all rightfully owned outputs\n// when the commitment gets confirmed.",
      "length": 737,
      "tokens": 111,
      "embedding": []
    },
    {
      "slug": "type LocalForceCloseSummary struct {",
      "content": "type LocalForceCloseSummary struct {\n\t// ChanPoint is the outpoint that created the channel which has been\n\t// force closed.\n\tChanPoint wire.OutPoint\n\n\t// CloseTx is the transaction which can be used to close the channel\n\t// on-chain. When we initiate a force close, this will be our latest\n\t// commitment state.\n\tCloseTx *wire.MsgTx\n\n\t// CommitResolution contains all the data required to sweep the output\n\t// to ourselves. Since this is our commitment transaction, we'll need\n\t// to wait a time delay before we can sweep the output.\n\t//\n\t// NOTE: If our commitment delivery output is below the dust limit,\n\t// then this will be nil.\n\tCommitResolution *CommitOutputResolution\n\n\t// HtlcResolutions contains all the data required to sweep any outgoing\n\t// HTLC's and incoming HTLc's we know the preimage to. For each of these\n\t// HTLC's, we'll need to go to the second level to sweep them fully.\n\tHtlcResolutions *HtlcResolutions\n\n\t// ChanSnapshot is a snapshot of the final state of the channel at the\n\t// time the summary was created.\n\tChanSnapshot channeldb.ChannelSnapshot\n\n\t// AnchorResolution contains the data required to sweep the anchor\n\t// output. If the channel type doesn't include anchors, the value of\n\t// this field will be nil.\n\tAnchorResolution *AnchorResolution\n}\n\n// ForceClose executes a unilateral closure of the transaction at the current\n// lowest commitment height of the channel. Following a force closure, all\n// state transitions, or modifications to the state update logs will be\n// rejected. Additionally, this function also returns a LocalForceCloseSummary\n// which includes the necessary details required to sweep all the time-locked\n// outputs within the commitment transaction.\n//\n// TODO(roasbeef): all methods need to abort if in dispute state\n// TODO(roasbeef): method to generate CloseSummaries for when the remote peer\n// does a unilateral close",
      "length": 1804,
      "tokens": 288,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) ForceClose() (*LocalForceCloseSummary, error) {",
      "content": "func (lc *LightningChannel) ForceClose() (*LocalForceCloseSummary, error) {\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\t// If we've detected local data loss for this channel, then we won't\n\t// allow a force close, as it may be the case that we have a dated\n\t// version of the commitment, or this is actually a channel shell.\n\tif lc.channelState.HasChanStatus(channeldb.ChanStatusLocalDataLoss) {\n\t\treturn nil, fmt.Errorf(\"cannot force close channel with \"+\n\t\t\t\"state: %v\", lc.channelState.ChanStatus())\n\t}\n\n\tcommitTx, err := lc.getSignedCommitTx()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlocalCommitment := lc.channelState.LocalCommitment\n\tsummary, err := NewLocalForceCloseSummary(\n\t\tlc.channelState, lc.Signer, commitTx,\n\t\tlocalCommitment.CommitHeight,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Set the channel state to indicate that the channel is now in a\n\t// contested state.\n\tlc.status = channelDispute\n\n\treturn summary, nil\n}\n\n// NewLocalForceCloseSummary generates a LocalForceCloseSummary from the given\n// channel state.  The passed commitTx must be a fully signed commitment\n// transaction corresponding to localCommit.",
      "length": 1012,
      "tokens": 142,
      "embedding": []
    },
    {
      "slug": "func NewLocalForceCloseSummary(chanState *channeldb.OpenChannel,",
      "content": "func NewLocalForceCloseSummary(chanState *channeldb.OpenChannel,\n\tsigner input.Signer, commitTx *wire.MsgTx, stateNum uint64) (\n\t*LocalForceCloseSummary, error) {\n\n\t// Re-derive the original pkScript for to-self output within the\n\t// commitment transaction. We'll need this to find the corresponding\n\t// output in the commitment transaction and potentially for creating\n\t// the sign descriptor.\n\tcsvTimeout := uint32(chanState.LocalChanCfg.CsvDelay)\n\n\t// We use the passed state num to derive our scripts, since in case\n\t// this is after recovery, our latest channels state might not be up to\n\t// date.\n\trevocation, err := chanState.RevocationProducer.AtIndex(stateNum)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tcommitPoint := input.ComputeCommitmentPoint(revocation[:])\n\tkeyRing := DeriveCommitmentKeys(\n\t\tcommitPoint, true, chanState.ChanType,\n\t\t&chanState.LocalChanCfg, &chanState.RemoteChanCfg,\n\t)\n\n\tvar leaseExpiry uint32\n\tif chanState.ChanType.HasLeaseExpiration() {\n\t\tleaseExpiry = chanState.ThawHeight\n\t}\n\ttoLocalScript, err := CommitScriptToSelf(\n\t\tchanState.ChanType, chanState.IsInitiator, keyRing.ToLocalKey,\n\t\tkeyRing.RevocationKey, csvTimeout, leaseExpiry,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Locate the output index of the delayed commitment output back to us.\n\t// We'll return the details of this output to the caller so they can\n\t// sweep it once it's mature.\n\tvar (\n\t\tdelayIndex uint32\n\t\tdelayOut   *wire.TxOut\n\t)\n\tfor i, txOut := range commitTx.TxOut {\n\t\tif !bytes.Equal(toLocalScript.PkScript, txOut.PkScript) {\n\t\t\tcontinue\n\t\t}\n\n\t\tdelayIndex = uint32(i)\n\t\tdelayOut = txOut\n\t\tbreak\n\t}\n\n\t// With the necessary information gathered above, create a new sign\n\t// descriptor which is capable of generating the signature the caller\n\t// needs to sweep this output. The hash cache, and input index are not\n\t// set as the caller will decide these values once sweeping the output.\n\t// If the output is non-existent (dust), have the sign descriptor be\n\t// nil.\n\tvar commitResolution *CommitOutputResolution\n\tif delayOut != nil {\n\t\tlocalBalance := delayOut.Value\n\t\tcommitResolution = &CommitOutputResolution{\n\t\t\tSelfOutPoint: wire.OutPoint{\n\t\t\t\tHash:  commitTx.TxHash(),\n\t\t\t\tIndex: delayIndex,\n\t\t\t},\n\t\t\tSelfOutputSignDesc: input.SignDescriptor{\n\t\t\t\tKeyDesc:       chanState.LocalChanCfg.DelayBasePoint,\n\t\t\t\tSingleTweak:   keyRing.LocalCommitKeyTweak,\n\t\t\t\tWitnessScript: toLocalScript.WitnessScript,\n\t\t\t\tOutput: &wire.TxOut{\n\t\t\t\t\tPkScript: delayOut.PkScript,\n\t\t\t\t\tValue:    localBalance,\n\t\t\t\t},\n\t\t\t\tHashType: txscript.SigHashAll,\n\t\t\t},\n\t\t\tMaturityDelay: csvTimeout,\n\t\t}\n\t}\n\n\t// Once the delay output has been found (if it exists), then we'll also\n\t// need to create a series of sign descriptors for any lingering\n\t// outgoing HTLC's that we'll need to claim as well. If this is after\n\t// recovery there is not much we can do with HTLCs, so we'll always\n\t// use what we have in our latest state when extracting resolutions.\n\tlocalCommit := chanState.LocalCommitment\n\thtlcResolutions, err := extractHtlcResolutions(\n\t\tchainfee.SatPerKWeight(localCommit.FeePerKw), true, signer,\n\t\tlocalCommit.Htlcs, keyRing, &chanState.LocalChanCfg,\n\t\t&chanState.RemoteChanCfg, commitTx, chanState.ChanType,\n\t\tchanState.IsInitiator, leaseExpiry,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tanchorResolution, err := NewAnchorResolution(\n\t\tchanState, commitTx,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &LocalForceCloseSummary{\n\t\tChanPoint:        chanState.FundingOutpoint,\n\t\tCloseTx:          commitTx,\n\t\tCommitResolution: commitResolution,\n\t\tHtlcResolutions:  htlcResolutions,\n\t\tChanSnapshot:     *chanState.Snapshot(),\n\t\tAnchorResolution: anchorResolution,\n\t}, nil\n}\n\n// CreateCloseProposal is used by both parties in a cooperative channel close\n// workflow to generate proposed close transactions and signatures. This method\n// should only be executed once all pending HTLCs (if any) on the channel have\n// been cleared/removed. Upon completion, the source channel will shift into\n// the \"closing\" state, which indicates that all incoming/outgoing HTLC\n// requests should be rejected. A signature for the closing transaction is\n// returned.\n//\n// TODO(roasbeef): caller should initiate signal to reject all incoming HTLCs,\n// settle any in flight.",
      "length": 4065,
      "tokens": 516,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) CreateCloseProposal(proposedFee btcutil.Amount,",
      "content": "func (lc *LightningChannel) CreateCloseProposal(proposedFee btcutil.Amount,\n\tlocalDeliveryScript []byte,\n\tremoteDeliveryScript []byte) (input.Signature, *chainhash.Hash,\n\tbtcutil.Amount, error) {\n\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\t// If we've already closed the channel, then ignore this request.\n\tif lc.status == channelClosed {\n\t\t// TODO(roasbeef): check to ensure no pending payments\n\t\treturn nil, nil, 0, ErrChanClosing\n\t}\n\n\t// Get the final balances after subtracting the proposed fee, taking\n\t// care not to persist the adjusted balance, as the feeRate may change\n\t// during the channel closing process.\n\tourBalance, theirBalance, err := CoopCloseBalance(\n\t\tlc.channelState.ChanType, lc.channelState.IsInitiator,\n\t\tproposedFee, lc.channelState.LocalCommitment,\n\t)\n\tif err != nil {\n\t\treturn nil, nil, 0, err\n\t}\n\n\tcloseTx := CreateCooperativeCloseTx(\n\t\tfundingTxIn(lc.channelState), lc.channelState.LocalChanCfg.DustLimit,\n\t\tlc.channelState.RemoteChanCfg.DustLimit, ourBalance, theirBalance,\n\t\tlocalDeliveryScript, remoteDeliveryScript,\n\t)\n\n\t// Ensure that the transaction doesn't explicitly violate any\n\t// consensus rules such as being too big, or having any value with a\n\t// negative output.\n\ttx := btcutil.NewTx(closeTx)\n\tif err := blockchain.CheckTransactionSanity(tx); err != nil {\n\t\treturn nil, nil, 0, err\n\t}\n\n\t// Finally, sign the completed cooperative closure transaction. As the\n\t// initiator we'll simply send our signature over to the remote party,\n\t// using the generated txid to be notified once the closure transaction\n\t// has been confirmed.\n\tlc.signDesc.SigHashes = input.NewTxSigHashesV0Only(closeTx)\n\tsig, err := lc.Signer.SignOutputRaw(closeTx, lc.signDesc)\n\tif err != nil {\n\t\treturn nil, nil, 0, err\n\t}\n\n\t// As everything checks out, indicate in the channel status that a\n\t// channel closure has been initiated.\n\tlc.status = channelClosing\n\n\tcloseTXID := closeTx.TxHash()\n\treturn sig, &closeTXID, ourBalance, nil\n}\n\n// CompleteCooperativeClose completes the cooperative closure of the target\n// active lightning channel. A fully signed closure transaction as well as the\n// signature itself are returned. Additionally, we also return our final\n// settled balance, which reflects any fees we may have paid.\n//\n// NOTE: The passed local and remote sigs are expected to be fully complete\n// signatures including the proper sighash byte.",
      "length": 2219,
      "tokens": 300,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) CompleteCooperativeClose(",
      "content": "func (lc *LightningChannel) CompleteCooperativeClose(\n\tlocalSig, remoteSig input.Signature,\n\tlocalDeliveryScript, remoteDeliveryScript []byte,\n\tproposedFee btcutil.Amount) (*wire.MsgTx, btcutil.Amount, error) {\n\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\t// If the channel is already closed, then ignore this request.\n\tif lc.status == channelClosed {\n\t\t// TODO(roasbeef): check to ensure no pending payments\n\t\treturn nil, 0, ErrChanClosing\n\t}\n\n\t// Get the final balances after subtracting the proposed fee.\n\tourBalance, theirBalance, err := CoopCloseBalance(\n\t\tlc.channelState.ChanType, lc.channelState.IsInitiator,\n\t\tproposedFee, lc.channelState.LocalCommitment,\n\t)\n\tif err != nil {\n\t\treturn nil, 0, err\n\t}\n\n\t// Create the transaction used to return the current settled balance\n\t// on this active channel back to both parties. In this current model,\n\t// the initiator pays full fees for the cooperative close transaction.\n\tcloseTx := CreateCooperativeCloseTx(\n\t\tfundingTxIn(lc.channelState), lc.channelState.LocalChanCfg.DustLimit,\n\t\tlc.channelState.RemoteChanCfg.DustLimit, ourBalance, theirBalance,\n\t\tlocalDeliveryScript, remoteDeliveryScript,\n\t)\n\n\t// Ensure that the transaction doesn't explicitly validate any\n\t// consensus rules such as being too big, or having any value with a\n\t// negative output.\n\ttx := btcutil.NewTx(closeTx)\n\tif err := blockchain.CheckTransactionSanity(tx); err != nil {\n\t\treturn nil, 0, err\n\t}\n\thashCache := input.NewTxSigHashesV0Only(closeTx)\n\n\t// Finally, construct the witness stack minding the order of the\n\t// pubkeys+sigs on the stack.\n\tourKey := lc.channelState.LocalChanCfg.MultiSigKey.PubKey.\n\t\tSerializeCompressed()\n\ttheirKey := lc.channelState.RemoteChanCfg.MultiSigKey.PubKey.\n\t\tSerializeCompressed()\n\twitness := input.SpendMultiSig(\n\t\tlc.signDesc.WitnessScript, ourKey, localSig, theirKey,\n\t\tremoteSig,\n\t)\n\tcloseTx.TxIn[0].Witness = witness\n\n\t// Validate the finalized transaction to ensure the output script is\n\t// properly met, and that the remote peer supplied a valid signature.\n\tprevOut := lc.signDesc.Output\n\tvm, err := txscript.NewEngine(\n\t\tprevOut.PkScript, closeTx, 0, txscript.StandardVerifyFlags, nil,\n\t\thashCache, prevOut.Value, txscript.NewCannedPrevOutputFetcher(\n\t\t\tprevOut.PkScript, prevOut.Value,\n\t\t),\n\t)\n\tif err != nil {\n\t\treturn nil, 0, err\n\t}\n\tif err := vm.Execute(); err != nil {\n\t\treturn nil, 0, err\n\t}\n\n\t// As the transaction is sane, and the scripts are valid we'll mark the\n\t// channel now as closed as the closure transaction should get into the\n\t// chain in a timely manner and possibly be re-broadcast by the wallet.\n\tlc.status = channelClosed\n\n\treturn closeTx, ourBalance, nil\n}\n\n// AnchorResolutions is a set of anchor resolutions that's being used when\n// sweeping anchors during local channel force close.",
      "length": 2637,
      "tokens": 334,
      "embedding": []
    },
    {
      "slug": "type AnchorResolutions struct {",
      "content": "type AnchorResolutions struct {\n\t// Local is the anchor resolution for the local commitment tx.\n\tLocal *AnchorResolution\n\n\t// Remote is the anchor resolution for the remote commitment tx.\n\tRemote *AnchorResolution\n\n\t// RemotePending is the anchor resolution for the remote pending\n\t// commitment tx. The value will be non-nil iff we've created a new\n\t// commitment tx for the remote party which they haven't ACKed yet.\n\tRemotePending *AnchorResolution\n}\n\n// NewAnchorResolutions returns a set of anchor resolutions wrapped in the\n// struct AnchorResolutions. Because we have no view on the mempool, we can\n// only blindly anchor all of these txes down. Caller needs to check the\n// returned values against nil to decide whether there exists an anchor\n// resolution for local/remote/pending remote commitment txes.",
      "length": 765,
      "tokens": 121,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) NewAnchorResolutions() (*AnchorResolutions,",
      "content": "func (lc *LightningChannel) NewAnchorResolutions() (*AnchorResolutions,\n\terror) {\n\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\tresolutions := &AnchorResolutions{}\n\n\t// Add anchor for local commitment tx, if any.\n\tlocalRes, err := NewAnchorResolution(\n\t\tlc.channelState, lc.channelState.LocalCommitment.CommitTx,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tresolutions.Local = localRes\n\n\t// Add anchor for remote commitment tx, if any.\n\tremoteRes, err := NewAnchorResolution(\n\t\tlc.channelState, lc.channelState.RemoteCommitment.CommitTx,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tresolutions.Remote = remoteRes\n\n\t// Add anchor for remote pending commitment tx, if any.\n\tremotePendingCommit, err := lc.channelState.RemoteCommitChainTip()\n\tif err != nil && err != channeldb.ErrNoPendingCommit {\n\t\treturn nil, err\n\t}\n\n\tif remotePendingCommit != nil {\n\t\tremotePendingRes, err := NewAnchorResolution(\n\t\t\tlc.channelState,\n\t\t\tremotePendingCommit.Commitment.CommitTx,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tresolutions.RemotePending = remotePendingRes\n\t}\n\n\treturn resolutions, nil\n}\n\n// NewAnchorResolution returns the information that is required to sweep the\n// local anchor.",
      "length": 1044,
      "tokens": 134,
      "embedding": []
    },
    {
      "slug": "func NewAnchorResolution(chanState *channeldb.OpenChannel,",
      "content": "func NewAnchorResolution(chanState *channeldb.OpenChannel,\n\tcommitTx *wire.MsgTx) (*AnchorResolution, error) {\n\n\t// Return nil resolution if the channel has no anchors.\n\tif !chanState.ChanType.HasAnchors() {\n\t\treturn nil, nil\n\t}\n\n\t// Derive our local anchor script.\n\tlocalAnchor, _, err := CommitScriptAnchors(\n\t\t&chanState.LocalChanCfg, &chanState.RemoteChanCfg,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Look up the script on the commitment transaction. It may not be\n\t// present if there is no output paying to us.\n\tfound, index := input.FindScriptOutputIndex(commitTx, localAnchor.PkScript)\n\tif !found {\n\t\treturn nil, nil\n\t}\n\n\toutPoint := &wire.OutPoint{\n\t\tHash:  commitTx.TxHash(),\n\t\tIndex: index,\n\t}\n\n\t// Instantiate the sign descriptor that allows sweeping of the anchor.\n\tsignDesc := &input.SignDescriptor{\n\t\tKeyDesc:       chanState.LocalChanCfg.MultiSigKey,\n\t\tWitnessScript: localAnchor.WitnessScript,\n\t\tOutput: &wire.TxOut{\n\t\t\tPkScript: localAnchor.PkScript,\n\t\t\tValue:    int64(anchorSize),\n\t\t},\n\t\tHashType: txscript.SigHashAll,\n\t}\n\n\t// Calculate commit tx weight. This commit tx doesn't yet include the\n\t// witness spending the funding output, so we add the (worst case)\n\t// weight for that too.\n\tutx := btcutil.NewTx(commitTx)\n\tweight := blockchain.GetTransactionWeight(utx) +\n\t\tinput.WitnessCommitmentTxWeight\n\n\t// Calculate commit tx fee.\n\tfee := chanState.Capacity\n\tfor _, out := range commitTx.TxOut {\n\t\tfee -= btcutil.Amount(out.Value)\n\t}\n\n\treturn &AnchorResolution{\n\t\tCommitAnchor:         *outPoint,\n\t\tAnchorSignDescriptor: *signDesc,\n\t\tCommitWeight:         weight,\n\t\tCommitFee:            fee,\n\t}, nil\n}\n\n// AvailableBalance returns the current balance available for sending within\n// the channel. By available balance, we mean that if at this very instance a\n// new commitment were to be created which evals all the log entries, what\n// would our available balance for adding an additional HTLC be. It takes into\n// account the fee that must be paid for adding this HTLC (if we're the\n// initiator), and that we cannot spend from the channel reserve. This method\n// is useful when deciding if a given channel can accept an HTLC in the\n// multi-hop forwarding scenario.",
      "length": 2070,
      "tokens": 285,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) AvailableBalance() lnwire.MilliSatoshi {",
      "content": "func (lc *LightningChannel) AvailableBalance() lnwire.MilliSatoshi {\n\tlc.RLock()\n\tdefer lc.RUnlock()\n\n\tbal, _ := lc.availableBalance()\n\treturn bal\n}\n\n// availableBalance is the private, non mutexed version of AvailableBalance.\n// This method is provided so methods that already hold the lock can access\n// this method. Additionally, the total weight of the next to be created\n// commitment is returned for accounting purposes.",
      "length": 347,
      "tokens": 54,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) availableBalance() (lnwire.MilliSatoshi, int64) {",
      "content": "func (lc *LightningChannel) availableBalance() (lnwire.MilliSatoshi, int64) {\n\t// We'll grab the current set of log updates that the remote has\n\t// ACKed.\n\tremoteACKedIndex := lc.localCommitChain.tip().theirMessageIndex\n\thtlcView := lc.fetchHTLCView(remoteACKedIndex,\n\t\tlc.localUpdateLog.logIndex)\n\n\t// Calculate our available balance from our local commitment.\n\t// TODO(halseth): could reuse parts validateCommitmentSanity to do this\n\t// balance calculation, as most of the logic is the same.\n\t//\n\t// NOTE: This is not always accurate, since the remote node can always\n\t// add updates concurrently, causing our balance to go down if we're\n\t// the initiator, but this is a problem on the protocol level.\n\tourLocalCommitBalance, commitWeight := lc.availableCommitmentBalance(\n\t\thtlcView, false,\n\t)\n\n\t// Do the same calculation from the remote commitment point of view.\n\tourRemoteCommitBalance, _ := lc.availableCommitmentBalance(\n\t\thtlcView, true,\n\t)\n\n\t// Return which ever balance is lowest.\n\tif ourRemoteCommitBalance < ourLocalCommitBalance {\n\t\treturn ourRemoteCommitBalance, commitWeight\n\t}\n\n\treturn ourLocalCommitBalance, commitWeight\n}\n\n// availableCommitmentBalance attempts to calculate the balance we have\n// available for HTLCs on the local/remote commitment given the htlcView. To\n// account for sending HTLCs of different sizes, it will report the balance\n// available for sending non-dust HTLCs, which will be manifested on the\n// commitment, increasing the commitment fee we must pay as an initiator,\n// eating into our balance. It will make sure we won't violate the channel\n// reserve constraints for this amount.",
      "length": 1514,
      "tokens": 213,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) availableCommitmentBalance(view *htlcView,",
      "content": "func (lc *LightningChannel) availableCommitmentBalance(view *htlcView,\n\tremoteChain bool) (lnwire.MilliSatoshi, int64) {\n\n\t// Compute the current balances for this commitment. This will take\n\t// into account HTLCs to determine the commit weight, which the\n\t// initiator must pay the fee for.\n\tourBalance, theirBalance, commitWeight, filteredView, err := lc.computeView(\n\t\tview, remoteChain, false,\n\t)\n\tif err != nil {\n\t\tlc.log.Errorf(\"Unable to fetch available balance: %v\", err)\n\t\treturn 0, 0\n\t}\n\n\t// We can never spend from the channel reserve, so we'll subtract it\n\t// from our available balance.\n\tourReserve := lnwire.NewMSatFromSatoshis(\n\t\tlc.channelState.LocalChanCfg.ChanReserve,\n\t)\n\tif ourReserve <= ourBalance {\n\t\tourBalance -= ourReserve\n\t} else {\n\t\tourBalance = 0\n\t}\n\n\t// Calculate the commitment fee in the case where we would add another\n\t// HTLC to the commitment, as only the balance remaining after this fee\n\t// has been paid is actually available for sending.\n\tfeePerKw := filteredView.feePerKw\n\thtlcCommitFee := lnwire.NewMSatFromSatoshis(\n\t\tfeePerKw.FeeForWeight(commitWeight + input.HTLCWeight),\n\t)\n\n\t// If we are the channel initiator, we must to subtract this commitment\n\t// fee from our available balance in order to ensure we can afford both\n\t// the value of the HTLC and the additional commitment fee from adding\n\t// the HTLC.\n\tif lc.channelState.IsInitiator {\n\t\t// There is an edge case where our non-zero balance is lower\n\t\t// than the htlcCommitFee, where we could still be sending dust\n\t\t// HTLCs, but we return 0 in this case. This is to avoid\n\t\t// lowering our balance even further, as this takes us into a\n\t\t// bad state where neither we nor our channel counterparty can\n\t\t// add HTLCs.\n\t\tif ourBalance < htlcCommitFee {\n\t\t\treturn 0, commitWeight\n\t\t}\n\n\t\treturn ourBalance - htlcCommitFee, commitWeight\n\t}\n\n\t// If we're not the initiator, we must check whether the remote has\n\t// enough balance to pay for the fee of our HTLC. We'll start by also\n\t// subtracting our counterparty's reserve from their balance.\n\ttheirReserve := lnwire.NewMSatFromSatoshis(\n\t\tlc.channelState.RemoteChanCfg.ChanReserve,\n\t)\n\tif theirReserve <= theirBalance {\n\t\ttheirBalance -= theirReserve\n\t} else {\n\t\ttheirBalance = 0\n\t}\n\n\t// We'll use the dustlimit and htlcFee to find the largest HTLC value\n\t// that will be considered dust on the commitment.\n\tdustlimit := lnwire.NewMSatFromSatoshis(\n\t\tlc.channelState.LocalChanCfg.DustLimit,\n\t)\n\n\t// For an extra HTLC fee to be paid on our commitment, the HTLC must be\n\t// large enough to make a non-dust HTLC timeout transaction.\n\thtlcFee := lnwire.NewMSatFromSatoshis(\n\t\tHtlcTimeoutFee(lc.channelState.ChanType, feePerKw),\n\t)\n\n\t// If we are looking at the remote commitment, we must use the remote\n\t// dust limit and the fee for adding an HTLC success transaction.\n\tif remoteChain {\n\t\tdustlimit = lnwire.NewMSatFromSatoshis(\n\t\t\tlc.channelState.RemoteChanCfg.DustLimit,\n\t\t)\n\t\thtlcFee = lnwire.NewMSatFromSatoshis(\n\t\t\tHtlcSuccessFee(lc.channelState.ChanType, feePerKw),\n\t\t)\n\t}\n\n\t// The HTLC output will be manifested on the commitment if it\n\t// is non-dust after paying the HTLC fee.\n\tnonDustHtlcAmt := dustlimit + htlcFee\n\n\t// If they cannot pay the fee if we add another non-dust HTLC, we'll\n\t// report our available balance just below the non-dust amount, to\n\t// avoid attempting HTLCs larger than this size.\n\tif theirBalance < htlcCommitFee && ourBalance >= nonDustHtlcAmt {\n\t\t// see https://github.com/lightning/bolts/issues/728\n\t\tourReportedBalance := nonDustHtlcAmt - 1\n\t\tlc.log.Infof(\"Reducing local balance (from %v to %v): \"+\n\t\t\t\"remote side does not have enough funds (%v < %v) to \"+\n\t\t\t\"pay for non-dust HTLC in case of unilateral close.\",\n\t\t\tourBalance, ourReportedBalance, theirBalance,\n\t\t\thtlcCommitFee)\n\t\tourBalance = ourReportedBalance\n\t}\n\n\treturn ourBalance, commitWeight\n}\n\n// StateSnapshot returns a snapshot of the current fully committed state within\n// the channel.",
      "length": 3758,
      "tokens": 554,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) StateSnapshot() *channeldb.ChannelSnapshot {",
      "content": "func (lc *LightningChannel) StateSnapshot() *channeldb.ChannelSnapshot {\n\tlc.RLock()\n\tdefer lc.RUnlock()\n\n\treturn lc.channelState.Snapshot()\n}\n\n// validateFeeRate ensures that if the passed fee is applied to the channel,\n// and a new commitment is created (which evaluates this fee), then the\n// initiator of the channel does not dip below their reserve.",
      "length": 273,
      "tokens": 43,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) validateFeeRate(feePerKw chainfee.SatPerKWeight) error {",
      "content": "func (lc *LightningChannel) validateFeeRate(feePerKw chainfee.SatPerKWeight) error {\n\t// We'll ensure that we can accommodate this new fee change, yet still\n\t// be above our reserve balance. Otherwise, we'll reject the fee\n\t// update.\n\tavailableBalance, txWeight := lc.availableBalance()\n\n\toldFee := lnwire.NewMSatFromSatoshis(\n\t\tlc.localCommitChain.tip().feePerKw.FeeForWeight(txWeight),\n\t)\n\n\t// Our base balance is the total amount of satoshis we can commit\n\t// towards fees before factoring in the channel reserve.\n\tbaseBalance := availableBalance + oldFee\n\n\t// Using the weight of the commitment transaction if we were to create\n\t// a commitment now, we'll compute our remaining balance if we apply\n\t// this new fee update.\n\tnewFee := lnwire.NewMSatFromSatoshis(\n\t\tfeePerKw.FeeForWeight(txWeight),\n\t)\n\n\t// If the total fee exceeds our available balance (taking into account\n\t// the fee from the last state), then we'll reject this update as it\n\t// would mean we need to trim our entire output.\n\tif newFee > baseBalance {\n\t\treturn fmt.Errorf(\"cannot apply fee_update=%v sat/kw, new fee \"+\n\t\t\t\"of %v is greater than balance of %v\", int64(feePerKw),\n\t\t\tnewFee, baseBalance)\n\t}\n\n\t// TODO(halseth): should fail if fee update is unreasonable,\n\t// as specified in BOLT#2.\n\t//  * COMMENT(roasbeef): can cross-check with our ideal fee rate\n\n\treturn nil\n}\n\n// UpdateFee initiates a fee update for this channel. Must only be called by\n// the channel initiator, and must be called before sending update_fee to\n// the remote.",
      "length": 1393,
      "tokens": 214,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) UpdateFee(feePerKw chainfee.SatPerKWeight) error {",
      "content": "func (lc *LightningChannel) UpdateFee(feePerKw chainfee.SatPerKWeight) error {\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\t// Only initiator can send fee update, so trying to send one as\n\t// non-initiator will fail.\n\tif !lc.channelState.IsInitiator {\n\t\treturn fmt.Errorf(\"local fee update as non-initiator\")\n\t}\n\n\t// Ensure that the passed fee rate meets our current requirements.\n\tif err := lc.validateFeeRate(feePerKw); err != nil {\n\t\treturn err\n\t}\n\n\tpd := &PaymentDescriptor{\n\t\tLogIndex:  lc.localUpdateLog.logIndex,\n\t\tAmount:    lnwire.NewMSatFromSatoshis(btcutil.Amount(feePerKw)),\n\t\tEntryType: FeeUpdate,\n\t}\n\n\tlc.localUpdateLog.appendUpdate(pd)\n\n\treturn nil\n}\n\n// ReceiveUpdateFee handles an updated fee sent from remote. This method will\n// return an error if called as channel initiator.",
      "length": 677,
      "tokens": 87,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) ReceiveUpdateFee(feePerKw chainfee.SatPerKWeight) error {",
      "content": "func (lc *LightningChannel) ReceiveUpdateFee(feePerKw chainfee.SatPerKWeight) error {\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\t// Only initiator can send fee update, and we must fail if we receive\n\t// fee update as initiator\n\tif lc.channelState.IsInitiator {\n\t\treturn fmt.Errorf(\"received fee update as initiator\")\n\t}\n\n\t// TODO(roasbeef): or just modify to use the other balance?\n\tpd := &PaymentDescriptor{\n\t\tLogIndex:  lc.remoteUpdateLog.logIndex,\n\t\tAmount:    lnwire.NewMSatFromSatoshis(btcutil.Amount(feePerKw)),\n\t\tEntryType: FeeUpdate,\n\t}\n\n\tlc.remoteUpdateLog.appendUpdate(pd)\n\n\treturn nil\n}\n\n// generateRevocation generates the revocation message for a given height.",
      "length": 555,
      "tokens": 66,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) generateRevocation(height uint64) (*lnwire.RevokeAndAck,",
      "content": "func (lc *LightningChannel) generateRevocation(height uint64) (*lnwire.RevokeAndAck,\n\terror) {\n\n\t// Now that we've accept a new state transition, we send the remote\n\t// party the revocation for our current commitment state.\n\trevocationMsg := &lnwire.RevokeAndAck{}\n\tcommitSecret, err := lc.channelState.RevocationProducer.AtIndex(height)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tcopy(revocationMsg.Revocation[:], commitSecret[:])\n\n\t// Along with this revocation, we'll also send the _next_ commitment\n\t// point that the remote party should use to create our next commitment\n\t// transaction. We use a +2 here as we already gave them a look ahead\n\t// of size one after the FundingLocked message was sent:\n\t//\n\t// 0: current revocation, 1: their \"next\" revocation, 2: this revocation\n\t//\n\t// We're revoking the current revocation. Once they receive this\n\t// message they'll set the \"current\" revocation for us to their stored\n\t// \"next\" revocation, and this revocation will become their new \"next\"\n\t// revocation.\n\t//\n\t// Put simply in the window slides to the left by one.\n\tnextCommitSecret, err := lc.channelState.RevocationProducer.AtIndex(\n\t\theight + 2,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trevocationMsg.NextRevocationKey = input.ComputeCommitmentPoint(nextCommitSecret[:])\n\trevocationMsg.ChanID = lnwire.NewChanIDFromOutPoint(\n\t\t&lc.channelState.FundingOutpoint)\n\n\treturn revocationMsg, nil\n}\n\n// CreateCooperativeCloseTx creates a transaction which if signed by both\n// parties, then broadcast cooperatively closes an active channel. The creation\n// of the closure transaction is modified by a boolean indicating if the party\n// constructing the channel is the initiator of the closure. Currently it is\n// expected that the initiator pays the transaction fees for the closing\n// transaction in full.",
      "length": 1679,
      "tokens": 244,
      "embedding": []
    },
    {
      "slug": "func CreateCooperativeCloseTx(fundingTxIn wire.TxIn,",
      "content": "func CreateCooperativeCloseTx(fundingTxIn wire.TxIn,\n\tlocalDust, remoteDust, ourBalance, theirBalance btcutil.Amount,\n\tourDeliveryScript, theirDeliveryScript []byte) *wire.MsgTx {\n\n\t// Construct the transaction to perform a cooperative closure of the\n\t// channel. In the event that one side doesn't have any settled funds\n\t// within the channel then a refund output for that particular side can\n\t// be omitted.\n\tcloseTx := wire.NewMsgTx(2)\n\tcloseTx.AddTxIn(&fundingTxIn)\n\n\t// Create both cooperative closure outputs, properly respecting the\n\t// dust limits of both parties.\n\tif ourBalance >= localDust {\n\t\tcloseTx.AddTxOut(&wire.TxOut{\n\t\t\tPkScript: ourDeliveryScript,\n\t\t\tValue:    int64(ourBalance),\n\t\t})\n\t}\n\tif theirBalance >= remoteDust {\n\t\tcloseTx.AddTxOut(&wire.TxOut{\n\t\t\tPkScript: theirDeliveryScript,\n\t\t\tValue:    int64(theirBalance),\n\t\t})\n\t}\n\n\ttxsort.InPlaceSort(closeTx)\n\n\treturn closeTx\n}\n\n// LocalBalanceDust returns true if when creating a co-op close transaction,\n// the balance of the local party will be dust after accounting for any anchor\n// outputs.",
      "length": 981,
      "tokens": 125,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) LocalBalanceDust() bool {",
      "content": "func (lc *LightningChannel) LocalBalanceDust() bool {\n\tlc.RLock()\n\tdefer lc.RUnlock()\n\n\tchanState := lc.channelState\n\tlocalBalance := chanState.LocalCommitment.LocalBalance.ToSatoshis()\n\n\t// If this is an anchor channel, and we're the initiator, then we'll\n\t// regain the stats allocated to the anchor outputs with the co-op\n\t// close transaction.\n\tif chanState.ChanType.HasAnchors() && chanState.IsInitiator {\n\t\tlocalBalance += 2 * anchorSize\n\t}\n\n\treturn localBalance <= chanState.LocalChanCfg.DustLimit\n}\n\n// RemoteBalanceDust returns true if when creating a co-op close transaction,\n// the balance of the remote party will be dust after accounting for any anchor\n// outputs.",
      "length": 605,
      "tokens": 81,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) RemoteBalanceDust() bool {",
      "content": "func (lc *LightningChannel) RemoteBalanceDust() bool {\n\tlc.RLock()\n\tdefer lc.RUnlock()\n\n\tchanState := lc.channelState\n\tremoteBalance := chanState.RemoteCommitment.RemoteBalance.ToSatoshis()\n\n\t// If this is an anchor channel, and they're the initiator, then we'll\n\t// regain the stats allocated to the anchor outputs with the co-op\n\t// close transaction.\n\tif chanState.ChanType.HasAnchors() && !chanState.IsInitiator {\n\t\tremoteBalance += 2 * anchorSize\n\t}\n\n\treturn remoteBalance <= chanState.RemoteChanCfg.DustLimit\n}\n\n// CalcFee returns the commitment fee to use for the given fee rate\n// (fee-per-kw).",
      "length": 530,
      "tokens": 68,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) CalcFee(feeRate chainfee.SatPerKWeight) btcutil.Amount {",
      "content": "func (lc *LightningChannel) CalcFee(feeRate chainfee.SatPerKWeight) btcutil.Amount {\n\treturn feeRate.FeeForWeight(CommitWeight(lc.channelState.ChanType))\n}\n\n// MaxFeeRate returns the maximum fee rate given an allocation of the channel\n// initiator's spendable balance along with the local reserve amount. This can\n// be useful to determine when we should stop proposing fee updates that exceed\n// our maximum allocation.\n//\n// NOTE: This should only be used for channels in which the local commitment is\n// the initiator.",
      "length": 427,
      "tokens": 65,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) MaxFeeRate(maxAllocation float64) chainfee.SatPerKWeight {",
      "content": "func (lc *LightningChannel) MaxFeeRate(maxAllocation float64) chainfee.SatPerKWeight {\n\tlc.RLock()\n\tdefer lc.RUnlock()\n\n\t// The maximum fee depends on the available balance that can be\n\t// committed towards fees. It takes into account our local reserve\n\t// balance.\n\tavailableBalance, weight := lc.availableBalance()\n\n\toldFee := lc.localCommitChain.tip().feePerKw.FeeForWeight(weight)\n\n\t// baseBalance is the maximum amount available for us to spend on fees.\n\tbaseBalance := availableBalance.ToSatoshis() + oldFee\n\n\tmaxFee := float64(baseBalance) * maxAllocation\n\n\t// Ensure the fee rate doesn't dip below the fee floor.\n\tmaxFeeRate := maxFee / (float64(weight) / 1000)\n\treturn chainfee.SatPerKWeight(\n\t\tmath.Max(maxFeeRate, float64(chainfee.FeePerKwFloor)),\n\t)\n}\n\n// IdealCommitFeeRate uses the current network fee, the minimum relay fee,\n// maximum fee allocation and anchor channel commitment fee rate to determine\n// the ideal fee to be used for the commitments of the channel.",
      "length": 870,
      "tokens": 118,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) IdealCommitFeeRate(netFeeRate, minRelayFeeRate,",
      "content": "func (lc *LightningChannel) IdealCommitFeeRate(netFeeRate, minRelayFeeRate,\n\tmaxAnchorCommitFeeRate chainfee.SatPerKWeight,\n\tmaxFeeAlloc float64) chainfee.SatPerKWeight {\n\n\t// Get the maximum fee rate that we can use given our max fee allocation\n\t// and given the local reserve balance that we must preserve.\n\tmaxFeeRate := lc.MaxFeeRate(maxFeeAlloc)\n\n\tvar commitFeeRate chainfee.SatPerKWeight\n\n\t// If the channel has anchor outputs then cap the fee rate at the\n\t// max anchor fee rate if that maximum is less than our max fee rate.\n\t// Otherwise, cap the fee rate at the max fee rate.\n\tswitch lc.channelState.ChanType.HasAnchors() &&\n\t\tmaxFeeRate > maxAnchorCommitFeeRate {\n\tcase true:\n\t\tcommitFeeRate = chainfee.SatPerKWeight(\n\t\t\tmath.Min(\n\t\t\t\tfloat64(netFeeRate),\n\t\t\t\tfloat64(maxAnchorCommitFeeRate),\n\t\t\t),\n\t\t)\n\n\tcase false:\n\t\tcommitFeeRate = chainfee.SatPerKWeight(\n\t\t\tmath.Min(float64(netFeeRate), float64(maxFeeRate)),\n\t\t)\n\t}\n\n\tif commitFeeRate >= minRelayFeeRate {\n\t\treturn commitFeeRate\n\t}\n\n\t// The commitment fee rate is below the minimum relay fee rate.\n\t// If the min relay fee rate is still below the maximum fee, then use\n\t// the minimum relay fee rate.\n\tif minRelayFeeRate <= maxFeeRate {\n\t\treturn minRelayFeeRate\n\t}\n\n\t// The minimum relay fee rate is more than the ideal maximum fee rate.\n\t// Check if it is smaller than the absolute maximum fee rate we can\n\t// use. If it is, then we use the minimum relay fee rate and we log a\n\t// warning to indicate that the max channel fee allocation option was\n\t// ignored.\n\tabsoluteMaxFee := lc.MaxFeeRate(1)\n\tif minRelayFeeRate <= absoluteMaxFee {\n\t\tlc.log.Warn(\"Ignoring max channel fee allocation to \" +\n\t\t\t\"ensure that the commitment fee is above the \" +\n\t\t\t\"minimum relay fee.\")\n\n\t\treturn minRelayFeeRate\n\t}\n\n\t// The absolute maximum fee rate we can pay is below the minimum\n\t// relay fee rate. The commitment tx will not be able to propagate.\n\t// To give the transaction the best chance, we use the absolute\n\t// maximum fee we have available and we log an error.\n\tlc.log.Errorf(\"The commitment fee rate of %s is below the current \"+\n\t\t\"minimum relay fee rate of %s. The max fee rate of %s will be\"+\n\t\t\"used.\", commitFeeRate, minRelayFeeRate, absoluteMaxFee)\n\n\treturn absoluteMaxFee\n}\n\n// RemoteNextRevocation returns the channelState's RemoteNextRevocation.",
      "length": 2178,
      "tokens": 331,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) RemoteNextRevocation() *btcec.PublicKey {",
      "content": "func (lc *LightningChannel) RemoteNextRevocation() *btcec.PublicKey {\n\tlc.RLock()\n\tdefer lc.RUnlock()\n\n\treturn lc.channelState.RemoteNextRevocation\n}\n\n// IsInitiator returns true if we were the ones that initiated the funding\n// workflow which led to the creation of this channel. Otherwise, it returns\n// false.",
      "length": 234,
      "tokens": 34,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) IsInitiator() bool {",
      "content": "func (lc *LightningChannel) IsInitiator() bool {\n\tlc.RLock()\n\tdefer lc.RUnlock()\n\n\treturn lc.channelState.IsInitiator\n}\n\n// CommitFeeRate returns the current fee rate of the commitment transaction in\n// units of sat-per-kw.",
      "length": 167,
      "tokens": 22,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) CommitFeeRate() chainfee.SatPerKWeight {",
      "content": "func (lc *LightningChannel) CommitFeeRate() chainfee.SatPerKWeight {\n\tlc.RLock()\n\tdefer lc.RUnlock()\n\n\treturn chainfee.SatPerKWeight(lc.channelState.LocalCommitment.FeePerKw)\n}\n\n// IsPending returns true if the channel's funding transaction has been fully\n// confirmed, and false otherwise.",
      "length": 214,
      "tokens": 23,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) IsPending() bool {",
      "content": "func (lc *LightningChannel) IsPending() bool {\n\tlc.RLock()\n\tdefer lc.RUnlock()\n\n\treturn lc.channelState.IsPending\n}\n\n// State provides access to the channel's internal state.",
      "length": 121,
      "tokens": 15,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) State() *channeldb.OpenChannel {",
      "content": "func (lc *LightningChannel) State() *channeldb.OpenChannel {\n\treturn lc.channelState\n}\n\n// MarkBorked marks the event when the channel as reached an irreconcilable\n// state, such as a channel breach or state desynchronization. Borked channels\n// should never be added to the switch.",
      "length": 216,
      "tokens": 35,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) MarkBorked() error {",
      "content": "func (lc *LightningChannel) MarkBorked() error {\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\treturn lc.channelState.MarkBorked()\n}\n\n// MarkCommitmentBroadcasted marks the channel as a commitment transaction has\n// been broadcast, either our own or the remote, and we should watch the chain\n// for it to confirm before taking any further action. It takes a boolean which\n// indicates whether we initiated the close.",
      "length": 344,
      "tokens": 53,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) MarkCommitmentBroadcasted(tx *wire.MsgTx,",
      "content": "func (lc *LightningChannel) MarkCommitmentBroadcasted(tx *wire.MsgTx,\n\tlocallyInitiated bool) error {\n\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\treturn lc.channelState.MarkCommitmentBroadcasted(tx, locallyInitiated)\n}\n\n// MarkCoopBroadcasted marks the channel as a cooperative close transaction has\n// been broadcast, and that we should watch the chain for it to confirm before\n// taking any further action. It takes a locally initiated bool which is true\n// if we initiated the cooperative close.",
      "length": 406,
      "tokens": 58,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) MarkCoopBroadcasted(tx *wire.MsgTx,",
      "content": "func (lc *LightningChannel) MarkCoopBroadcasted(tx *wire.MsgTx,\n\tlocalInitiated bool) error {\n\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\treturn lc.channelState.MarkCoopBroadcasted(tx, localInitiated)\n}\n\n// MarkDataLoss marks sets the channel status to LocalDataLoss and stores the\n// passed commitPoint for use to retrieve funds in case the remote force closes\n// the channel.",
      "length": 292,
      "tokens": 40,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) MarkDataLoss(commitPoint *btcec.PublicKey) error {",
      "content": "func (lc *LightningChannel) MarkDataLoss(commitPoint *btcec.PublicKey) error {\n\tlc.Lock()\n\tdefer lc.Unlock()\n\n\treturn lc.channelState.MarkDataLoss(commitPoint)\n}\n\n// ActiveHtlcs returns a slice of HTLC's which are currently active on *both*\n// commitment transactions.",
      "length": 182,
      "tokens": 22,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) ActiveHtlcs() []channeldb.HTLC {",
      "content": "func (lc *LightningChannel) ActiveHtlcs() []channeldb.HTLC {\n\tlc.RLock()\n\tdefer lc.RUnlock()\n\n\treturn lc.channelState.ActiveHtlcs()\n}\n\n// LocalChanReserve returns our local ChanReserve requirement for the remote party.",
      "length": 151,
      "tokens": 17,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) LocalChanReserve() btcutil.Amount {",
      "content": "func (lc *LightningChannel) LocalChanReserve() btcutil.Amount {\n\treturn lc.channelState.LocalChanCfg.ChanReserve\n}\n\n// NextLocalHtlcIndex returns the next unallocated local htlc index. To ensure\n// this always returns the next index that has been not been allocated, this\n// will first try to examine any pending commitments, before falling back to the\n// last locked-in local commitment.",
      "length": 318,
      "tokens": 47,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) NextLocalHtlcIndex() (uint64, error) {",
      "content": "func (lc *LightningChannel) NextLocalHtlcIndex() (uint64, error) {\n\tlc.RLock()\n\tdefer lc.RUnlock()\n\n\treturn lc.channelState.NextLocalHtlcIndex()\n}\n\n// FwdMinHtlc returns the minimum HTLC value required by the remote node, i.e.\n// the minimum value HTLC we can forward on this channel.",
      "length": 210,
      "tokens": 30,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) FwdMinHtlc() lnwire.MilliSatoshi {",
      "content": "func (lc *LightningChannel) FwdMinHtlc() lnwire.MilliSatoshi {\n\treturn lc.channelState.LocalChanCfg.MinHTLC\n}\n\n// unsignedLocalUpdates retrieves the unsigned local updates that we should\n// store upon receiving a revocation. This function is called from\n// ReceiveRevocation. remoteMessageIndex is the height into the local update\n// log that the remote commitment chain tip includes. localMessageIndex\n// is the height into the local update log that the local commitment tail\n// includes. Our local updates that are unsigned by the remote should\n// have height greater than or equal to localMessageIndex (not on our commit),\n// and height less than remoteMessageIndex (on the remote commit).\n//\n// NOTE: remoteMessageIndex is the height on the tip because this is called\n// before the tail is advanced to the tip during ReceiveRevocation.",
      "length": 763,
      "tokens": 118,
      "embedding": []
    },
    {
      "slug": "func (lc *LightningChannel) unsignedLocalUpdates(remoteMessageIndex,",
      "content": "func (lc *LightningChannel) unsignedLocalUpdates(remoteMessageIndex,\n\tlocalMessageIndex uint64, chanID lnwire.ChannelID) []channeldb.LogUpdate {\n\n\tvar localPeerUpdates []channeldb.LogUpdate\n\tfor e := lc.localUpdateLog.Front(); e != nil; e = e.Next() {\n\t\tpd := e.Value.(*PaymentDescriptor)\n\n\t\t// We don't save add updates as they are restored from the\n\t\t// remote commitment in restoreStateLogs.\n\t\tif pd.EntryType == Add {\n\t\t\tcontinue\n\t\t}\n\n\t\t// This is a settle/fail that is on the remote commitment, but\n\t\t// not on the local commitment. We expect this update to be\n\t\t// covered in the next commitment signature that the remote\n\t\t// sends.\n\t\tif pd.LogIndex < remoteMessageIndex && pd.LogIndex >= localMessageIndex {\n\t\t\tlogUpdate := channeldb.LogUpdate{\n\t\t\t\tLogIndex: pd.LogIndex,\n\t\t\t}\n\n\t\t\tswitch pd.EntryType {\n\t\t\tcase FeeUpdate:\n\t\t\t\tlogUpdate.UpdateMsg = &lnwire.UpdateFee{\n\t\t\t\t\tChanID:   chanID,\n\t\t\t\t\tFeePerKw: uint32(pd.Amount.ToSatoshis()),\n\t\t\t\t}\n\t\t\tcase Settle:\n\t\t\t\tlogUpdate.UpdateMsg = &lnwire.UpdateFulfillHTLC{\n\t\t\t\t\tChanID:          chanID,\n\t\t\t\t\tID:              pd.ParentIndex,\n\t\t\t\t\tPaymentPreimage: pd.RPreimage,\n\t\t\t\t}\n\t\t\tcase Fail:\n\t\t\t\tlogUpdate.UpdateMsg = &lnwire.UpdateFailHTLC{\n\t\t\t\t\tChanID: chanID,\n\t\t\t\t\tID:     pd.ParentIndex,\n\t\t\t\t\tReason: pd.FailReason,\n\t\t\t\t}\n\t\t\tcase MalformedFail:\n\t\t\t\tlogUpdate.UpdateMsg = &lnwire.UpdateFailMalformedHTLC{\n\t\t\t\t\tChanID:       chanID,\n\t\t\t\t\tID:           pd.ParentIndex,\n\t\t\t\t\tShaOnionBlob: pd.ShaOnionBlob,\n\t\t\t\t\tFailureCode:  pd.FailCode,\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tlocalPeerUpdates = append(localPeerUpdates, logUpdate)\n\t\t}\n\t}\n\n\treturn localPeerUpdates\n}\n",
      "length": 1472,
      "tokens": 159,
      "embedding": []
    }
  ]
}