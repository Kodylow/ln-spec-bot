{
  "filepath": "../implementations/go/lnd/channeldb/migration_01_to_11/migrations.go",
  "package": "migration_01_to_11",
  "sections": [
    {
      "slug": "func MigrateNodeAndEdgeUpdateIndex(tx kvdb.RwTx) error {",
      "content": "func MigrateNodeAndEdgeUpdateIndex(tx kvdb.RwTx) error {\n\t// First, we'll populating the node portion of the new index. Before we\n\t// can add new values to the index, we'll first create the new bucket\n\t// where these items will be housed.\n\tnodes, err := tx.CreateTopLevelBucket(nodeBucket)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to create node bucket: %v\", err)\n\t}\n\tnodeUpdateIndex, err := nodes.CreateBucketIfNotExists(\n\t\tnodeUpdateIndexBucket,\n\t)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to create node update index: %v\", err)\n\t}\n\n\tlog.Infof(\"Populating new node update index bucket\")\n\n\t// Now that we know the bucket has been created, we'll iterate over the\n\t// entire node bucket so we can add the (updateTime || nodePub) key\n\t// into the node update index.\n\terr = nodes.ForEach(func(nodePub, nodeInfo []byte) error {\n\t\tif len(nodePub) != 33 {\n\t\t\treturn nil\n\t\t}\n\n\t\tlog.Tracef(\"Adding %x to node update index\", nodePub)\n\n\t\t// The first 8 bytes of a node's serialize data is the update\n\t\t// time, so we can extract that without decoding the entire\n\t\t// structure.\n\t\tupdateTime := nodeInfo[:8]\n\n\t\t// Now that we have the update time, we can construct the key\n\t\t// to insert into the index.\n\t\tvar indexKey [8 + 33]byte\n\t\tcopy(indexKey[:8], updateTime)\n\t\tcopy(indexKey[8:], nodePub)\n\n\t\treturn nodeUpdateIndex.Put(indexKey[:], nil)\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to update node indexes: %v\", err)\n\t}\n\n\tlog.Infof(\"Populating new edge update index bucket\")\n\n\t// With the set of nodes updated, we'll now update all edges to have a\n\t// corresponding entry in the edge update index.\n\tedges, err := tx.CreateTopLevelBucket(edgeBucket)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to create edge bucket: %v\", err)\n\t}\n\tedgeUpdateIndex, err := edges.CreateBucketIfNotExists(\n\t\tedgeUpdateIndexBucket,\n\t)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to create edge update index: %v\", err)\n\t}\n\n\t// We'll now run through each edge policy in the database, and update\n\t// the index to ensure each edge has the proper record.\n\terr = edges.ForEach(func(edgeKey, edgePolicyBytes []byte) error {\n\t\tif len(edgeKey) != 41 {\n\t\t\treturn nil\n\t\t}\n\n\t\t// Now that we know this is the proper record, we'll grab the\n\t\t// channel ID (last 8 bytes of the key), and then decode the\n\t\t// edge policy so we can access the update time.\n\t\tchanID := edgeKey[33:]\n\t\tedgePolicyReader := bytes.NewReader(edgePolicyBytes)\n\n\t\tedgePolicy, err := deserializeChanEdgePolicy(\n\t\t\tedgePolicyReader, nodes,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tlog.Tracef(\"Adding chan_id=%v to edge update index\",\n\t\t\tedgePolicy.ChannelID)\n\n\t\t// We'll now construct the index key using the channel ID, and\n\t\t// the last time it was updated: (updateTime || chanID).\n\t\tvar indexKey [8 + 8]byte\n\t\tbyteOrder.PutUint64(\n\t\t\tindexKey[:], uint64(edgePolicy.LastUpdate.Unix()),\n\t\t)\n\t\tcopy(indexKey[8:], chanID)\n\n\t\treturn edgeUpdateIndex.Put(indexKey[:], nil)\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to update edge indexes: %v\", err)\n\t}\n\n\tlog.Infof(\"Migration to node and edge update indexes complete!\")\n\n\treturn nil\n}\n\n// MigrateInvoiceTimeSeries is a database migration that assigns all existing\n// invoices an index in the add and/or the settle index. Additionally, all\n// existing invoices will have their bytes padded out in order to encode the\n// add+settle index as well as the amount paid.",
      "length": 3200,
      "tokens": 488,
      "embedding": []
    },
    {
      "slug": "func MigrateInvoiceTimeSeries(tx kvdb.RwTx) error {",
      "content": "func MigrateInvoiceTimeSeries(tx kvdb.RwTx) error {\n\tinvoices, err := tx.CreateTopLevelBucket(invoiceBucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\taddIndex, err := invoices.CreateBucketIfNotExists(\n\t\taddIndexBucket,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\tsettleIndex, err := invoices.CreateBucketIfNotExists(\n\t\tsettleIndexBucket,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tlog.Infof(\"Migrating invoice database to new time series format\")\n\n\t// Now that we have all the buckets we need, we'll run through each\n\t// invoice in the database, and update it to reflect the new format\n\t// expected post migration.\n\t// NOTE: we store the converted invoices and put them back into the\n\t// database after the loop, since modifying the bucket within the\n\t// ForEach loop is not safe.\n\tvar invoicesKeys [][]byte\n\tvar invoicesValues [][]byte\n\terr = invoices.ForEach(func(invoiceNum, invoiceBytes []byte) error {\n\t\t// If this is a sub bucket, then we'll skip it.\n\t\tif invoiceBytes == nil {\n\t\t\treturn nil\n\t\t}\n\n\t\t// First, we'll make a copy of the encoded invoice bytes.\n\t\tinvoiceBytesCopy := make([]byte, len(invoiceBytes))\n\t\tcopy(invoiceBytesCopy, invoiceBytes)\n\n\t\t// With the bytes copied over, we'll append 24 additional\n\t\t// bytes. We do this so we can decode the invoice under the new\n\t\t// serialization format.\n\t\tpadding := bytes.Repeat([]byte{0}, 24)\n\t\tinvoiceBytesCopy = append(invoiceBytesCopy, padding...)\n\n\t\tinvoiceReader := bytes.NewReader(invoiceBytesCopy)\n\t\tinvoice, err := deserializeInvoiceLegacy(invoiceReader)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to decode invoice: %v\", err)\n\t\t}\n\n\t\t// Now that we have the fully decoded invoice, we can update\n\t\t// the various indexes that we're added, and finally the\n\t\t// invoice itself before re-inserting it.\n\n\t\t// First, we'll get the new sequence in the addIndex in order\n\t\t// to create the proper mapping.\n\t\tnextAddSeqNo, err := addIndex.NextSequence()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tvar seqNoBytes [8]byte\n\t\tbyteOrder.PutUint64(seqNoBytes[:], nextAddSeqNo)\n\t\terr = addIndex.Put(seqNoBytes[:], invoiceNum[:])\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tlog.Tracef(\"Adding invoice (preimage=%x, add_index=%v) to add \"+\n\t\t\t\"time series\", invoice.Terms.PaymentPreimage[:],\n\t\t\tnextAddSeqNo)\n\n\t\t// Next, we'll check if the invoice has been settled or not. If\n\t\t// so, then we'll also add it to the settle index.\n\t\tvar nextSettleSeqNo uint64\n\t\tif invoice.Terms.State == ContractSettled {\n\t\t\tnextSettleSeqNo, err = settleIndex.NextSequence()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tvar seqNoBytes [8]byte\n\t\t\tbyteOrder.PutUint64(seqNoBytes[:], nextSettleSeqNo)\n\t\t\terr := settleIndex.Put(seqNoBytes[:], invoiceNum)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tinvoice.AmtPaid = invoice.Terms.Value\n\n\t\t\tlog.Tracef(\"Adding invoice (preimage=%x, \"+\n\t\t\t\t\"settle_index=%v) to add time series\",\n\t\t\t\tinvoice.Terms.PaymentPreimage[:],\n\t\t\t\tnextSettleSeqNo)\n\t\t}\n\n\t\t// Finally, we'll update the invoice itself with the new\n\t\t// indexing information as well as the amount paid if it has\n\t\t// been settled or not.\n\t\tinvoice.AddIndex = nextAddSeqNo\n\t\tinvoice.SettleIndex = nextSettleSeqNo\n\n\t\t// We've fully migrated an invoice, so we'll now update the\n\t\t// invoice in-place.\n\t\tvar b bytes.Buffer\n\t\tif err := serializeInvoiceLegacy(&b, &invoice); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Save the key and value pending update for after the ForEach\n\t\t// is done.\n\t\tinvoicesKeys = append(invoicesKeys, invoiceNum)\n\t\tinvoicesValues = append(invoicesValues, b.Bytes())\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Now put the converted invoices into the DB.\n\tfor i := range invoicesKeys {\n\t\tkey := invoicesKeys[i]\n\t\tvalue := invoicesValues[i]\n\t\tif err := invoices.Put(key, value); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tlog.Infof(\"Migration to invoice time series index complete!\")\n\n\treturn nil\n}\n\n// MigrateInvoiceTimeSeriesOutgoingPayments is a follow up to the\n// migrateInvoiceTimeSeries migration. As at the time of writing, the\n// OutgoingPayment struct embeddeds an instance of the Invoice struct. As a\n// result, we also need to migrate the internal invoice to the new format.",
      "length": 3919,
      "tokens": 559,
      "embedding": []
    },
    {
      "slug": "func MigrateInvoiceTimeSeriesOutgoingPayments(tx kvdb.RwTx) error {",
      "content": "func MigrateInvoiceTimeSeriesOutgoingPayments(tx kvdb.RwTx) error {\n\tpayBucket := tx.ReadWriteBucket(paymentBucket)\n\tif payBucket == nil {\n\t\treturn nil\n\t}\n\n\tlog.Infof(\"Migrating invoice database to new outgoing payment format\")\n\n\t// We store the keys and values we want to modify since it is not safe\n\t// to modify them directly within the ForEach loop.\n\tvar paymentKeys [][]byte\n\tvar paymentValues [][]byte\n\terr := payBucket.ForEach(func(payID, paymentBytes []byte) error {\n\t\tlog.Tracef(\"Migrating payment %x\", payID[:])\n\n\t\t// The internal invoices for each payment only contain a\n\t\t// populated contract term, and creation date, as a result,\n\t\t// most of the bytes will be \"empty\".\n\n\t\t// We'll calculate the end of the invoice index assuming a\n\t\t// \"minimal\" index that's embedded within the greater\n\t\t// OutgoingPayment. The breakdown is:\n\t\t//  3 bytes empty var bytes, 16 bytes creation date, 16 bytes\n\t\t//  settled date, 32 bytes payment pre-image, 8 bytes value, 1\n\t\t//  byte settled.\n\t\tendOfInvoiceIndex := 1 + 1 + 1 + 16 + 16 + 32 + 8 + 1\n\n\t\t// We'll now extract the prefix of the pure invoice embedded\n\t\t// within.\n\t\tinvoiceBytes := paymentBytes[:endOfInvoiceIndex]\n\n\t\t// With the prefix extracted, we'll copy over the invoice, and\n\t\t// also add padding for the new 24 bytes of fields, and finally\n\t\t// append the remainder of the outgoing payment.\n\t\tpaymentCopy := make([]byte, len(invoiceBytes))\n\t\tcopy(paymentCopy[:], invoiceBytes)\n\n\t\tpadding := bytes.Repeat([]byte{0}, 24)\n\t\tpaymentCopy = append(paymentCopy, padding...)\n\t\tpaymentCopy = append(\n\t\t\tpaymentCopy, paymentBytes[endOfInvoiceIndex:]...,\n\t\t)\n\n\t\t// At this point, we now have the new format of the outgoing\n\t\t// payments, we'll attempt to deserialize it to ensure the\n\t\t// bytes are properly formatted.\n\t\tpaymentReader := bytes.NewReader(paymentCopy)\n\t\t_, err := deserializeOutgoingPayment(paymentReader)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to deserialize payment: %v\", err)\n\t\t}\n\n\t\t// Now that we know the modifications was successful, we'll\n\t\t// store it to our slice of keys and values, and write it back\n\t\t// to disk in the new format after the ForEach loop is over.\n\t\tpaymentKeys = append(paymentKeys, payID)\n\t\tpaymentValues = append(paymentValues, paymentCopy)\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Finally store the updated payments to the bucket.\n\tfor i := range paymentKeys {\n\t\tkey := paymentKeys[i]\n\t\tvalue := paymentValues[i]\n\t\tif err := payBucket.Put(key, value); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tlog.Infof(\"Migration to outgoing payment invoices complete!\")\n\n\treturn nil\n}\n\n// MigrateEdgePolicies is a migration function that will update the edges\n// bucket. It ensure that edges with unknown policies will also have an entry\n// in the bucket. After the migration, there will be two edge entries for\n// every channel, regardless of whether the policies are known.",
      "length": 2731,
      "tokens": 420,
      "embedding": []
    },
    {
      "slug": "func MigrateEdgePolicies(tx kvdb.RwTx) error {",
      "content": "func MigrateEdgePolicies(tx kvdb.RwTx) error {\n\tnodes := tx.ReadWriteBucket(nodeBucket)\n\tif nodes == nil {\n\t\treturn nil\n\t}\n\n\tedges := tx.ReadWriteBucket(edgeBucket)\n\tif edges == nil {\n\t\treturn nil\n\t}\n\n\tedgeIndex := edges.NestedReadWriteBucket(edgeIndexBucket)\n\tif edgeIndex == nil {\n\t\treturn nil\n\t}\n\n\t// checkKey gets the policy from the database with a low-level call\n\t// so that it is still possible to distinguish between unknown and\n\t// not present.\n\tcheckKey := func(channelId uint64, keyBytes []byte) error {\n\t\tvar channelID [8]byte\n\t\tbyteOrder.PutUint64(channelID[:], channelId)\n\n\t\t_, err := fetchChanEdgePolicy(edges,\n\t\t\tchannelID[:], keyBytes, nodes)\n\n\t\tif err == ErrEdgeNotFound {\n\t\t\tlog.Tracef(\"Adding unknown edge policy present for node %x, channel %v\",\n\t\t\t\tkeyBytes, channelId)\n\n\t\t\terr := putChanEdgePolicyUnknown(edges, channelId, keyBytes)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\treturn nil\n\t\t}\n\n\t\treturn err\n\t}\n\n\t// Iterate over all channels and check both edge policies.\n\terr := edgeIndex.ForEach(func(chanID, edgeInfoBytes []byte) error {\n\t\tinfoReader := bytes.NewReader(edgeInfoBytes)\n\t\tedgeInfo, err := deserializeChanEdgeInfo(infoReader)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tfor _, key := range [][]byte{edgeInfo.NodeKey1Bytes[:],\n\t\t\tedgeInfo.NodeKey2Bytes[:]} {\n\n\t\t\tif err := checkKey(edgeInfo.ChannelID, key); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t})\n\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to update edge policies: %v\", err)\n\t}\n\n\tlog.Infof(\"Migration of edge policies complete!\")\n\n\treturn nil\n}\n\n// PaymentStatusesMigration is a database migration intended for adding payment\n// statuses for each existing payment entity in bucket to be able control\n// transitions of statuses and prevent cases such as double payment",
      "length": 1650,
      "tokens": 228,
      "embedding": []
    },
    {
      "slug": "func PaymentStatusesMigration(tx kvdb.RwTx) error {",
      "content": "func PaymentStatusesMigration(tx kvdb.RwTx) error {\n\t// Get the bucket dedicated to storing statuses of payments,\n\t// where a key is payment hash, value is payment status.\n\tpaymentStatuses, err := tx.CreateTopLevelBucket(paymentStatusBucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tlog.Infof(\"Migrating database to support payment statuses\")\n\n\tcircuitAddKey := []byte(\"circuit-adds\")\n\tcircuits := tx.ReadWriteBucket(circuitAddKey)\n\tif circuits != nil {\n\t\tlog.Infof(\"Marking all known circuits with status InFlight\")\n\n\t\terr = circuits.ForEach(func(k, v []byte) error {\n\t\t\t// Parse the first 8 bytes as the short chan ID for the\n\t\t\t// circuit. We'll skip all short chan IDs are not\n\t\t\t// locally initiated, which includes all non-zero short\n\t\t\t// chan ids.\n\t\t\tchanID := binary.BigEndian.Uint64(k[:8])\n\t\t\tif chanID != 0 {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\t// The payment hash is the third item in the serialized\n\t\t\t// payment circuit. The first two items are an AddRef\n\t\t\t// (10 bytes) and the incoming circuit key (16 bytes).\n\t\t\tconst payHashOffset = 10 + 16\n\n\t\t\tpaymentHash := v[payHashOffset : payHashOffset+32]\n\n\t\t\treturn paymentStatuses.Put(\n\t\t\t\tpaymentHash[:], StatusInFlight.Bytes(),\n\t\t\t)\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tlog.Infof(\"Marking all existing payments with status Completed\")\n\n\t// Get the bucket dedicated to storing payments\n\tbucket := tx.ReadWriteBucket(paymentBucket)\n\tif bucket == nil {\n\t\treturn nil\n\t}\n\n\t// For each payment in the bucket, deserialize the payment and mark it\n\t// as completed.\n\terr = bucket.ForEach(func(k, v []byte) error {\n\t\t// Ignores if it is sub-bucket.\n\t\tif v == nil {\n\t\t\treturn nil\n\t\t}\n\n\t\tr := bytes.NewReader(v)\n\t\tpayment, err := deserializeOutgoingPayment(r)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Calculate payment hash for current payment.\n\t\tpaymentHash := sha256.Sum256(payment.PaymentPreimage[:])\n\n\t\t// Update status for current payment to completed. If it fails,\n\t\t// the migration is aborted and the payment bucket is returned\n\t\t// to its previous state.\n\t\treturn paymentStatuses.Put(paymentHash[:], StatusSucceeded.Bytes())\n\t})\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tlog.Infof(\"Migration of payment statuses complete!\")\n\n\treturn nil\n}\n\n// MigratePruneEdgeUpdateIndex is a database migration that attempts to resolve\n// some lingering bugs with regards to edge policies and their update index.\n// Stale entries within the edge update index were not being properly pruned due\n// to a miscalculation on the offset of an edge's policy last update. This\n// migration also fixes the case where the public keys within edge policies were\n// being serialized with an extra byte, causing an even greater error when\n// attempting to perform the offset calculation described earlier.",
      "length": 2583,
      "tokens": 388,
      "embedding": []
    },
    {
      "slug": "func MigratePruneEdgeUpdateIndex(tx kvdb.RwTx) error {",
      "content": "func MigratePruneEdgeUpdateIndex(tx kvdb.RwTx) error {\n\t// To begin the migration, we'll retrieve the update index bucket. If it\n\t// does not exist, we have nothing left to do so we can simply exit.\n\tedges := tx.ReadWriteBucket(edgeBucket)\n\tif edges == nil {\n\t\treturn nil\n\t}\n\tedgeUpdateIndex := edges.NestedReadWriteBucket(edgeUpdateIndexBucket)\n\tif edgeUpdateIndex == nil {\n\t\treturn nil\n\t}\n\n\t// Retrieve some buckets that will be needed later on. These should\n\t// already exist given the assumption that the buckets above do as\n\t// well.\n\tedgeIndex, err := edges.CreateBucketIfNotExists(edgeIndexBucket)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error creating edge index bucket: %s\", err)\n\t}\n\tif edgeIndex == nil {\n\t\treturn fmt.Errorf(\"unable to create/fetch edge index \" +\n\t\t\t\"bucket\")\n\t}\n\tnodes, err := tx.CreateTopLevelBucket(nodeBucket)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to make node bucket\")\n\t}\n\n\tlog.Info(\"Migrating database to properly prune edge update index\")\n\n\t// We'll need to properly prune all the outdated entries within the edge\n\t// update index. To do so, we'll gather all of the existing policies\n\t// within the graph to re-populate them later on.\n\tvar edgeKeys [][]byte\n\terr = edges.ForEach(func(edgeKey, edgePolicyBytes []byte) error {\n\t\t// All valid entries are indexed by a public key (33 bytes)\n\t\t// followed by a channel ID (8 bytes), so we'll skip any entries\n\t\t// with keys that do not match this.\n\t\tif len(edgeKey) != 33+8 {\n\t\t\treturn nil\n\t\t}\n\n\t\tedgeKeys = append(edgeKeys, edgeKey)\n\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to gather existing edge policies: %v\",\n\t\t\terr)\n\t}\n\n\tlog.Info(\"Constructing set of edge update entries to purge.\")\n\n\t// Build the set of keys that we will remove from the edge update index.\n\t// This will include all keys contained within the bucket.\n\tvar updateKeysToRemove [][]byte\n\terr = edgeUpdateIndex.ForEach(func(updKey, _ []byte) error {\n\t\tupdateKeysToRemove = append(updateKeysToRemove, updKey)\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to gather existing edge updates: %v\",\n\t\t\terr)\n\t}\n\n\tlog.Infof(\"Removing %d entries from edge update index.\",\n\t\tlen(updateKeysToRemove))\n\n\t// With the set of keys contained in the edge update index constructed,\n\t// we'll proceed in purging all of them from the index.\n\tfor _, updKey := range updateKeysToRemove {\n\t\tif err := edgeUpdateIndex.Delete(updKey); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tlog.Infof(\"Repopulating edge update index with %d valid entries.\",\n\t\tlen(edgeKeys))\n\n\t// For each edge key, we'll retrieve the policy, deserialize it, and\n\t// re-add it to the different buckets. By doing so, we'll ensure that\n\t// all existing edge policies are serialized correctly within their\n\t// respective buckets and that the correct entries are populated within\n\t// the edge update index.\n\tfor _, edgeKey := range edgeKeys {\n\t\tedgePolicyBytes := edges.Get(edgeKey)\n\n\t\t// Skip any entries with unknown policies as there will not be\n\t\t// any entries for them in the edge update index.\n\t\tif bytes.Equal(edgePolicyBytes[:], unknownPolicy) {\n\t\t\tcontinue\n\t\t}\n\n\t\tedgePolicy, err := deserializeChanEdgePolicy(\n\t\t\tbytes.NewReader(edgePolicyBytes), nodes,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t_, err = updateEdgePolicy(tx, edgePolicy)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tlog.Info(\"Migration to properly prune edge update index complete!\")\n\n\treturn nil\n}\n\n// MigrateOptionalChannelCloseSummaryFields migrates the serialized format of\n// ChannelCloseSummary to a format where optional fields' presence is indicated\n// with boolean markers.",
      "length": 3413,
      "tokens": 517,
      "embedding": []
    },
    {
      "slug": "func MigrateOptionalChannelCloseSummaryFields(tx kvdb.RwTx) error {",
      "content": "func MigrateOptionalChannelCloseSummaryFields(tx kvdb.RwTx) error {\n\tclosedChanBucket := tx.ReadWriteBucket(closedChannelBucket)\n\tif closedChanBucket == nil {\n\t\treturn nil\n\t}\n\n\tlog.Info(\"Migrating to new closed channel format...\")\n\n\t// We store the converted keys and values and put them back into the\n\t// database after the loop, since modifying the bucket within the\n\t// ForEach loop is not safe.\n\tvar closedChansKeys [][]byte\n\tvar closedChansValues [][]byte\n\terr := closedChanBucket.ForEach(func(chanID, summary []byte) error {\n\t\tr := bytes.NewReader(summary)\n\n\t\t// Read the old (v6) format from the database.\n\t\tc, err := deserializeCloseChannelSummaryV6(r)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Serialize using the new format, and put back into the\n\t\t// bucket.\n\t\tvar b bytes.Buffer\n\t\tif err := serializeChannelCloseSummary(&b, c); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Now that we know the modifications was successful, we'll\n\t\t// Store the key and value to our slices, and write it back to\n\t\t// disk in the new format after the ForEach loop is over.\n\t\tclosedChansKeys = append(closedChansKeys, chanID)\n\t\tclosedChansValues = append(closedChansValues, b.Bytes())\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to update closed channels: %v\", err)\n\t}\n\n\t// Now put the new format back into the DB.\n\tfor i := range closedChansKeys {\n\t\tkey := closedChansKeys[i]\n\t\tvalue := closedChansValues[i]\n\t\tif err := closedChanBucket.Put(key, value); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tlog.Info(\"Migration to new closed channel format complete!\")\n\n\treturn nil\n}\n\nvar messageStoreBucket = []byte(\"message-store\")\n\n// MigrateGossipMessageStoreKeys migrates the key format for gossip messages\n// found in the message store to a new one that takes into consideration the of\n// the message being stored.",
      "length": 1681,
      "tokens": 253,
      "embedding": []
    },
    {
      "slug": "func MigrateGossipMessageStoreKeys(tx kvdb.RwTx) error {",
      "content": "func MigrateGossipMessageStoreKeys(tx kvdb.RwTx) error {\n\t// We'll start by retrieving the bucket in which these messages are\n\t// stored within. If there isn't one, there's nothing left for us to do\n\t// so we can avoid the migration.\n\tmessageStore := tx.ReadWriteBucket(messageStoreBucket)\n\tif messageStore == nil {\n\t\treturn nil\n\t}\n\n\tlog.Info(\"Migrating to the gossip message store new key format\")\n\n\t// Otherwise we'll proceed with the migration. We'll start by coalescing\n\t// all the current messages within the store, which are indexed by the\n\t// public key of the peer which they should be sent to, followed by the\n\t// short channel ID of the channel for which the message belongs to. We\n\t// should only expect to find channel announcement signatures as that\n\t// was the only support message type previously.\n\tmsgs := make(map[[33 + 8]byte]*lnwire.AnnounceSignatures)\n\terr := messageStore.ForEach(func(k, v []byte) error {\n\t\tvar msgKey [33 + 8]byte\n\t\tcopy(msgKey[:], k)\n\n\t\tmsg := &lnwire.AnnounceSignatures{}\n\t\tif err := msg.Decode(bytes.NewReader(v), 0); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tmsgs[msgKey] = msg\n\n\t\treturn nil\n\n\t})\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Then, we'll go over all of our messages, remove their previous entry,\n\t// and add another with the new key format. Once we've done this for\n\t// every message, we can consider the migration complete.\n\tfor oldMsgKey, msg := range msgs {\n\t\tif err := messageStore.Delete(oldMsgKey[:]); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Construct the new key for which we'll find this message with\n\t\t// in the store. It'll be the same as the old, but we'll also\n\t\t// include the message type.\n\t\tvar msgType [2]byte\n\t\tbinary.BigEndian.PutUint16(msgType[:], uint16(msg.MsgType()))\n\t\tnewMsgKey := append(oldMsgKey[:], msgType[:]...)\n\n\t\t// Serialize the message with its wire encoding.\n\t\tvar b bytes.Buffer\n\t\tif _, err := lnwire.WriteMessage(&b, msg, 0); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif err := messageStore.Put(newMsgKey, b.Bytes()); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tlog.Info(\"Migration to the gossip message store new key format complete!\")\n\n\treturn nil\n}\n\n// MigrateOutgoingPayments moves the OutgoingPayments into a new bucket format\n// where they all reside in a top-level bucket indexed by the payment hash. In\n// this sub-bucket we store information relevant to this payment, such as the\n// payment status.\n//\n// Since the router cannot handle resumed payments that have the status\n// InFlight (we have no PaymentAttemptInfo available for pre-migration\n// payments) we delete those statuses, so only Completed payments remain in the\n// new bucket structure.",
      "length": 2488,
      "tokens": 398,
      "embedding": []
    },
    {
      "slug": "func MigrateOutgoingPayments(tx kvdb.RwTx) error {",
      "content": "func MigrateOutgoingPayments(tx kvdb.RwTx) error {\n\tlog.Infof(\"Migrating outgoing payments to new bucket structure\")\n\n\toldPayments := tx.ReadWriteBucket(paymentBucket)\n\n\t// Return early if there are no payments to migrate.\n\tif oldPayments == nil {\n\t\tlog.Infof(\"No outgoing payments found, nothing to migrate.\")\n\t\treturn nil\n\t}\n\n\tnewPayments, err := tx.CreateTopLevelBucket(paymentsRootBucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Helper method to get the source pubkey. We define it such that we\n\t// only attempt to fetch it if needed.\n\tsourcePub := func() ([33]byte, error) {\n\t\tvar pub [33]byte\n\t\tnodes := tx.ReadWriteBucket(nodeBucket)\n\t\tif nodes == nil {\n\t\t\treturn pub, ErrGraphNotFound\n\t\t}\n\n\t\tselfPub := nodes.Get(sourceKey)\n\t\tif selfPub == nil {\n\t\t\treturn pub, ErrSourceNodeNotSet\n\t\t}\n\t\tcopy(pub[:], selfPub[:])\n\t\treturn pub, nil\n\t}\n\n\terr = oldPayments.ForEach(func(k, v []byte) error {\n\t\t// Ignores if it is sub-bucket.\n\t\tif v == nil {\n\t\t\treturn nil\n\t\t}\n\n\t\t// Read the old payment format.\n\t\tr := bytes.NewReader(v)\n\t\tpayment, err := deserializeOutgoingPayment(r)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Calculate payment hash from the payment preimage.\n\t\tpaymentHash := sha256.Sum256(payment.PaymentPreimage[:])\n\n\t\t// Now create and add a PaymentCreationInfo to the bucket.\n\t\tc := &PaymentCreationInfo{\n\t\t\tPaymentHash:    paymentHash,\n\t\t\tValue:          payment.Terms.Value,\n\t\t\tCreationDate:   payment.CreationDate,\n\t\t\tPaymentRequest: payment.PaymentRequest,\n\t\t}\n\n\t\tvar infoBuf bytes.Buffer\n\t\tif err := serializePaymentCreationInfo(&infoBuf, c); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tsourcePubKey, err := sourcePub()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Do the same for the PaymentAttemptInfo.\n\t\ttotalAmt := payment.Terms.Value + payment.Fee\n\t\trt := Route{\n\t\t\tTotalTimeLock: payment.TimeLockLength,\n\t\t\tTotalAmount:   totalAmt,\n\t\t\tSourcePubKey:  sourcePubKey,\n\t\t\tHops:          []*Hop{},\n\t\t}\n\t\tfor _, hop := range payment.Path {\n\t\t\trt.Hops = append(rt.Hops, &Hop{\n\t\t\t\tPubKeyBytes:  hop,\n\t\t\t\tAmtToForward: totalAmt,\n\t\t\t})\n\t\t}\n\n\t\t// Since the old format didn't store the fee for individual\n\t\t// hops, we let the last hop eat the whole fee for the total to\n\t\t// add up.\n\t\tif len(rt.Hops) > 0 {\n\t\t\trt.Hops[len(rt.Hops)-1].AmtToForward = payment.Terms.Value\n\t\t}\n\n\t\t// Since we don't have the session key for old payments, we\n\t\t// create a random one to be able to serialize the attempt\n\t\t// info.\n\t\tpriv, _ := btcec.NewPrivateKey()\n\t\ts := &PaymentAttemptInfo{\n\t\t\tPaymentID:  0,    // unknown.\n\t\t\tSessionKey: priv, // unknown.\n\t\t\tRoute:      rt,\n\t\t}\n\n\t\tvar attemptBuf bytes.Buffer\n\t\tif err := serializePaymentAttemptInfoMigration9(&attemptBuf, s); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Reuse the existing payment sequence number.\n\t\tvar seqNum [8]byte\n\t\tcopy(seqNum[:], k)\n\n\t\t// Create a bucket indexed by the payment hash.\n\t\tbucket, err := newPayments.CreateBucket(paymentHash[:])\n\n\t\t// If the bucket already exists, it means that we are migrating\n\t\t// from a database containing duplicate payments to a payment\n\t\t// hash. To keep this information, we store such duplicate\n\t\t// payments in a sub-bucket.\n\t\tif err == kvdb.ErrBucketExists {\n\t\t\tpHashBucket := newPayments.NestedReadWriteBucket(paymentHash[:])\n\n\t\t\t// Create a bucket for duplicate payments within this\n\t\t\t// payment hash's bucket.\n\t\t\tdup, err := pHashBucket.CreateBucketIfNotExists(\n\t\t\t\tpaymentDuplicateBucket,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t// Each duplicate will get its own sub-bucket within\n\t\t\t// this bucket, so use their sequence number to index\n\t\t\t// them by.\n\t\t\tbucket, err = dup.CreateBucket(seqNum[:])\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t} else if err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Store the payment's information to the bucket.\n\t\terr = bucket.Put(paymentSequenceKey, seqNum[:])\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\terr = bucket.Put(paymentCreationInfoKey, infoBuf.Bytes())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\terr = bucket.Put(paymentAttemptInfoKey, attemptBuf.Bytes())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\terr = bucket.Put(paymentSettleInfoKey, payment.PaymentPreimage[:])\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// To continue producing unique sequence numbers, we set the sequence\n\t// of the new bucket to that of the old one.\n\tseq := oldPayments.Sequence()\n\tif err := newPayments.SetSequence(seq); err != nil {\n\t\treturn err\n\t}\n\n\t// Now we delete the old buckets. Deleting the payment status buckets\n\t// deletes all payment statuses other than Complete.\n\terr = tx.DeleteTopLevelBucket(paymentStatusBucket)\n\tif err != nil && err != kvdb.ErrBucketNotFound {\n\t\treturn err\n\t}\n\n\t// Finally delete the old payment bucket.\n\terr = tx.DeleteTopLevelBucket(paymentBucket)\n\tif err != nil && err != kvdb.ErrBucketNotFound {\n\t\treturn err\n\t}\n\n\tlog.Infof(\"Migration of outgoing payment bucket structure completed!\")\n\treturn nil\n}\n",
      "length": 4636,
      "tokens": 659,
      "embedding": []
    }
  ]
}