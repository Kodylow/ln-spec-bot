{
  "filepath": "../implementations/go/lnd/channeldb/migration30/migration_test.go",
  "package": "migration30",
  "sections": [
    {
      "slug": "type (",
      "content": "type (\n\tbeforeMigrationFunc func(db kvdb.Backend) error\n\tafterMigrationFunc  func(t *testing.T, db kvdb.Backend)\n)\n\n// TestMigrateRevocationLog provide a comprehensive test for the revocation log\n// migration. The revocation logs are stored inside a deeply nested bucket, and\n// can be accessed via nodePub:chainHash:fundingOutpoint:revocationLogBucket.\n// Based on each value in the chain, we'd end up in a different db state. This\n// test alters nodePub, fundingOutpoint, and revocationLogBucket to test\n// against possible db states, leaving the chainHash staying the same as it's\n// less likely to be changed. In specific, we test based on whether we have one\n// or two peers(nodePub). For each peer, we test whether we have one or two\n// channels(fundingOutpoint). And for each channel, we test 5 cases based on\n// the revocation migration states(see buildChannelCases). The total states\n// grow quickly and the test may take longer than 5min.",
      "length": 927,
      "tokens": 141,
      "embedding": []
    },
    {
      "slug": "func TestMigrateRevocationLog(t *testing.T) {",
      "content": "func TestMigrateRevocationLog(t *testing.T) {\n\tt.Parallel()\n\n\ttestCases := make([]*testCase, 0)\n\n\t// Create two peers, each has two channels.\n\talice1, alice2 := createTwoChannels()\n\tbob1, bob2 := createTwoChannels()\n\n\t// Sort the two peers to match the order saved in boltdb.\n\tif bytes.Compare(\n\t\talice1.IdentityPub.SerializeCompressed(),\n\t\tbob1.IdentityPub.SerializeCompressed(),\n\t) > 0 {\n\n\t\talice1, bob1 = bob1, alice1\n\t\talice2, bob2 = bob2, alice2\n\t}\n\n\t// Build test cases for two peers. Each peer is independent so we\n\t// combine the test cases based on its current db state. This would\n\t// create a total of 30x30=900 cases.\n\tfor _, p1 := range buildPeerCases(alice1, alice2, false) {\n\t\tfor _, p2 := range buildPeerCases(bob1, bob2, p1.unfinished) {\n\t\t\tsetups := make([]beforeMigrationFunc, 0)\n\t\t\tsetups = append(setups, p1.setups...)\n\t\t\tsetups = append(setups, p2.setups...)\n\n\t\t\tasserters := make([]afterMigrationFunc, 0)\n\t\t\tasserters = append(asserters, p1.asserters...)\n\t\t\tasserters = append(asserters, p2.asserters...)\n\n\t\t\tname := fmt.Sprintf(\"alice: %s, bob: %s\",\n\t\t\t\tp1.name, p2.name)\n\n\t\t\ttc := &testCase{\n\t\t\t\tname:      name,\n\t\t\t\tsetups:    setups,\n\t\t\t\tasserters: asserters,\n\t\t\t}\n\t\t\ttestCases = append(testCases, tc)\n\t\t}\n\t}\n\n\tfmt.Printf(\"Running %d test cases...\\n\", len(testCases))\n\tfmt.Printf(\"withAmtData is set to: %v\\n\", withAmtData)\n\n\tfor i, tc := range testCases {\n\t\ttc := tc\n\n\t\t// Construct a test case name that can be easily traced.\n\t\tname := fmt.Sprintf(\"case_%d\", i)\n\t\tfmt.Println(name, tc.name)\n\n\t\tsuccess := t.Run(name, func(t *testing.T) {\n\t\t\t// Log the test's actual name on failure.\n\t\t\tt.Log(\"Test setup: \", tc.name)\n\n\t\t\tbeforeMigration := func(db kvdb.Backend) error {\n\t\t\t\tfor _, setup := range tc.setups {\n\t\t\t\t\tif err := setup(db); err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\tafterMigration := func(db kvdb.Backend) error {\n\t\t\t\tfor _, asserter := range tc.asserters {\n\t\t\t\t\tasserter(t, db)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\tcfg := &MigrateRevLogConfigImpl{\n\t\t\t\tNoAmountData: !withAmtData,\n\t\t\t}\n\n\t\t\tmigtest.ApplyMigrationWithDB(\n\t\t\t\tt,\n\t\t\t\tbeforeMigration,\n\t\t\t\tafterMigration,\n\t\t\t\tfunc(db kvdb.Backend) error {\n\t\t\t\t\treturn MigrateRevocationLog(db, cfg)\n\t\t\t\t},\n\t\t\t\tfalse,\n\t\t\t)\n\t\t})\n\t\tif !success {\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// TestValidateMigration checks that the function `validateMigration` behaves\n// as expected.",
      "length": 2218,
      "tokens": 293,
      "embedding": []
    },
    {
      "slug": "func TestValidateMigration(t *testing.T) {",
      "content": "func TestValidateMigration(t *testing.T) {\n\tc := createTestChannel(nil)\n\n\ttestCases := []struct {\n\t\tname       string\n\t\tsetup      func(db kvdb.Backend) error\n\t\texpectFail bool\n\t}{\n\t\t{\n\t\t\t// Finished prior to v0.15.0.\n\t\t\tname: \"valid migration\",\n\t\t\tsetup: func(db kvdb.Backend) error {\n\t\t\t\treturn createFinished(db, c, true)\n\t\t\t},\n\t\t\texpectFail: false,\n\t\t},\n\t\t{\n\t\t\t// Finished after to v0.15.0.\n\t\t\tname: \"valid migration after v0.15.0\",\n\t\t\tsetup: func(db kvdb.Backend) error {\n\t\t\t\treturn createFinished(db, c, false)\n\t\t\t},\n\t\t\texpectFail: false,\n\t\t},\n\t\t{\n\t\t\t// Missing logs prior to v0.15.0.\n\t\t\tname: \"invalid migration\",\n\t\t\tsetup: func(db kvdb.Backend) error {\n\t\t\t\treturn createNotFinished(db, c, true)\n\t\t\t},\n\t\t\texpectFail: true,\n\t\t},\n\t\t{\n\t\t\t// Missing logs after to v0.15.0.\n\t\t\tname: \"invalid migration after v0.15.0\",\n\t\t\tsetup: func(db kvdb.Backend) error {\n\t\t\t\treturn createNotFinished(db, c, false)\n\t\t\t},\n\t\t\texpectFail: true,\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\ttc := tc\n\n\t\t// Create a test db.\n\t\tcdb, err := migtest.MakeDB(t)\n\t\trequire.NoError(t, err, \"failed to create test db\")\n\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\t// Setup test logs.\n\t\t\terr := tc.setup(cdb)\n\t\t\trequire.NoError(t, err, \"failed to setup\")\n\n\t\t\t// Call the actual function and check the error is\n\t\t\t// returned as expected.\n\t\t\terr = kvdb.Update(cdb, validateMigration, func() {})\n\n\t\t\tif tc.expectFail {\n\t\t\t\trequire.Error(t, err)\n\t\t\t} else {\n\t\t\t\trequire.NoError(t, err)\n\t\t\t}\n\t\t})\n\t}\n}\n\n// createTwoChannels creates two channels that have the same chainHash and\n// IdentityPub, simulating having two channels under the same peer.",
      "length": 1500,
      "tokens": 208,
      "embedding": []
    },
    {
      "slug": "func createTwoChannels() (*mig26.OpenChannel, *mig26.OpenChannel) {",
      "content": "func createTwoChannels() (*mig26.OpenChannel, *mig26.OpenChannel) {\n\t// Create two channels under the same peer.\n\tc1 := createTestChannel(nil)\n\tc2 := createTestChannel(c1.IdentityPub)\n\n\t// If c1 is greater than c2, boltdb will put c2 before c1.\n\tif bytes.Compare(\n\t\tc1.FundingOutpoint.Hash[:],\n\t\tc2.FundingOutpoint.Hash[:],\n\t) > 0 {\n\n\t\tc1, c2 = c2, c1\n\t}\n\n\treturn c1, c2\n}\n\n// channelTestCase defines a single test case given a particular channel state.",
      "length": 369,
      "tokens": 57,
      "embedding": []
    },
    {
      "slug": "type channelTestCase struct {",
      "content": "type channelTestCase struct {\n\tname       string\n\tsetup      beforeMigrationFunc\n\tasserter   afterMigrationFunc\n\tunfinished bool\n}\n\n// buildChannelCases builds five channel test cases. These cases can be viewed\n// as basic units that are used to build more complex test cases based on\n// number of channels and peers.",
      "length": 279,
      "tokens": 42,
      "embedding": []
    },
    {
      "slug": "func buildChannelCases(c *mig26.OpenChannel,",
      "content": "func buildChannelCases(c *mig26.OpenChannel,\n\toverwrite bool) []*channelTestCase {\n\n\t// assertNewLogs is a helper closure that checks the old bucket and the\n\t// two new logs are saved.\n\tassertNewLogs := func(t *testing.T, db kvdb.Backend) {\n\t\t// Check that the old bucket is removed.\n\t\tassertOldLogBucketDeleted(t, db, c)\n\n\t\tl := fetchNewLog(t, db, c, logHeight1)\n\t\tassertRevocationLog(t, newLog1, l)\n\n\t\tl = fetchNewLog(t, db, c, logHeight2)\n\t\tassertRevocationLog(t, newLog2, l)\n\t}\n\n\t// case1 defines a case where we don't have a chanBucket.\n\tcase1 := &channelTestCase{\n\t\tname: \"no channel\",\n\t\tsetup: func(db kvdb.Backend) error {\n\t\t\treturn setupTestLogs(db, nil, nil, nil)\n\t\t},\n\t\t// No need to assert anything.\n\t\tasserter: func(t *testing.T, db kvdb.Backend) {},\n\t}\n\n\t// case2 defines a case when the chanBucket has no old revocation logs.\n\tcase2 := &channelTestCase{\n\t\tname: \"empty old logs\",\n\t\tsetup: func(db kvdb.Backend) error {\n\t\t\treturn setupTestLogs(db, c, nil, nil)\n\t\t},\n\t\t// No need to assert anything.\n\t\tasserter: func(t *testing.T, db kvdb.Backend) {},\n\t}\n\n\t// case3 defines a case when the chanBucket has finished its migration.\n\tcase3 := &channelTestCase{\n\t\tname: \"finished migration\",\n\t\tsetup: func(db kvdb.Backend) error {\n\t\t\treturn createFinished(db, c, true)\n\t\t},\n\t\tasserter: func(t *testing.T, db kvdb.Backend) {\n\t\t\t// Check that the old bucket is removed.\n\t\t\tassertOldLogBucketDeleted(t, db, c)\n\n\t\t\t// Fetch the new log. We should see\n\t\t\t// OurOutputIndex matching the testOurIndex\n\t\t\t// value, indicating that for migrated logs we\n\t\t\t// won't touch them.\n\t\t\t//\n\t\t\t// NOTE: when the log is created before\n\t\t\t// migration, OurOutputIndex would be\n\t\t\t// testOurIndex rather than OutputIndexEmpty.\n\t\t\tl := fetchNewLog(t, db, c, logHeight1)\n\t\t\trequire.EqualValues(\n\t\t\t\tt, testOurIndex, l.OurOutputIndex,\n\t\t\t\t\"expected log to be NOT overwritten\",\n\t\t\t)\n\n\t\t\t// Fetch the new log. We should see\n\t\t\t// TheirOutputIndex matching the testTheirIndex\n\t\t\t// value, indicating that for migrated logs we\n\t\t\t// won't touch them.\n\t\t\t//\n\t\t\t// NOTE: when the log is created before\n\t\t\t// migration, TheirOutputIndex would be\n\t\t\t// testTheirIndex rather than OutputIndexEmpty.\n\t\t\tl = fetchNewLog(t, db, c, logHeight2)\n\t\t\trequire.EqualValues(\n\t\t\t\tt, testTheirIndex, l.TheirOutputIndex,\n\t\t\t\t\"expected log to be NOT overwritten\",\n\t\t\t)\n\t\t},\n\t}\n\n\t// case4 defines a case when the chanBucket has both old and new logs,\n\t// which happens when the migration is ongoing.\n\tcase4 := &channelTestCase{\n\t\tname: \"unfinished migration\",\n\t\tsetup: func(db kvdb.Backend) error {\n\t\t\treturn createNotFinished(db, c, true)\n\t\t},\n\t\tasserter: func(t *testing.T, db kvdb.Backend) {\n\t\t\t// Check that the old bucket is removed.\n\t\t\tassertOldLogBucketDeleted(t, db, c)\n\n\t\t\t// Fetch the new log. We should see\n\t\t\t// OurOutputIndex matching the testOurIndex\n\t\t\t// value, indicating that for migrated logs we\n\t\t\t// won't touch them.\n\t\t\t//\n\t\t\t// NOTE: when the log is created before\n\t\t\t// migration, OurOutputIndex would be\n\t\t\t// testOurIndex rather than OutputIndexEmpty.\n\t\t\tl := fetchNewLog(t, db, c, logHeight1)\n\t\t\trequire.EqualValues(\n\t\t\t\tt, testOurIndex, l.OurOutputIndex,\n\t\t\t\t\"expected log to be NOT overwritten\",\n\t\t\t)\n\n\t\t\t// We expect to have one new log.\n\t\t\tl = fetchNewLog(t, db, c, logHeight2)\n\t\t\tassertRevocationLog(t, newLog2, l)\n\t\t},\n\t\tunfinished: true,\n\t}\n\n\t// case5 defines a case when the chanBucket has no new logs, which\n\t// happens when we haven't migrated anything for this bucket yet.\n\tcase5 := &channelTestCase{\n\t\tname: \"initial migration\",\n\t\tsetup: func(db kvdb.Backend) error {\n\t\t\treturn createNotStarted(db, c, true)\n\t\t},\n\t\tasserter:   assertNewLogs,\n\t\tunfinished: true,\n\t}\n\n\t// Check that the already migrated logs are overwritten. For two\n\t// channels sorted and stored in boltdb, when the first channel has\n\t// unfinished migrations, even channel two has migrated logs, they will\n\t// be overwritten to make sure the data stay consistent.\n\tif overwrite {\n\t\tcase3.name += \" overwritten\"\n\t\tcase3.asserter = assertNewLogs\n\n\t\tcase4.name += \" overwritten\"\n\t\tcase4.asserter = assertNewLogs\n\t}\n\n\treturn []*channelTestCase{case1, case2, case3, case4, case5}\n}\n\n// testCase defines a case for a particular db state that we want to test based\n// on whether we have one or two peers, one or two channels for each peer, and\n// the particular state for each channel.",
      "length": 4165,
      "tokens": 605,
      "embedding": []
    },
    {
      "slug": "type testCase struct {",
      "content": "type testCase struct {\n\t// name has the format: peer: [channel state].\n\tname string\n\n\t// setups is a list of setup functions we'd run sequentially to provide\n\t// the initial db state.\n\tsetups []beforeMigrationFunc\n\n\t// asserters is a list of assertions we'd perform after the migration\n\t// function has been called.\n\tasserters []afterMigrationFunc\n\n\t// unfinished specifies that the test case is testing a case where the\n\t// revocation migration is considered unfinished. This is useful if\n\t// it's used to construct a larger test case where there's a following\n\t// case with a state of finished, we can then test that the revocation\n\t// logs are overwritten even if the state says finished.\n\tunfinished bool\n}\n\n// buildPeerCases builds test cases based on whether we have one or two\n// channels saved under this peer. When there's one channel, we have 5 states,\n// and when there are two, we have 25 states, a total of 30 cases.",
      "length": 885,
      "tokens": 154,
      "embedding": []
    },
    {
      "slug": "func buildPeerCases(c1, c2 *mig26.OpenChannel, unfinished bool) []*testCase {",
      "content": "func buildPeerCases(c1, c2 *mig26.OpenChannel, unfinished bool) []*testCase {\n\ttestCases := make([]*testCase, 0)\n\n\t// Single peer with one channel.\n\tfor _, c := range buildChannelCases(c1, unfinished) {\n\t\tname := fmt.Sprintf(\"[channel: %s]\", c.name)\n\t\ttc := &testCase{\n\t\t\tname:       name,\n\t\t\tsetups:     []beforeMigrationFunc{c.setup},\n\t\t\tasserters:  []afterMigrationFunc{c.asserter},\n\t\t\tunfinished: c.unfinished,\n\t\t}\n\t\ttestCases = append(testCases, tc)\n\t}\n\n\t// Single peer with two channels.\n\ttestCases = append(\n\t\ttestCases, buildTwoChannelCases(c1, c2, unfinished)...,\n\t)\n\n\treturn testCases\n}\n\n// buildTwoChannelCases takes two channels to build test cases that covers all\n// combinations of the two channels' state. Since each channel has 5 states,\n// this will give us a total 25 states.",
      "length": 691,
      "tokens": 91,
      "embedding": []
    },
    {
      "slug": "func buildTwoChannelCases(c1, c2 *mig26.OpenChannel,",
      "content": "func buildTwoChannelCases(c1, c2 *mig26.OpenChannel,\n\tunfinished bool) []*testCase {\n\n\ttestCases := make([]*testCase, 0)\n\n\t// buildCase is a helper closure that contructs a test case based on\n\t// the two smaller test cases.\n\tbuildCase := func(tc1, tc2 *channelTestCase) {\n\t\tsetups := make([]beforeMigrationFunc, 0)\n\t\tsetups = append(setups, tc1.setup)\n\t\tsetups = append(setups, tc2.setup)\n\n\t\tasserters := make([]afterMigrationFunc, 0)\n\t\tasserters = append(asserters, tc1.asserter)\n\t\tasserters = append(asserters, tc2.asserter)\n\n\t\t// If any of the test cases has unfinished state, the test case\n\t\t// would have a state of unfinished, indicating any peers after\n\t\t// this one must overwrite their revocation logs.\n\t\tunfinished := tc1.unfinished || tc2.unfinished\n\n\t\tname := fmt.Sprintf(\"[channelOne: %s] [channelTwo: %s]\",\n\t\t\ttc1.name, tc2.name)\n\n\t\ttc := &testCase{\n\t\t\tname:       name,\n\t\t\tsetups:     setups,\n\t\t\tasserters:  asserters,\n\t\t\tunfinished: unfinished,\n\t\t}\n\t\ttestCases = append(testCases, tc)\n\t}\n\n\t// Build channel cases for both of the channels and combine them.\n\tfor _, tc1 := range buildChannelCases(c1, unfinished) {\n\t\t// The second channel's already migrated logs will be\n\t\t// overwritten if the first channel has unfinished state, which\n\t\t// are case4 and case5.\n\t\tunfinished := unfinished || tc1.unfinished\n\t\tfor _, tc2 := range buildChannelCases(c2, unfinished) {\n\t\t\tbuildCase(tc1, tc2)\n\t\t}\n\t}\n\n\treturn testCases\n}\n\n// assertOldLogBucketDeleted asserts that the given channel's old revocation\n// log bucket doesn't exist.",
      "length": 1437,
      "tokens": 197,
      "embedding": []
    },
    {
      "slug": "func assertOldLogBucketDeleted(t testing.TB, cdb kvdb.Backend,",
      "content": "func assertOldLogBucketDeleted(t testing.TB, cdb kvdb.Backend,\n\tc *mig26.OpenChannel) {\n\n\tvar logBucket kvdb.RBucket\n\terr := kvdb.Update(cdb, func(tx kvdb.RwTx) error {\n\t\tchanBucket, err := mig25.FetchChanBucket(tx, &c.OpenChannel)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tlogBucket = chanBucket.NestedReadBucket(\n\t\t\trevocationLogBucketDeprecated,\n\t\t)\n\t\treturn err\n\t}, func() {})\n\n\trequire.NoError(t, err, \"read bucket failed\")\n\trequire.Nil(t, logBucket, \"expected old bucket to be deleted\")\n}\n\n// fetchNewLog asserts a revocation log can be found using the given updateNum\n// for the specified channel.",
      "length": 518,
      "tokens": 68,
      "embedding": []
    },
    {
      "slug": "func fetchNewLog(t testing.TB, cdb kvdb.Backend,",
      "content": "func fetchNewLog(t testing.TB, cdb kvdb.Backend,\n\tc *mig26.OpenChannel, updateNum uint64) RevocationLog {\n\n\tvar newLog RevocationLog\n\terr := kvdb.Update(cdb, func(tx kvdb.RwTx) error {\n\t\tchanBucket, err := mig25.FetchChanBucket(tx, &c.OpenChannel)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tlogBucket, err := fetchLogBucket(chanBucket)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tnewLog, err = fetchRevocationLog(logBucket, updateNum)\n\t\treturn err\n\t}, func() {})\n\n\trequire.NoError(t, err, \"failed to query revocation log\")\n\n\treturn newLog\n}\n\n// assertRevocationLog asserts two revocation logs are equal.",
      "length": 522,
      "tokens": 69,
      "embedding": []
    },
    {
      "slug": "func assertRevocationLog(t testing.TB, want, got RevocationLog) {",
      "content": "func assertRevocationLog(t testing.TB, want, got RevocationLog) {\n\trequire.Equal(t, want.OurOutputIndex, got.OurOutputIndex,\n\t\t\"wrong OurOutputIndex\")\n\trequire.Equal(t, want.TheirOutputIndex, got.TheirOutputIndex,\n\t\t\"wrong TheirOutputIndex\")\n\trequire.Equal(t, want.CommitTxHash, got.CommitTxHash,\n\t\t\"wrong CommitTxHash\")\n\trequire.Equal(t, want.TheirBalance, got.TheirBalance,\n\t\t\"wrong TheirBalance\")\n\trequire.Equal(t, want.OurBalance, got.OurBalance,\n\t\t\"wrong OurBalance\")\n\trequire.Equal(t, len(want.HTLCEntries), len(got.HTLCEntries),\n\t\t\"wrong HTLCEntries length\")\n\n\tfor i, expectedHTLC := range want.HTLCEntries {\n\t\thtlc := got.HTLCEntries[i]\n\t\trequire.Equal(t, expectedHTLC.Amt, htlc.Amt, \"wrong Amt\")\n\t\trequire.Equal(t, expectedHTLC.RHash, htlc.RHash, \"wrong RHash\")\n\t\trequire.Equal(t, expectedHTLC.Incoming, htlc.Incoming,\n\t\t\t\"wrong Incoming\")\n\t\trequire.Equal(t, expectedHTLC.OutputIndex, htlc.OutputIndex,\n\t\t\t\"wrong OutputIndex\")\n\t\trequire.Equal(t, expectedHTLC.RefundTimeout, htlc.RefundTimeout,\n\t\t\t\"wrong RefundTimeout\")\n\t}\n}\n\n// BenchmarkMigration creates a benchmark test for the migration. The test uses\n// the flag `-benchtime` to specify how many revocation logs we want to test.",
      "length": 1099,
      "tokens": 94,
      "embedding": []
    },
    {
      "slug": "func BenchmarkMigration(b *testing.B) {",
      "content": "func BenchmarkMigration(b *testing.B) {\n\t// Stop the timer and start it again later when the actual migration\n\t// starts.\n\tb.StopTimer()\n\n\t// Gather number of records by reading `-benchtime` flag.\n\tnumLogs := b.N\n\n\t// Create a mock store.\n\tmockStore := &mockStore{}\n\tmockStore.On(\"AddNextEntry\", mock.Anything).Return(nil)\n\tmockStore.On(\"Encode\", mock.Anything).Return(nil)\n\n\t// Build the test data.\n\toldLogs := make([]mig.ChannelCommitment, numLogs)\n\tbeforeMigration := func(db kvdb.Backend) error {\n\t\tfmt.Printf(\"\\nBuilding test data for %d logs...\\n\", numLogs)\n\t\tdefer fmt.Println(\"Finished building test data, migrating...\")\n\n\t\t// We use a mock store here to bypass the check in\n\t\t// `AddNextEntry` so we don't need a \"read\" preimage here. This\n\t\t// shouldn't affect our benchmark result as the migration will\n\t\t// load the actual store from db.\n\t\tc := createTestChannel(nil)\n\t\tc.RevocationStore = mockStore\n\n\t\t// Create the test logs.\n\t\tfor i := 0; i < numLogs; i++ {\n\t\t\toldLog := oldLog2\n\t\t\toldLog.CommitHeight = uint64(i)\n\t\t\toldLogs[i] = oldLog\n\t\t}\n\n\t\treturn setupTestLogs(db, c, oldLogs, nil)\n\t}\n\n\tcfg := &MigrateRevLogConfigImpl{\n\t\tNoAmountData: !withAmtData,\n\t}\n\n\t// Run the migration test.\n\tmigtest.ApplyMigrationWithDB(\n\t\tb,\n\t\tbeforeMigration,\n\t\tnil,\n\t\tfunc(db kvdb.Backend) error {\n\t\t\tb.StartTimer()\n\t\t\tdefer b.StopTimer()\n\n\t\t\treturn MigrateRevocationLog(db, cfg)\n\t\t},\n\t\tfalse,\n\t)\n}\n",
      "length": 1303,
      "tokens": 173,
      "embedding": []
    }
  ]
}