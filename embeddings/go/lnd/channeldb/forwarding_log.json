{
  "filepath": "../implementations/go/lnd/channeldb/forwarding_log.go",
  "package": "channeldb",
  "sections": [
    {
      "slug": "func (d *DB) ForwardingLog() *ForwardingLog {",
      "content": "func (d *DB) ForwardingLog() *ForwardingLog {\n\treturn &ForwardingLog{\n\t\tdb: d,\n\t}\n}\n\n// ForwardingLog is a time series database that logs the fulfillment of payment\n// circuits by a lightning network daemon. The log contains a series of\n// forwarding events which map a timestamp to a forwarding event. A forwarding\n// event describes which channels were used to create+settle a circuit, and the\n// amount involved. Subtracting the outgoing amount from the incoming amount\n// reveals the fee charged for the forwarding service.",
      "length": 471,
      "tokens": 78,
      "embedding": []
    },
    {
      "slug": "type ForwardingLog struct {",
      "content": "type ForwardingLog struct {\n\tdb *DB\n}\n\n// ForwardingEvent is an event in the forwarding log's time series. Each\n// forwarding event logs the creation and tear-down of a payment circuit. A\n// circuit is created once an incoming HTLC has been fully forwarded, and\n// destroyed once the payment has been settled.",
      "length": 275,
      "tokens": 49,
      "embedding": []
    },
    {
      "slug": "type ForwardingEvent struct {",
      "content": "type ForwardingEvent struct {\n\t// Timestamp is the settlement time of this payment circuit.\n\tTimestamp time.Time\n\n\t// IncomingChanID is the incoming channel ID of the payment circuit.\n\tIncomingChanID lnwire.ShortChannelID\n\n\t// OutgoingChanID is the outgoing channel ID of the payment circuit.\n\tOutgoingChanID lnwire.ShortChannelID\n\n\t// AmtIn is the amount of the incoming HTLC. Subtracting this from the\n\t// outgoing amount gives the total fees of this payment circuit.\n\tAmtIn lnwire.MilliSatoshi\n\n\t// AmtOut is the amount of the outgoing HTLC. Subtracting the incoming\n\t// amount from this gives the total fees for this payment circuit.\n\tAmtOut lnwire.MilliSatoshi\n}\n\n// encodeForwardingEvent writes out the target forwarding event to the passed\n// io.Writer, using the expected DB format. Note that the timestamp isn't\n// serialized as this will be the key value within the bucket.",
      "length": 833,
      "tokens": 126,
      "embedding": []
    },
    {
      "slug": "func encodeForwardingEvent(w io.Writer, f *ForwardingEvent) error {",
      "content": "func encodeForwardingEvent(w io.Writer, f *ForwardingEvent) error {\n\treturn WriteElements(\n\t\tw, f.IncomingChanID, f.OutgoingChanID, f.AmtIn, f.AmtOut,\n\t)\n}\n\n// decodeForwardingEvent attempts to decode the raw bytes of a serialized\n// forwarding event into the target ForwardingEvent. Note that the timestamp\n// won't be decoded, as the caller is expected to set this due to the bucket\n// structure of the forwarding log.",
      "length": 344,
      "tokens": 53,
      "embedding": []
    },
    {
      "slug": "func decodeForwardingEvent(r io.Reader, f *ForwardingEvent) error {",
      "content": "func decodeForwardingEvent(r io.Reader, f *ForwardingEvent) error {\n\treturn ReadElements(\n\t\tr, &f.IncomingChanID, &f.OutgoingChanID, &f.AmtIn, &f.AmtOut,\n\t)\n}\n\n// AddForwardingEvents adds a series of forwarding events to the database.\n// Before inserting, the set of events will be sorted according to their\n// timestamp. This ensures that all writes to disk are sequential.",
      "length": 299,
      "tokens": 44,
      "embedding": []
    },
    {
      "slug": "func (f *ForwardingLog) AddForwardingEvents(events []ForwardingEvent) error {",
      "content": "func (f *ForwardingLog) AddForwardingEvents(events []ForwardingEvent) error {\n\t// Before we create the database transaction, we'll ensure that the set\n\t// of forwarding events are properly sorted according to their\n\t// timestamp and that no duplicate timestamps exist to avoid collisions\n\t// in the key we are going to store the events under.\n\tmakeUniqueTimestamps(events)\n\n\tvar timestamp [8]byte\n\n\treturn kvdb.Batch(f.db.Backend, func(tx kvdb.RwTx) error {\n\t\t// First, we'll fetch the bucket that stores our time series\n\t\t// log.\n\t\tlogBucket, err := tx.CreateTopLevelBucket(\n\t\t\tforwardingLogBucket,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// With the bucket obtained, we can now begin to write out the\n\t\t// series of events.\n\t\tfor _, event := range events {\n\t\t\terr := storeEvent(logBucket, event, timestamp[:])\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t})\n}\n\n// storeEvent tries to store a forwarding event into the given bucket by trying\n// to avoid collisions. If a key for the event timestamp already exists in the\n// database, the timestamp is incremented in nanosecond intervals until a \"free\"\n// slot is found.",
      "length": 1024,
      "tokens": 169,
      "embedding": []
    },
    {
      "slug": "func storeEvent(bucket walletdb.ReadWriteBucket, event ForwardingEvent,",
      "content": "func storeEvent(bucket walletdb.ReadWriteBucket, event ForwardingEvent,\n\ttimestampScratchSpace []byte) error {\n\n\t// First, we'll serialize this timestamp into our\n\t// timestamp buffer.\n\tbyteOrder.PutUint64(\n\t\ttimestampScratchSpace, uint64(event.Timestamp.UnixNano()),\n\t)\n\n\t// Next we'll loop until we find a \"free\" slot in the bucket to store\n\t// the event under. This should almost never happen unless we're running\n\t// on a system that has a very bad system clock that doesn't properly\n\t// resolve to nanosecond scale. We try up to 100 times (which would come\n\t// to a maximum shift of 0.1 microsecond which is acceptable for most\n\t// use cases). If we don't find a free slot, we just give up and let\n\t// the collision happen. Something must be wrong with the data in that\n\t// case, even on a very fast machine forwarding payments _will_ take a\n\t// few microseconds at least so we should find a nanosecond slot\n\t// somewhere.\n\tconst maxTries = 100\n\ttries := 0\n\tfor tries < maxTries {\n\t\tval := bucket.Get(timestampScratchSpace)\n\t\tif val == nil {\n\t\t\tbreak\n\t\t}\n\n\t\t// Collision, try the next nanosecond timestamp.\n\t\tnextNano := event.Timestamp.UnixNano() + 1\n\t\tevent.Timestamp = time.Unix(0, nextNano)\n\t\tbyteOrder.PutUint64(timestampScratchSpace, uint64(nextNano))\n\t\ttries++\n\t}\n\n\t// With the key encoded, we'll then encode the event\n\t// into our buffer, then write it out to disk.\n\tvar eventBytes [forwardingEventSize]byte\n\teventBuf := bytes.NewBuffer(eventBytes[0:0:forwardingEventSize])\n\terr := encodeForwardingEvent(eventBuf, &event)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn bucket.Put(timestampScratchSpace, eventBuf.Bytes())\n}\n\n// ForwardingEventQuery represents a query to the forwarding log payment\n// circuit time series database. The query allows a caller to retrieve all\n// records for a particular time slice, offset in that time slice, limiting the\n// total number of responses returned.",
      "length": 1778,
      "tokens": 270,
      "embedding": []
    },
    {
      "slug": "type ForwardingEventQuery struct {",
      "content": "type ForwardingEventQuery struct {\n\t// StartTime is the start time of the time slice.\n\tStartTime time.Time\n\n\t// EndTime is the end time of the time slice.\n\tEndTime time.Time\n\n\t// IndexOffset is the offset within the time slice to start at. This\n\t// can be used to start the response at a particular record.\n\tIndexOffset uint32\n\n\t// NumMaxEvents is the max number of events to return.\n\tNumMaxEvents uint32\n}\n\n// ForwardingLogTimeSlice is the response to a forwarding query. It includes\n// the original query, the set  events that match the query, and an integer\n// which represents the offset index of the last item in the set of returned\n// events. This integer allows callers to resume their query using this offset\n// in the event that the query's response exceeds the max number of returnable\n// events.",
      "length": 752,
      "tokens": 133,
      "embedding": []
    },
    {
      "slug": "type ForwardingLogTimeSlice struct {",
      "content": "type ForwardingLogTimeSlice struct {\n\tForwardingEventQuery\n\n\t// ForwardingEvents is the set of events in our time series that answer\n\t// the query embedded above.\n\tForwardingEvents []ForwardingEvent\n\n\t// LastIndexOffset is the index of the last element in the set of\n\t// returned ForwardingEvents above. Callers can use this to resume\n\t// their query in the event that the time slice has too many events to\n\t// fit into a single response.\n\tLastIndexOffset uint32\n}\n\n// Query allows a caller to query the forwarding event time series for a\n// particular time slice. The caller can control the precise time as well as\n// the number of events to be returned.\n//\n// TODO(roasbeef): rename?",
      "length": 631,
      "tokens": 108,
      "embedding": []
    },
    {
      "slug": "func (f *ForwardingLog) Query(q ForwardingEventQuery) (ForwardingLogTimeSlice, error) {",
      "content": "func (f *ForwardingLog) Query(q ForwardingEventQuery) (ForwardingLogTimeSlice, error) {\n\tvar resp ForwardingLogTimeSlice\n\n\t// If the user provided an index offset, then we'll not know how many\n\t// records we need to skip. We'll also keep track of the record offset\n\t// as that's part of the final return value.\n\trecordsToSkip := q.IndexOffset\n\trecordOffset := q.IndexOffset\n\n\terr := kvdb.View(f.db, func(tx kvdb.RTx) error {\n\t\t// If the bucket wasn't found, then there aren't any events to\n\t\t// be returned.\n\t\tlogBucket := tx.ReadBucket(forwardingLogBucket)\n\t\tif logBucket == nil {\n\t\t\treturn ErrNoForwardingEvents\n\t\t}\n\n\t\t// We'll be using a cursor to seek into the database, so we'll\n\t\t// populate byte slices that represent the start of the key\n\t\t// space we're interested in, and the end.\n\t\tvar startTime, endTime [8]byte\n\t\tbyteOrder.PutUint64(startTime[:], uint64(q.StartTime.UnixNano()))\n\t\tbyteOrder.PutUint64(endTime[:], uint64(q.EndTime.UnixNano()))\n\n\t\t// If we know that a set of log events exists, then we'll begin\n\t\t// our seek through the log in order to satisfy the query.\n\t\t// We'll continue until either we reach the end of the range,\n\t\t// or reach our max number of events.\n\t\tlogCursor := logBucket.ReadCursor()\n\t\ttimestamp, events := logCursor.Seek(startTime[:])\n\t\tfor ; timestamp != nil && bytes.Compare(timestamp, endTime[:]) <= 0; timestamp, events = logCursor.Next() {\n\t\t\t// If our current return payload exceeds the max number\n\t\t\t// of events, then we'll exit now.\n\t\t\tif uint32(len(resp.ForwardingEvents)) >= q.NumMaxEvents {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\t// If we're not yet past the user defined offset, then\n\t\t\t// we'll continue to seek forward.\n\t\t\tif recordsToSkip > 0 {\n\t\t\t\trecordsToSkip--\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tcurrentTime := time.Unix(\n\t\t\t\t0, int64(byteOrder.Uint64(timestamp)),\n\t\t\t)\n\n\t\t\t// At this point, we've skipped enough records to start\n\t\t\t// to collate our query. For each record, we'll\n\t\t\t// increment the final record offset so the querier can\n\t\t\t// utilize pagination to seek further.\n\t\t\treadBuf := bytes.NewReader(events)\n\t\t\tfor readBuf.Len() != 0 {\n\t\t\t\tvar event ForwardingEvent\n\t\t\t\terr := decodeForwardingEvent(readBuf, &event)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\tevent.Timestamp = currentTime\n\t\t\t\tresp.ForwardingEvents = append(resp.ForwardingEvents, event)\n\n\t\t\t\trecordOffset++\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t}, func() {\n\t\tresp = ForwardingLogTimeSlice{\n\t\t\tForwardingEventQuery: q,\n\t\t}\n\t})\n\tif err != nil && err != ErrNoForwardingEvents {\n\t\treturn ForwardingLogTimeSlice{}, err\n\t}\n\n\tresp.LastIndexOffset = recordOffset\n\n\treturn resp, nil\n}\n\n// makeUniqueTimestamps takes a slice of forwarding events, sorts it by the\n// event timestamps and then makes sure there are no duplicates in the\n// timestamps. If duplicates are found, some of the timestamps are increased on\n// the nanosecond scale until only unique values remain. This is a fix to\n// address the problem that in some environments (looking at you, Windows) the\n// system clock has such a bad resolution that two serial invocations of\n// time.Now() might return the same timestamp, even if some time has elapsed\n// between the calls.",
      "length": 2956,
      "tokens": 438,
      "embedding": []
    },
    {
      "slug": "func makeUniqueTimestamps(events []ForwardingEvent) {",
      "content": "func makeUniqueTimestamps(events []ForwardingEvent) {\n\tsort.Slice(events, func(i, j int) bool {\n\t\treturn events[i].Timestamp.Before(events[j].Timestamp)\n\t})\n\n\t// Now that we know the events are sorted by timestamp, we can go\n\t// through the list and fix all duplicates until only unique values\n\t// remain.\n\tfor outer := 0; outer < len(events)-1; outer++ {\n\t\tcurrent := events[outer].Timestamp.UnixNano()\n\t\tnext := events[outer+1].Timestamp.UnixNano()\n\n\t\t// We initially sorted the slice. So if the current is now\n\t\t// greater or equal to the next one, it's either because it's a\n\t\t// duplicate or because we increased the current in the last\n\t\t// iteration.\n\t\tif current >= next {\n\t\t\tnext = current + 1\n\t\t\tevents[outer+1].Timestamp = time.Unix(0, next)\n\t\t}\n\t}\n}\n",
      "length": 687,
      "tokens": 107,
      "embedding": []
    }
  ]
}