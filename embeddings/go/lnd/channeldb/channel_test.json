{
  "filepath": "../implementations/go/lnd/channeldb/channel_test.go",
  "package": "channeldb",
  "sections": [
    {
      "slug": "type testChannelParams struct {",
      "content": "type testChannelParams struct {\n\t// channel is the channel that will be written to disk.\n\tchannel *OpenChannel\n\n\t// addr is the address that the channel will be synced pending with.\n\taddr *net.TCPAddr\n\n\t// pendingHeight is the height that the channel should be recorded as\n\t// pending.\n\tpendingHeight uint32\n\n\t// openChannel is set to true if the channel should be fully marked as\n\t// open if this is false, the channel will be left in pending state.\n\topenChannel bool\n}\n\n// testChannelOption is a functional option which can be used to alter the\n// default channel that is creates for testing.",
      "length": 546,
      "tokens": 96,
      "embedding": []
    },
    {
      "slug": "type testChannelOption func(params *testChannelParams)",
      "content": "type testChannelOption func(params *testChannelParams)\n\n// pendingHeightOption is an option which can be used to set the height the\n// channel is marked as pending at.",
      "length": 110,
      "tokens": 21,
      "embedding": []
    },
    {
      "slug": "func pendingHeightOption(height uint32) testChannelOption {",
      "content": "func pendingHeightOption(height uint32) testChannelOption {\n\treturn func(params *testChannelParams) {\n\t\tparams.pendingHeight = height\n\t}\n}\n\n// openChannelOption is an option which can be used to create a test channel\n// that is open.",
      "length": 167,
      "tokens": 27,
      "embedding": []
    },
    {
      "slug": "func openChannelOption() testChannelOption {",
      "content": "func openChannelOption() testChannelOption {\n\treturn func(params *testChannelParams) {\n\t\tparams.openChannel = true\n\t}\n}\n\n// localHtlcsOption is an option which allows setting of htlcs on the local\n// commitment.",
      "length": 160,
      "tokens": 24,
      "embedding": []
    },
    {
      "slug": "func localHtlcsOption(htlcs []HTLC) testChannelOption {",
      "content": "func localHtlcsOption(htlcs []HTLC) testChannelOption {\n\treturn func(params *testChannelParams) {\n\t\tparams.channel.LocalCommitment.Htlcs = htlcs\n\t}\n}\n\n// remoteHtlcsOption is an option which allows setting of htlcs on the remote\n// commitment.",
      "length": 181,
      "tokens": 24,
      "embedding": []
    },
    {
      "slug": "func remoteHtlcsOption(htlcs []HTLC) testChannelOption {",
      "content": "func remoteHtlcsOption(htlcs []HTLC) testChannelOption {\n\treturn func(params *testChannelParams) {\n\t\tparams.channel.RemoteCommitment.Htlcs = htlcs\n\t}\n}\n\n// loadFwdPkgs is a helper method that reads all forwarding packages for a\n// particular packager.",
      "length": 188,
      "tokens": 25,
      "embedding": []
    },
    {
      "slug": "func loadFwdPkgs(t *testing.T, db kvdb.Backend,",
      "content": "func loadFwdPkgs(t *testing.T, db kvdb.Backend,\n\tpackager FwdPackager) []*FwdPkg {\n\n\tvar (\n\t\tfwdPkgs []*FwdPkg\n\t\terr     error\n\t)\n\n\terr = kvdb.View(db, func(tx kvdb.RTx) error {\n\t\tfwdPkgs, err = packager.LoadFwdPkgs(tx)\n\t\treturn err\n\t}, func() {})\n\trequire.NoError(t, err, \"unable to load fwd pkgs\")\n\n\treturn fwdPkgs\n}\n\n// localShutdownOption is an option which sets the local upfront shutdown\n// script for the channel.",
      "length": 355,
      "tokens": 53,
      "embedding": []
    },
    {
      "slug": "func localShutdownOption(addr lnwire.DeliveryAddress) testChannelOption {",
      "content": "func localShutdownOption(addr lnwire.DeliveryAddress) testChannelOption {\n\treturn func(params *testChannelParams) {\n\t\tparams.channel.LocalShutdownScript = addr\n\t}\n}\n\n// remoteShutdownOption is an option which sets the remote upfront shutdown\n// script for the channel.",
      "length": 188,
      "tokens": 25,
      "embedding": []
    },
    {
      "slug": "func remoteShutdownOption(addr lnwire.DeliveryAddress) testChannelOption {",
      "content": "func remoteShutdownOption(addr lnwire.DeliveryAddress) testChannelOption {\n\treturn func(params *testChannelParams) {\n\t\tparams.channel.RemoteShutdownScript = addr\n\t}\n}\n\n// fundingPointOption is an option which sets the funding outpoint of the\n// channel.",
      "length": 172,
      "tokens": 23,
      "embedding": []
    },
    {
      "slug": "func fundingPointOption(chanPoint wire.OutPoint) testChannelOption {",
      "content": "func fundingPointOption(chanPoint wire.OutPoint) testChannelOption {\n\treturn func(params *testChannelParams) {\n\t\tparams.channel.FundingOutpoint = chanPoint\n\t}\n}\n\n// channelIDOption is an option which sets the short channel ID of the channel.\nvar channelIDOption = func(chanID lnwire.ShortChannelID) testChannelOption {\n\treturn func(params *testChannelParams) {\n\t\tparams.channel.ShortChannelID = chanID\n\t}\n}\n\n// createTestChannel writes a test channel to the database. It takes a set of\n// functional options which can be used to overwrite the default of creating\n// a pending channel that was broadcast at height 100.",
      "length": 534,
      "tokens": 76,
      "embedding": []
    },
    {
      "slug": "func createTestChannel(t *testing.T, cdb *ChannelStateDB,",
      "content": "func createTestChannel(t *testing.T, cdb *ChannelStateDB,\n\topts ...testChannelOption) *OpenChannel {\n\n\t// Create a default set of parameters.\n\tparams := &testChannelParams{\n\t\tchannel:       createTestChannelState(t, cdb),\n\t\taddr:          defaultAddr,\n\t\topenChannel:   false,\n\t\tpendingHeight: uint32(defaultPendingHeight),\n\t}\n\n\t// Apply all functional options to the test channel params.\n\tfor _, o := range opts {\n\t\to(params)\n\t}\n\n\t// Mark the channel as pending.\n\terr := params.channel.SyncPending(params.addr, params.pendingHeight)\n\tif err != nil {\n\t\tt.Fatalf(\"unable to save and serialize channel \"+\n\t\t\t\"state: %v\", err)\n\t}\n\n\t// If the parameters do not specify that we should open the channel\n\t// fully, we return the pending channel.\n\tif !params.openChannel {\n\t\treturn params.channel\n\t}\n\n\t// Mark the channel as open with the short channel id provided.\n\terr = params.channel.MarkAsOpen(params.channel.ShortChannelID)\n\trequire.NoError(t, err, \"unable to mark channel open\")\n\n\treturn params.channel\n}\n",
      "length": 911,
      "tokens": 120,
      "embedding": []
    },
    {
      "slug": "func createTestChannelState(t *testing.T, cdb *ChannelStateDB) *OpenChannel {",
      "content": "func createTestChannelState(t *testing.T, cdb *ChannelStateDB) *OpenChannel {\n\t// Simulate 1000 channel updates.\n\tproducer, err := shachain.NewRevocationProducerFromBytes(key[:])\n\trequire.NoError(t, err, \"could not get producer\")\n\tstore := shachain.NewRevocationStore()\n\tfor i := 0; i < 1; i++ {\n\t\tpreImage, err := producer.AtIndex(uint64(i))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"could not get \"+\n\t\t\t\t\"preimage: %v\", err)\n\t\t}\n\n\t\tif err := store.AddNextEntry(preImage); err != nil {\n\t\t\tt.Fatalf(\"could not add entry: %v\", err)\n\t\t}\n\t}\n\n\tlocalCfg := ChannelConfig{\n\t\tChannelConstraints: ChannelConstraints{\n\t\t\tDustLimit:        btcutil.Amount(rand.Int63()),\n\t\t\tMaxPendingAmount: lnwire.MilliSatoshi(rand.Int63()),\n\t\t\tChanReserve:      btcutil.Amount(rand.Int63()),\n\t\t\tMinHTLC:          lnwire.MilliSatoshi(rand.Int63()),\n\t\t\tMaxAcceptedHtlcs: uint16(rand.Int31()),\n\t\t\tCsvDelay:         uint16(rand.Int31()),\n\t\t},\n\t\tMultiSigKey: keychain.KeyDescriptor{\n\t\t\tPubKey: privKey.PubKey(),\n\t\t},\n\t\tRevocationBasePoint: keychain.KeyDescriptor{\n\t\t\tPubKey: privKey.PubKey(),\n\t\t},\n\t\tPaymentBasePoint: keychain.KeyDescriptor{\n\t\t\tPubKey: privKey.PubKey(),\n\t\t},\n\t\tDelayBasePoint: keychain.KeyDescriptor{\n\t\t\tPubKey: privKey.PubKey(),\n\t\t},\n\t\tHtlcBasePoint: keychain.KeyDescriptor{\n\t\t\tPubKey: privKey.PubKey(),\n\t\t},\n\t}\n\tremoteCfg := ChannelConfig{\n\t\tChannelConstraints: ChannelConstraints{\n\t\t\tDustLimit:        btcutil.Amount(rand.Int63()),\n\t\t\tMaxPendingAmount: lnwire.MilliSatoshi(rand.Int63()),\n\t\t\tChanReserve:      btcutil.Amount(rand.Int63()),\n\t\t\tMinHTLC:          lnwire.MilliSatoshi(rand.Int63()),\n\t\t\tMaxAcceptedHtlcs: uint16(rand.Int31()),\n\t\t\tCsvDelay:         uint16(rand.Int31()),\n\t\t},\n\t\tMultiSigKey: keychain.KeyDescriptor{\n\t\t\tPubKey: privKey.PubKey(),\n\t\t\tKeyLocator: keychain.KeyLocator{\n\t\t\t\tFamily: keychain.KeyFamilyMultiSig,\n\t\t\t\tIndex:  9,\n\t\t\t},\n\t\t},\n\t\tRevocationBasePoint: keychain.KeyDescriptor{\n\t\t\tPubKey: privKey.PubKey(),\n\t\t\tKeyLocator: keychain.KeyLocator{\n\t\t\t\tFamily: keychain.KeyFamilyRevocationBase,\n\t\t\t\tIndex:  8,\n\t\t\t},\n\t\t},\n\t\tPaymentBasePoint: keychain.KeyDescriptor{\n\t\t\tPubKey: privKey.PubKey(),\n\t\t\tKeyLocator: keychain.KeyLocator{\n\t\t\t\tFamily: keychain.KeyFamilyPaymentBase,\n\t\t\t\tIndex:  7,\n\t\t\t},\n\t\t},\n\t\tDelayBasePoint: keychain.KeyDescriptor{\n\t\t\tPubKey: privKey.PubKey(),\n\t\t\tKeyLocator: keychain.KeyLocator{\n\t\t\t\tFamily: keychain.KeyFamilyDelayBase,\n\t\t\t\tIndex:  6,\n\t\t\t},\n\t\t},\n\t\tHtlcBasePoint: keychain.KeyDescriptor{\n\t\t\tPubKey: privKey.PubKey(),\n\t\t\tKeyLocator: keychain.KeyLocator{\n\t\t\t\tFamily: keychain.KeyFamilyHtlcBase,\n\t\t\t\tIndex:  5,\n\t\t\t},\n\t\t},\n\t}\n\n\tchanID := lnwire.NewShortChanIDFromInt(uint64(rand.Int63()))\n\n\t// Increment the uniqueOutputIndex so we always get a unique value for\n\t// the funding outpoint.\n\tuniqueOutputIndex.Add(1)\n\top := wire.OutPoint{Hash: key, Index: uniqueOutputIndex.Load()}\n\n\treturn &OpenChannel{\n\t\tChanType:          SingleFunderBit | FrozenBit,\n\t\tChainHash:         key,\n\t\tFundingOutpoint:   op,\n\t\tShortChannelID:    chanID,\n\t\tIsInitiator:       true,\n\t\tIsPending:         true,\n\t\tIdentityPub:       pubKey,\n\t\tCapacity:          btcutil.Amount(10000),\n\t\tLocalChanCfg:      localCfg,\n\t\tRemoteChanCfg:     remoteCfg,\n\t\tTotalMSatSent:     8,\n\t\tTotalMSatReceived: 2,\n\t\tLocalCommitment: ChannelCommitment{\n\t\t\tCommitHeight:  0,\n\t\t\tLocalBalance:  lnwire.MilliSatoshi(9000),\n\t\t\tRemoteBalance: lnwire.MilliSatoshi(3000),\n\t\t\tCommitFee:     btcutil.Amount(rand.Int63()),\n\t\t\tFeePerKw:      btcutil.Amount(5000),\n\t\t\tCommitTx:      channels.TestFundingTx,\n\t\t\tCommitSig:     bytes.Repeat([]byte{1}, 71),\n\t\t},\n\t\tRemoteCommitment: ChannelCommitment{\n\t\t\tCommitHeight:  0,\n\t\t\tLocalBalance:  lnwire.MilliSatoshi(3000),\n\t\t\tRemoteBalance: lnwire.MilliSatoshi(9000),\n\t\t\tCommitFee:     btcutil.Amount(rand.Int63()),\n\t\t\tFeePerKw:      btcutil.Amount(5000),\n\t\t\tCommitTx:      channels.TestFundingTx,\n\t\t\tCommitSig:     bytes.Repeat([]byte{1}, 71),\n\t\t},\n\t\tNumConfsRequired:        4,\n\t\tRemoteCurrentRevocation: privKey.PubKey(),\n\t\tRemoteNextRevocation:    privKey.PubKey(),\n\t\tRevocationProducer:      producer,\n\t\tRevocationStore:         store,\n\t\tDb:                      cdb,\n\t\tPackager:                NewChannelPackager(chanID),\n\t\tFundingTxn:              channels.TestFundingTx,\n\t\tThawHeight:              uint32(defaultPendingHeight),\n\t\tInitialLocalBalance:     lnwire.MilliSatoshi(9000),\n\t\tInitialRemoteBalance:    lnwire.MilliSatoshi(3000),\n\t}\n}\n",
      "length": 4136,
      "tokens": 297,
      "embedding": []
    },
    {
      "slug": "func TestOpenChannelPutGetDelete(t *testing.T) {",
      "content": "func TestOpenChannelPutGetDelete(t *testing.T) {\n\tt.Parallel()\n\n\tfullDB, err := MakeTestDB(t)\n\trequire.NoError(t, err, \"unable to make test database\")\n\n\tcdb := fullDB.ChannelStateDB()\n\n\t// Create the test channel state, with additional htlcs on the local\n\t// and remote commitment.\n\tlocalHtlcs := []HTLC{\n\t\t{Signature: testSig.Serialize(),\n\t\t\tIncoming:      true,\n\t\t\tAmt:           10,\n\t\t\tRHash:         key,\n\t\t\tRefundTimeout: 1,\n\t\t\tOnionBlob:     []byte(\"onionblob\"),\n\t\t},\n\t}\n\n\tremoteHtlcs := []HTLC{\n\t\t{\n\t\t\tSignature:     testSig.Serialize(),\n\t\t\tIncoming:      false,\n\t\t\tAmt:           10,\n\t\t\tRHash:         key,\n\t\t\tRefundTimeout: 1,\n\t\t\tOnionBlob:     []byte(\"onionblob\"),\n\t\t},\n\t}\n\n\tstate := createTestChannel(\n\t\tt, cdb,\n\t\tremoteHtlcsOption(remoteHtlcs),\n\t\tlocalHtlcsOption(localHtlcs),\n\t)\n\n\topenChannels, err := cdb.FetchOpenChannels(state.IdentityPub)\n\trequire.NoError(t, err, \"unable to fetch open channel\")\n\n\tnewState := openChannels[0]\n\n\t// The decoded channel state should be identical to what we stored\n\t// above.\n\tif !reflect.DeepEqual(state, newState) {\n\t\tt.Fatalf(\"channel state doesn't match:: %v vs %v\",\n\t\t\tspew.Sdump(state), spew.Sdump(newState))\n\t}\n\n\t// We'll also test that the channel is properly able to hot swap the\n\t// next revocation for the state machine. This tests the initial\n\t// post-funding revocation exchange.\n\tnextRevKey, err := btcec.NewPrivateKey()\n\trequire.NoError(t, err, \"unable to create new private key\")\n\tif err := state.InsertNextRevocation(nextRevKey.PubKey()); err != nil {\n\t\tt.Fatalf(\"unable to update revocation: %v\", err)\n\t}\n\n\topenChannels, err = cdb.FetchOpenChannels(state.IdentityPub)\n\trequire.NoError(t, err, \"unable to fetch open channel\")\n\tupdatedChan := openChannels[0]\n\n\t// Ensure that the revocation was set properly.\n\tif !nextRevKey.PubKey().IsEqual(updatedChan.RemoteNextRevocation) {\n\t\tt.Fatalf(\"next revocation wasn't updated\")\n\t}\n\n\t// Finally to wrap up the test, delete the state of the channel within\n\t// the database. This involves \"closing\" the channel which removes all\n\t// written state, and creates a small \"summary\" elsewhere within the\n\t// database.\n\tcloseSummary := &ChannelCloseSummary{\n\t\tChanPoint:         state.FundingOutpoint,\n\t\tRemotePub:         state.IdentityPub,\n\t\tSettledBalance:    btcutil.Amount(500),\n\t\tTimeLockedBalance: btcutil.Amount(10000),\n\t\tIsPending:         false,\n\t\tCloseType:         CooperativeClose,\n\t}\n\tif err := state.CloseChannel(closeSummary); err != nil {\n\t\tt.Fatalf(\"unable to close channel: %v\", err)\n\t}\n\n\t// As the channel is now closed, attempting to fetch all open channels\n\t// for our fake node ID should return an empty slice.\n\topenChans, err := cdb.FetchOpenChannels(state.IdentityPub)\n\trequire.NoError(t, err, \"unable to fetch open channels\")\n\tif len(openChans) != 0 {\n\t\tt.Fatalf(\"all channels not deleted, found %v\", len(openChans))\n\t}\n\n\t// Additionally, attempting to fetch all the open channels globally\n\t// should yield no results.\n\topenChans, err = cdb.FetchAllChannels()\n\tif err != nil {\n\t\tt.Fatal(\"unable to fetch all open chans\")\n\t}\n\tif len(openChans) != 0 {\n\t\tt.Fatalf(\"all channels not deleted, found %v\", len(openChans))\n\t}\n}\n\n// TestOptionalShutdown tests the reading and writing of channels with and\n// without optional shutdown script fields.",
      "length": 3113,
      "tokens": 381,
      "embedding": []
    },
    {
      "slug": "func TestOptionalShutdown(t *testing.T) {",
      "content": "func TestOptionalShutdown(t *testing.T) {\n\tlocal := lnwire.DeliveryAddress([]byte(\"local shutdown script\"))\n\tremote := lnwire.DeliveryAddress([]byte(\"remote shutdown script\"))\n\n\tif _, err := rand.Read(remote); err != nil {\n\t\tt.Fatalf(\"Could not create random script: %v\", err)\n\t}\n\n\ttests := []struct {\n\t\tname           string\n\t\tlocalShutdown  lnwire.DeliveryAddress\n\t\tremoteShutdown lnwire.DeliveryAddress\n\t}{\n\t\t{\n\t\t\tname:           \"no shutdown scripts\",\n\t\t\tlocalShutdown:  nil,\n\t\t\tremoteShutdown: nil,\n\t\t},\n\t\t{\n\t\t\tname:           \"local shutdown script\",\n\t\t\tlocalShutdown:  local,\n\t\t\tremoteShutdown: nil,\n\t\t},\n\t\t{\n\t\t\tname:           \"remote shutdown script\",\n\t\t\tlocalShutdown:  nil,\n\t\t\tremoteShutdown: remote,\n\t\t},\n\t\t{\n\t\t\tname:           \"both scripts set\",\n\t\t\tlocalShutdown:  local,\n\t\t\tremoteShutdown: remote,\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\ttest := test\n\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tfullDB, err := MakeTestDB(t)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"unable to make test database: %v\", err)\n\t\t\t}\n\n\t\t\tcdb := fullDB.ChannelStateDB()\n\n\t\t\t// Create a channel with upfront scripts set as\n\t\t\t// specified in the test.\n\t\t\tstate := createTestChannel(\n\t\t\t\tt, cdb,\n\t\t\t\tlocalShutdownOption(test.localShutdown),\n\t\t\t\tremoteShutdownOption(test.remoteShutdown),\n\t\t\t)\n\n\t\t\topenChannels, err := cdb.FetchOpenChannels(\n\t\t\t\tstate.IdentityPub,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"unable to fetch open\"+\n\t\t\t\t\t\" channel: %v\", err)\n\t\t\t}\n\n\t\t\tif len(openChannels) != 1 {\n\t\t\t\tt.Fatalf(\"Expected one channel open,\"+\n\t\t\t\t\t\" got: %v\", len(openChannels))\n\t\t\t}\n\n\t\t\tif !bytes.Equal(openChannels[0].LocalShutdownScript,\n\t\t\t\ttest.localShutdown) {\n\n\t\t\t\tt.Fatalf(\"Expected local: %x, got: %x\",\n\t\t\t\t\ttest.localShutdown,\n\t\t\t\t\topenChannels[0].LocalShutdownScript)\n\t\t\t}\n\n\t\t\tif !bytes.Equal(openChannels[0].RemoteShutdownScript,\n\t\t\t\ttest.remoteShutdown) {\n\n\t\t\t\tt.Fatalf(\"Expected remote: %x, got: %x\",\n\t\t\t\t\ttest.remoteShutdown,\n\t\t\t\t\topenChannels[0].RemoteShutdownScript)\n\t\t\t}\n\t\t})\n\t}\n}\n",
      "length": 1842,
      "tokens": 196,
      "embedding": []
    },
    {
      "slug": "func assertCommitmentEqual(t *testing.T, a, b *ChannelCommitment) {",
      "content": "func assertCommitmentEqual(t *testing.T, a, b *ChannelCommitment) {\n\tif !reflect.DeepEqual(a, b) {\n\t\t_, _, line, _ := runtime.Caller(1)\n\t\tt.Fatalf(\"line %v: commitments don't match: %v vs %v\",\n\t\t\tline, spew.Sdump(a), spew.Sdump(b))\n\t}\n}\n\n// assertRevocationLogEntryEqual asserts that, for all the fields of a given\n// revocation log entry, their values match those on a given ChannelCommitment.",
      "length": 318,
      "tokens": 46,
      "embedding": []
    },
    {
      "slug": "func assertRevocationLogEntryEqual(t *testing.T, c *ChannelCommitment,",
      "content": "func assertRevocationLogEntryEqual(t *testing.T, c *ChannelCommitment,\n\tr *RevocationLog) {\n\n\t// Check the common fields.\n\trequire.EqualValues(\n\t\tt, r.CommitTxHash, c.CommitTx.TxHash(), \"CommitTx mismatch\",\n\t)\n\n\t// Now check the common fields from the HTLCs.\n\trequire.Equal(t, len(r.HTLCEntries), len(c.Htlcs), \"HTLCs len mismatch\")\n\tfor i, rHtlc := range r.HTLCEntries {\n\t\tcHtlc := c.Htlcs[i]\n\t\trequire.Equal(t, rHtlc.RHash, cHtlc.RHash, \"RHash mismatch\")\n\t\trequire.Equal(t, rHtlc.Amt, cHtlc.Amt.ToSatoshis(),\n\t\t\t\"Amt mismatch\")\n\t\trequire.Equal(t, rHtlc.RefundTimeout, cHtlc.RefundTimeout,\n\t\t\t\"RefundTimeout mismatch\")\n\t\trequire.EqualValues(t, rHtlc.OutputIndex, cHtlc.OutputIndex,\n\t\t\t\"OutputIndex mismatch\")\n\t\trequire.Equal(t, rHtlc.Incoming, cHtlc.Incoming,\n\t\t\t\"Incoming mismatch\")\n\t}\n}\n",
      "length": 697,
      "tokens": 67,
      "embedding": []
    },
    {
      "slug": "func TestChannelStateTransition(t *testing.T) {",
      "content": "func TestChannelStateTransition(t *testing.T) {\n\tt.Parallel()\n\n\tfullDB, err := MakeTestDB(t)\n\trequire.NoError(t, err, \"unable to make test database\")\n\n\tcdb := fullDB.ChannelStateDB()\n\n\t// First create a minimal channel, then perform a full sync in order to\n\t// persist the data.\n\tchannel := createTestChannel(t, cdb)\n\n\t// Add some HTLCs which were added during this new state transition.\n\t// Half of the HTLCs are incoming, while the other half are outgoing.\n\tvar (\n\t\thtlcs   []HTLC\n\t\thtlcAmt lnwire.MilliSatoshi\n\t)\n\tfor i := uint32(0); i < 10; i++ {\n\t\tvar incoming bool\n\t\tif i > 5 {\n\t\t\tincoming = true\n\t\t}\n\t\thtlc := HTLC{\n\t\t\tSignature:     testSig.Serialize(),\n\t\t\tIncoming:      incoming,\n\t\t\tAmt:           10,\n\t\t\tRHash:         key,\n\t\t\tRefundTimeout: i,\n\t\t\tOutputIndex:   int32(i * 3),\n\t\t\tLogIndex:      uint64(i * 2),\n\t\t\tHtlcIndex:     uint64(i),\n\t\t}\n\t\thtlc.OnionBlob = make([]byte, 10)\n\t\tcopy(htlc.OnionBlob[:], bytes.Repeat([]byte{2}, 10))\n\t\thtlcs = append(htlcs, htlc)\n\t\thtlcAmt += htlc.Amt\n\t}\n\n\t// Create a new channel delta which includes the above HTLCs, some\n\t// balance updates, and an increment of the current commitment height.\n\t// Additionally, modify the signature and commitment transaction.\n\tnewSequence := uint32(129498)\n\tnewSig := bytes.Repeat([]byte{3}, 71)\n\tnewTx := channel.LocalCommitment.CommitTx.Copy()\n\tnewTx.TxIn[0].Sequence = newSequence\n\tcommitment := ChannelCommitment{\n\t\tCommitHeight:    1,\n\t\tLocalLogIndex:   2,\n\t\tLocalHtlcIndex:  1,\n\t\tRemoteLogIndex:  2,\n\t\tRemoteHtlcIndex: 1,\n\t\tLocalBalance:    lnwire.MilliSatoshi(1e8),\n\t\tRemoteBalance:   lnwire.MilliSatoshi(1e8),\n\t\tCommitFee:       55,\n\t\tFeePerKw:        99,\n\t\tCommitTx:        newTx,\n\t\tCommitSig:       newSig,\n\t\tHtlcs:           htlcs,\n\t}\n\n\t// First update the local node's broadcastable state and also add a\n\t// CommitDiff remote node's as well in order to simulate a proper state\n\t// transition.\n\tunsignedAckedUpdates := []LogUpdate{\n\t\t{\n\t\t\tLogIndex: 2,\n\t\t\tUpdateMsg: &lnwire.UpdateAddHTLC{\n\t\t\t\tChanID:    lnwire.ChannelID{1, 2, 3},\n\t\t\t\tExtraData: make([]byte, 0),\n\t\t\t},\n\t\t},\n\t}\n\n\t_, err = channel.UpdateCommitment(&commitment, unsignedAckedUpdates)\n\trequire.NoError(t, err, \"unable to update commitment\")\n\n\t// Assert that update is correctly written to the database.\n\tdbUnsignedAckedUpdates, err := channel.UnsignedAckedUpdates()\n\trequire.NoError(t, err, \"unable to fetch dangling remote updates\")\n\tif len(dbUnsignedAckedUpdates) != 1 {\n\t\tt.Fatalf(\"unexpected number of dangling remote updates\")\n\t}\n\tif !reflect.DeepEqual(\n\t\tdbUnsignedAckedUpdates[0], unsignedAckedUpdates[0],\n\t) {\n\n\t\tt.Fatalf(\"unexpected update: expected %v, got %v\",\n\t\t\tspew.Sdump(unsignedAckedUpdates[0]),\n\t\t\tspew.Sdump(dbUnsignedAckedUpdates))\n\t}\n\n\t// The balances, new update, the HTLCs and the changes to the fake\n\t// commitment transaction along with the modified signature should all\n\t// have been updated.\n\tupdatedChannel, err := cdb.FetchOpenChannels(channel.IdentityPub)\n\trequire.NoError(t, err, \"unable to fetch updated channel\")\n\tassertCommitmentEqual(t, &commitment, &updatedChannel[0].LocalCommitment)\n\tnumDiskUpdates, err := updatedChannel[0].CommitmentHeight()\n\trequire.NoError(t, err, \"unable to read commitment height from disk\")\n\tif numDiskUpdates != uint64(commitment.CommitHeight) {\n\t\tt.Fatalf(\"num disk updates doesn't match: %v vs %v\",\n\t\t\tnumDiskUpdates, commitment.CommitHeight)\n\t}\n\n\t// Attempting to query for a commitment diff should return\n\t// ErrNoPendingCommit as we haven't yet created a new state for them.\n\t_, err = channel.RemoteCommitChainTip()\n\tif err != ErrNoPendingCommit {\n\t\tt.Fatalf(\"expected ErrNoPendingCommit, instead got %v\", err)\n\t}\n\n\t// To simulate us extending a new state to the remote party, we'll also\n\t// create a new commit diff for them.\n\tremoteCommit := commitment\n\tremoteCommit.LocalBalance = lnwire.MilliSatoshi(2e8)\n\tremoteCommit.RemoteBalance = lnwire.MilliSatoshi(3e8)\n\tremoteCommit.CommitHeight = 1\n\tcommitDiff := &CommitDiff{\n\t\tCommitment: remoteCommit,\n\t\tCommitSig: &lnwire.CommitSig{\n\t\t\tChanID:    lnwire.ChannelID(key),\n\t\t\tCommitSig: wireSig,\n\t\t\tHtlcSigs: []lnwire.Sig{\n\t\t\t\twireSig,\n\t\t\t\twireSig,\n\t\t\t},\n\t\t\tExtraData: make([]byte, 0),\n\t\t},\n\t\tLogUpdates: []LogUpdate{\n\t\t\t{\n\t\t\t\tLogIndex: 1,\n\t\t\t\tUpdateMsg: &lnwire.UpdateAddHTLC{\n\t\t\t\t\tID:        1,\n\t\t\t\t\tAmount:    lnwire.NewMSatFromSatoshis(100),\n\t\t\t\t\tExpiry:    25,\n\t\t\t\t\tExtraData: make([]byte, 0),\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\tLogIndex: 2,\n\t\t\t\tUpdateMsg: &lnwire.UpdateAddHTLC{\n\t\t\t\t\tID:        2,\n\t\t\t\t\tAmount:    lnwire.NewMSatFromSatoshis(200),\n\t\t\t\t\tExpiry:    50,\n\t\t\t\t\tExtraData: make([]byte, 0),\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\tOpenedCircuitKeys: []models.CircuitKey{},\n\t\tClosedCircuitKeys: []models.CircuitKey{},\n\t}\n\tcopy(commitDiff.LogUpdates[0].UpdateMsg.(*lnwire.UpdateAddHTLC).PaymentHash[:],\n\t\tbytes.Repeat([]byte{1}, 32))\n\tcopy(commitDiff.LogUpdates[1].UpdateMsg.(*lnwire.UpdateAddHTLC).PaymentHash[:],\n\t\tbytes.Repeat([]byte{2}, 32))\n\tif err := channel.AppendRemoteCommitChain(commitDiff); err != nil {\n\t\tt.Fatalf(\"unable to add to commit chain: %v\", err)\n\t}\n\n\t// The commitment tip should now match the commitment that we just\n\t// inserted.\n\tdiskCommitDiff, err := channel.RemoteCommitChainTip()\n\trequire.NoError(t, err, \"unable to fetch commit diff\")\n\tif !reflect.DeepEqual(commitDiff, diskCommitDiff) {\n\t\tt.Fatalf(\"commit diffs don't match: %v vs %v\", spew.Sdump(remoteCommit),\n\t\t\tspew.Sdump(diskCommitDiff))\n\t}\n\n\t// We'll save the old remote commitment as this will be added to the\n\t// revocation log shortly.\n\toldRemoteCommit := channel.RemoteCommitment\n\n\t// Next, write to the log which tracks the necessary revocation state\n\t// needed to rectify any fishy behavior by the remote party. Modify the\n\t// current uncollapsed revocation state to simulate a state transition\n\t// by the remote party.\n\tchannel.RemoteCurrentRevocation = channel.RemoteNextRevocation\n\tnewPriv, err := btcec.NewPrivateKey()\n\trequire.NoError(t, err, \"unable to generate key\")\n\tchannel.RemoteNextRevocation = newPriv.PubKey()\n\n\tfwdPkg := NewFwdPkg(channel.ShortChanID(), oldRemoteCommit.CommitHeight,\n\t\tdiskCommitDiff.LogUpdates, nil)\n\n\terr = channel.AdvanceCommitChainTail(\n\t\tfwdPkg, nil, dummyLocalOutputIndex, dummyRemoteOutIndex,\n\t)\n\trequire.NoError(t, err, \"unable to append to revocation log\")\n\n\t// At this point, the remote commit chain should be nil, and the posted\n\t// remote commitment should match the one we added as a diff above.\n\tif _, err := channel.RemoteCommitChainTip(); err != ErrNoPendingCommit {\n\t\tt.Fatalf(\"expected ErrNoPendingCommit, instead got %v\", err)\n\t}\n\n\t// We should be able to fetch the channel delta created above by its\n\t// update number with all the state properly reconstructed.\n\tdiskPrevCommit, _, err := channel.FindPreviousState(\n\t\toldRemoteCommit.CommitHeight,\n\t)\n\trequire.NoError(t, err, \"unable to fetch past delta\")\n\n\t// Check the output indexes are saved as expected.\n\trequire.EqualValues(\n\t\tt, dummyLocalOutputIndex, diskPrevCommit.OurOutputIndex,\n\t)\n\trequire.EqualValues(\n\t\tt, dummyRemoteOutIndex, diskPrevCommit.TheirOutputIndex,\n\t)\n\n\t// The two deltas (the original vs the on-disk version) should\n\t// identical, and all HTLC data should properly be retained.\n\tassertRevocationLogEntryEqual(t, &oldRemoteCommit, diskPrevCommit)\n\n\t// The state number recovered from the tail of the revocation log\n\t// should be identical to this current state.\n\tlogTailHeight, err := channel.revocationLogTailCommitHeight()\n\trequire.NoError(t, err, \"unable to retrieve log\")\n\tif logTailHeight != oldRemoteCommit.CommitHeight {\n\t\tt.Fatal(\"update number doesn't match\")\n\t}\n\n\toldRemoteCommit = channel.RemoteCommitment\n\n\t// Next modify the posted diff commitment slightly, then create a new\n\t// commitment diff and advance the tail.\n\tcommitDiff.Commitment.CommitHeight = 2\n\tcommitDiff.Commitment.LocalBalance -= htlcAmt\n\tcommitDiff.Commitment.RemoteBalance += htlcAmt\n\tcommitDiff.LogUpdates = []LogUpdate{}\n\tif err := channel.AppendRemoteCommitChain(commitDiff); err != nil {\n\t\tt.Fatalf(\"unable to add to commit chain: %v\", err)\n\t}\n\n\tfwdPkg = NewFwdPkg(channel.ShortChanID(), oldRemoteCommit.CommitHeight, nil, nil)\n\n\terr = channel.AdvanceCommitChainTail(\n\t\tfwdPkg, nil, dummyLocalOutputIndex, dummyRemoteOutIndex,\n\t)\n\trequire.NoError(t, err, \"unable to append to revocation log\")\n\n\t// Once again, fetch the state and ensure it has been properly updated.\n\tprevCommit, _, err := channel.FindPreviousState(\n\t\toldRemoteCommit.CommitHeight,\n\t)\n\trequire.NoError(t, err, \"unable to fetch past delta\")\n\n\t// Check the output indexes are saved as expected.\n\trequire.EqualValues(\n\t\tt, dummyLocalOutputIndex, diskPrevCommit.OurOutputIndex,\n\t)\n\trequire.EqualValues(\n\t\tt, dummyRemoteOutIndex, diskPrevCommit.TheirOutputIndex,\n\t)\n\n\tassertRevocationLogEntryEqual(t, &oldRemoteCommit, prevCommit)\n\n\t// Once again, state number recovered from the tail of the revocation\n\t// log should be identical to this current state.\n\tlogTailHeight, err = channel.revocationLogTailCommitHeight()\n\trequire.NoError(t, err, \"unable to retrieve log\")\n\tif logTailHeight != oldRemoteCommit.CommitHeight {\n\t\tt.Fatal(\"update number doesn't match\")\n\t}\n\n\t// The revocation state stored on-disk should now also be identical.\n\tupdatedChannel, err = cdb.FetchOpenChannels(channel.IdentityPub)\n\trequire.NoError(t, err, \"unable to fetch updated channel\")\n\tif !channel.RemoteCurrentRevocation.IsEqual(updatedChannel[0].RemoteCurrentRevocation) {\n\t\tt.Fatalf(\"revocation state was not synced\")\n\t}\n\tif !channel.RemoteNextRevocation.IsEqual(updatedChannel[0].RemoteNextRevocation) {\n\t\tt.Fatalf(\"revocation state was not synced\")\n\t}\n\n\t// At this point, we should have 2 forwarding packages added.\n\tfwdPkgs := loadFwdPkgs(t, cdb.backend, channel.Packager)\n\trequire.Len(t, fwdPkgs, 2, \"wrong number of forwarding packages\")\n\n\t// Now attempt to delete the channel from the database.\n\tcloseSummary := &ChannelCloseSummary{\n\t\tChanPoint:         channel.FundingOutpoint,\n\t\tRemotePub:         channel.IdentityPub,\n\t\tSettledBalance:    btcutil.Amount(500),\n\t\tTimeLockedBalance: btcutil.Amount(10000),\n\t\tIsPending:         false,\n\t\tCloseType:         RemoteForceClose,\n\t}\n\tif err := updatedChannel[0].CloseChannel(closeSummary); err != nil {\n\t\tt.Fatalf(\"unable to delete updated channel: %v\", err)\n\t}\n\n\t// If we attempt to fetch the target channel again, it shouldn't be\n\t// found.\n\tchannels, err := cdb.FetchOpenChannels(channel.IdentityPub)\n\trequire.NoError(t, err, \"unable to fetch updated channels\")\n\tif len(channels) != 0 {\n\t\tt.Fatalf(\"%v channels, found, but none should be\",\n\t\t\tlen(channels))\n\t}\n\n\t// Attempting to find previous states on the channel should fail as the\n\t// revocation log has been deleted.\n\t_, _, err = updatedChannel[0].FindPreviousState(\n\t\toldRemoteCommit.CommitHeight,\n\t)\n\tif err == nil {\n\t\tt.Fatal(\"revocation log search should have failed\")\n\t}\n\n\t// All forwarding packages of this channel has been deleted too.\n\tfwdPkgs = loadFwdPkgs(t, cdb.backend, channel.Packager)\n\trequire.Empty(t, fwdPkgs, \"no forwarding packages should exist\")\n}\n",
      "length": 10609,
      "tokens": 1203,
      "embedding": []
    },
    {
      "slug": "func TestFetchPendingChannels(t *testing.T) {",
      "content": "func TestFetchPendingChannels(t *testing.T) {\n\tt.Parallel()\n\n\tfullDB, err := MakeTestDB(t)\n\trequire.NoError(t, err, \"unable to make test database\")\n\n\tcdb := fullDB.ChannelStateDB()\n\n\t// Create a pending channel that was broadcast at height 99.\n\tconst broadcastHeight = 99\n\tcreateTestChannel(t, cdb, pendingHeightOption(broadcastHeight))\n\n\tpendingChannels, err := cdb.FetchPendingChannels()\n\trequire.NoError(t, err, \"unable to list pending channels\")\n\n\tif len(pendingChannels) != 1 {\n\t\tt.Fatalf(\"incorrect number of pending channels: expecting %v,\"+\n\t\t\t\"got %v\", 1, len(pendingChannels))\n\t}\n\n\t// The broadcast height of the pending channel should have been set\n\t// properly.\n\tif pendingChannels[0].FundingBroadcastHeight != broadcastHeight {\n\t\tt.Fatalf(\"broadcast height mismatch: expected %v, got %v\",\n\t\t\tpendingChannels[0].FundingBroadcastHeight,\n\t\t\tbroadcastHeight)\n\t}\n\n\tchanOpenLoc := lnwire.ShortChannelID{\n\t\tBlockHeight: 5,\n\t\tTxIndex:     10,\n\t\tTxPosition:  15,\n\t}\n\terr = pendingChannels[0].MarkAsOpen(chanOpenLoc)\n\trequire.NoError(t, err, \"unable to mark channel as open\")\n\n\tif pendingChannels[0].IsPending {\n\t\tt.Fatalf(\"channel marked open should no longer be pending\")\n\t}\n\n\tif pendingChannels[0].ShortChanID() != chanOpenLoc {\n\t\tt.Fatalf(\"channel opening height not updated: expected %v, \"+\n\t\t\t\"got %v\", spew.Sdump(pendingChannels[0].ShortChanID()),\n\t\t\tchanOpenLoc)\n\t}\n\n\t// Next, we'll re-fetch the channel to ensure that the open height was\n\t// properly set.\n\topenChans, err := cdb.FetchAllChannels()\n\trequire.NoError(t, err, \"unable to fetch channels\")\n\tif openChans[0].ShortChanID() != chanOpenLoc {\n\t\tt.Fatalf(\"channel opening heights don't match: expected %v, \"+\n\t\t\t\"got %v\", spew.Sdump(openChans[0].ShortChanID()),\n\t\t\tchanOpenLoc)\n\t}\n\tif openChans[0].FundingBroadcastHeight != broadcastHeight {\n\t\tt.Fatalf(\"broadcast height mismatch: expected %v, got %v\",\n\t\t\topenChans[0].FundingBroadcastHeight,\n\t\t\tbroadcastHeight)\n\t}\n\n\tpendingChannels, err = cdb.FetchPendingChannels()\n\trequire.NoError(t, err, \"unable to list pending channels\")\n\n\tif len(pendingChannels) != 0 {\n\t\tt.Fatalf(\"incorrect number of pending channels: expecting %v,\"+\n\t\t\t\"got %v\", 0, len(pendingChannels))\n\t}\n}\n",
      "length": 2073,
      "tokens": 229,
      "embedding": []
    },
    {
      "slug": "func TestFetchClosedChannels(t *testing.T) {",
      "content": "func TestFetchClosedChannels(t *testing.T) {\n\tt.Parallel()\n\n\tfullDB, err := MakeTestDB(t)\n\trequire.NoError(t, err, \"unable to make test database\")\n\n\tcdb := fullDB.ChannelStateDB()\n\n\t// Create an open channel in the database.\n\tstate := createTestChannel(t, cdb, openChannelOption())\n\n\t// Next, close the channel by including a close channel summary in the\n\t// database.\n\tsummary := &ChannelCloseSummary{\n\t\tChanPoint:         state.FundingOutpoint,\n\t\tClosingTXID:       rev,\n\t\tRemotePub:         state.IdentityPub,\n\t\tCapacity:          state.Capacity,\n\t\tSettledBalance:    state.LocalCommitment.LocalBalance.ToSatoshis(),\n\t\tTimeLockedBalance: state.RemoteCommitment.LocalBalance.ToSatoshis() + 10000,\n\t\tCloseType:         RemoteForceClose,\n\t\tIsPending:         true,\n\t\tLocalChanConfig:   state.LocalChanCfg,\n\t}\n\tif err := state.CloseChannel(summary); err != nil {\n\t\tt.Fatalf(\"unable to close channel: %v\", err)\n\t}\n\n\t// Query the database to ensure that the channel has now been properly\n\t// closed. We should get the same result whether querying for pending\n\t// channels only, or not.\n\tpendingClosed, err := cdb.FetchClosedChannels(true)\n\trequire.NoError(t, err, \"failed fetching closed channels\")\n\tif len(pendingClosed) != 1 {\n\t\tt.Fatalf(\"incorrect number of pending closed channels: expecting %v,\"+\n\t\t\t\"got %v\", 1, len(pendingClosed))\n\t}\n\tif !reflect.DeepEqual(summary, pendingClosed[0]) {\n\t\tt.Fatalf(\"database summaries don't match: expected %v got %v\",\n\t\t\tspew.Sdump(summary), spew.Sdump(pendingClosed[0]))\n\t}\n\tclosed, err := cdb.FetchClosedChannels(false)\n\trequire.NoError(t, err, \"failed fetching all closed channels\")\n\tif len(closed) != 1 {\n\t\tt.Fatalf(\"incorrect number of closed channels: expecting %v, \"+\n\t\t\t\"got %v\", 1, len(closed))\n\t}\n\tif !reflect.DeepEqual(summary, closed[0]) {\n\t\tt.Fatalf(\"database summaries don't match: expected %v got %v\",\n\t\t\tspew.Sdump(summary), spew.Sdump(closed[0]))\n\t}\n\n\t// Mark the channel as fully closed.\n\terr = cdb.MarkChanFullyClosed(&state.FundingOutpoint)\n\trequire.NoError(t, err, \"failed fully closing channel\")\n\n\t// The channel should no longer be considered pending, but should still\n\t// be retrieved when fetching all the closed channels.\n\tclosed, err = cdb.FetchClosedChannels(false)\n\trequire.NoError(t, err, \"failed fetching closed channels\")\n\tif len(closed) != 1 {\n\t\tt.Fatalf(\"incorrect number of closed channels: expecting %v, \"+\n\t\t\t\"got %v\", 1, len(closed))\n\t}\n\tpendingClose, err := cdb.FetchClosedChannels(true)\n\trequire.NoError(t, err, \"failed fetching channels pending close\")\n\tif len(pendingClose) != 0 {\n\t\tt.Fatalf(\"incorrect number of closed channels: expecting %v, \"+\n\t\t\t\"got %v\", 0, len(closed))\n\t}\n}\n\n// TestFetchWaitingCloseChannels ensures that the correct channels that are\n// waiting to be closed are returned.",
      "length": 2657,
      "tokens": 310,
      "embedding": []
    },
    {
      "slug": "func TestFetchWaitingCloseChannels(t *testing.T) {",
      "content": "func TestFetchWaitingCloseChannels(t *testing.T) {\n\tt.Parallel()\n\n\tconst numChannels = 2\n\tconst broadcastHeight = 99\n\n\t// We'll start by creating two channels within our test database. One of\n\t// them will have their funding transaction confirmed on-chain, while\n\t// the other one will remain unconfirmed.\n\tfullDB, err := MakeTestDB(t)\n\trequire.NoError(t, err, \"unable to make test database\")\n\n\tcdb := fullDB.ChannelStateDB()\n\n\tchannels := make([]*OpenChannel, numChannels)\n\tfor i := 0; i < numChannels; i++ {\n\t\t// Create a pending channel in the database at the broadcast\n\t\t// height.\n\t\tchannels[i] = createTestChannel(\n\t\t\tt, cdb, pendingHeightOption(broadcastHeight),\n\t\t)\n\t}\n\n\t// We'll only confirm the first one.\n\tchannelConf := lnwire.ShortChannelID{\n\t\tBlockHeight: broadcastHeight + 1,\n\t\tTxIndex:     10,\n\t\tTxPosition:  15,\n\t}\n\tif err := channels[0].MarkAsOpen(channelConf); err != nil {\n\t\tt.Fatalf(\"unable to mark channel as open: %v\", err)\n\t}\n\n\t// Then, we'll mark the channels as if their commitments were broadcast.\n\t// This would happen in the event of a force close and should make the\n\t// channels enter a state of waiting close.\n\tfor _, channel := range channels {\n\t\tcloseTx := wire.NewMsgTx(2)\n\t\tcloseTx.AddTxIn(\n\t\t\t&wire.TxIn{\n\t\t\t\tPreviousOutPoint: channel.FundingOutpoint,\n\t\t\t},\n\t\t)\n\n\t\tif err := channel.MarkCommitmentBroadcasted(closeTx, true); err != nil {\n\t\t\tt.Fatalf(\"unable to mark commitment broadcast: %v\", err)\n\t\t}\n\n\t\t// Now try to marking a coop close with a nil tx. This should\n\t\t// succeed, but it shouldn't exit when queried.\n\t\tif err = channel.MarkCoopBroadcasted(nil, true); err != nil {\n\t\t\tt.Fatalf(\"unable to mark nil coop broadcast: %v\", err)\n\t\t}\n\t\t_, err := channel.BroadcastedCooperative()\n\t\tif err != ErrNoCloseTx {\n\t\t\tt.Fatalf(\"expected no closing tx error, got: %v\", err)\n\t\t}\n\n\t\t// Finally, modify the close tx deterministically  and also mark\n\t\t// it as coop closed. Later we will test that distinct\n\t\t// transactions are returned for both coop and force closes.\n\t\tcloseTx.TxIn[0].PreviousOutPoint.Index ^= 1\n\t\tif err := channel.MarkCoopBroadcasted(closeTx, true); err != nil {\n\t\t\tt.Fatalf(\"unable to mark coop broadcast: %v\", err)\n\t\t}\n\t}\n\n\t// Now, we'll fetch all the channels waiting to be closed from the\n\t// database. We should expect to see both channels above, even if any of\n\t// them haven't had their funding transaction confirm on-chain.\n\twaitingCloseChannels, err := cdb.FetchWaitingCloseChannels()\n\trequire.NoError(t, err, \"unable to fetch all waiting close channels\")\n\tif len(waitingCloseChannels) != numChannels {\n\t\tt.Fatalf(\"expected %d channels waiting to be closed, got %d\", 2,\n\t\t\tlen(waitingCloseChannels))\n\t}\n\texpectedChannels := make(map[wire.OutPoint]struct{})\n\tfor _, channel := range channels {\n\t\texpectedChannels[channel.FundingOutpoint] = struct{}{}\n\t}\n\tfor _, channel := range waitingCloseChannels {\n\t\tif _, ok := expectedChannels[channel.FundingOutpoint]; !ok {\n\t\t\tt.Fatalf(\"expected channel %v to be waiting close\",\n\t\t\t\tchannel.FundingOutpoint)\n\t\t}\n\n\t\tchanPoint := channel.FundingOutpoint\n\n\t\t// Assert that the force close transaction is retrievable.\n\t\tforceCloseTx, err := channel.BroadcastedCommitment()\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"Unable to retrieve commitment: %v\", err)\n\t\t}\n\n\t\tif forceCloseTx.TxIn[0].PreviousOutPoint != chanPoint {\n\t\t\tt.Fatalf(\"expected outpoint %v, got %v\",\n\t\t\t\tchanPoint,\n\t\t\t\tforceCloseTx.TxIn[0].PreviousOutPoint)\n\t\t}\n\n\t\t// Assert that the coop close transaction is retrievable.\n\t\tcoopCloseTx, err := channel.BroadcastedCooperative()\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unable to retrieve coop close: %v\", err)\n\t\t}\n\n\t\tchanPoint.Index ^= 1\n\t\tif coopCloseTx.TxIn[0].PreviousOutPoint != chanPoint {\n\t\t\tt.Fatalf(\"expected outpoint %v, got %v\",\n\t\t\t\tchanPoint,\n\t\t\t\tcoopCloseTx.TxIn[0].PreviousOutPoint)\n\t\t}\n\t}\n}\n\n// TestRefresh asserts that Refresh updates the in-memory state of another\n// OpenChannel to reflect a preceding call to MarkOpen on a different\n// OpenChannel.",
      "length": 3790,
      "tokens": 514,
      "embedding": []
    },
    {
      "slug": "func TestRefresh(t *testing.T) {",
      "content": "func TestRefresh(t *testing.T) {\n\tt.Parallel()\n\n\tfullDB, err := MakeTestDB(t)\n\trequire.NoError(t, err, \"unable to make test database\")\n\n\tcdb := fullDB.ChannelStateDB()\n\n\t// First create a test channel.\n\tstate := createTestChannel(t, cdb)\n\n\t// Next, locate the pending channel with the database.\n\tpendingChannels, err := cdb.FetchPendingChannels()\n\tif err != nil {\n\t\tt.Fatalf(\"unable to load pending channels; %v\", err)\n\t}\n\n\tvar pendingChannel *OpenChannel\n\tfor _, channel := range pendingChannels {\n\t\tif channel.FundingOutpoint == state.FundingOutpoint {\n\t\t\tpendingChannel = channel\n\t\t\tbreak\n\t\t}\n\t}\n\tif pendingChannel == nil {\n\t\tt.Fatalf(\"unable to find pending channel with funding \"+\n\t\t\t\"outpoint=%v: %v\", state.FundingOutpoint, err)\n\t}\n\n\t// Next, simulate the confirmation of the channel by marking it as\n\t// pending within the database.\n\tchanOpenLoc := lnwire.ShortChannelID{\n\t\tBlockHeight: 105,\n\t\tTxIndex:     10,\n\t\tTxPosition:  15,\n\t}\n\n\terr = state.MarkAsOpen(chanOpenLoc)\n\trequire.NoError(t, err, \"unable to mark channel open\")\n\n\t// The short_chan_id of the receiver to MarkAsOpen should reflect the\n\t// open location, but the other pending channel should remain unchanged.\n\tif state.ShortChanID() == pendingChannel.ShortChanID() {\n\t\tt.Fatalf(\"pending channel short_chan_ID should not have been \" +\n\t\t\t\"updated before refreshing short_chan_id\")\n\t}\n\n\t// Now that the receiver's short channel id has been updated, check to\n\t// ensure that the channel packager's source has been updated as well.\n\t// This ensures that the packager will read and write to buckets\n\t// corresponding to the new short chan id, instead of the prior.\n\tif state.Packager.(*ChannelPackager).source != chanOpenLoc {\n\t\tt.Fatalf(\"channel packager source was not updated: want %v, \"+\n\t\t\t\"got %v\", chanOpenLoc,\n\t\t\tstate.Packager.(*ChannelPackager).source)\n\t}\n\n\t// Now, refresh the state of the pending channel.\n\terr = pendingChannel.Refresh()\n\trequire.NoError(t, err, \"unable to refresh short_chan_id\")\n\n\t// This should result in both OpenChannel's now having the same\n\t// ShortChanID.\n\tif state.ShortChanID() != pendingChannel.ShortChanID() {\n\t\tt.Fatalf(\"expected pending channel short_chan_id to be \"+\n\t\t\t\"refreshed: want %v, got %v\", state.ShortChanID(),\n\t\t\tpendingChannel.ShortChanID())\n\t}\n\n\t// Check to ensure that the _other_ OpenChannel channel packager's\n\t// source has also been updated after the refresh. This ensures that the\n\t// other packagers will read and write to buckets corresponding to the\n\t// updated short chan id.\n\tif pendingChannel.Packager.(*ChannelPackager).source != chanOpenLoc {\n\t\tt.Fatalf(\"channel packager source was not updated: want %v, \"+\n\t\t\t\"got %v\", chanOpenLoc,\n\t\t\tpendingChannel.Packager.(*ChannelPackager).source)\n\t}\n\n\t// Check to ensure that this channel is no longer pending and this field\n\t// is up to date.\n\tif pendingChannel.IsPending {\n\t\tt.Fatalf(\"channel pending state wasn't updated: want false got true\")\n\t}\n}\n\n// TestCloseInitiator tests the setting of close initiator statuses for\n// cooperative closes and local force closes.",
      "length": 2930,
      "tokens": 396,
      "embedding": []
    },
    {
      "slug": "func TestCloseInitiator(t *testing.T) {",
      "content": "func TestCloseInitiator(t *testing.T) {\n\ttests := []struct {\n\t\tname string\n\t\t// updateChannel is called to update the channel as broadcast,\n\t\t// cooperatively or not, based on the test's requirements.\n\t\tupdateChannel    func(c *OpenChannel) error\n\t\texpectedStatuses []ChannelStatus\n\t}{\n\t\t{\n\t\t\tname: \"local coop close\",\n\t\t\t// Mark the channel as cooperatively closed, initiated\n\t\t\t// by the local party.\n\t\t\tupdateChannel: func(c *OpenChannel) error {\n\t\t\t\treturn c.MarkCoopBroadcasted(\n\t\t\t\t\t&wire.MsgTx{}, true,\n\t\t\t\t)\n\t\t\t},\n\t\t\texpectedStatuses: []ChannelStatus{\n\t\t\t\tChanStatusLocalCloseInitiator,\n\t\t\t\tChanStatusCoopBroadcasted,\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"remote coop close\",\n\t\t\t// Mark the channel as cooperatively closed, initiated\n\t\t\t// by the remote party.\n\t\t\tupdateChannel: func(c *OpenChannel) error {\n\t\t\t\treturn c.MarkCoopBroadcasted(\n\t\t\t\t\t&wire.MsgTx{}, false,\n\t\t\t\t)\n\t\t\t},\n\t\t\texpectedStatuses: []ChannelStatus{\n\t\t\t\tChanStatusRemoteCloseInitiator,\n\t\t\t\tChanStatusCoopBroadcasted,\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"local force close\",\n\t\t\t// Mark the channel's commitment as broadcast with\n\t\t\t// local initiator.\n\t\t\tupdateChannel: func(c *OpenChannel) error {\n\t\t\t\treturn c.MarkCommitmentBroadcasted(\n\t\t\t\t\t&wire.MsgTx{}, true,\n\t\t\t\t)\n\t\t\t},\n\t\t\texpectedStatuses: []ChannelStatus{\n\t\t\t\tChanStatusLocalCloseInitiator,\n\t\t\t\tChanStatusCommitBroadcasted,\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\ttest := test\n\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tt.Parallel()\n\n\t\t\tfullDB, err := MakeTestDB(t)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"unable to make test database: %v\",\n\t\t\t\t\terr)\n\t\t\t}\n\n\t\t\tcdb := fullDB.ChannelStateDB()\n\n\t\t\t// Create an open channel.\n\t\t\tchannel := createTestChannel(\n\t\t\t\tt, cdb, openChannelOption(),\n\t\t\t)\n\n\t\t\terr = test.updateChannel(channel)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t\t}\n\n\t\t\t// Lookup open channels in the database.\n\t\t\tdbChans, err := fetchChannels(\n\t\t\t\tcdb, pendingChannelFilter(false),\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t\t}\n\t\t\tif len(dbChans) != 1 {\n\t\t\t\tt.Fatalf(\"expected 1 channel, got: %v\",\n\t\t\t\t\tlen(dbChans))\n\t\t\t}\n\n\t\t\t// Check that the statuses that we expect were written\n\t\t\t// to disk.\n\t\t\tfor _, status := range test.expectedStatuses {\n\t\t\t\tif !dbChans[0].HasChanStatus(status) {\n\t\t\t\t\tt.Fatalf(\"expected channel to have \"+\n\t\t\t\t\t\t\"status: %v, has status: %v\",\n\t\t\t\t\t\tstatus, dbChans[0].chanStatus)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\n// TestCloseChannelStatus tests setting of a channel status on the historical\n// channel on channel close.",
      "length": 2360,
      "tokens": 288,
      "embedding": []
    },
    {
      "slug": "func TestCloseChannelStatus(t *testing.T) {",
      "content": "func TestCloseChannelStatus(t *testing.T) {\n\tfullDB, err := MakeTestDB(t)\n\tif err != nil {\n\t\tt.Fatalf(\"unable to make test database: %v\",\n\t\t\terr)\n\t}\n\n\tcdb := fullDB.ChannelStateDB()\n\n\t// Create an open channel.\n\tchannel := createTestChannel(\n\t\tt, cdb, openChannelOption(),\n\t)\n\n\tif err := channel.CloseChannel(\n\t\t&ChannelCloseSummary{\n\t\t\tChanPoint: channel.FundingOutpoint,\n\t\t\tRemotePub: channel.IdentityPub,\n\t\t}, ChanStatusRemoteCloseInitiator,\n\t); err != nil {\n\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t}\n\n\thistChan, err := channel.Db.FetchHistoricalChannel(\n\t\t&channel.FundingOutpoint,\n\t)\n\trequire.NoError(t, err, \"unexpected error\")\n\n\tif !histChan.HasChanStatus(ChanStatusRemoteCloseInitiator) {\n\t\tt.Fatalf(\"channel should have status\")\n\t}\n}\n\n// TestHasChanStatus asserts the behavior of HasChanStatus by checking the\n// behavior of various status flags in addition to the special case of\n// ChanStatusDefault which is treated like a flag in the code base even though\n// it isn't.",
      "length": 903,
      "tokens": 112,
      "embedding": []
    },
    {
      "slug": "func TestHasChanStatus(t *testing.T) {",
      "content": "func TestHasChanStatus(t *testing.T) {\n\ttests := []struct {\n\t\tname   string\n\t\tstatus ChannelStatus\n\t\texpHas map[ChannelStatus]bool\n\t}{\n\t\t{\n\t\t\tname:   \"default\",\n\t\t\tstatus: ChanStatusDefault,\n\t\t\texpHas: map[ChannelStatus]bool{\n\t\t\t\tChanStatusDefault: true,\n\t\t\t\tChanStatusBorked:  false,\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"single flag\",\n\t\t\tstatus: ChanStatusBorked,\n\t\t\texpHas: map[ChannelStatus]bool{\n\t\t\t\tChanStatusDefault: false,\n\t\t\t\tChanStatusBorked:  true,\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"multiple flags\",\n\t\t\tstatus: ChanStatusBorked | ChanStatusLocalDataLoss,\n\t\t\texpHas: map[ChannelStatus]bool{\n\t\t\t\tChanStatusDefault:       false,\n\t\t\t\tChanStatusBorked:        true,\n\t\t\t\tChanStatusLocalDataLoss: true,\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\ttest := test\n\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tc := &OpenChannel{\n\t\t\t\tchanStatus: test.status,\n\t\t\t}\n\n\t\t\tfor status, expHas := range test.expHas {\n\t\t\t\thas := c.HasChanStatus(status)\n\t\t\t\tif has == expHas {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tt.Fatalf(\"expected chan status to \"+\n\t\t\t\t\t\"have %s? %t, got: %t\",\n\t\t\t\t\tstatus, expHas, has)\n\t\t\t}\n\t\t})\n\t}\n}\n\n// TestKeyLocatorEncoding tests that we are able to serialize a given\n// keychain.KeyLocator. After successfully encoding, we check that the decode\n// output arrives at the same initial KeyLocator.",
      "length": 1185,
      "tokens": 140,
      "embedding": []
    },
    {
      "slug": "func TestKeyLocatorEncoding(t *testing.T) {",
      "content": "func TestKeyLocatorEncoding(t *testing.T) {\n\tkeyLoc := keychain.KeyLocator{\n\t\tFamily: keychain.KeyFamilyRevocationRoot,\n\t\tIndex:  keyLocIndex,\n\t}\n\n\t// First, we'll encode the KeyLocator into a buffer.\n\tvar (\n\t\tb   bytes.Buffer\n\t\tbuf [8]byte\n\t)\n\n\terr := EKeyLocator(&b, &keyLoc, &buf)\n\trequire.NoError(t, err, \"unable to encode key locator\")\n\n\t// Next, we'll attempt to decode the bytes into a new KeyLocator.\n\tr := bytes.NewReader(b.Bytes())\n\tvar decodedKeyLoc keychain.KeyLocator\n\n\terr = DKeyLocator(r, &decodedKeyLoc, &buf, 8)\n\trequire.NoError(t, err, \"unable to decode key locator\")\n\n\t// Finally, we'll compare that the original KeyLocator and the decoded\n\t// version are equal.\n\trequire.Equal(t, keyLoc, decodedKeyLoc)\n}\n\n// TestFinalHtlcs tests final htlc storage and retrieval.",
      "length": 713,
      "tokens": 94,
      "embedding": []
    },
    {
      "slug": "func TestFinalHtlcs(t *testing.T) {",
      "content": "func TestFinalHtlcs(t *testing.T) {\n\tt.Parallel()\n\n\tfullDB, err := MakeTestDB(t, OptionStoreFinalHtlcResolutions(true))\n\trequire.NoError(t, err, \"unable to make test database\")\n\n\tcdb := fullDB.ChannelStateDB()\n\n\tchanID := lnwire.ShortChannelID{\n\t\tBlockHeight: 1,\n\t\tTxIndex:     2,\n\t\tTxPosition:  3,\n\t}\n\n\t// Test unknown htlc lookup.\n\tconst unknownHtlcID = 999\n\n\t_, err = cdb.LookupFinalHtlc(chanID, unknownHtlcID)\n\trequire.ErrorIs(t, err, ErrHtlcUnknown)\n\n\t// Test offchain final htlcs.\n\tconst offchainHtlcID = 1\n\n\terr = kvdb.Update(cdb.backend, func(tx kvdb.RwTx) error {\n\t\tbucket, err := fetchFinalHtlcsBucketRw(\n\t\t\ttx, chanID,\n\t\t)\n\t\trequire.NoError(t, err)\n\n\t\treturn putFinalHtlc(bucket, offchainHtlcID, FinalHtlcInfo{\n\t\t\tSettled:  true,\n\t\t\tOffchain: true,\n\t\t})\n\t}, func() {})\n\trequire.NoError(t, err)\n\n\tinfo, err := cdb.LookupFinalHtlc(chanID, offchainHtlcID)\n\trequire.NoError(t, err)\n\trequire.True(t, info.Settled)\n\trequire.True(t, info.Offchain)\n\n\t// Test onchain final htlcs.\n\tconst onchainHtlcID = 2\n\n\terr = cdb.PutOnchainFinalHtlcOutcome(chanID, onchainHtlcID, true)\n\trequire.NoError(t, err)\n\n\tinfo, err = cdb.LookupFinalHtlc(chanID, onchainHtlcID)\n\trequire.NoError(t, err)\n\trequire.True(t, info.Settled)\n\trequire.False(t, info.Offchain)\n\n\t// Test unknown htlc lookup for existing channel.\n\t_, err = cdb.LookupFinalHtlc(chanID, unknownHtlcID)\n\trequire.ErrorIs(t, err, ErrHtlcUnknown)\n}\n",
      "length": 1304,
      "tokens": 137,
      "embedding": []
    }
  ]
}