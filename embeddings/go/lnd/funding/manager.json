{
  "filepath": "../implementations/go/lnd/funding/manager.go",
  "package": "funding",
  "sections": [
    {
      "slug": "func WriteOutpoint(w io.Writer, o *wire.OutPoint) error {",
      "content": "func WriteOutpoint(w io.Writer, o *wire.OutPoint) error {\n\tscratch := make([]byte, 4)\n\n\tif err := wire.WriteVarBytes(w, 0, o.Hash[:]); err != nil {\n\t\treturn err\n\t}\n\n\tbyteOrder.PutUint32(scratch, o.Index)\n\t_, err := w.Write(scratch)\n\treturn err\n}\n\nconst (\n\t// MinBtcRemoteDelay is the minimum CSV delay we will require the remote\n\t// to use for its commitment transaction.\n\tMinBtcRemoteDelay uint16 = 144\n\n\t// MaxBtcRemoteDelay is the maximum CSV delay we will require the remote\n\t// to use for its commitment transaction.\n\tMaxBtcRemoteDelay uint16 = 2016\n\n\t// MinLtcRemoteDelay is the minimum Litecoin CSV delay we will require\n\t// the remote to use for its commitment transaction.\n\tMinLtcRemoteDelay uint16 = 576\n\n\t// MaxLtcRemoteDelay is the maximum Litecoin CSV delay we will require\n\t// the remote to use for its commitment transaction.\n\tMaxLtcRemoteDelay uint16 = 8064\n\n\t// MinChanFundingSize is the smallest channel that we'll allow to be\n\t// created over the RPC interface.\n\tMinChanFundingSize = btcutil.Amount(20000)\n\n\t// MaxBtcFundingAmount is a soft-limit of the maximum channel size\n\t// currently accepted on the Bitcoin chain within the Lightning\n\t// Protocol. This limit is defined in BOLT-0002, and serves as an\n\t// initial precautionary limit while implementations are battle tested\n\t// in the real world.\n\tMaxBtcFundingAmount = btcutil.Amount(1<<24) - 1\n\n\t// MaxBtcFundingAmountWumbo is a soft-limit on the maximum size of wumbo\n\t// channels. This limit is 10 BTC and is the only thing standing between\n\t// you and limitless channel size (apart from 21 million cap).\n\tMaxBtcFundingAmountWumbo = btcutil.Amount(1000000000)\n\n\t// MaxLtcFundingAmount is a soft-limit of the maximum channel size\n\t// currently accepted on the Litecoin chain within the Lightning\n\t// Protocol.\n\tMaxLtcFundingAmount = MaxBtcFundingAmount *\n\t\tchainreg.BtcToLtcConversionRate\n\n\t// TODO(roasbeef): tune.\n\tmsgBufferSize = 50\n\n\t// maxWaitNumBlocksFundingConf is the maximum number of blocks to wait\n\t// for the funding transaction to be confirmed before forgetting\n\t// channels that aren't initiated by us. 2016 blocks is ~2 weeks.\n\tmaxWaitNumBlocksFundingConf = 2016\n\n\t// pendingChansLimit is the maximum number of pending channels that we\n\t// can have. After this point, pending channel opens will start to be\n\t// rejected.\n\tpendingChansLimit = 1_000\n)\n\nvar (\n\t// ErrFundingManagerShuttingDown is an error returned when attempting to\n\t// process a funding request/message but the funding manager has already\n\t// been signaled to shut down.\n\tErrFundingManagerShuttingDown = errors.New(\"funding manager shutting \" +\n\t\t\"down\")\n\n\t// ErrConfirmationTimeout is an error returned when we as a responder\n\t// are waiting for a funding transaction to confirm, but too many\n\t// blocks pass without confirmation.\n\tErrConfirmationTimeout = errors.New(\"timeout waiting for funding \" +\n\t\t\"confirmation\")\n\n\t// errUpfrontShutdownScriptNotSupported is returned if an upfront\n\t// shutdown script is set for a peer that does not support the feature\n\t// bit.\n\terrUpfrontShutdownScriptNotSupported = errors.New(\"peer does not \" +\n\t\t\"support option upfront shutdown script\")\n\n\tzeroID [32]byte\n)\n\n// reservationWithCtx encapsulates a pending channel reservation. This wrapper\n// struct is used internally within the funding manager to track and progress\n// the funding workflow initiated by incoming/outgoing methods from the target\n// peer. Additionally, this struct houses a response and error channel which is\n// used to respond to the caller in the case a channel workflow is initiated\n// via a local signal such as RPC.\n//\n// TODO(roasbeef): actually use the context package\n//   - deadlines, etc.",
      "length": 3515,
      "tokens": 522,
      "embedding": []
    },
    {
      "slug": "type reservationWithCtx struct {",
      "content": "type reservationWithCtx struct {\n\treservation *lnwallet.ChannelReservation\n\tpeer        lnpeer.Peer\n\n\tchanAmt btcutil.Amount\n\n\t// forwardingPolicy is the policy provided by the initFundingMsg.\n\tforwardingPolicy htlcswitch.ForwardingPolicy\n\n\t// Constraints we require for the remote.\n\tremoteCsvDelay    uint16\n\tremoteMinHtlc     lnwire.MilliSatoshi\n\tremoteMaxValue    lnwire.MilliSatoshi\n\tremoteMaxHtlcs    uint16\n\tremoteChanReserve btcutil.Amount\n\n\t// maxLocalCsv is the maximum csv we will accept from the remote.\n\tmaxLocalCsv uint16\n\n\t// channelType is the explicit channel type proposed by the initiator of\n\t// the channel.\n\tchannelType *lnwire.ChannelType\n\n\tupdateMtx   sync.RWMutex\n\tlastUpdated time.Time\n\n\tupdates chan *lnrpc.OpenStatusUpdate\n\terr     chan error\n}\n\n// isLocked checks the reservation's timestamp to determine whether it is\n// locked.",
      "length": 793,
      "tokens": 89,
      "embedding": []
    },
    {
      "slug": "func (r *reservationWithCtx) isLocked() bool {",
      "content": "func (r *reservationWithCtx) isLocked() bool {\n\tr.updateMtx.RLock()\n\tdefer r.updateMtx.RUnlock()\n\n\t// The time zero value represents a locked reservation.\n\treturn r.lastUpdated.IsZero()\n}\n\n// updateTimestamp updates the reservation's timestamp with the current time.",
      "length": 212,
      "tokens": 25,
      "embedding": []
    },
    {
      "slug": "func (r *reservationWithCtx) updateTimestamp() {",
      "content": "func (r *reservationWithCtx) updateTimestamp() {\n\tr.updateMtx.Lock()\n\tdefer r.updateMtx.Unlock()\n\n\tr.lastUpdated = time.Now()\n}\n\n// InitFundingMsg is sent by an outside subsystem to the funding manager in\n// order to kick off a funding workflow with a specified target peer. The\n// original request which defines the parameters of the funding workflow are\n// embedded within this message giving the funding manager full context w.r.t\n// the workflow.",
      "length": 391,
      "tokens": 61,
      "embedding": []
    },
    {
      "slug": "type InitFundingMsg struct {",
      "content": "type InitFundingMsg struct {\n\t// Peer is the peer that we want to open a channel to.\n\tPeer lnpeer.Peer\n\n\t// TargetPubkey is the public key of the peer.\n\tTargetPubkey *btcec.PublicKey\n\n\t// ChainHash is the target genesis hash for this channel.\n\tChainHash chainhash.Hash\n\n\t// SubtractFees set to true means that fees will be subtracted\n\t// from the LocalFundingAmt.\n\tSubtractFees bool\n\n\t// LocalFundingAmt is the size of the channel.\n\tLocalFundingAmt btcutil.Amount\n\n\t// BaseFee is the base fee charged for routing payments regardless of\n\t// the number of milli-satoshis sent.\n\tBaseFee *uint64\n\n\t// FeeRate is the fee rate in ppm (parts per million) that will be\n\t// charged proportionally based on the value of each forwarded HTLC, the\n\t// lowest possible rate is 0 with a granularity of 0.000001\n\t// (millionths).\n\tFeeRate *uint64\n\n\t// PushAmt is the amount pushed to the counterparty.\n\tPushAmt lnwire.MilliSatoshi\n\n\t// FundingFeePerKw is the fee for the funding transaction.\n\tFundingFeePerKw chainfee.SatPerKWeight\n\n\t// Private determines whether or not this channel will be private.\n\tPrivate bool\n\n\t// MinHtlcIn is the minimum incoming HTLC that we accept.\n\tMinHtlcIn lnwire.MilliSatoshi\n\n\t// RemoteCsvDelay is the CSV delay we require for the remote peer.\n\tRemoteCsvDelay uint16\n\n\t// RemoteChanReserve is the channel reserve we required for the remote\n\t// peer.\n\tRemoteChanReserve btcutil.Amount\n\n\t// MinConfs indicates the minimum number of confirmations that each\n\t// output selected to fund the channel should satisfy.\n\tMinConfs int32\n\n\t// ShutdownScript is an optional upfront shutdown script for the\n\t// channel. This value is optional, so may be nil.\n\tShutdownScript lnwire.DeliveryAddress\n\n\t// MaxValueInFlight is the maximum amount of coins in MilliSatoshi\n\t// that can be pending within the channel. It only applies to the\n\t// remote party.\n\tMaxValueInFlight lnwire.MilliSatoshi\n\n\t// MaxHtlcs is the maximum number of HTLCs that the remote peer\n\t// can offer us.\n\tMaxHtlcs uint16\n\n\t// MaxLocalCsv is the maximum local csv delay we will accept from our\n\t// peer.\n\tMaxLocalCsv uint16\n\n\t// ChanFunder is an optional channel funder that allows the caller to\n\t// control exactly how the channel funding is carried out. If not\n\t// specified, then the default chanfunding.WalletAssembler will be\n\t// used.\n\tChanFunder chanfunding.Assembler\n\n\t// PendingChanID is not all zeroes (the default value), then this will\n\t// be the pending channel ID used for the funding flow within the wire\n\t// protocol.\n\tPendingChanID [32]byte\n\n\t// ChannelType allows the caller to use an explicit channel type for the\n\t// funding negotiation. This type will only be observed if BOTH sides\n\t// support explicit channel type negotiation.\n\tChannelType *lnwire.ChannelType\n\n\t// Updates is a channel which updates to the opening status of the\n\t// channel are sent on.\n\tUpdates chan *lnrpc.OpenStatusUpdate\n\n\t// Err is a channel which errors encountered during the funding flow are\n\t// sent on.\n\tErr chan error\n}\n\n// fundingMsg is sent by the ProcessFundingMsg function and packages a\n// funding-specific lnwire.Message along with the lnpeer.Peer that sent it.",
      "length": 3018,
      "tokens": 469,
      "embedding": []
    },
    {
      "slug": "type fundingMsg struct {",
      "content": "type fundingMsg struct {\n\tmsg  lnwire.Message\n\tpeer lnpeer.Peer\n}\n\n// pendingChannels is a map instantiated per-peer which tracks all active\n// pending single funded channels indexed by their pending channel identifier,\n// which is a set of 32-bytes generated via a CSPRNG.",
      "length": 242,
      "tokens": 38,
      "embedding": []
    },
    {
      "slug": "type pendingChannels map[[32]byte]*reservationWithCtx",
      "content": "type pendingChannels map[[32]byte]*reservationWithCtx\n\n// serializedPubKey is used within the FundingManager's activeReservations list\n// to identify the nodes with which the FundingManager is actively working to\n// initiate new channels.",
      "length": 181,
      "tokens": 26,
      "embedding": []
    },
    {
      "slug": "type serializedPubKey [33]byte",
      "content": "type serializedPubKey [33]byte\n\n// newSerializedKey creates a new serialized public key from an instance of a\n// live pubkey object.",
      "length": 99,
      "tokens": 17,
      "embedding": []
    },
    {
      "slug": "func newSerializedKey(pubKey *btcec.PublicKey) serializedPubKey {",
      "content": "func newSerializedKey(pubKey *btcec.PublicKey) serializedPubKey {\n\tvar s serializedPubKey\n\tcopy(s[:], pubKey.SerializeCompressed())\n\treturn s\n}\n\n// Config defines the configuration for the FundingManager. All elements\n// within the configuration MUST be non-nil for the FundingManager to carry out\n// its duties.",
      "length": 239,
      "tokens": 34,
      "embedding": []
    },
    {
      "slug": "type Config struct {",
      "content": "type Config struct {\n\t// NoWumboChans indicates if we're to reject all incoming wumbo channel\n\t// requests, and also reject all outgoing wumbo channel requests.\n\tNoWumboChans bool\n\n\t// IDKey is the PublicKey that is used to identify this node within the\n\t// Lightning Network.\n\tIDKey *btcec.PublicKey\n\n\t// IDKeyLoc is the locator for the key that is used to identify this\n\t// node within the LightningNetwork.\n\tIDKeyLoc keychain.KeyLocator\n\n\t// Wallet handles the parts of the funding process that involves moving\n\t// funds from on-chain transaction outputs into Lightning channels.\n\tWallet *lnwallet.LightningWallet\n\n\t// PublishTransaction facilitates the process of broadcasting a\n\t// transaction to the network.\n\tPublishTransaction func(*wire.MsgTx, string) error\n\n\t// UpdateLabel updates the label that a transaction has in our wallet,\n\t// overwriting any existing labels.\n\tUpdateLabel func(chainhash.Hash, string) error\n\n\t// FeeEstimator calculates appropriate fee rates based on historical\n\t// transaction information.\n\tFeeEstimator chainfee.Estimator\n\n\t// Notifier is used by the FundingManager to determine when the\n\t// channel's funding transaction has been confirmed on the blockchain\n\t// so that the channel creation process can be completed.\n\tNotifier chainntnfs.ChainNotifier\n\n\t// SignMessage signs an arbitrary message with a given public key. The\n\t// actual digest signed is the double sha-256 of the message. In the\n\t// case that the private key corresponding to the passed public key\n\t// cannot be located, then an error is returned.\n\t//\n\t// TODO(roasbeef): should instead pass on this responsibility to a\n\t// distinct sub-system?\n\tSignMessage func(keyLoc keychain.KeyLocator,\n\t\tmsg []byte, doubleHash bool) (*ecdsa.Signature, error)\n\n\t// CurrentNodeAnnouncement should return the latest, fully signed node\n\t// announcement from the backing Lightning Network node.\n\tCurrentNodeAnnouncement func() (lnwire.NodeAnnouncement, error)\n\n\t// SendAnnouncement is used by the FundingManager to send announcement\n\t// messages to the Gossiper to possibly broadcast to the greater\n\t// network. A set of optional message fields can be provided to populate\n\t// any information within the graph that is not included in the gossip\n\t// message.\n\tSendAnnouncement func(msg lnwire.Message,\n\t\toptionalFields ...discovery.OptionalMsgField) chan error\n\n\t// NotifyWhenOnline allows the FundingManager to register with a\n\t// subsystem that will notify it when the peer comes online. This is\n\t// used when sending the fundingLocked message, since it MUST be\n\t// delivered after the funding transaction is confirmed.\n\t//\n\t// NOTE: The peerChan channel must be buffered.\n\tNotifyWhenOnline func(peer [33]byte, peerChan chan<- lnpeer.Peer)\n\n\t// FindChannel queries the database for the channel with the given\n\t// channel ID. Providing the node's public key is an optimization that\n\t// prevents deserializing and scanning through all possible channels.\n\tFindChannel func(node *btcec.PublicKey,\n\t\tchanID lnwire.ChannelID) (*channeldb.OpenChannel, error)\n\n\t// TempChanIDSeed is a cryptographically random string of bytes that's\n\t// used as a seed to generate pending channel ID's.\n\tTempChanIDSeed [32]byte\n\n\t// DefaultRoutingPolicy is the default routing policy used when\n\t// initially announcing channels.\n\tDefaultRoutingPolicy htlcswitch.ForwardingPolicy\n\n\t// DefaultMinHtlcIn is the default minimum incoming htlc value that is\n\t// set as a channel parameter.\n\tDefaultMinHtlcIn lnwire.MilliSatoshi\n\n\t// NumRequiredConfs is a function closure that helps the funding\n\t// manager decide how many confirmations it should require for a\n\t// channel extended to it. The function is able to take into account\n\t// the amount of the channel, and any funds we'll be pushed in the\n\t// process to determine how many confirmations we'll require.\n\tNumRequiredConfs func(btcutil.Amount, lnwire.MilliSatoshi) uint16\n\n\t// RequiredRemoteDelay is a function that maps the total amount in a\n\t// proposed channel to the CSV delay that we'll require for the remote\n\t// party. Naturally a larger channel should require a higher CSV delay\n\t// in order to give us more time to claim funds in the case of a\n\t// contract breach.\n\tRequiredRemoteDelay func(btcutil.Amount) uint16\n\n\t// RequiredRemoteChanReserve is a function closure that, given the\n\t// channel capacity and dust limit, will return an appropriate amount\n\t// for the remote peer's required channel reserve that is to be adhered\n\t// to at all times.\n\tRequiredRemoteChanReserve func(capacity,\n\t\tdustLimit btcutil.Amount) btcutil.Amount\n\n\t// RequiredRemoteMaxValue is a function closure that, given the channel\n\t// capacity, returns the amount of MilliSatoshis that our remote peer\n\t// can have in total outstanding HTLCs with us.\n\tRequiredRemoteMaxValue func(btcutil.Amount) lnwire.MilliSatoshi\n\n\t// RequiredRemoteMaxHTLCs is a function closure that, given the channel\n\t// capacity, returns the number of maximum HTLCs the remote peer can\n\t// offer us.\n\tRequiredRemoteMaxHTLCs func(btcutil.Amount) uint16\n\n\t// WatchNewChannel is to be called once a new channel enters the final\n\t// funding stage: waiting for on-chain confirmation. This method sends\n\t// the channel to the ChainArbitrator so it can watch for any on-chain\n\t// events related to the channel. We also provide the public key of the\n\t// node we're establishing a channel with for reconnection purposes.\n\tWatchNewChannel func(*channeldb.OpenChannel, *btcec.PublicKey) error\n\n\t// ReportShortChanID allows the funding manager to report the confirmed\n\t// short channel ID of a formerly pending zero-conf channel to outside\n\t// sub-systems.\n\tReportShortChanID func(wire.OutPoint) error\n\n\t// ZombieSweeperInterval is the periodic time interval in which the\n\t// zombie sweeper is run.\n\tZombieSweeperInterval time.Duration\n\n\t// ReservationTimeout is the length of idle time that must pass before\n\t// a reservation is considered a zombie.\n\tReservationTimeout time.Duration\n\n\t// MinChanSize is the smallest channel size that we'll accept as an\n\t// inbound channel. We have such a parameter, as otherwise, nodes could\n\t// flood us with very small channels that would never really be usable\n\t// due to fees.\n\tMinChanSize btcutil.Amount\n\n\t// MaxChanSize is the largest channel size that we'll accept as an\n\t// inbound channel. We have such a parameter, so that you may decide how\n\t// WUMBO you would like your channel.\n\tMaxChanSize btcutil.Amount\n\n\t// MaxPendingChannels is the maximum number of pending channels we\n\t// allow for each peer.\n\tMaxPendingChannels int\n\n\t// RejectPush is set true if the fundingmanager should reject any\n\t// incoming channels having a non-zero push amount.\n\tRejectPush bool\n\n\t// MaxLocalCSVDelay is the maximum csv delay we will allow for our\n\t// commit output. Channels that exceed this value will be failed.\n\tMaxLocalCSVDelay uint16\n\n\t// NotifyOpenChannelEvent informs the ChannelNotifier when channels\n\t// transition from pending open to open.\n\tNotifyOpenChannelEvent func(wire.OutPoint)\n\n\t// OpenChannelPredicate is a predicate on the lnwire.OpenChannel message\n\t// and on the requesting node's public key that returns a bool which\n\t// tells the funding manager whether or not to accept the channel.\n\tOpenChannelPredicate chanacceptor.ChannelAcceptor\n\n\t// NotifyPendingOpenChannelEvent informs the ChannelNotifier when\n\t// channels enter a pending state.\n\tNotifyPendingOpenChannelEvent func(wire.OutPoint,\n\t\t*channeldb.OpenChannel)\n\n\t// EnableUpfrontShutdown specifies whether the upfront shutdown script\n\t// is enabled.\n\tEnableUpfrontShutdown bool\n\n\t// RegisteredChains keeps track of all chains that have been registered\n\t// with the daemon.\n\tRegisteredChains *chainreg.ChainRegistry\n\n\t// MaxAnchorsCommitFeeRate is the max commitment fee rate we'll use as\n\t// the initiator for channels of the anchor type.\n\tMaxAnchorsCommitFeeRate chainfee.SatPerKWeight\n\n\t// DeleteAliasEdge allows the Manager to delete an alias channel edge\n\t// from the graph. It also returns our local to-be-deleted policy.\n\tDeleteAliasEdge func(scid lnwire.ShortChannelID) (\n\t\t*channeldb.ChannelEdgePolicy, error)\n\n\t// AliasManager is an implementation of the aliasHandler interface that\n\t// abstracts away the handling of many alias functions.\n\tAliasManager aliasHandler\n}\n\n// Manager acts as an orchestrator/bridge between the wallet's\n// 'ChannelReservation' workflow, and the wire protocol's funding initiation\n// messages. Any requests to initiate the funding workflow for a channel,\n// either kicked-off locally or remotely are handled by the funding manager.\n// Once a channel's funding workflow has been completed, any local callers, the\n// local peer, and possibly the remote peer are notified of the completion of\n// the channel workflow. Additionally, any temporary or permanent access\n// controls between the wallet and remote peers are enforced via the funding\n// manager.",
      "length": 8642,
      "tokens": 1225,
      "embedding": []
    },
    {
      "slug": "type Manager struct {",
      "content": "type Manager struct {\n\tstarted sync.Once\n\tstopped sync.Once\n\n\t// cfg is a copy of the configuration struct that the FundingManager\n\t// was initialized with.\n\tcfg *Config\n\n\t// chanIDKey is a cryptographically random key that's used to generate\n\t// temporary channel ID's.\n\tchanIDKey [32]byte\n\n\t// chanIDNonce is a nonce that's incremented for each new funding\n\t// reservation created.\n\tnonceMtx    sync.RWMutex\n\tchanIDNonce uint64\n\n\t// activeReservations is a map which houses the state of all pending\n\t// funding workflows.\n\tactiveReservations map[serializedPubKey]pendingChannels\n\n\t// signedReservations is a utility map that maps the permanent channel\n\t// ID of a funding reservation to its temporary channel ID. This is\n\t// required as mid funding flow, we switch to referencing the channel\n\t// by its full channel ID once the commitment transactions have been\n\t// signed by both parties.\n\tsignedReservations map[lnwire.ChannelID][32]byte\n\n\t// resMtx guards both of the maps above to ensure that all access is\n\t// goroutine safe.\n\tresMtx sync.RWMutex\n\n\t// fundingMsgs is a channel that relays fundingMsg structs from\n\t// external sub-systems using the ProcessFundingMsg call.\n\tfundingMsgs chan *fundingMsg\n\n\t// fundingRequests is a channel used to receive channel initiation\n\t// requests from a local subsystem within the daemon.\n\tfundingRequests chan *InitFundingMsg\n\n\t// newChanBarriers is a map from a channel ID to a 'barrier' which will\n\t// be signalled once the channel is fully open. This barrier acts as a\n\t// synchronization point for any incoming/outgoing HTLCs before the\n\t// channel has been fully opened.\n\tbarrierMtx      sync.RWMutex\n\tnewChanBarriers map[lnwire.ChannelID]chan struct{}\n\n\tlocalDiscoveryMtx     sync.Mutex\n\tlocalDiscoverySignals map[lnwire.ChannelID]chan struct{}\n\n\thandleFundingLockedMtx      sync.RWMutex\n\thandleFundingLockedBarriers map[lnwire.ChannelID]struct{}\n\n\tquit chan struct{}\n\twg   sync.WaitGroup\n}\n\n// channelOpeningState represents the different states a channel can be in\n// between the funding transaction has been confirmed and the channel is\n// announced to the network and ready to be used.",
      "length": 2060,
      "tokens": 286,
      "embedding": []
    },
    {
      "slug": "type channelOpeningState uint8",
      "content": "type channelOpeningState uint8\n\nconst (\n\t// markedOpen is the opening state of a channel if the funding\n\t// transaction is confirmed on-chain, but fundingLocked is not yet\n\t// successfully sent to the other peer.\n\tmarkedOpen channelOpeningState = iota\n\n\t// fundingLockedSent is the opening state of a channel if the\n\t// fundingLocked message has successfully been sent to the other peer,\n\t// but we still haven't announced the channel to the network.\n\tfundingLockedSent\n\n\t// addedToRouterGraph is the opening state of a channel if the\n\t// channel has been successfully added to the router graph\n\t// immediately after the fundingLocked message has been sent, but\n\t// we still haven't announced the channel to the network.\n\taddedToRouterGraph\n)\n",
      "length": 694,
      "tokens": 112,
      "embedding": []
    },
    {
      "slug": "func (c channelOpeningState) String() string {",
      "content": "func (c channelOpeningState) String() string {\n\tswitch c {\n\tcase markedOpen:\n\t\treturn \"markedOpen\"\n\tcase fundingLockedSent:\n\t\treturn \"fundingLocked\"\n\tcase addedToRouterGraph:\n\t\treturn \"addedToRouterGraph\"\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\n// NewFundingManager creates and initializes a new instance of the\n// fundingManager.",
      "length": 265,
      "tokens": 32,
      "embedding": []
    },
    {
      "slug": "func NewFundingManager(cfg Config) (*Manager, error) {",
      "content": "func NewFundingManager(cfg Config) (*Manager, error) {\n\treturn &Manager{\n\t\tcfg:                         &cfg,\n\t\tchanIDKey:                   cfg.TempChanIDSeed,\n\t\tactiveReservations:          make(map[serializedPubKey]pendingChannels),\n\t\tsignedReservations:          make(map[lnwire.ChannelID][32]byte),\n\t\tnewChanBarriers:             make(map[lnwire.ChannelID]chan struct{}),\n\t\tfundingMsgs:                 make(chan *fundingMsg, msgBufferSize),\n\t\tfundingRequests:             make(chan *InitFundingMsg, msgBufferSize),\n\t\tlocalDiscoverySignals:       make(map[lnwire.ChannelID]chan struct{}),\n\t\thandleFundingLockedBarriers: make(map[lnwire.ChannelID]struct{}),\n\t\tquit:                        make(chan struct{}),\n\t}, nil\n}\n\n// Start launches all helper goroutines required for handling requests sent\n// to the funding manager.",
      "length": 757,
      "tokens": 48,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) Start() error {",
      "content": "func (f *Manager) Start() error {\n\tvar err error\n\tf.started.Do(func() {\n\t\tlog.Info(\"Funding manager starting\")\n\t\terr = f.start()\n\t})\n\treturn err\n}\n",
      "length": 106,
      "tokens": 15,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) start() error {",
      "content": "func (f *Manager) start() error {\n\t// Upon restart, the Funding Manager will check the database to load any\n\t// channels that were  waiting for their funding transactions to be\n\t// confirmed on the blockchain at the time when the daemon last went\n\t// down.\n\t// TODO(roasbeef): store height that funding finished?\n\t//  * would then replace call below\n\tallChannels, err := f.cfg.Wallet.Cfg.Database.FetchAllChannels()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tfor _, channel := range allChannels {\n\t\tchanID := lnwire.NewChanIDFromOutPoint(&channel.FundingOutpoint)\n\n\t\t// For any channels that were in a pending state when the\n\t\t// daemon was last connected, the Funding Manager will\n\t\t// re-initialize the channel barriers, and republish the\n\t\t// funding transaction if we're the initiator.\n\t\tif channel.IsPending {\n\t\t\tf.barrierMtx.Lock()\n\t\t\tlog.Tracef(\"Loading pending ChannelPoint(%v), \"+\n\t\t\t\t\"creating chan barrier\",\n\t\t\t\tchannel.FundingOutpoint)\n\n\t\t\tf.newChanBarriers[chanID] = make(chan struct{})\n\t\t\tf.barrierMtx.Unlock()\n\n\t\t\tf.localDiscoveryMtx.Lock()\n\t\t\tf.localDiscoverySignals[chanID] = make(chan struct{})\n\t\t\tf.localDiscoveryMtx.Unlock()\n\n\t\t\t// Rebroadcast the funding transaction for any pending\n\t\t\t// channel that we initiated. No error will be returned\n\t\t\t// if the transaction already has been broadcast.\n\t\t\tchanType := channel.ChanType\n\t\t\tif chanType.IsSingleFunder() &&\n\t\t\t\tchanType.HasFundingTx() &&\n\t\t\t\tchannel.IsInitiator {\n\n\t\t\t\tf.rebroadcastFundingTx(channel)\n\t\t\t}\n\t\t} else if channel.ChanType.IsSingleFunder() &&\n\t\t\tchannel.ChanType.HasFundingTx() &&\n\t\t\tchannel.IsZeroConf() && channel.IsInitiator &&\n\t\t\t!channel.ZeroConfConfirmed() {\n\n\t\t\t// Rebroadcast the funding transaction for unconfirmed\n\t\t\t// zero-conf channels if we have the funding tx and are\n\t\t\t// also the initiator.\n\t\t\tf.rebroadcastFundingTx(channel)\n\t\t}\n\n\t\t// We will restart the funding state machine for all channels,\n\t\t// which will wait for the channel's funding transaction to be\n\t\t// confirmed on the blockchain, and transmit the messages\n\t\t// necessary for the channel to be operational.\n\t\tf.wg.Add(1)\n\t\tgo f.advanceFundingState(channel, chanID, nil)\n\t}\n\n\tf.wg.Add(1) // TODO(roasbeef): tune\n\tgo f.reservationCoordinator()\n\n\treturn nil\n}\n\n// Stop signals all helper goroutines to execute a graceful shutdown. This\n// method will block until all goroutines have exited.",
      "length": 2248,
      "tokens": 284,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) Stop() error {",
      "content": "func (f *Manager) Stop() error {\n\tf.stopped.Do(func() {\n\t\tlog.Info(\"Funding manager shutting down\")\n\t\tclose(f.quit)\n\t\tf.wg.Wait()\n\t})\n\n\treturn nil\n}\n\n// rebroadcastFundingTx publishes the funding tx on startup for each\n// unconfirmed channel.",
      "length": 199,
      "tokens": 25,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) rebroadcastFundingTx(c *channeldb.OpenChannel) {",
      "content": "func (f *Manager) rebroadcastFundingTx(c *channeldb.OpenChannel) {\n\tvar fundingTxBuf bytes.Buffer\n\terr := c.FundingTxn.Serialize(&fundingTxBuf)\n\tif err != nil {\n\t\tlog.Errorf(\"Unable to serialize funding transaction %v: %v\",\n\t\t\tc.FundingTxn.TxHash(), err)\n\n\t\t// Clear the buffer of any bytes that were written before the\n\t\t// serialization error to prevent logging an incomplete\n\t\t// transaction.\n\t\tfundingTxBuf.Reset()\n\t} else {\n\t\tlog.Debugf(\"Rebroadcasting funding tx for ChannelPoint(%v): \"+\n\t\t\t\"%x\", c.FundingOutpoint, fundingTxBuf.Bytes())\n\t}\n\n\t// Set a nil short channel ID at this stage because we do not know it\n\t// until our funding tx confirms.\n\tlabel := labels.MakeLabel(labels.LabelTypeChannelOpen, nil)\n\n\terr = f.cfg.PublishTransaction(c.FundingTxn, label)\n\tif err != nil {\n\t\tlog.Errorf(\"Unable to rebroadcast funding tx %x for \"+\n\t\t\t\"ChannelPoint(%v): %v\", fundingTxBuf.Bytes(),\n\t\t\tc.FundingOutpoint, err)\n\t}\n}\n\n// nextPendingChanID returns the next free pending channel ID to be used to\n// identify a particular future channel funding workflow.",
      "length": 963,
      "tokens": 127,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) nextPendingChanID() [32]byte {",
      "content": "func (f *Manager) nextPendingChanID() [32]byte {\n\t// Obtain a fresh nonce. We do this by encoding the current nonce\n\t// counter, then incrementing it by one.\n\tf.nonceMtx.Lock()\n\tvar nonce [8]byte\n\tbinary.LittleEndian.PutUint64(nonce[:], f.chanIDNonce)\n\tf.chanIDNonce++\n\tf.nonceMtx.Unlock()\n\n\t// We'll generate the next pending channelID by \"encrypting\" 32-bytes\n\t// of zeroes which'll extract 32 random bytes from our stream cipher.\n\tvar (\n\t\tnextChanID [32]byte\n\t\tzeroes     [32]byte\n\t)\n\tsalsa20.XORKeyStream(nextChanID[:], zeroes[:], nonce[:], &f.chanIDKey)\n\n\treturn nextChanID\n}\n\n// CancelPeerReservations cancels all active reservations associated with the\n// passed node. This will ensure any outputs which have been pre committed,\n// (and thus locked from coin selection), are properly freed.",
      "length": 727,
      "tokens": 96,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) CancelPeerReservations(nodePub [33]byte) {",
      "content": "func (f *Manager) CancelPeerReservations(nodePub [33]byte) {\n\tlog.Debugf(\"Cancelling all reservations for peer %x\", nodePub[:])\n\n\tf.resMtx.Lock()\n\tdefer f.resMtx.Unlock()\n\n\t// We'll attempt to look up this node in the set of active\n\t// reservations.  If they don't have any, then there's no further work\n\t// to be done.\n\tnodeReservations, ok := f.activeReservations[nodePub]\n\tif !ok {\n\t\tlog.Debugf(\"No active reservations for node: %x\", nodePub[:])\n\t\treturn\n\t}\n\n\t// If they do have any active reservations, then we'll cancel all of\n\t// them (which releases any locked UTXO's), and also delete it from the\n\t// reservation map.\n\tfor pendingID, resCtx := range nodeReservations {\n\t\tif err := resCtx.reservation.Cancel(); err != nil {\n\t\t\tlog.Errorf(\"unable to cancel reservation for \"+\n\t\t\t\t\"node=%x: %v\", nodePub[:], err)\n\t\t}\n\n\t\tresCtx.err <- fmt.Errorf(\"peer disconnected\")\n\t\tdelete(nodeReservations, pendingID)\n\t}\n\n\t// Finally, we'll delete the node itself from the set of reservations.\n\tdelete(f.activeReservations, nodePub)\n}\n\n// failFundingFlow will fail the active funding flow with the target peer,\n// identified by its unique temporary channel ID. This method will send an\n// error to the remote peer, and also remove the reservation from our set of\n// pending reservations.\n//\n// TODO(roasbeef): if peer disconnects, and haven't yet broadcast funding\n// transaction, then all reservations should be cleared.",
      "length": 1314,
      "tokens": 194,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) failFundingFlow(peer lnpeer.Peer, tempChanID [32]byte,",
      "content": "func (f *Manager) failFundingFlow(peer lnpeer.Peer, tempChanID [32]byte,\n\tfundingErr error) {\n\n\tlog.Debugf(\"Failing funding flow for pending_id=%x: %v\",\n\t\ttempChanID, fundingErr)\n\n\tctx, err := f.cancelReservationCtx(\n\t\tpeer.IdentityKey(), tempChanID, false,\n\t)\n\tif err != nil {\n\t\tlog.Errorf(\"unable to cancel reservation: %v\", err)\n\t}\n\n\t// In case the case where the reservation existed, send the funding\n\t// error on the error channel.\n\tif ctx != nil {\n\t\tctx.err <- fundingErr\n\t}\n\n\t// We only send the exact error if it is part of out whitelisted set of\n\t// errors (lnwire.FundingError or lnwallet.ReservationError).\n\tvar msg lnwire.ErrorData\n\tswitch e := fundingErr.(type) {\n\t// Let the actual error message be sent to the remote for the\n\t// whitelisted types.\n\tcase lnwallet.ReservationError:\n\t\tmsg = lnwire.ErrorData(e.Error())\n\tcase lnwire.FundingError:\n\t\tmsg = lnwire.ErrorData(e.Error())\n\tcase chanacceptor.ChanAcceptError:\n\t\tmsg = lnwire.ErrorData(e.Error())\n\n\t// For all other error types we just send a generic error.\n\tdefault:\n\t\tmsg = lnwire.ErrorData(\"funding failed due to internal error\")\n\t}\n\n\terrMsg := &lnwire.Error{\n\t\tChanID: tempChanID,\n\t\tData:   msg,\n\t}\n\n\tlog.Debugf(\"Sending funding error to peer (%x): %v\",\n\t\tpeer.IdentityKey().SerializeCompressed(), spew.Sdump(errMsg))\n\tif err := peer.SendMessage(false, errMsg); err != nil {\n\t\tlog.Errorf(\"unable to send error message to peer %v\", err)\n\t}\n}\n\n// reservationCoordinator is the primary goroutine tasked with progressing the\n// funding workflow between the wallet, and any outside peers or local callers.\n//\n// NOTE: This MUST be run as a goroutine.",
      "length": 1495,
      "tokens": 210,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) reservationCoordinator() {",
      "content": "func (f *Manager) reservationCoordinator() {\n\tdefer f.wg.Done()\n\n\tzombieSweepTicker := time.NewTicker(f.cfg.ZombieSweeperInterval)\n\tdefer zombieSweepTicker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase fmsg := <-f.fundingMsgs:\n\t\t\tswitch msg := fmsg.msg.(type) {\n\t\t\tcase *lnwire.OpenChannel:\n\t\t\t\tf.handleFundingOpen(fmsg.peer, msg)\n\n\t\t\tcase *lnwire.AcceptChannel:\n\t\t\t\tf.handleFundingAccept(fmsg.peer, msg)\n\n\t\t\tcase *lnwire.FundingCreated:\n\t\t\t\tf.handleFundingCreated(fmsg.peer, msg)\n\n\t\t\tcase *lnwire.FundingSigned:\n\t\t\t\tf.handleFundingSigned(fmsg.peer, msg)\n\n\t\t\tcase *lnwire.FundingLocked:\n\t\t\t\tf.wg.Add(1)\n\t\t\t\tgo f.handleFundingLocked(fmsg.peer, msg)\n\n\t\t\tcase *lnwire.Warning:\n\t\t\t\tf.handleWarningMsg(fmsg.peer, msg)\n\n\t\t\tcase *lnwire.Error:\n\t\t\t\tf.handleErrorMsg(fmsg.peer, msg)\n\t\t\t}\n\t\tcase req := <-f.fundingRequests:\n\t\t\tf.handleInitFundingMsg(req)\n\n\t\tcase <-zombieSweepTicker.C:\n\t\t\tf.pruneZombieReservations()\n\n\t\tcase <-f.quit:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// advanceFundingState will advance the channel through the steps after the\n// funding transaction is broadcasted, up until the point where the channel is\n// ready for operation. This includes waiting for the funding transaction to\n// confirm, sending funding locked to the peer, adding the channel to the\n// router graph, and announcing the channel. The updateChan can be set non-nil\n// to get OpenStatusUpdates.\n//\n// NOTE: This MUST be run as a goroutine.",
      "length": 1295,
      "tokens": 141,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) advanceFundingState(channel *channeldb.OpenChannel,",
      "content": "func (f *Manager) advanceFundingState(channel *channeldb.OpenChannel,\n\tpendingChanID [32]byte, updateChan chan<- *lnrpc.OpenStatusUpdate) {\n\n\tdefer f.wg.Done()\n\n\t// If the channel is still pending we must wait for the funding\n\t// transaction to confirm.\n\tif channel.IsPending {\n\t\terr := f.advancePendingChannelState(channel, pendingChanID)\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"Unable to advance pending state of \"+\n\t\t\t\t\"ChannelPoint(%v): %v\",\n\t\t\t\tchannel.FundingOutpoint, err)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// We create the state-machine object which wraps the database state.\n\tlnChannel, err := lnwallet.NewLightningChannel(\n\t\tnil, channel, nil,\n\t)\n\tif err != nil {\n\t\tlog.Errorf(\"Unable to create LightningChannel(%v): %v\",\n\t\t\tchannel.FundingOutpoint, err)\n\t\treturn\n\t}\n\n\tfor {\n\t\tchannelState, shortChanID, err := f.getChannelOpeningState(\n\t\t\t&channel.FundingOutpoint,\n\t\t)\n\t\tif err == channeldb.ErrChannelNotFound {\n\t\t\t// Channel not in fundingManager's opening database,\n\t\t\t// meaning it was successfully announced to the\n\t\t\t// network.\n\t\t\t// TODO(halseth): could do graph consistency check\n\t\t\t// here, and re-add the edge if missing.\n\t\t\tlog.Debugf(\"ChannelPoint(%v) with chan_id=%x not \"+\n\t\t\t\t\"found in opening database, assuming already \"+\n\t\t\t\t\"announced to the network\",\n\t\t\t\tchannel.FundingOutpoint, pendingChanID)\n\t\t\treturn\n\t\t} else if err != nil {\n\t\t\tlog.Errorf(\"Unable to query database for \"+\n\t\t\t\t\"channel opening state(%v): %v\",\n\t\t\t\tchannel.FundingOutpoint, err)\n\t\t\treturn\n\t\t}\n\n\t\t// If we did find the channel in the opening state database, we\n\t\t// have seen the funding transaction being confirmed, but there\n\t\t// are still steps left of the setup procedure. We continue the\n\t\t// procedure where we left off.\n\t\terr = f.stateStep(\n\t\t\tchannel, lnChannel, shortChanID, pendingChanID,\n\t\t\tchannelState, updateChan,\n\t\t)\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"Unable to advance state(%v): %v\",\n\t\t\t\tchannel.FundingOutpoint, err)\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// stateStep advances the confirmed channel one step in the funding state\n// machine. This method is synchronous and the new channel opening state will\n// have been written to the database when it successfully returns. The\n// updateChan can be set non-nil to get OpenStatusUpdates.",
      "length": 2074,
      "tokens": 283,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) stateStep(channel *channeldb.OpenChannel,",
      "content": "func (f *Manager) stateStep(channel *channeldb.OpenChannel,\n\tlnChannel *lnwallet.LightningChannel,\n\tshortChanID *lnwire.ShortChannelID, pendingChanID [32]byte,\n\tchannelState channelOpeningState,\n\tupdateChan chan<- *lnrpc.OpenStatusUpdate) error {\n\n\tchanID := lnwire.NewChanIDFromOutPoint(&channel.FundingOutpoint)\n\tlog.Debugf(\"Channel(%v) with ShortChanID %v has opening state %v\",\n\t\tchanID, shortChanID, channelState)\n\n\tswitch channelState {\n\t// The funding transaction was confirmed, but we did not successfully\n\t// send the fundingLocked message to the peer, so let's do that now.\n\tcase markedOpen:\n\t\terr := f.sendFundingLocked(channel, lnChannel)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed sending fundingLocked: %v\",\n\t\t\t\terr)\n\t\t}\n\n\t\t// As the fundingLocked message is now sent to the peer, the\n\t\t// channel is moved to the next state of the state machine. It\n\t\t// will be moved to the last state (actually deleted from the\n\t\t// database) after the channel is finally announced.\n\t\terr = f.saveChannelOpeningState(\n\t\t\t&channel.FundingOutpoint, fundingLockedSent,\n\t\t\tshortChanID,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"error setting channel state to\"+\n\t\t\t\t\" fundingLockedSent: %v\", err)\n\t\t}\n\n\t\tlog.Debugf(\"Channel(%v) with ShortChanID %v: successfully \"+\n\t\t\t\"sent FundingLocked\", chanID, shortChanID)\n\n\t\treturn nil\n\n\t// fundingLocked was sent to peer, but the channel was not added to the\n\t// router graph and the channel announcement was not sent.\n\tcase fundingLockedSent:\n\t\t// We must wait until we've received the peer's funding locked\n\t\t// before sending a channel_update according to BOLT#07.\n\t\treceived, err := f.receivedFundingLocked(\n\t\t\tchannel.IdentityPub, chanID,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to check if funding locked \"+\n\t\t\t\t\"was received: %v\", err)\n\t\t}\n\n\t\tif !received {\n\t\t\t// We haven't received FundingLocked, so we'll continue\n\t\t\t// to the next iteration of the loop after sleeping for\n\t\t\t// checkPeerFundingLockInterval.\n\t\t\tselect {\n\t\t\tcase <-time.After(checkPeerFundingLockInterval):\n\t\t\tcase <-f.quit:\n\t\t\t\treturn ErrFundingManagerShuttingDown\n\t\t\t}\n\n\t\t\treturn nil\n\t\t}\n\n\t\tvar peerAlias *lnwire.ShortChannelID\n\t\tif channel.IsZeroConf() {\n\t\t\t// We'll need to wait until funding_locked has been\n\t\t\t// received and the peer lets us know the alias they\n\t\t\t// want to use for the channel. With this information,\n\t\t\t// we can then construct a ChannelUpdate for them.\n\t\t\t// If an alias does not yet exist, we'll just return,\n\t\t\t// letting the next iteration of the loop check again.\n\t\t\tvar defaultAlias lnwire.ShortChannelID\n\t\t\tchanID := lnwire.NewChanIDFromOutPoint(\n\t\t\t\t&channel.FundingOutpoint,\n\t\t\t)\n\t\t\tfoundAlias, _ := f.cfg.AliasManager.GetPeerAlias(\n\t\t\t\tchanID,\n\t\t\t)\n\t\t\tif foundAlias == defaultAlias {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\tpeerAlias = &foundAlias\n\t\t}\n\n\t\terr = f.addToRouterGraph(channel, shortChanID, peerAlias, nil)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed adding to \"+\n\t\t\t\t\"router graph: %v\", err)\n\t\t}\n\n\t\t// As the channel is now added to the ChannelRouter's topology,\n\t\t// the channel is moved to the next state of the state machine.\n\t\t// It will be moved to the last state (actually deleted from\n\t\t// the database) after the channel is finally announced.\n\t\terr = f.saveChannelOpeningState(\n\t\t\t&channel.FundingOutpoint, addedToRouterGraph,\n\t\t\tshortChanID,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"error setting channel state to\"+\n\t\t\t\t\" addedToRouterGraph: %v\", err)\n\t\t}\n\n\t\tlog.Debugf(\"Channel(%v) with ShortChanID %v: successfully \"+\n\t\t\t\"added to router graph\", chanID, shortChanID)\n\n\t\t// Give the caller a final update notifying them that\n\t\t// the channel is now open.\n\t\t// TODO(roasbeef): only notify after recv of funding locked?\n\t\tfundingPoint := channel.FundingOutpoint\n\t\tcp := &lnrpc.ChannelPoint{\n\t\t\tFundingTxid: &lnrpc.ChannelPoint_FundingTxidBytes{\n\t\t\t\tFundingTxidBytes: fundingPoint.Hash[:],\n\t\t\t},\n\t\t\tOutputIndex: fundingPoint.Index,\n\t\t}\n\n\t\tif updateChan != nil {\n\t\t\tupd := &lnrpc.OpenStatusUpdate{\n\t\t\t\tUpdate: &lnrpc.OpenStatusUpdate_ChanOpen{\n\t\t\t\t\tChanOpen: &lnrpc.ChannelOpenUpdate{\n\t\t\t\t\t\tChannelPoint: cp,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\tPendingChanId: pendingChanID[:],\n\t\t\t}\n\n\t\t\tselect {\n\t\t\tcase updateChan <- upd:\n\t\t\tcase <-f.quit:\n\t\t\t\treturn ErrFundingManagerShuttingDown\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\n\t// The channel was added to the Router's topology, but the channel\n\t// announcement was not sent.\n\tcase addedToRouterGraph:\n\t\tif channel.IsZeroConf() {\n\t\t\t// If this is a zero-conf channel, then we will wait\n\t\t\t// for it to be confirmed before announcing it to the\n\t\t\t// greater network.\n\t\t\terr := f.waitForZeroConfChannel(channel, pendingChanID)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed waiting for zero \"+\n\t\t\t\t\t\"channel: %v\", err)\n\t\t\t}\n\n\t\t\t// Update the local shortChanID variable such that\n\t\t\t// annAfterSixConfs uses the confirmed SCID.\n\t\t\tconfirmedScid := channel.ZeroConfRealScid()\n\t\t\tshortChanID = &confirmedScid\n\t\t}\n\n\t\terr := f.annAfterSixConfs(channel, shortChanID)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"error sending channel \"+\n\t\t\t\t\"announcement: %v\", err)\n\t\t}\n\n\t\t// We delete the channel opening state from our internal\n\t\t// database as the opening process has succeeded. We can do\n\t\t// this because we assume the AuthenticatedGossiper queues the\n\t\t// announcement messages, and persists them in case of a daemon\n\t\t// shutdown.\n\t\terr = f.deleteChannelOpeningState(&channel.FundingOutpoint)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"error deleting channel state: %v\",\n\t\t\t\terr)\n\t\t}\n\n\t\tlog.Debugf(\"Channel(%v) with ShortChanID %v: successfully \"+\n\t\t\t\"announced\", chanID, shortChanID)\n\n\t\treturn nil\n\t}\n\n\treturn fmt.Errorf(\"undefined channelState: %v\", channelState)\n}\n\n// advancePendingChannelState waits for a pending channel's funding tx to\n// confirm, and marks it open in the database when that happens.",
      "length": 5542,
      "tokens": 727,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) advancePendingChannelState(",
      "content": "func (f *Manager) advancePendingChannelState(\n\tchannel *channeldb.OpenChannel, pendingChanID [32]byte) error {\n\n\tif channel.IsZeroConf() {\n\t\t// Persist the alias to the alias database.\n\t\tbaseScid := channel.ShortChannelID\n\t\terr := f.cfg.AliasManager.AddLocalAlias(\n\t\t\tbaseScid, baseScid, true,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"error adding local alias to \"+\n\t\t\t\t\"store: %v\", err)\n\t\t}\n\n\t\t// We don't wait for zero-conf channels to be confirmed and\n\t\t// instead immediately proceed with the rest of the funding\n\t\t// flow. The channel opening state is stored under the alias\n\t\t// SCID.\n\t\terr = f.saveChannelOpeningState(\n\t\t\t&channel.FundingOutpoint, markedOpen,\n\t\t\t&channel.ShortChannelID,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"error setting zero-conf channel \"+\n\t\t\t\t\"state to markedOpen: %v\", err)\n\t\t}\n\n\t\t// The ShortChannelID is already set since it's an alias, but\n\t\t// we still need to mark the channel as no longer pending.\n\t\terr = channel.MarkAsOpen(channel.ShortChannelID)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"error setting zero-conf channel's \"+\n\t\t\t\t\"pending flag to false: %v\", err)\n\t\t}\n\n\t\t// Inform the ChannelNotifier that the channel has transitioned\n\t\t// from pending open to open.\n\t\tf.cfg.NotifyOpenChannelEvent(channel.FundingOutpoint)\n\n\t\t// Find and close the discoverySignal for this channel such\n\t\t// that FundingLocked messages will be processed.\n\t\tchanID := lnwire.NewChanIDFromOutPoint(\n\t\t\t&channel.FundingOutpoint,\n\t\t)\n\t\tf.localDiscoveryMtx.Lock()\n\t\tdefer f.localDiscoveryMtx.Unlock()\n\t\tif discoverySignal, ok := f.localDiscoverySignals[chanID]; ok {\n\t\t\tclose(discoverySignal)\n\t\t}\n\n\t\treturn nil\n\t}\n\n\tconfChannel, err := f.waitForFundingWithTimeout(channel)\n\tif err == ErrConfirmationTimeout {\n\t\treturn f.fundingTimeout(channel, pendingChanID)\n\t} else if err != nil {\n\t\treturn fmt.Errorf(\"error waiting for funding \"+\n\t\t\t\"confirmation for ChannelPoint(%v): %v\",\n\t\t\tchannel.FundingOutpoint, err)\n\t}\n\n\t// Success, funding transaction was confirmed.\n\tchanID := lnwire.NewChanIDFromOutPoint(&channel.FundingOutpoint)\n\tlog.Debugf(\"ChannelID(%v) is now fully confirmed! \"+\n\t\t\"(shortChanID=%v)\", chanID, confChannel.shortChanID)\n\n\terr = f.handleFundingConfirmation(channel, confChannel)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to handle funding \"+\n\t\t\t\"confirmation for ChannelPoint(%v): %v\",\n\t\t\tchannel.FundingOutpoint, err)\n\t}\n\n\treturn nil\n}\n\n// ProcessFundingMsg sends a message to the internal fundingManager goroutine,\n// allowing it to handle the lnwire.Message.",
      "length": 2380,
      "tokens": 290,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) ProcessFundingMsg(msg lnwire.Message, peer lnpeer.Peer) {",
      "content": "func (f *Manager) ProcessFundingMsg(msg lnwire.Message, peer lnpeer.Peer) {\n\tselect {\n\tcase f.fundingMsgs <- &fundingMsg{msg, peer}:\n\tcase <-f.quit:\n\t\treturn\n\t}\n}\n\n// handleFundingOpen creates an initial 'ChannelReservation' within the wallet,\n// then responds to the source peer with an accept channel message progressing\n// the funding workflow.\n//\n// TODO(roasbeef): add error chan to all, let channelManager handle\n// error+propagate.",
      "length": 350,
      "tokens": 51,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) handleFundingOpen(peer lnpeer.Peer,",
      "content": "func (f *Manager) handleFundingOpen(peer lnpeer.Peer,\n\tmsg *lnwire.OpenChannel) {\n\n\t// Check number of pending channels to be smaller than maximum allowed\n\t// number and send ErrorGeneric to remote peer if condition is\n\t// violated.\n\tpeerPubKey := peer.IdentityKey()\n\tpeerIDKey := newSerializedKey(peerPubKey)\n\n\tamt := msg.FundingAmount\n\n\t// We get all pending channels for this peer. This is the list of the\n\t// active reservations and the channels pending open in the database.\n\tf.resMtx.RLock()\n\treservations := f.activeReservations[peerIDKey]\n\n\t// We don't count reservations that were created from a canned funding\n\t// shim. The user has registered the shim and therefore expects this\n\t// channel to arrive.\n\tnumPending := 0\n\tfor _, res := range reservations {\n\t\tif !res.reservation.IsCannedShim() {\n\t\t\tnumPending++\n\t\t}\n\t}\n\tf.resMtx.RUnlock()\n\n\t// Also count the channels that are already pending. There we don't know\n\t// the underlying intent anymore, unfortunately.\n\tchannels, err := f.cfg.Wallet.Cfg.Database.FetchOpenChannels(peerPubKey)\n\tif err != nil {\n\t\tf.failFundingFlow(\n\t\t\tpeer, msg.PendingChannelID, err,\n\t\t)\n\t\treturn\n\t}\n\n\tfor _, c := range channels {\n\t\t// Pending channels that have a non-zero thaw height were also\n\t\t// created through a canned funding shim. Those also don't\n\t\t// count towards the DoS protection limit.\n\t\t//\n\t\t// TODO(guggero): Properly store the funding type (wallet, shim,\n\t\t// PSBT) on the channel so we don't need to use the thaw height.\n\t\tif c.IsPending && c.ThawHeight == 0 {\n\t\t\tnumPending++\n\t\t}\n\t}\n\n\t// TODO(roasbeef): modify to only accept a _single_ pending channel per\n\t// block unless white listed\n\tif numPending >= f.cfg.MaxPendingChannels {\n\t\tf.failFundingFlow(\n\t\t\tpeer, msg.PendingChannelID,\n\t\t\tlnwire.ErrMaxPendingChannels,\n\t\t)\n\t\treturn\n\t}\n\n\t// Ensure that the pendingChansLimit is respected.\n\tpendingChans, err := f.cfg.Wallet.Cfg.Database.FetchPendingChannels()\n\tif err != nil {\n\t\tf.failFundingFlow(\n\t\t\tpeer, msg.PendingChannelID, err,\n\t\t)\n\n\t\treturn\n\t}\n\n\tif len(pendingChans) > pendingChansLimit {\n\t\tf.failFundingFlow(\n\t\t\tpeer, msg.PendingChannelID,\n\t\t\tlnwire.ErrMaxPendingChannels,\n\t\t)\n\n\t\treturn\n\t}\n\n\t// We'll also reject any requests to create channels until we're fully\n\t// synced to the network as we won't be able to properly validate the\n\t// confirmation of the funding transaction.\n\tisSynced, _, err := f.cfg.Wallet.IsSynced()\n\tif err != nil || !isSynced {\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"unable to query wallet: %v\", err)\n\t\t}\n\t\terr := errors.New(\"Synchronizing blockchain\")\n\t\tf.failFundingFlow(\n\t\t\tpeer, msg.PendingChannelID,\n\t\t\terr,\n\t\t)\n\t\treturn\n\t}\n\n\t// Ensure that the remote party respects our maximum channel size.\n\tif amt > f.cfg.MaxChanSize {\n\t\tf.failFundingFlow(\n\t\t\tpeer, msg.PendingChannelID,\n\t\t\tlnwallet.ErrChanTooLarge(amt, f.cfg.MaxChanSize),\n\t\t)\n\t\treturn\n\t}\n\n\t// We'll, also ensure that the remote party isn't attempting to propose\n\t// a channel that's below our current min channel size.\n\tif amt < f.cfg.MinChanSize {\n\t\tf.failFundingFlow(\n\t\t\tpeer, msg.PendingChannelID,\n\t\t\tlnwallet.ErrChanTooSmall(amt, f.cfg.MinChanSize),\n\t\t)\n\t\treturn\n\t}\n\n\t// If request specifies non-zero push amount and 'rejectpush' is set,\n\t// signal an error.\n\tif f.cfg.RejectPush && msg.PushAmount > 0 {\n\t\tf.failFundingFlow(\n\t\t\tpeer, msg.PendingChannelID,\n\t\t\tlnwallet.ErrNonZeroPushAmount(),\n\t\t)\n\t\treturn\n\t}\n\n\t// Send the OpenChannel request to the ChannelAcceptor to determine\n\t// whether this node will accept the channel.\n\tchanReq := &chanacceptor.ChannelAcceptRequest{\n\t\tNode:        peer.IdentityKey(),\n\t\tOpenChanMsg: msg,\n\t}\n\n\t// Query our channel acceptor to determine whether we should reject\n\t// the channel.\n\tacceptorResp := f.cfg.OpenChannelPredicate.Accept(chanReq)\n\tif acceptorResp.RejectChannel() {\n\t\tf.failFundingFlow(\n\t\t\tpeer, msg.PendingChannelID,\n\t\t\tacceptorResp.ChanAcceptError,\n\t\t)\n\t\treturn\n\t}\n\n\tlog.Infof(\"Recv'd fundingRequest(amt=%v, push=%v, delay=%v, \"+\n\t\t\"pendingId=%x) from peer(%x)\", amt, msg.PushAmount,\n\t\tmsg.CsvDelay, msg.PendingChannelID,\n\t\tpeer.IdentityKey().SerializeCompressed())\n\n\t// Attempt to initialize a reservation within the wallet. If the wallet\n\t// has insufficient resources to create the channel, then the\n\t// reservation attempt may be rejected. Note that since we're on the\n\t// responding side of a single funder workflow, we don't commit any\n\t// funds to the channel ourselves.\n\t//\n\t// Before we init the channel, we'll also check to see what commitment\n\t// format we can use with this peer. This is dependent on *both* us and\n\t// the remote peer are signaling the proper feature bit if we're using\n\t// implicit negotiation, and simply the channel type sent over if we're\n\t// using explicit negotiation.\n\twasExplicit, _, commitType, err := negotiateCommitmentType(\n\t\tmsg.ChannelType, peer.LocalFeatures(), peer.RemoteFeatures(),\n\t\tfalse,\n\t)\n\tif err != nil {\n\t\t// TODO(roasbeef): should be using soft errors\n\t\tlog.Errorf(\"channel type negotiation failed: %v\", err)\n\t\tf.failFundingFlow(peer, msg.PendingChannelID, err)\n\t\treturn\n\t}\n\n\tvar scidFeatureVal bool\n\tif hasFeatures(\n\t\tpeer.LocalFeatures(), peer.RemoteFeatures(),\n\t\tlnwire.ScidAliasOptional,\n\t) {\n\n\t\tscidFeatureVal = true\n\t}\n\n\tvar (\n\t\tchanTypeFeatureBits *lnwire.ChannelType\n\t\tzeroConf            bool\n\t\tscid                bool\n\t)\n\n\tif wasExplicit {\n\t\t// Only echo back a channel type in AcceptChannel if we actually\n\t\t// used explicit negotiation above.\n\t\tchanTypeFeatureBits = msg.ChannelType\n\n\t\t// Check if the channel type includes the zero-conf or\n\t\t// scid-alias bits.\n\t\tfeatureVec := lnwire.RawFeatureVector(*chanTypeFeatureBits)\n\t\tzeroConf = featureVec.IsSet(lnwire.ZeroConfRequired)\n\t\tscid = featureVec.IsSet(lnwire.ScidAliasRequired)\n\n\t\t// If the zero-conf channel type was negotiated, ensure that\n\t\t// the acceptor allows it.\n\t\tif zeroConf && !acceptorResp.ZeroConf {\n\t\t\t// Fail the funding flow.\n\t\t\tflowErr := fmt.Errorf(\"channel acceptor blocked \" +\n\t\t\t\t\"zero-conf channel negotiation\")\n\t\t\tf.failFundingFlow(\n\t\t\t\tpeer, msg.PendingChannelID, flowErr,\n\t\t\t)\n\t\t\treturn\n\t\t}\n\n\t\t// If the zero-conf channel type wasn't negotiated and the\n\t\t// fundee still wants a zero-conf channel, perform more checks.\n\t\t// Require that both sides have the scid-alias feature bit set.\n\t\t// We don't require anchors here - this is for compatibility\n\t\t// with LDK.\n\t\tif !zeroConf && acceptorResp.ZeroConf {\n\t\t\tif !scidFeatureVal {\n\t\t\t\t// Fail the funding flow.\n\t\t\t\tflowErr := fmt.Errorf(\"scid-alias feature \" +\n\t\t\t\t\t\"must be negotiated for zero-conf\")\n\t\t\t\tf.failFundingFlow(\n\t\t\t\t\tpeer, msg.PendingChannelID, flowErr,\n\t\t\t\t)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Set zeroConf to true to enable the zero-conf flow.\n\t\t\tzeroConf = true\n\t\t}\n\t}\n\n\t// Sending the option-scid-alias channel type for a public channel is\n\t// disallowed.\n\tpublic := msg.ChannelFlags&lnwire.FFAnnounceChannel != 0\n\tif public && scid {\n\t\terr = fmt.Errorf(\"option-scid-alias chantype for public \" +\n\t\t\t\"channel\")\n\t\tlog.Error(err)\n\t\tf.failFundingFlow(peer, msg.PendingChannelID, err)\n\t\treturn\n\t}\n\n\treq := &lnwallet.InitFundingReserveMsg{\n\t\tChainHash:        &msg.ChainHash,\n\t\tPendingChanID:    msg.PendingChannelID,\n\t\tNodeID:           peer.IdentityKey(),\n\t\tNodeAddr:         peer.Address(),\n\t\tLocalFundingAmt:  0,\n\t\tRemoteFundingAmt: amt,\n\t\tCommitFeePerKw:   chainfee.SatPerKWeight(msg.FeePerKiloWeight),\n\t\tFundingFeePerKw:  0,\n\t\tPushMSat:         msg.PushAmount,\n\t\tFlags:            msg.ChannelFlags,\n\t\tMinConfs:         1,\n\t\tCommitType:       commitType,\n\t\tZeroConf:         zeroConf,\n\t\tOptionScidAlias:  scid,\n\t\tScidAliasFeature: scidFeatureVal,\n\t}\n\n\treservation, err := f.cfg.Wallet.InitChannelReservation(req)\n\tif err != nil {\n\t\tlog.Errorf(\"Unable to initialize reservation: %v\", err)\n\t\tf.failFundingFlow(peer, msg.PendingChannelID, err)\n\t\treturn\n\t}\n\n\tlog.Debugf(\"Initialized channel reservation: zeroConf=%v, psbt=%v, \"+\n\t\t\"cannedShim=%v\", reservation.IsZeroConf(),\n\t\treservation.IsPsbt(), reservation.IsCannedShim())\n\n\tif zeroConf {\n\t\t// Store an alias for zero-conf channels. Other option-scid\n\t\t// channels will do this at a later point.\n\t\taliasScid, err := f.cfg.AliasManager.RequestAlias()\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"Unable to request alias: %v\", err)\n\t\t\tf.failFundingFlow(peer, msg.PendingChannelID, err)\n\t\t\treturn\n\t\t}\n\n\t\treservation.AddAlias(aliasScid)\n\t}\n\n\t// As we're the responder, we get to specify the number of confirmations\n\t// that we require before both of us consider the channel open. We'll\n\t// use our mapping to derive the proper number of confirmations based on\n\t// the amount of the channel, and also if any funds are being pushed to\n\t// us. If a depth value was set by our channel acceptor, we will use\n\t// that value instead.\n\tnumConfsReq := f.cfg.NumRequiredConfs(msg.FundingAmount, msg.PushAmount)\n\tif acceptorResp.MinAcceptDepth != 0 {\n\t\tnumConfsReq = acceptorResp.MinAcceptDepth\n\t}\n\n\t// We'll ignore the min_depth calculated above if this is a zero-conf\n\t// channel.\n\tif zeroConf {\n\t\tnumConfsReq = 0\n\t}\n\n\treservation.SetNumConfsRequired(numConfsReq)\n\n\t// We'll also validate and apply all the constraints the initiating\n\t// party is attempting to dictate for our commitment transaction.\n\tchannelConstraints := &channeldb.ChannelConstraints{\n\t\tDustLimit:        msg.DustLimit,\n\t\tChanReserve:      msg.ChannelReserve,\n\t\tMaxPendingAmount: msg.MaxValueInFlight,\n\t\tMinHTLC:          msg.HtlcMinimum,\n\t\tMaxAcceptedHtlcs: msg.MaxAcceptedHTLCs,\n\t\tCsvDelay:         msg.CsvDelay,\n\t}\n\terr = reservation.CommitConstraints(\n\t\tchannelConstraints, f.cfg.MaxLocalCSVDelay, true,\n\t)\n\tif err != nil {\n\t\tlog.Errorf(\"Unacceptable channel constraints: %v\", err)\n\t\tf.failFundingFlow(peer, msg.PendingChannelID, err)\n\t\treturn\n\t}\n\n\t// Check whether the peer supports upfront shutdown, and get a new\n\t// wallet address if our node is configured to set shutdown addresses by\n\t// default. We use the upfront shutdown script provided by our channel\n\t// acceptor (if any) in lieu of user input.\n\tshutdown, err := getUpfrontShutdownScript(\n\t\tf.cfg.EnableUpfrontShutdown, peer, acceptorResp.UpfrontShutdown,\n\t\tf.selectShutdownScript,\n\t)\n\tif err != nil {\n\t\tf.failFundingFlow(\n\t\t\tpeer, msg.PendingChannelID,\n\t\t\tfmt.Errorf(\"getUpfrontShutdownScript error: %v\", err),\n\t\t)\n\t\treturn\n\t}\n\treservation.SetOurUpfrontShutdown(shutdown)\n\n\t// If a script enforced channel lease is being proposed, we'll need to\n\t// validate its custom TLV records.\n\tif commitType == lnwallet.CommitmentTypeScriptEnforcedLease {\n\t\tif msg.LeaseExpiry == nil {\n\t\t\terr := errors.New(\"missing lease expiry\")\n\t\t\tf.failFundingFlow(peer, msg.PendingChannelID, err)\n\t\t\treturn\n\t\t}\n\n\t\t// If we had a shim registered for this channel prior to\n\t\t// receiving its corresponding OpenChannel message, then we'll\n\t\t// validate the proposed LeaseExpiry against what was registered\n\t\t// in our shim.\n\t\tif reservation.LeaseExpiry() != 0 {\n\t\t\tif uint32(*msg.LeaseExpiry) !=\n\t\t\t\treservation.LeaseExpiry() {\n\n\t\t\t\terr := errors.New(\"lease expiry mismatch\")\n\t\t\t\tf.failFundingFlow(\n\t\t\t\t\tpeer, msg.PendingChannelID, err,\n\t\t\t\t)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\tlog.Infof(\"Requiring %v confirmations for pendingChan(%x): \"+\n\t\t\"amt=%v, push_amt=%v, committype=%v, upfrontShutdown=%x\",\n\t\tnumConfsReq, msg.PendingChannelID, amt, msg.PushAmount,\n\t\tcommitType, msg.UpfrontShutdownScript)\n\n\t// Generate our required constraints for the remote party, using the\n\t// values provided by the channel acceptor if they are non-zero.\n\tremoteCsvDelay := f.cfg.RequiredRemoteDelay(amt)\n\tif acceptorResp.CSVDelay != 0 {\n\t\tremoteCsvDelay = acceptorResp.CSVDelay\n\t}\n\n\t// If our default dust limit was above their ChannelReserve, we change\n\t// it to the ChannelReserve. We must make sure the ChannelReserve we\n\t// send in the AcceptChannel message is above both dust limits.\n\t// Therefore, take the maximum of msg.DustLimit and our dust limit.\n\t//\n\t// NOTE: Even with this bounding, the ChannelAcceptor may return an\n\t// BOLT#02-invalid ChannelReserve.\n\tmaxDustLimit := reservation.OurContribution().DustLimit\n\tif msg.DustLimit > maxDustLimit {\n\t\tmaxDustLimit = msg.DustLimit\n\t}\n\n\tchanReserve := f.cfg.RequiredRemoteChanReserve(amt, maxDustLimit)\n\tif acceptorResp.Reserve != 0 {\n\t\tchanReserve = acceptorResp.Reserve\n\t}\n\n\tremoteMaxValue := f.cfg.RequiredRemoteMaxValue(amt)\n\tif acceptorResp.InFlightTotal != 0 {\n\t\tremoteMaxValue = acceptorResp.InFlightTotal\n\t}\n\n\tmaxHtlcs := f.cfg.RequiredRemoteMaxHTLCs(amt)\n\tif acceptorResp.HtlcLimit != 0 {\n\t\tmaxHtlcs = acceptorResp.HtlcLimit\n\t}\n\n\t// Default to our default minimum hltc value, replacing it with the\n\t// channel acceptor's value if it is set.\n\tminHtlc := f.cfg.DefaultMinHtlcIn\n\tif acceptorResp.MinHtlcIn != 0 {\n\t\tminHtlc = acceptorResp.MinHtlcIn\n\t}\n\n\t// If we are handling a FundingOpen request then we need to\n\t// specify the default channel fees since they are not provided\n\t// by the responder interactively.\n\tforwardingPolicy := htlcswitch.ForwardingPolicy{\n\t\tBaseFee: f.cfg.DefaultRoutingPolicy.BaseFee,\n\t\tFeeRate: f.cfg.DefaultRoutingPolicy.FeeRate,\n\t}\n\n\t// Once the reservation has been created successfully, we add it to\n\t// this peer's map of pending reservations to track this particular\n\t// reservation until either abort or completion.\n\tf.resMtx.Lock()\n\tif _, ok := f.activeReservations[peerIDKey]; !ok {\n\t\tf.activeReservations[peerIDKey] = make(pendingChannels)\n\t}\n\tresCtx := &reservationWithCtx{\n\t\treservation:       reservation,\n\t\tchanAmt:           amt,\n\t\tforwardingPolicy:  forwardingPolicy,\n\t\tremoteCsvDelay:    remoteCsvDelay,\n\t\tremoteMinHtlc:     minHtlc,\n\t\tremoteMaxValue:    remoteMaxValue,\n\t\tremoteMaxHtlcs:    maxHtlcs,\n\t\tremoteChanReserve: chanReserve,\n\t\tmaxLocalCsv:       f.cfg.MaxLocalCSVDelay,\n\t\tchannelType:       msg.ChannelType,\n\t\terr:               make(chan error, 1),\n\t\tpeer:              peer,\n\t}\n\tf.activeReservations[peerIDKey][msg.PendingChannelID] = resCtx\n\tf.resMtx.Unlock()\n\n\t// Update the timestamp once the fundingOpenMsg has been handled.\n\tdefer resCtx.updateTimestamp()\n\n\t// With our parameters set, we'll now process their contribution so we\n\t// can move the funding workflow ahead.\n\tremoteContribution := &lnwallet.ChannelContribution{\n\t\tFundingAmount:        amt,\n\t\tFirstCommitmentPoint: msg.FirstCommitmentPoint,\n\t\tChannelConfig: &channeldb.ChannelConfig{\n\t\t\tChannelConstraints: channeldb.ChannelConstraints{\n\t\t\t\tDustLimit:        msg.DustLimit,\n\t\t\t\tMaxPendingAmount: remoteMaxValue,\n\t\t\t\tChanReserve:      chanReserve,\n\t\t\t\tMinHTLC:          minHtlc,\n\t\t\t\tMaxAcceptedHtlcs: maxHtlcs,\n\t\t\t\tCsvDelay:         remoteCsvDelay,\n\t\t\t},\n\t\t\tMultiSigKey: keychain.KeyDescriptor{\n\t\t\t\tPubKey: copyPubKey(msg.FundingKey),\n\t\t\t},\n\t\t\tRevocationBasePoint: keychain.KeyDescriptor{\n\t\t\t\tPubKey: copyPubKey(msg.RevocationPoint),\n\t\t\t},\n\t\t\tPaymentBasePoint: keychain.KeyDescriptor{\n\t\t\t\tPubKey: copyPubKey(msg.PaymentPoint),\n\t\t\t},\n\t\t\tDelayBasePoint: keychain.KeyDescriptor{\n\t\t\t\tPubKey: copyPubKey(msg.DelayedPaymentPoint),\n\t\t\t},\n\t\t\tHtlcBasePoint: keychain.KeyDescriptor{\n\t\t\t\tPubKey: copyPubKey(msg.HtlcPoint),\n\t\t\t},\n\t\t},\n\t\tUpfrontShutdown: msg.UpfrontShutdownScript,\n\t}\n\terr = reservation.ProcessSingleContribution(remoteContribution)\n\tif err != nil {\n\t\tlog.Errorf(\"unable to add contribution reservation: %v\", err)\n\t\tf.failFundingFlow(peer, msg.PendingChannelID, err)\n\t\treturn\n\t}\n\n\tlog.Infof(\"Sending fundingResp for pending_id(%x)\",\n\t\tmsg.PendingChannelID)\n\tlog.Debugf(\"Remote party accepted commitment constraints: %v\",\n\t\tspew.Sdump(remoteContribution.ChannelConfig.ChannelConstraints))\n\n\t// With the initiator's contribution recorded, respond with our\n\t// contribution in the next message of the workflow.\n\tourContribution := reservation.OurContribution()\n\tfundingAccept := lnwire.AcceptChannel{\n\t\tPendingChannelID:      msg.PendingChannelID,\n\t\tDustLimit:             ourContribution.DustLimit,\n\t\tMaxValueInFlight:      remoteMaxValue,\n\t\tChannelReserve:        chanReserve,\n\t\tMinAcceptDepth:        uint32(numConfsReq),\n\t\tHtlcMinimum:           minHtlc,\n\t\tCsvDelay:              remoteCsvDelay,\n\t\tMaxAcceptedHTLCs:      maxHtlcs,\n\t\tFundingKey:            ourContribution.MultiSigKey.PubKey,\n\t\tRevocationPoint:       ourContribution.RevocationBasePoint.PubKey,\n\t\tPaymentPoint:          ourContribution.PaymentBasePoint.PubKey,\n\t\tDelayedPaymentPoint:   ourContribution.DelayBasePoint.PubKey,\n\t\tHtlcPoint:             ourContribution.HtlcBasePoint.PubKey,\n\t\tFirstCommitmentPoint:  ourContribution.FirstCommitmentPoint,\n\t\tUpfrontShutdownScript: ourContribution.UpfrontShutdown,\n\t\tChannelType:           chanTypeFeatureBits,\n\t\tLeaseExpiry:           msg.LeaseExpiry,\n\t}\n\n\tif err := peer.SendMessage(true, &fundingAccept); err != nil {\n\t\tlog.Errorf(\"unable to send funding response to peer: %v\", err)\n\t\tf.failFundingFlow(peer, msg.PendingChannelID, err)\n\t\treturn\n\t}\n}\n\n// handleFundingAccept processes a response to the workflow initiation sent by\n// the remote peer. This message then queues a message with the funding\n// outpoint, and a commitment signature to the remote peer.",
      "length": 16299,
      "tokens": 1861,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) handleFundingAccept(peer lnpeer.Peer,",
      "content": "func (f *Manager) handleFundingAccept(peer lnpeer.Peer,\n\tmsg *lnwire.AcceptChannel) {\n\n\tpendingChanID := msg.PendingChannelID\n\tpeerKey := peer.IdentityKey()\n\tvar peerKeyBytes []byte\n\tif peerKey != nil {\n\t\tpeerKeyBytes = peerKey.SerializeCompressed()\n\t}\n\n\tresCtx, err := f.getReservationCtx(peerKey, pendingChanID)\n\tif err != nil {\n\t\tlog.Warnf(\"Can't find reservation (peerKey:%x, chan_id:%v)\",\n\t\t\tpeerKeyBytes, pendingChanID)\n\t\treturn\n\t}\n\n\t// Update the timestamp once the fundingAcceptMsg has been handled.\n\tdefer resCtx.updateTimestamp()\n\n\tlog.Infof(\"Recv'd fundingResponse for pending_id(%x)\",\n\t\tpendingChanID[:])\n\n\t// Perform some basic validation of any custom TLV records included.\n\t//\n\t// TODO: Return errors as funding.Error to give context to remote peer?\n\tif resCtx.channelType != nil {\n\t\t// We'll want to quickly check that the ChannelType echoed by\n\t\t// the channel request recipient matches what we proposed.\n\t\tif msg.ChannelType == nil {\n\t\t\terr := errors.New(\"explicit channel type not echoed \" +\n\t\t\t\t\"back\")\n\t\t\tf.failFundingFlow(peer, msg.PendingChannelID, err)\n\t\t\treturn\n\t\t}\n\t\tproposedFeatures := lnwire.RawFeatureVector(*resCtx.channelType)\n\t\tackedFeatures := lnwire.RawFeatureVector(*msg.ChannelType)\n\t\tif !proposedFeatures.Equals(&ackedFeatures) {\n\t\t\terr := errors.New(\"channel type mismatch\")\n\t\t\tf.failFundingFlow(peer, msg.PendingChannelID, err)\n\t\t\treturn\n\t\t}\n\n\t\t// We'll want to do the same with the LeaseExpiry if one should\n\t\t// be set.\n\t\tif resCtx.reservation.LeaseExpiry() != 0 {\n\t\t\tif msg.LeaseExpiry == nil {\n\t\t\t\terr := errors.New(\"lease expiry not echoed \" +\n\t\t\t\t\t\"back\")\n\t\t\t\tf.failFundingFlow(\n\t\t\t\t\tpeer, msg.PendingChannelID, err,\n\t\t\t\t)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif uint32(*msg.LeaseExpiry) !=\n\t\t\t\tresCtx.reservation.LeaseExpiry() {\n\n\t\t\t\terr := errors.New(\"lease expiry mismatch\")\n\t\t\t\tf.failFundingFlow(\n\t\t\t\t\tpeer, msg.PendingChannelID, err,\n\t\t\t\t)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t} else if msg.ChannelType != nil {\n\t\t// The spec isn't too clear about whether it's okay to set the\n\t\t// channel type in the accept_channel response if we didn't\n\t\t// explicitly set it in the open_channel message. For now, let's\n\t\t// just log the problem instead of failing the funding flow.\n\t\t_, implicitChannelType := implicitNegotiateCommitmentType(\n\t\t\tpeer.LocalFeatures(), peer.RemoteFeatures(),\n\t\t)\n\n\t\t// We pass in false here as the funder since at this point, we\n\t\t// didn't set a chan type ourselves, so falling back to\n\t\t// implicit funding is acceptable.\n\t\t_, _, negotiatedChannelType, err := negotiateCommitmentType(\n\t\t\tmsg.ChannelType, peer.LocalFeatures(),\n\t\t\tpeer.RemoteFeatures(), false,\n\t\t)\n\t\tif err != nil {\n\t\t\terr := errors.New(\"received unexpected channel type\")\n\t\t\tf.failFundingFlow(peer, msg.PendingChannelID, err)\n\t\t\treturn\n\t\t}\n\n\t\t// Even though we don't expect a channel type to be set when we\n\t\t// didn't send one in the first place, we check that it's the\n\t\t// same type we'd have arrived through implicit negotiation. If\n\t\t// it's another type, we fail the flow.\n\t\tif implicitChannelType != negotiatedChannelType {\n\t\t\terr := errors.New(\"negotiated unexpected channel type\")\n\t\t\tf.failFundingFlow(peer, msg.PendingChannelID, err)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// The required number of confirmations should not be greater than the\n\t// maximum number of confirmations required by the ChainNotifier to\n\t// properly dispatch confirmations.\n\tif msg.MinAcceptDepth > chainntnfs.MaxNumConfs {\n\t\terr := lnwallet.ErrNumConfsTooLarge(\n\t\t\tmsg.MinAcceptDepth, chainntnfs.MaxNumConfs,\n\t\t)\n\t\tlog.Warnf(\"Unacceptable channel constraints: %v\", err)\n\t\tf.failFundingFlow(peer, msg.PendingChannelID, err)\n\t\treturn\n\t}\n\n\t// Check that zero-conf channels have minimum depth set to 0.\n\tif resCtx.reservation.IsZeroConf() && msg.MinAcceptDepth != 0 {\n\t\terr = fmt.Errorf(\"zero-conf channel has min_depth non-zero\")\n\t\tlog.Warn(err)\n\t\tf.failFundingFlow(peer, msg.PendingChannelID, err)\n\t\treturn\n\t}\n\n\t// Fail early if minimum depth is set to 0 and the channel is not\n\t// zero-conf.\n\tif !resCtx.reservation.IsZeroConf() && msg.MinAcceptDepth == 0 {\n\t\terr = fmt.Errorf(\"non-zero-conf channel has min depth zero\")\n\t\tlog.Warn(err)\n\t\tf.failFundingFlow(peer, msg.PendingChannelID, err)\n\t\treturn\n\t}\n\n\t// We'll also specify the responder's preference for the number of\n\t// required confirmations, and also the set of channel constraints\n\t// they've specified for commitment states we can create.\n\tresCtx.reservation.SetNumConfsRequired(uint16(msg.MinAcceptDepth))\n\tchannelConstraints := &channeldb.ChannelConstraints{\n\t\tDustLimit:        msg.DustLimit,\n\t\tChanReserve:      msg.ChannelReserve,\n\t\tMaxPendingAmount: msg.MaxValueInFlight,\n\t\tMinHTLC:          msg.HtlcMinimum,\n\t\tMaxAcceptedHtlcs: msg.MaxAcceptedHTLCs,\n\t\tCsvDelay:         msg.CsvDelay,\n\t}\n\terr = resCtx.reservation.CommitConstraints(\n\t\tchannelConstraints, resCtx.maxLocalCsv, false,\n\t)\n\tif err != nil {\n\t\tlog.Warnf(\"Unacceptable channel constraints: %v\", err)\n\t\tf.failFundingFlow(peer, msg.PendingChannelID, err)\n\t\treturn\n\t}\n\n\t// The remote node has responded with their portion of the channel\n\t// contribution. At this point, we can process their contribution which\n\t// allows us to construct and sign both the commitment transaction, and\n\t// the funding transaction.\n\tremoteContribution := &lnwallet.ChannelContribution{\n\t\tFirstCommitmentPoint: msg.FirstCommitmentPoint,\n\t\tChannelConfig: &channeldb.ChannelConfig{\n\t\t\tChannelConstraints: channeldb.ChannelConstraints{\n\t\t\t\tDustLimit:        msg.DustLimit,\n\t\t\t\tMaxPendingAmount: resCtx.remoteMaxValue,\n\t\t\t\tChanReserve:      resCtx.remoteChanReserve,\n\t\t\t\tMinHTLC:          resCtx.remoteMinHtlc,\n\t\t\t\tMaxAcceptedHtlcs: resCtx.remoteMaxHtlcs,\n\t\t\t\tCsvDelay:         resCtx.remoteCsvDelay,\n\t\t\t},\n\t\t\tMultiSigKey: keychain.KeyDescriptor{\n\t\t\t\tPubKey: copyPubKey(msg.FundingKey),\n\t\t\t},\n\t\t\tRevocationBasePoint: keychain.KeyDescriptor{\n\t\t\t\tPubKey: copyPubKey(msg.RevocationPoint),\n\t\t\t},\n\t\t\tPaymentBasePoint: keychain.KeyDescriptor{\n\t\t\t\tPubKey: copyPubKey(msg.PaymentPoint),\n\t\t\t},\n\t\t\tDelayBasePoint: keychain.KeyDescriptor{\n\t\t\t\tPubKey: copyPubKey(msg.DelayedPaymentPoint),\n\t\t\t},\n\t\t\tHtlcBasePoint: keychain.KeyDescriptor{\n\t\t\t\tPubKey: copyPubKey(msg.HtlcPoint),\n\t\t\t},\n\t\t},\n\t\tUpfrontShutdown: msg.UpfrontShutdownScript,\n\t}\n\terr = resCtx.reservation.ProcessContribution(remoteContribution)\n\n\t// The wallet has detected that a PSBT funding process was requested by\n\t// the user and has halted the funding process after negotiating the\n\t// multisig keys. We now have everything that is needed for the user to\n\t// start constructing a PSBT that sends to the multisig funding address.\n\tvar psbtIntent *chanfunding.PsbtIntent\n\tif psbtErr, ok := err.(*lnwallet.PsbtFundingRequired); ok {\n\t\t// Return the information that is needed by the user to\n\t\t// construct the PSBT back to the caller.\n\t\taddr, amt, packet, err := psbtErr.Intent.FundingParams()\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"Unable to process PSBT funding params \"+\n\t\t\t\t\"for contribution from %x: %v\", peerKeyBytes,\n\t\t\t\terr)\n\t\t\tf.failFundingFlow(peer, msg.PendingChannelID, err)\n\t\t\treturn\n\t\t}\n\t\tvar buf bytes.Buffer\n\t\terr = packet.Serialize(&buf)\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"Unable to serialize PSBT for \"+\n\t\t\t\t\"contribution from %x: %v\", peerKeyBytes, err)\n\t\t\tf.failFundingFlow(peer, msg.PendingChannelID, err)\n\t\t\treturn\n\t\t}\n\t\tresCtx.updates <- &lnrpc.OpenStatusUpdate{\n\t\t\tPendingChanId: pendingChanID[:],\n\t\t\tUpdate: &lnrpc.OpenStatusUpdate_PsbtFund{\n\t\t\t\tPsbtFund: &lnrpc.ReadyForPsbtFunding{\n\t\t\t\t\tFundingAddress: addr.EncodeAddress(),\n\t\t\t\t\tFundingAmount:  amt,\n\t\t\t\t\tPsbt:           buf.Bytes(),\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t\tpsbtIntent = psbtErr.Intent\n\t} else if err != nil {\n\t\tlog.Errorf(\"Unable to process contribution from %x: %v\",\n\t\t\tpeerKeyBytes, err)\n\t\tf.failFundingFlow(peer, msg.PendingChannelID, err)\n\t\treturn\n\t}\n\n\tlog.Infof(\"pendingChan(%x): remote party proposes num_confs=%v, \"+\n\t\t\"csv_delay=%v\", pendingChanID[:], msg.MinAcceptDepth,\n\t\tmsg.CsvDelay)\n\tlog.Debugf(\"Remote party accepted commitment constraints: %v\",\n\t\tspew.Sdump(remoteContribution.ChannelConfig.ChannelConstraints))\n\n\t// If the user requested funding through a PSBT, we cannot directly\n\t// continue now and need to wait for the fully funded and signed PSBT\n\t// to arrive. To not block any other channels from opening, we wait in\n\t// a separate goroutine.\n\tif psbtIntent != nil {\n\t\tf.wg.Add(1)\n\t\tgo func() {\n\t\t\tdefer f.wg.Done()\n\n\t\t\tf.waitForPsbt(psbtIntent, resCtx, pendingChanID)\n\t\t}()\n\n\t\t// With the new goroutine spawned, we can now exit to unblock\n\t\t// the main event loop.\n\t\treturn\n\t}\n\n\t// In a normal, non-PSBT funding flow, we can jump directly to the next\n\t// step where we expect our contribution to be finalized.\n\tf.continueFundingAccept(resCtx, pendingChanID)\n}\n\n// waitForPsbt blocks until either a signed PSBT arrives, an error occurs or\n// the funding manager shuts down. In the case of a valid PSBT, the funding flow\n// is continued.\n//\n// NOTE: This method must be called as a goroutine.",
      "length": 8584,
      "tokens": 1006,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) waitForPsbt(intent *chanfunding.PsbtIntent,",
      "content": "func (f *Manager) waitForPsbt(intent *chanfunding.PsbtIntent,\n\tresCtx *reservationWithCtx, pendingChanID [32]byte) {\n\n\t// failFlow is a helper that logs an error message with the current\n\t// context and then fails the funding flow.\n\tpeerKey := resCtx.peer.IdentityKey()\n\tfailFlow := func(errMsg string, cause error) {\n\t\tlog.Errorf(\"Unable to handle funding accept message \"+\n\t\t\t\"for peer_key=%x, pending_chan_id=%x: %s: %v\",\n\t\t\tpeerKey.SerializeCompressed(), pendingChanID, errMsg,\n\t\t\tcause)\n\t\tf.failFundingFlow(resCtx.peer, pendingChanID, cause)\n\t}\n\n\t// We'll now wait until the intent has received the final and complete\n\t// funding transaction. If the channel is closed without any error being\n\t// sent, we know everything's going as expected.\n\tselect {\n\tcase err := <-intent.PsbtReady:\n\t\tswitch err {\n\t\t// If the user canceled the funding reservation, we need to\n\t\t// inform the other peer about us canceling the reservation.\n\t\tcase chanfunding.ErrUserCanceled:\n\t\t\tfailFlow(\"aborting PSBT flow\", err)\n\t\t\treturn\n\n\t\t// If the remote canceled the funding reservation, we don't need\n\t\t// to send another fail message. But we want to inform the user\n\t\t// about what happened.\n\t\tcase chanfunding.ErrRemoteCanceled:\n\t\t\tlog.Infof(\"Remote canceled, aborting PSBT flow \"+\n\t\t\t\t\"for peer_key=%x, pending_chan_id=%x\",\n\t\t\t\tpeerKey.SerializeCompressed(), pendingChanID)\n\t\t\treturn\n\n\t\t// Nil error means the flow continues normally now.\n\t\tcase nil:\n\n\t\t// For any other error, we'll fail the funding flow.\n\t\tdefault:\n\t\t\tfailFlow(\"error waiting for PSBT flow\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// A non-nil error means we can continue the funding flow.\n\t\t// Notify the wallet so it can prepare everything we need to\n\t\t// continue.\n\t\terr = resCtx.reservation.ProcessPsbt()\n\t\tif err != nil {\n\t\t\tfailFlow(\"error continuing PSBT flow\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// We are now ready to continue the funding flow.\n\t\tf.continueFundingAccept(resCtx, pendingChanID)\n\n\t// Handle a server shutdown as well because the reservation won't\n\t// survive a restart as it's in memory only.\n\tcase <-f.quit:\n\t\tlog.Errorf(\"Unable to handle funding accept message \"+\n\t\t\t\"for peer_key=%x, pending_chan_id=%x: funding manager \"+\n\t\t\t\"shutting down\", peerKey.SerializeCompressed(),\n\t\t\tpendingChanID)\n\t\treturn\n\t}\n}\n\n// continueFundingAccept continues the channel funding flow once our\n// contribution is finalized, the channel output is known and the funding\n// transaction is signed.",
      "length": 2296,
      "tokens": 317,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) continueFundingAccept(resCtx *reservationWithCtx,",
      "content": "func (f *Manager) continueFundingAccept(resCtx *reservationWithCtx,\n\tpendingChanID [32]byte) {\n\n\t// Now that we have their contribution, we can extract, then send over\n\t// both the funding out point and our signature for their version of\n\t// the commitment transaction to the remote peer.\n\toutPoint := resCtx.reservation.FundingOutpoint()\n\t_, sig := resCtx.reservation.OurSignatures()\n\n\t// A new channel has almost finished the funding process. In order to\n\t// properly synchronize with the writeHandler goroutine, we add a new\n\t// channel to the barriers map which will be closed once the channel is\n\t// fully open.\n\tf.barrierMtx.Lock()\n\tchannelID := lnwire.NewChanIDFromOutPoint(outPoint)\n\tlog.Debugf(\"Creating chan barrier for ChanID(%v)\", channelID)\n\tf.newChanBarriers[channelID] = make(chan struct{})\n\tf.barrierMtx.Unlock()\n\n\t// The next message that advances the funding flow will reference the\n\t// channel via its permanent channel ID, so we'll set up this mapping\n\t// so we can retrieve the reservation context once we get the\n\t// FundingSigned message.\n\tf.resMtx.Lock()\n\tf.signedReservations[channelID] = pendingChanID\n\tf.resMtx.Unlock()\n\n\tlog.Infof(\"Generated ChannelPoint(%v) for pending_id(%x)\", outPoint,\n\t\tpendingChanID[:])\n\n\tvar err error\n\tfundingCreated := &lnwire.FundingCreated{\n\t\tPendingChannelID: pendingChanID,\n\t\tFundingPoint:     *outPoint,\n\t}\n\tfundingCreated.CommitSig, err = lnwire.NewSigFromSignature(sig)\n\tif err != nil {\n\t\tlog.Errorf(\"Unable to parse signature: %v\", err)\n\t\tf.failFundingFlow(resCtx.peer, pendingChanID, err)\n\t\treturn\n\t}\n\tif err := resCtx.peer.SendMessage(true, fundingCreated); err != nil {\n\t\tlog.Errorf(\"Unable to send funding complete message: %v\", err)\n\t\tf.failFundingFlow(resCtx.peer, pendingChanID, err)\n\t\treturn\n\t}\n}\n\n// handleFundingCreated progresses the funding workflow when the daemon is on\n// the responding side of a single funder workflow. Once this message has been\n// processed, a signature is sent to the remote peer allowing it to broadcast\n// the funding transaction, progressing the workflow into the final stage.",
      "length": 1959,
      "tokens": 255,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) handleFundingCreated(peer lnpeer.Peer,",
      "content": "func (f *Manager) handleFundingCreated(peer lnpeer.Peer,\n\tmsg *lnwire.FundingCreated) {\n\n\tpeerKey := peer.IdentityKey()\n\tpendingChanID := msg.PendingChannelID\n\n\tresCtx, err := f.getReservationCtx(peerKey, pendingChanID)\n\tif err != nil {\n\t\tlog.Warnf(\"can't find reservation (peer_id:%v, chan_id:%x)\",\n\t\t\tpeerKey, pendingChanID[:])\n\t\treturn\n\t}\n\n\t// The channel initiator has responded with the funding outpoint of the\n\t// final funding transaction, as well as a signature for our version of\n\t// the commitment transaction. So at this point, we can validate the\n\t// initiator's commitment transaction, then send our own if it's valid.\n\t// TODO(roasbeef): make case (p vs P) consistent throughout\n\tfundingOut := msg.FundingPoint\n\tlog.Infof(\"completing pending_id(%x) with ChannelPoint(%v)\",\n\t\tpendingChanID[:], fundingOut)\n\n\tcommitSig, err := msg.CommitSig.ToSignature()\n\tif err != nil {\n\t\tlog.Errorf(\"unable to parse signature: %v\", err)\n\t\tf.failFundingFlow(peer, pendingChanID, err)\n\t\treturn\n\t}\n\n\t// With all the necessary data available, attempt to advance the\n\t// funding workflow to the next stage. If this succeeds then the\n\t// funding transaction will broadcast after our next message.\n\t// CompleteReservationSingle will also mark the channel as 'IsPending'\n\t// in the database.\n\tcompleteChan, err := resCtx.reservation.CompleteReservationSingle(\n\t\t&fundingOut, commitSig,\n\t)\n\tif err != nil {\n\t\t// TODO(roasbeef): better error logging: peerID, channelID, etc.\n\t\tlog.Errorf(\"unable to complete single reservation: %v\", err)\n\t\tf.failFundingFlow(peer, pendingChanID, err)\n\t\treturn\n\t}\n\n\t// Get forwarding policy before deleting the reservation context.\n\tforwardingPolicy := resCtx.forwardingPolicy\n\n\t// The channel is marked IsPending in the database, and can be removed\n\t// from the set of active reservations.\n\tf.deleteReservationCtx(peerKey, msg.PendingChannelID)\n\n\t// If something goes wrong before the funding transaction is confirmed,\n\t// we use this convenience method to delete the pending OpenChannel\n\t// from the database.\n\tdeleteFromDatabase := func() {\n\t\tlocalBalance := completeChan.LocalCommitment.LocalBalance.ToSatoshis()\n\t\tcloseInfo := &channeldb.ChannelCloseSummary{\n\t\t\tChanPoint:               completeChan.FundingOutpoint,\n\t\t\tChainHash:               completeChan.ChainHash,\n\t\t\tRemotePub:               completeChan.IdentityPub,\n\t\t\tCloseType:               channeldb.FundingCanceled,\n\t\t\tCapacity:                completeChan.Capacity,\n\t\t\tSettledBalance:          localBalance,\n\t\t\tRemoteCurrentRevocation: completeChan.RemoteCurrentRevocation,\n\t\t\tRemoteNextRevocation:    completeChan.RemoteNextRevocation,\n\t\t\tLocalChanConfig:         completeChan.LocalChanCfg,\n\t\t}\n\n\t\t// Close the channel with us as the initiator because we are\n\t\t// deciding to exit the funding flow due to an internal error.\n\t\tif err := completeChan.CloseChannel(\n\t\t\tcloseInfo, channeldb.ChanStatusLocalCloseInitiator,\n\t\t); err != nil {\n\t\t\tlog.Errorf(\"Failed closing channel %v: %v\",\n\t\t\t\tcompleteChan.FundingOutpoint, err)\n\t\t}\n\t}\n\n\t// A new channel has almost finished the funding process. In order to\n\t// properly synchronize with the writeHandler goroutine, we add a new\n\t// channel to the barriers map which will be closed once the channel is\n\t// fully open.\n\tf.barrierMtx.Lock()\n\tchannelID := lnwire.NewChanIDFromOutPoint(&fundingOut)\n\tlog.Debugf(\"Creating chan barrier for ChanID(%v)\", channelID)\n\tf.newChanBarriers[channelID] = make(chan struct{})\n\tf.barrierMtx.Unlock()\n\n\tlog.Infof(\"sending FundingSigned for pending_id(%x) over \"+\n\t\t\"ChannelPoint(%v)\", pendingChanID[:], fundingOut)\n\n\t// With their signature for our version of the commitment transaction\n\t// verified, we can now send over our signature to the remote peer.\n\t_, sig := resCtx.reservation.OurSignatures()\n\tourCommitSig, err := lnwire.NewSigFromSignature(sig)\n\tif err != nil {\n\t\tlog.Errorf(\"unable to parse signature: %v\", err)\n\t\tf.failFundingFlow(peer, pendingChanID, err)\n\t\tdeleteFromDatabase()\n\t\treturn\n\t}\n\n\tfundingSigned := &lnwire.FundingSigned{\n\t\tChanID:    channelID,\n\t\tCommitSig: ourCommitSig,\n\t}\n\tif err := peer.SendMessage(true, fundingSigned); err != nil {\n\t\tlog.Errorf(\"unable to send FundingSigned message: %v\", err)\n\t\tf.failFundingFlow(peer, pendingChanID, err)\n\t\tdeleteFromDatabase()\n\t\treturn\n\t}\n\n\t// With a permanent channel id established we can save the respective\n\t// forwarding policy in the database. In the channel announcement phase\n\t// this forwarding policy is retrieved and applied.\n\terr = f.saveInitialFwdingPolicy(channelID, &forwardingPolicy)\n\tif err != nil {\n\t\tlog.Errorf(\"Unable to store the forwarding policy: %v\", err)\n\t}\n\n\t// Now that we've sent over our final signature for this channel, we'll\n\t// send it to the ChainArbitrator so it can watch for any on-chain\n\t// actions during this final confirmation stage.\n\tif err := f.cfg.WatchNewChannel(completeChan, peerKey); err != nil {\n\t\tlog.Errorf(\"Unable to send new ChannelPoint(%v) for \"+\n\t\t\t\"arbitration: %v\", fundingOut, err)\n\t}\n\n\t// Create an entry in the local discovery map so we can ensure that we\n\t// process the channel confirmation fully before we receive a funding\n\t// locked message.\n\tf.localDiscoveryMtx.Lock()\n\tf.localDiscoverySignals[channelID] = make(chan struct{})\n\tf.localDiscoveryMtx.Unlock()\n\n\t// Inform the ChannelNotifier that the channel has entered\n\t// pending open state.\n\tf.cfg.NotifyPendingOpenChannelEvent(fundingOut, completeChan)\n\n\t// At this point we have sent our last funding message to the\n\t// initiating peer before the funding transaction will be broadcast.\n\t// With this last message, our job as the responder is now complete.\n\t// We'll wait for the funding transaction to reach the specified number\n\t// of confirmations, then start normal operations.\n\t//\n\t// When we get to this point we have sent the signComplete message to\n\t// the channel funder, and BOLT#2 specifies that we MUST remember the\n\t// channel for reconnection. The channel is already marked\n\t// as pending in the database, so in case of a disconnect or restart,\n\t// we will continue waiting for the confirmation the next time we start\n\t// the funding manager. In case the funding transaction never appears\n\t// on the blockchain, we must forget this channel. We therefore\n\t// completely forget about this channel if we haven't seen the funding\n\t// transaction in 288 blocks (~ 48 hrs), by canceling the reservation\n\t// and canceling the wait for the funding confirmation.\n\tf.wg.Add(1)\n\tgo f.advanceFundingState(completeChan, pendingChanID, nil)\n}\n\n// handleFundingSigned processes the final message received in a single funder\n// workflow. Once this message is processed, the funding transaction is\n// broadcast. Once the funding transaction reaches a sufficient number of\n// confirmations, a message is sent to the responding peer along with a compact\n// encoding of the location of the channel within the blockchain.",
      "length": 6645,
      "tokens": 858,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) handleFundingSigned(peer lnpeer.Peer,",
      "content": "func (f *Manager) handleFundingSigned(peer lnpeer.Peer,\n\tmsg *lnwire.FundingSigned) {\n\n\t// As the funding signed message will reference the reservation by its\n\t// permanent channel ID, we'll need to perform an intermediate look up\n\t// before we can obtain the reservation.\n\tf.resMtx.Lock()\n\tpendingChanID, ok := f.signedReservations[msg.ChanID]\n\tdelete(f.signedReservations, msg.ChanID)\n\tf.resMtx.Unlock()\n\tif !ok {\n\t\terr := fmt.Errorf(\"unable to find signed reservation for \"+\n\t\t\t\"chan_id=%x\", msg.ChanID)\n\t\tlog.Warnf(err.Error())\n\t\tf.failFundingFlow(peer, msg.ChanID, err)\n\t\treturn\n\t}\n\n\tpeerKey := peer.IdentityKey()\n\tresCtx, err := f.getReservationCtx(peerKey, pendingChanID)\n\tif err != nil {\n\t\tlog.Warnf(\"Unable to find reservation (peer_id:%v, \"+\n\t\t\t\"chan_id:%x)\", peerKey, pendingChanID[:])\n\t\t// TODO: add ErrChanNotFound?\n\t\tf.failFundingFlow(peer, pendingChanID, err)\n\t\treturn\n\t}\n\n\t// Create an entry in the local discovery map so we can ensure that we\n\t// process the channel confirmation fully before we receive a funding\n\t// locked message.\n\tfundingPoint := resCtx.reservation.FundingOutpoint()\n\tpermChanID := lnwire.NewChanIDFromOutPoint(fundingPoint)\n\tf.localDiscoveryMtx.Lock()\n\tf.localDiscoverySignals[permChanID] = make(chan struct{})\n\tf.localDiscoveryMtx.Unlock()\n\n\t// We have to store the forwardingPolicy before the reservation context\n\t// is deleted. The policy will then be read and applied in\n\t// newChanAnnouncement.\n\terr = f.saveInitialFwdingPolicy(permChanID, &resCtx.forwardingPolicy)\n\tif err != nil {\n\t\tlog.Errorf(\"Unable to store the forwarding policy: %v\", err)\n\t}\n\n\t// The remote peer has responded with a signature for our commitment\n\t// transaction. We'll verify the signature for validity, then commit\n\t// the state to disk as we can now open the channel.\n\tcommitSig, err := msg.CommitSig.ToSignature()\n\tif err != nil {\n\t\tlog.Errorf(\"Unable to parse signature: %v\", err)\n\t\tf.failFundingFlow(peer, pendingChanID, err)\n\t\treturn\n\t}\n\n\tcompleteChan, err := resCtx.reservation.CompleteReservation(\n\t\tnil, commitSig,\n\t)\n\tif err != nil {\n\t\tlog.Errorf(\"Unable to complete reservation sign \"+\n\t\t\t\"complete: %v\", err)\n\t\tf.failFundingFlow(peer, pendingChanID, err)\n\t\treturn\n\t}\n\n\t// The channel is now marked IsPending in the database, and we can\n\t// delete it from our set of active reservations.\n\tf.deleteReservationCtx(peerKey, pendingChanID)\n\n\t// Broadcast the finalized funding transaction to the network, but only\n\t// if we actually have the funding transaction.\n\tif completeChan.ChanType.HasFundingTx() {\n\t\tfundingTx := completeChan.FundingTxn\n\t\tvar fundingTxBuf bytes.Buffer\n\t\tif err := fundingTx.Serialize(&fundingTxBuf); err != nil {\n\t\t\tlog.Errorf(\"Unable to serialize funding \"+\n\t\t\t\t\"transaction %v: %v\", fundingTx.TxHash(), err)\n\n\t\t\t// Clear the buffer of any bytes that were written\n\t\t\t// before the serialization error to prevent logging an\n\t\t\t// incomplete transaction.\n\t\t\tfundingTxBuf.Reset()\n\t\t}\n\n\t\tlog.Infof(\"Broadcasting funding tx for ChannelPoint(%v): %x\",\n\t\t\tcompleteChan.FundingOutpoint, fundingTxBuf.Bytes())\n\n\t\t// Set a nil short channel ID at this stage because we do not\n\t\t// know it until our funding tx confirms.\n\t\tlabel := labels.MakeLabel(\n\t\t\tlabels.LabelTypeChannelOpen, nil,\n\t\t)\n\n\t\terr = f.cfg.PublishTransaction(fundingTx, label)\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"Unable to broadcast funding tx %x for \"+\n\t\t\t\t\"ChannelPoint(%v): %v\", fundingTxBuf.Bytes(),\n\t\t\t\tcompleteChan.FundingOutpoint, err)\n\n\t\t\t// We failed to broadcast the funding transaction, but\n\t\t\t// watch the channel regardless, in case the\n\t\t\t// transaction made it to the network. We will retry\n\t\t\t// broadcast at startup.\n\t\t\t//\n\t\t\t// TODO(halseth): retry more often? Handle with CPFP?\n\t\t\t// Just delete from the DB?\n\t\t}\n\t}\n\n\t// Now that we have a finalized reservation for this funding flow,\n\t// we'll send the to be active channel to the ChainArbitrator so it can\n\t// watch for any on-chain actions before the channel has fully\n\t// confirmed.\n\tif err := f.cfg.WatchNewChannel(completeChan, peerKey); err != nil {\n\t\tlog.Errorf(\"Unable to send new ChannelPoint(%v) for \"+\n\t\t\t\"arbitration: %v\", fundingPoint, err)\n\t}\n\n\tlog.Infof(\"Finalizing pending_id(%x) over ChannelPoint(%v), \"+\n\t\t\"waiting for channel open on-chain\", pendingChanID[:],\n\t\tfundingPoint)\n\n\t// Send an update to the upstream client that the negotiation process\n\t// is over.\n\t//\n\t// TODO(roasbeef): add abstraction over updates to accommodate\n\t// long-polling, or SSE, etc.\n\tupd := &lnrpc.OpenStatusUpdate{\n\t\tUpdate: &lnrpc.OpenStatusUpdate_ChanPending{\n\t\t\tChanPending: &lnrpc.PendingUpdate{\n\t\t\t\tTxid:        fundingPoint.Hash[:],\n\t\t\t\tOutputIndex: fundingPoint.Index,\n\t\t\t},\n\t\t},\n\t\tPendingChanId: pendingChanID[:],\n\t}\n\n\tselect {\n\tcase resCtx.updates <- upd:\n\t\t// Inform the ChannelNotifier that the channel has entered\n\t\t// pending open state.\n\t\tf.cfg.NotifyPendingOpenChannelEvent(*fundingPoint, completeChan)\n\tcase <-f.quit:\n\t\treturn\n\t}\n\n\t// At this point we have broadcast the funding transaction and done all\n\t// necessary processing.\n\tf.wg.Add(1)\n\tgo f.advanceFundingState(completeChan, pendingChanID, resCtx.updates)\n}\n\n// confirmedChannel wraps a confirmed funding transaction, as well as the short\n// channel ID which identifies that channel into a single struct. We'll use\n// this to pass around the final state of a channel after it has been\n// confirmed.",
      "length": 5132,
      "tokens": 663,
      "embedding": []
    },
    {
      "slug": "type confirmedChannel struct {",
      "content": "type confirmedChannel struct {\n\t// shortChanID expresses where in the block the funding transaction was\n\t// located.\n\tshortChanID lnwire.ShortChannelID\n\n\t// fundingTx is the funding transaction that created the channel.\n\tfundingTx *wire.MsgTx\n}\n\n// fundingTimeout is called when callers of waitForFundingWithTimeout receive\n// an ErrConfirmationTimeout. It is used to clean-up channel state and mark the\n// channel as closed. The error is only returned for the responder of the\n// channel flow.",
      "length": 452,
      "tokens": 67,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) fundingTimeout(c *channeldb.OpenChannel,",
      "content": "func (f *Manager) fundingTimeout(c *channeldb.OpenChannel,\n\tpendingID [32]byte) error {\n\n\t// We'll get a timeout if the number of blocks mined since the channel\n\t// was initiated reaches maxWaitNumBlocksFundingConf and we are not the\n\t// channel initiator.\n\tlocalBalance := c.LocalCommitment.LocalBalance.ToSatoshis()\n\tcloseInfo := &channeldb.ChannelCloseSummary{\n\t\tChainHash:               c.ChainHash,\n\t\tChanPoint:               c.FundingOutpoint,\n\t\tRemotePub:               c.IdentityPub,\n\t\tCapacity:                c.Capacity,\n\t\tSettledBalance:          localBalance,\n\t\tCloseType:               channeldb.FundingCanceled,\n\t\tRemoteCurrentRevocation: c.RemoteCurrentRevocation,\n\t\tRemoteNextRevocation:    c.RemoteNextRevocation,\n\t\tLocalChanConfig:         c.LocalChanCfg,\n\t}\n\n\t// Close the channel with us as the initiator because we are timing the\n\t// channel out.\n\tif err := c.CloseChannel(\n\t\tcloseInfo, channeldb.ChanStatusLocalCloseInitiator,\n\t); err != nil {\n\t\treturn fmt.Errorf(\"failed closing channel %v: %v\",\n\t\t\tc.FundingOutpoint, err)\n\t}\n\n\ttimeoutErr := fmt.Errorf(\"timeout waiting for funding tx (%v) to \"+\n\t\t\"confirm\", c.FundingOutpoint)\n\n\t// When the peer comes online, we'll notify it that we are now\n\t// considering the channel flow canceled.\n\tf.wg.Add(1)\n\tgo func() {\n\t\tdefer f.wg.Done()\n\n\t\tpeer, err := f.waitForPeerOnline(c.IdentityPub)\n\t\tswitch err {\n\t\t// We're already shutting down, so we can just return.\n\t\tcase ErrFundingManagerShuttingDown:\n\t\t\treturn\n\n\t\t// nil error means we continue on.\n\t\tcase nil:\n\n\t\t// For unexpected errors, we print the error and still try to\n\t\t// fail the funding flow.\n\t\tdefault:\n\t\t\tlog.Errorf(\"Unexpected error while waiting for peer \"+\n\t\t\t\t\"to come online: %v\", err)\n\t\t}\n\n\t\t// TODO(halseth): should this send be made\n\t\t// reliable?\n\n\t\t// The reservation won't exist at this point, but we'll send an\n\t\t// Error message over anyways with ChanID set to pendingID.\n\t\tf.failFundingFlow(peer, pendingID, timeoutErr)\n\t}()\n\n\treturn timeoutErr\n}\n\n// waitForFundingWithTimeout is a wrapper around waitForFundingConfirmation and\n// waitForTimeout that will return ErrConfirmationTimeout if we are not the\n// channel initiator and the maxWaitNumBlocksFundingConf has passed from the\n// funding broadcast height. In case of confirmation, the short channel ID of\n// the channel and the funding transaction will be returned.",
      "length": 2235,
      "tokens": 280,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) waitForFundingWithTimeout(",
      "content": "func (f *Manager) waitForFundingWithTimeout(\n\tch *channeldb.OpenChannel) (*confirmedChannel, error) {\n\n\tconfChan := make(chan *confirmedChannel)\n\ttimeoutChan := make(chan error, 1)\n\tcancelChan := make(chan struct{})\n\n\tf.wg.Add(1)\n\tgo f.waitForFundingConfirmation(ch, cancelChan, confChan)\n\n\t// If we are not the initiator, we have no money at stake and will\n\t// timeout waiting for the funding transaction to confirm after a\n\t// while.\n\tif !ch.IsInitiator && !ch.IsZeroConf() {\n\t\tf.wg.Add(1)\n\t\tgo f.waitForTimeout(ch, cancelChan, timeoutChan)\n\t}\n\tdefer close(cancelChan)\n\n\tselect {\n\tcase err := <-timeoutChan:\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn nil, ErrConfirmationTimeout\n\n\tcase <-f.quit:\n\t\t// The fundingManager is shutting down, and will resume wait on\n\t\t// startup.\n\t\treturn nil, ErrFundingManagerShuttingDown\n\n\tcase confirmedChannel, ok := <-confChan:\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"waiting for funding\" +\n\t\t\t\t\"confirmation failed\")\n\t\t}\n\t\treturn confirmedChannel, nil\n\t}\n}\n\n// makeFundingScript re-creates the funding script for the funding transaction\n// of the target channel.",
      "length": 1020,
      "tokens": 137,
      "embedding": []
    },
    {
      "slug": "func makeFundingScript(channel *channeldb.OpenChannel) ([]byte, error) {",
      "content": "func makeFundingScript(channel *channeldb.OpenChannel) ([]byte, error) {\n\tlocalKey := channel.LocalChanCfg.MultiSigKey.PubKey.SerializeCompressed()\n\tremoteKey := channel.RemoteChanCfg.MultiSigKey.PubKey.SerializeCompressed()\n\n\tmultiSigScript, err := input.GenMultiSigScript(localKey, remoteKey)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn input.WitnessScriptHash(multiSigScript)\n}\n\n// waitForFundingConfirmation handles the final stages of the channel funding\n// process once the funding transaction has been broadcast. The primary\n// function of waitForFundingConfirmation is to wait for blockchain\n// confirmation, and then to notify the other systems that must be notified\n// when a channel has become active for lightning transactions.\n// The wait can be canceled by closing the cancelChan. In case of success,\n// a *lnwire.ShortChannelID will be passed to confChan.\n//\n// NOTE: This MUST be run as a goroutine.",
      "length": 826,
      "tokens": 108,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) waitForFundingConfirmation(",
      "content": "func (f *Manager) waitForFundingConfirmation(\n\tcompleteChan *channeldb.OpenChannel, cancelChan <-chan struct{},\n\tconfChan chan<- *confirmedChannel) {\n\n\tdefer f.wg.Done()\n\tdefer close(confChan)\n\n\t// Register with the ChainNotifier for a notification once the funding\n\t// transaction reaches `numConfs` confirmations.\n\ttxid := completeChan.FundingOutpoint.Hash\n\tfundingScript, err := makeFundingScript(completeChan)\n\tif err != nil {\n\t\tlog.Errorf(\"unable to create funding script for \"+\n\t\t\t\"ChannelPoint(%v): %v\", completeChan.FundingOutpoint,\n\t\t\terr)\n\t\treturn\n\t}\n\tnumConfs := uint32(completeChan.NumConfsRequired)\n\n\t// If the underlying channel is a zero-conf channel, we'll set numConfs\n\t// to 6, since it will be zero here.\n\tif completeChan.IsZeroConf() {\n\t\tnumConfs = 6\n\t}\n\n\tconfNtfn, err := f.cfg.Notifier.RegisterConfirmationsNtfn(\n\t\t&txid, fundingScript, numConfs,\n\t\tcompleteChan.BroadcastHeight(),\n\t)\n\tif err != nil {\n\t\tlog.Errorf(\"Unable to register for confirmation of \"+\n\t\t\t\"ChannelPoint(%v): %v\", completeChan.FundingOutpoint,\n\t\t\terr)\n\t\treturn\n\t}\n\n\tlog.Infof(\"Waiting for funding tx (%v) to reach %v confirmations\",\n\t\ttxid, numConfs)\n\n\tvar confDetails *chainntnfs.TxConfirmation\n\tvar ok bool\n\n\t// Wait until the specified number of confirmations has been reached,\n\t// we get a cancel signal, or the wallet signals a shutdown.\n\tselect {\n\tcase confDetails, ok = <-confNtfn.Confirmed:\n\t\t// fallthrough\n\n\tcase <-cancelChan:\n\t\tlog.Warnf(\"canceled waiting for funding confirmation, \"+\n\t\t\t\"stopping funding flow for ChannelPoint(%v)\",\n\t\t\tcompleteChan.FundingOutpoint)\n\t\treturn\n\n\tcase <-f.quit:\n\t\tlog.Warnf(\"fundingManager shutting down, stopping funding \"+\n\t\t\t\"flow for ChannelPoint(%v)\",\n\t\t\tcompleteChan.FundingOutpoint)\n\t\treturn\n\t}\n\n\tif !ok {\n\t\tlog.Warnf(\"ChainNotifier shutting down, cannot complete \"+\n\t\t\t\"funding flow for ChannelPoint(%v)\",\n\t\t\tcompleteChan.FundingOutpoint)\n\t\treturn\n\t}\n\n\tfundingPoint := completeChan.FundingOutpoint\n\tlog.Infof(\"ChannelPoint(%v) is now active: ChannelID(%v)\",\n\t\tfundingPoint, lnwire.NewChanIDFromOutPoint(&fundingPoint))\n\n\t// With the block height and the transaction index known, we can\n\t// construct the compact chanID which is used on the network to unique\n\t// identify channels.\n\tshortChanID := lnwire.ShortChannelID{\n\t\tBlockHeight: confDetails.BlockHeight,\n\t\tTxIndex:     confDetails.TxIndex,\n\t\tTxPosition:  uint16(fundingPoint.Index),\n\t}\n\n\tselect {\n\tcase confChan <- &confirmedChannel{\n\t\tshortChanID: shortChanID,\n\t\tfundingTx:   confDetails.Tx,\n\t}:\n\tcase <-f.quit:\n\t\treturn\n\t}\n}\n\n// waitForTimeout will close the timeout channel if maxWaitNumBlocksFundingConf\n// has passed from the broadcast height of the given channel. In case of error,\n// the error is sent on timeoutChan. The wait can be canceled by closing the\n// cancelChan.\n//\n// NOTE: timeoutChan MUST be buffered.\n// NOTE: This MUST be run as a goroutine.",
      "length": 2719,
      "tokens": 327,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) waitForTimeout(completeChan *channeldb.OpenChannel,",
      "content": "func (f *Manager) waitForTimeout(completeChan *channeldb.OpenChannel,\n\tcancelChan <-chan struct{}, timeoutChan chan<- error) {\n\n\tdefer f.wg.Done()\n\n\tepochClient, err := f.cfg.Notifier.RegisterBlockEpochNtfn(nil)\n\tif err != nil {\n\t\ttimeoutChan <- fmt.Errorf(\"unable to register for epoch \"+\n\t\t\t\"notification: %v\", err)\n\t\treturn\n\t}\n\n\tdefer epochClient.Cancel()\n\n\t// On block maxHeight we will cancel the funding confirmation wait.\n\tbroadcastHeight := completeChan.BroadcastHeight()\n\tmaxHeight := broadcastHeight + maxWaitNumBlocksFundingConf\n\tfor {\n\t\tselect {\n\t\tcase epoch, ok := <-epochClient.Epochs:\n\t\t\tif !ok {\n\t\t\t\ttimeoutChan <- fmt.Errorf(\"epoch client \" +\n\t\t\t\t\t\"shutting down\")\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Close the timeout channel and exit if the block is\n\t\t\t// above the max height.\n\t\t\tif uint32(epoch.Height) >= maxHeight {\n\t\t\t\tlog.Warnf(\"Waited for %v blocks without \"+\n\t\t\t\t\t\"seeing funding transaction confirmed,\"+\n\t\t\t\t\t\" cancelling.\",\n\t\t\t\t\tmaxWaitNumBlocksFundingConf)\n\n\t\t\t\t// Notify the caller of the timeout.\n\t\t\t\tclose(timeoutChan)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// TODO: If we are the channel initiator implement\n\t\t\t// a method for recovering the funds from the funding\n\t\t\t// transaction\n\n\t\tcase <-cancelChan:\n\t\t\treturn\n\n\t\tcase <-f.quit:\n\t\t\t// The fundingManager is shutting down, will resume\n\t\t\t// waiting for the funding transaction on startup.\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// makeLabelForTx updates the label for the confirmed funding transaction. If\n// we opened the channel, and lnd's wallet published our funding tx (which is\n// not the case for some channels) then we update our transaction label with\n// our short channel ID, which is known now that our funding transaction has\n// confirmed. We do not label transactions we did not publish, because our\n// wallet has no knowledge of them.",
      "length": 1656,
      "tokens": 237,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) makeLabelForTx(c *channeldb.OpenChannel) {",
      "content": "func (f *Manager) makeLabelForTx(c *channeldb.OpenChannel) {\n\tif c.IsInitiator && c.ChanType.HasFundingTx() {\n\t\tshortChanID := c.ShortChanID()\n\n\t\t// For zero-conf channels, we'll use the actually-confirmed\n\t\t// short channel id.\n\t\tif c.IsZeroConf() {\n\t\t\tshortChanID = c.ZeroConfRealScid()\n\t\t}\n\n\t\tlabel := labels.MakeLabel(\n\t\t\tlabels.LabelTypeChannelOpen, &shortChanID,\n\t\t)\n\n\t\terr := f.cfg.UpdateLabel(c.FundingOutpoint.Hash, label)\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"unable to update label: %v\", err)\n\t\t}\n\t}\n}\n\n// handleFundingConfirmation marks a channel as open in the database, and set\n// the channelOpeningState markedOpen. In addition it will report the now\n// decided short channel ID to the switch, and close the local discovery signal\n// for this channel.",
      "length": 677,
      "tokens": 92,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) handleFundingConfirmation(",
      "content": "func (f *Manager) handleFundingConfirmation(\n\tcompleteChan *channeldb.OpenChannel,\n\tconfChannel *confirmedChannel) error {\n\n\tfundingPoint := completeChan.FundingOutpoint\n\tchanID := lnwire.NewChanIDFromOutPoint(&fundingPoint)\n\n\t// TODO(roasbeef): ideally persistent state update for chan above\n\t// should be abstracted\n\n\t// Now that that the channel has been fully confirmed, we'll request\n\t// that the wallet fully verify this channel to ensure that it can be\n\t// used.\n\terr := f.cfg.Wallet.ValidateChannel(completeChan, confChannel.fundingTx)\n\tif err != nil {\n\t\t// TODO(roasbeef): delete chan state?\n\t\treturn fmt.Errorf(\"unable to validate channel: %v\", err)\n\t}\n\n\t// Now that the channel has been validated, we'll persist an alias for\n\t// this channel if the option-scid-alias feature-bit was negotiated.\n\tif completeChan.NegotiatedAliasFeature() {\n\t\taliasScid, err := f.cfg.AliasManager.RequestAlias()\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to request alias: %v\", err)\n\t\t}\n\n\t\terr = f.cfg.AliasManager.AddLocalAlias(\n\t\t\taliasScid, confChannel.shortChanID, true,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to request alias: %v\", err)\n\t\t}\n\t}\n\n\t// The funding transaction now being confirmed, we add this channel to\n\t// the fundingManager's internal persistent state machine that we use\n\t// to track the remaining process of the channel opening. This is\n\t// useful to resume the opening process in case of restarts. We set the\n\t// opening state before we mark the channel opened in the database,\n\t// such that we can receover from one of the db writes failing.\n\terr = f.saveChannelOpeningState(\n\t\t&fundingPoint, markedOpen, &confChannel.shortChanID,\n\t)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error setting channel state to \"+\n\t\t\t\"markedOpen: %v\", err)\n\t}\n\n\t// Now that the channel has been fully confirmed and we successfully\n\t// saved the opening state, we'll mark it as open within the database.\n\terr = completeChan.MarkAsOpen(confChannel.shortChanID)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error setting channel pending flag to \"+\n\t\t\t\"false:\t%v\", err)\n\t}\n\n\t// Update the confirmed funding transaction label.\n\tf.makeLabelForTx(completeChan)\n\n\t// Inform the ChannelNotifier that the channel has transitioned from\n\t// pending open to open.\n\tf.cfg.NotifyOpenChannelEvent(completeChan.FundingOutpoint)\n\n\t// Close the discoverySignal channel, indicating to a separate\n\t// goroutine that the channel now is marked as open in the database\n\t// and that it is acceptable to process funding locked messages\n\t// from the peer.\n\tf.localDiscoveryMtx.Lock()\n\tif discoverySignal, ok := f.localDiscoverySignals[chanID]; ok {\n\t\tclose(discoverySignal)\n\t}\n\tf.localDiscoveryMtx.Unlock()\n\n\treturn nil\n}\n\n// sendFundingLocked creates and sends the fundingLocked message.\n// This should be called after the funding transaction has been confirmed,\n// and the channelState is 'markedOpen'.",
      "length": 2755,
      "tokens": 380,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) sendFundingLocked(completeChan *channeldb.OpenChannel,",
      "content": "func (f *Manager) sendFundingLocked(completeChan *channeldb.OpenChannel,\n\tchannel *lnwallet.LightningChannel) error {\n\n\tchanID := lnwire.NewChanIDFromOutPoint(&completeChan.FundingOutpoint)\n\n\tvar peerKey [33]byte\n\tcopy(peerKey[:], completeChan.IdentityPub.SerializeCompressed())\n\n\t// Next, we'll send over the funding locked message which marks that we\n\t// consider the channel open by presenting the remote party with our\n\t// next revocation key. Without the revocation key, the remote party\n\t// will be unable to propose state transitions.\n\tnextRevocation, err := channel.NextRevocationKey()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to create next revocation: %v\", err)\n\t}\n\tfundingLockedMsg := lnwire.NewFundingLocked(chanID, nextRevocation)\n\n\t// If the channel negotiated the option-scid-alias feature bit, we'll\n\t// send a TLV segment that includes an alias the peer can use in their\n\t// invoice hop hints. We'll send the first alias we find for the\n\t// channel since it does not matter which alias we send. We'll error\n\t// out in the odd case that no aliases are found.\n\tif completeChan.NegotiatedAliasFeature() {\n\t\taliases := f.cfg.AliasManager.GetAliases(\n\t\t\tcompleteChan.ShortChanID(),\n\t\t)\n\t\tif len(aliases) == 0 {\n\t\t\treturn fmt.Errorf(\"no aliases found\")\n\t\t}\n\n\t\t// We can use a pointer to aliases since GetAliases returns a\n\t\t// copy of the alias slice.\n\t\tfundingLockedMsg.AliasScid = &aliases[0]\n\t}\n\n\t// If the peer has disconnected before we reach this point, we will need\n\t// to wait for him to come back online before sending the fundingLocked\n\t// message. This is special for fundingLocked, since failing to send any\n\t// of the previous messages in the funding flow just cancels the flow.\n\t// But now the funding transaction is confirmed, the channel is open\n\t// and we have to make sure the peer gets the fundingLocked message when\n\t// it comes back online. This is also crucial during restart of lnd,\n\t// where we might try to resend the fundingLocked message before the\n\t// server has had the time to connect to the peer. We keep trying to\n\t// send fundingLocked until we succeed, or the fundingManager is shut\n\t// down.\n\tfor {\n\t\tpeer, err := f.waitForPeerOnline(completeChan.IdentityPub)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tlocalAlias := peer.LocalFeatures().HasFeature(\n\t\t\tlnwire.ScidAliasOptional,\n\t\t)\n\t\tremoteAlias := peer.RemoteFeatures().HasFeature(\n\t\t\tlnwire.ScidAliasOptional,\n\t\t)\n\n\t\t// We could also refresh the channel state instead of checking\n\t\t// whether the feature was negotiated, but this saves us a\n\t\t// database read.\n\t\tif fundingLockedMsg.AliasScid == nil && localAlias &&\n\t\t\tremoteAlias {\n\n\t\t\t// If an alias was not assigned above and the scid\n\t\t\t// alias feature was negotiated, check if we already\n\t\t\t// have an alias stored in case handleFundingLocked was\n\t\t\t// called before this. If an alias exists, use that in\n\t\t\t// funding_locked. Otherwise, request and store an\n\t\t\t// alias and use that.\n\t\t\taliases := f.cfg.AliasManager.GetAliases(\n\t\t\t\tcompleteChan.ShortChannelID,\n\t\t\t)\n\t\t\tif len(aliases) == 0 {\n\t\t\t\t// No aliases were found.\n\t\t\t\talias, err := f.cfg.AliasManager.RequestAlias()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\terr = f.cfg.AliasManager.AddLocalAlias(\n\t\t\t\t\talias, completeChan.ShortChannelID,\n\t\t\t\t\tfalse,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\tfundingLockedMsg.AliasScid = &alias\n\t\t\t} else {\n\t\t\t\tfundingLockedMsg.AliasScid = &aliases[0]\n\t\t\t}\n\t\t}\n\n\t\tlog.Infof(\"Peer(%x) is online, sending FundingLocked \"+\n\t\t\t\"for ChannelID(%v)\", peerKey, chanID)\n\n\t\tif err := peer.SendMessage(true, fundingLockedMsg); err == nil {\n\t\t\t// Sending succeeded, we can break out and continue the\n\t\t\t// funding flow.\n\t\t\tbreak\n\t\t}\n\n\t\tlog.Warnf(\"Unable to send fundingLocked to peer %x: %v. \"+\n\t\t\t\"Will retry when online\", peerKey, err)\n\t}\n\n\treturn nil\n}\n\n// receivedFundingLocked checks whether or not we've received a FundingLocked\n// from the remote peer. If we have, RemoteNextRevocation will be set.",
      "length": 3774,
      "tokens": 549,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) receivedFundingLocked(node *btcec.PublicKey,",
      "content": "func (f *Manager) receivedFundingLocked(node *btcec.PublicKey,\n\tchanID lnwire.ChannelID) (bool, error) {\n\n\t// If the funding manager has exited, return an error to stop looping.\n\t// Note that the peer may appear as online while the funding manager\n\t// has stopped due to the shutdown order in the server.\n\tselect {\n\tcase <-f.quit:\n\t\treturn false, ErrFundingManagerShuttingDown\n\tdefault:\n\t}\n\n\t// Avoid a tight loop if peer is offline.\n\tif _, err := f.waitForPeerOnline(node); err != nil {\n\t\tlog.Errorf(\"Wait for peer online failed: %v\", err)\n\t\treturn false, err\n\t}\n\n\tchannel, err := f.cfg.FindChannel(node, chanID)\n\tif err != nil {\n\t\tlog.Errorf(\"Unable to locate ChannelID(%v) to determine if \"+\n\t\t\t\"FundingLocked was received\", chanID)\n\t\treturn false, err\n\t}\n\n\treturn channel.RemoteNextRevocation != nil, nil\n}\n\n// extractAnnounceParams extracts the various channel announcement and update\n// parameters that will be needed to construct a ChannelAnnouncement and a\n// ChannelUpdate.",
      "length": 890,
      "tokens": 135,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) extractAnnounceParams(c *channeldb.OpenChannel) (",
      "content": "func (f *Manager) extractAnnounceParams(c *channeldb.OpenChannel) (\n\tlnwire.MilliSatoshi, lnwire.MilliSatoshi) {\n\n\t// We'll obtain the min HTLC value we can forward in our direction, as\n\t// we'll use this value within our ChannelUpdate. This constraint is\n\t// originally set by the remote node, as it will be the one that will\n\t// need to determine the smallest HTLC it deems economically relevant.\n\tfwdMinHTLC := c.LocalChanCfg.MinHTLC\n\n\t// We don't necessarily want to go as low as the remote party allows.\n\t// Check it against our default forwarding policy.\n\tif fwdMinHTLC < f.cfg.DefaultRoutingPolicy.MinHTLCOut {\n\t\tfwdMinHTLC = f.cfg.DefaultRoutingPolicy.MinHTLCOut\n\t}\n\n\t// We'll obtain the max HTLC value we can forward in our direction, as\n\t// we'll use this value within our ChannelUpdate. This value must be <=\n\t// channel capacity and <= the maximum in-flight msats set by the peer.\n\tfwdMaxHTLC := c.LocalChanCfg.MaxPendingAmount\n\tcapacityMSat := lnwire.NewMSatFromSatoshis(c.Capacity)\n\tif fwdMaxHTLC > capacityMSat {\n\t\tfwdMaxHTLC = capacityMSat\n\t}\n\n\treturn fwdMinHTLC, fwdMaxHTLC\n}\n\n// addToRouterGraph sends a ChannelAnnouncement and a ChannelUpdate to the\n// gossiper so that the channel is added to the Router's internal graph.\n// These announcement messages are NOT broadcasted to the greater network,\n// only to the channel counter party. The proofs required to announce the\n// channel to the greater network will be created and sent in annAfterSixConfs.\n// The peerAlias is used for zero-conf channels to give the counter-party a\n// ChannelUpdate they understand. ourPolicy may be set for various\n// option-scid-alias channels to re-use the same policy.",
      "length": 1569,
      "tokens": 238,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) addToRouterGraph(completeChan *channeldb.OpenChannel,",
      "content": "func (f *Manager) addToRouterGraph(completeChan *channeldb.OpenChannel,\n\tshortChanID *lnwire.ShortChannelID,\n\tpeerAlias *lnwire.ShortChannelID,\n\tourPolicy *channeldb.ChannelEdgePolicy) error {\n\n\tchanID := lnwire.NewChanIDFromOutPoint(&completeChan.FundingOutpoint)\n\n\tfwdMinHTLC, fwdMaxHTLC := f.extractAnnounceParams(completeChan)\n\n\tann, err := f.newChanAnnouncement(\n\t\tf.cfg.IDKey, completeChan.IdentityPub,\n\t\t&completeChan.LocalChanCfg.MultiSigKey,\n\t\tcompleteChan.RemoteChanCfg.MultiSigKey.PubKey, *shortChanID,\n\t\tchanID, fwdMinHTLC, fwdMaxHTLC, ourPolicy,\n\t)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error generating channel \"+\n\t\t\t\"announcement: %v\", err)\n\t}\n\n\t// Send ChannelAnnouncement and ChannelUpdate to the gossiper to add\n\t// to the Router's topology.\n\terrChan := f.cfg.SendAnnouncement(\n\t\tann.chanAnn, discovery.ChannelCapacity(completeChan.Capacity),\n\t\tdiscovery.ChannelPoint(completeChan.FundingOutpoint),\n\t)\n\tselect {\n\tcase err := <-errChan:\n\t\tif err != nil {\n\t\t\tif routing.IsError(err, routing.ErrOutdated,\n\t\t\t\trouting.ErrIgnored) {\n\n\t\t\t\tlog.Debugf(\"Router rejected \"+\n\t\t\t\t\t\"ChannelAnnouncement: %v\", err)\n\t\t\t} else {\n\t\t\t\treturn fmt.Errorf(\"error sending channel \"+\n\t\t\t\t\t\"announcement: %v\", err)\n\t\t\t}\n\t\t}\n\tcase <-f.quit:\n\t\treturn ErrFundingManagerShuttingDown\n\t}\n\n\terrChan = f.cfg.SendAnnouncement(\n\t\tann.chanUpdateAnn, discovery.RemoteAlias(peerAlias),\n\t)\n\tselect {\n\tcase err := <-errChan:\n\t\tif err != nil {\n\t\t\tif routing.IsError(err, routing.ErrOutdated,\n\t\t\t\trouting.ErrIgnored) {\n\n\t\t\t\tlog.Debugf(\"Router rejected \"+\n\t\t\t\t\t\"ChannelUpdate: %v\", err)\n\t\t\t} else {\n\t\t\t\treturn fmt.Errorf(\"error sending channel \"+\n\t\t\t\t\t\"update: %v\", err)\n\t\t\t}\n\t\t}\n\tcase <-f.quit:\n\t\treturn ErrFundingManagerShuttingDown\n\t}\n\n\treturn nil\n}\n\n// annAfterSixConfs broadcasts the necessary channel announcement messages to\n// the network after 6 confs. Should be called after the fundingLocked message\n// is sent and the channel is added to the router graph (channelState is\n// 'addedToRouterGraph') and the channel is ready to be used. This is the last\n// step in the channel opening process, and the opening state will be deleted\n// from the database if successful.",
      "length": 2010,
      "tokens": 224,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) annAfterSixConfs(completeChan *channeldb.OpenChannel,",
      "content": "func (f *Manager) annAfterSixConfs(completeChan *channeldb.OpenChannel,\n\tshortChanID *lnwire.ShortChannelID) error {\n\n\t// If this channel is not meant to be announced to the greater network,\n\t// we'll only send our NodeAnnouncement to our counterparty to ensure we\n\t// don't leak any of our information.\n\tannounceChan := completeChan.ChannelFlags&lnwire.FFAnnounceChannel != 0\n\tif !announceChan {\n\t\tlog.Debugf(\"Will not announce private channel %v.\",\n\t\t\tshortChanID.ToUint64())\n\n\t\tpeer, err := f.waitForPeerOnline(completeChan.IdentityPub)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tnodeAnn, err := f.cfg.CurrentNodeAnnouncement()\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to retrieve current node \"+\n\t\t\t\t\"announcement: %v\", err)\n\t\t}\n\n\t\tchanID := lnwire.NewChanIDFromOutPoint(\n\t\t\t&completeChan.FundingOutpoint,\n\t\t)\n\t\tpubKey := peer.PubKey()\n\t\tlog.Debugf(\"Sending our NodeAnnouncement for \"+\n\t\t\t\"ChannelID(%v) to %x\", chanID, pubKey)\n\n\t\t// TODO(halseth): make reliable. If the peer is not online this\n\t\t// will fail, and the opening process will stop. Should instead\n\t\t// block here, waiting for the peer to come online.\n\t\tif err := peer.SendMessage(true, &nodeAnn); err != nil {\n\t\t\treturn fmt.Errorf(\"unable to send node announcement \"+\n\t\t\t\t\"to peer %x: %v\", pubKey, err)\n\t\t}\n\n\t\t// For private channels we do not announce the channel policy\n\t\t// to the network but still need to delete them from the\n\t\t// database.\n\t\terr = f.deleteInitialFwdingPolicy(chanID)\n\t\tif err != nil {\n\t\t\tlog.Infof(\"Could not delete channel fees \"+\n\t\t\t\t\"for chanId %x.\", chanID)\n\t\t}\n\t} else {\n\t\t// Otherwise, we'll wait until the funding transaction has\n\t\t// reached 6 confirmations before announcing it.\n\t\tnumConfs := uint32(completeChan.NumConfsRequired)\n\t\tif numConfs < 6 {\n\t\t\tnumConfs = 6\n\t\t}\n\t\ttxid := completeChan.FundingOutpoint.Hash\n\t\tlog.Debugf(\"Will announce channel %v after ChannelPoint\"+\n\t\t\t\"(%v) has gotten %d confirmations\",\n\t\t\tshortChanID.ToUint64(), completeChan.FundingOutpoint,\n\t\t\tnumConfs)\n\n\t\tfundingScript, err := makeFundingScript(completeChan)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to create funding script \"+\n\t\t\t\t\"for ChannelPoint(%v): %v\",\n\t\t\t\tcompleteChan.FundingOutpoint, err)\n\t\t}\n\n\t\t// Register with the ChainNotifier for a notification once the\n\t\t// funding transaction reaches at least 6 confirmations.\n\t\tconfNtfn, err := f.cfg.Notifier.RegisterConfirmationsNtfn(\n\t\t\t&txid, fundingScript, numConfs,\n\t\t\tcompleteChan.BroadcastHeight(),\n\t\t)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to register for \"+\n\t\t\t\t\"confirmation of ChannelPoint(%v): %v\",\n\t\t\t\tcompleteChan.FundingOutpoint, err)\n\t\t}\n\n\t\t// Wait until 6 confirmations has been reached or the wallet\n\t\t// signals a shutdown.\n\t\tselect {\n\t\tcase _, ok := <-confNtfn.Confirmed:\n\t\t\tif !ok {\n\t\t\t\treturn fmt.Errorf(\"ChainNotifier shutting \"+\n\t\t\t\t\t\"down, cannot complete funding flow \"+\n\t\t\t\t\t\"for ChannelPoint(%v)\",\n\t\t\t\t\tcompleteChan.FundingOutpoint)\n\t\t\t}\n\t\t\t// Fallthrough.\n\n\t\tcase <-f.quit:\n\t\t\treturn fmt.Errorf(\"%v, stopping funding flow for \"+\n\t\t\t\t\"ChannelPoint(%v)\",\n\t\t\t\tErrFundingManagerShuttingDown,\n\t\t\t\tcompleteChan.FundingOutpoint)\n\t\t}\n\n\t\tfundingPoint := completeChan.FundingOutpoint\n\t\tchanID := lnwire.NewChanIDFromOutPoint(&fundingPoint)\n\n\t\tlog.Infof(\"Announcing ChannelPoint(%v), short_chan_id=%v\",\n\t\t\t&fundingPoint, shortChanID)\n\n\t\t// If this is a non-zero-conf option-scid-alias channel, we'll\n\t\t// delete the mappings the gossiper uses so that ChannelUpdates\n\t\t// with aliases won't be accepted. This is done elsewhere for\n\t\t// zero-conf channels.\n\t\tisScidFeature := completeChan.NegotiatedAliasFeature()\n\t\tisZeroConf := completeChan.IsZeroConf()\n\t\tif isScidFeature && !isZeroConf {\n\t\t\tbaseScid := completeChan.ShortChanID()\n\t\t\terr := f.cfg.AliasManager.DeleteSixConfs(baseScid)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed deleting six confs \"+\n\t\t\t\t\t\"maps: %v\", err)\n\t\t\t}\n\n\t\t\t// We'll delete the edge and add it again via\n\t\t\t// addToRouterGraph. This is because the peer may have\n\t\t\t// sent us a ChannelUpdate with an alias and we don't\n\t\t\t// want to relay this.\n\t\t\tourPolicy, err := f.cfg.DeleteAliasEdge(baseScid)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed deleting real edge \"+\n\t\t\t\t\t\"for alias channel from graph: %v\",\n\t\t\t\t\terr)\n\t\t\t}\n\n\t\t\terr = f.addToRouterGraph(\n\t\t\t\tcompleteChan, &baseScid, nil, ourPolicy,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to re-add to \"+\n\t\t\t\t\t\"router graph: %v\", err)\n\t\t\t}\n\t\t}\n\n\t\t// Create and broadcast the proofs required to make this channel\n\t\t// public and usable for other nodes for routing.\n\t\terr = f.announceChannel(\n\t\t\tf.cfg.IDKey, completeChan.IdentityPub,\n\t\t\t&completeChan.LocalChanCfg.MultiSigKey,\n\t\t\tcompleteChan.RemoteChanCfg.MultiSigKey.PubKey,\n\t\t\t*shortChanID, chanID,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"channel announcement failed: %w\",\n\t\t\t\terr)\n\t\t}\n\n\t\tlog.Debugf(\"Channel with ChannelPoint(%v), short_chan_id=%v \"+\n\t\t\t\"sent to gossiper\", &fundingPoint, shortChanID)\n\t}\n\n\treturn nil\n}\n\n// waitForZeroConfChannel is called when the state is addedToRouterGraph with\n// a zero-conf channel. This will wait for the real confirmation, add the\n// confirmed SCID to the router graph, and then announce after six confs.",
      "length": 4939,
      "tokens": 621,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) waitForZeroConfChannel(c *channeldb.OpenChannel,",
      "content": "func (f *Manager) waitForZeroConfChannel(c *channeldb.OpenChannel,\n\tpendingID [32]byte) error {\n\n\t// First we'll check whether the channel is confirmed on-chain. If it\n\t// is already confirmed, the chainntnfs subsystem will return with the\n\t// confirmed tx. Otherwise, we'll wait here until confirmation occurs.\n\tconfChan, err := f.waitForFundingWithTimeout(c)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error waiting for zero-conf funding \"+\n\t\t\t\"confirmation for ChannelPoint(%v): %v\",\n\t\t\tc.FundingOutpoint, err)\n\t}\n\n\t// We'll need to refresh the channel state so that things are properly\n\t// populated when validating the channel state. Otherwise, a panic may\n\t// occur due to inconsistency in the OpenChannel struct.\n\terr = c.Refresh()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to refresh channel state: %v\", err)\n\t}\n\n\t// Now that we have the confirmed transaction and the proper SCID,\n\t// we'll call ValidateChannel to ensure the confirmed tx is properly\n\t// formatted.\n\terr = f.cfg.Wallet.ValidateChannel(c, confChan.fundingTx)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to validate zero-conf channel: \"+\n\t\t\t\"%v\", err)\n\t}\n\n\t// Once we know the confirmed ShortChannelID, we'll need to save it to\n\t// the database and refresh the OpenChannel struct with it.\n\terr = c.MarkRealScid(confChan.shortChanID)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to set confirmed SCID for zero \"+\n\t\t\t\"channel: %v\", err)\n\t}\n\n\t// Six confirmations have been reached. If this channel is public,\n\t// we'll delete some of the alias mappings the gossiper uses.\n\tisPublic := c.ChannelFlags&lnwire.FFAnnounceChannel != 0\n\tif isPublic {\n\t\terr = f.cfg.AliasManager.DeleteSixConfs(c.ShortChannelID)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to delete base alias after \"+\n\t\t\t\t\"six confirmations: %v\", err)\n\t\t}\n\n\t\t// TODO: Make this atomic!\n\t\tourPolicy, err := f.cfg.DeleteAliasEdge(c.ShortChanID())\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to delete alias edge from \"+\n\t\t\t\t\"graph: %v\", err)\n\t\t}\n\n\t\t// We'll need to update the graph with the new ShortChannelID\n\t\t// via an addToRouterGraph call. We don't pass in the peer's\n\t\t// alias since we'll be using the confirmed SCID from now on\n\t\t// regardless if it's public or not.\n\t\terr = f.addToRouterGraph(\n\t\t\tc, &confChan.shortChanID, nil, ourPolicy,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed adding confirmed zero-conf \"+\n\t\t\t\t\"SCID to router graph: %v\", err)\n\t\t}\n\t}\n\n\t// Since we have now marked down the confirmed SCID, we'll also need to\n\t// tell the Switch to refresh the relevant ChannelLink so that forwards\n\t// under the confirmed SCID are possible if this is a public channel.\n\terr = f.cfg.ReportShortChanID(c.FundingOutpoint)\n\tif err != nil {\n\t\t// This should only fail if the link is not found in the\n\t\t// Switch's linkIndex map. If this is the case, then the peer\n\t\t// has gone offline and the next time the link is loaded, it\n\t\t// will have a refreshed state. Just log an error here.\n\t\tlog.Errorf(\"unable to report scid for zero-conf channel \"+\n\t\t\t\"channel: %v\", err)\n\t}\n\n\t// Update the confirmed transaction's label.\n\tf.makeLabelForTx(c)\n\n\treturn nil\n}\n\n// handleFundingLocked finalizes the channel funding process and enables the\n// channel to enter normal operating mode.",
      "length": 3074,
      "tokens": 478,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) handleFundingLocked(peer lnpeer.Peer,",
      "content": "func (f *Manager) handleFundingLocked(peer lnpeer.Peer,\n\tmsg *lnwire.FundingLocked) {\n\n\tdefer f.wg.Done()\n\tlog.Debugf(\"Received FundingLocked for ChannelID(%v) from \"+\n\t\t\"peer %x\", msg.ChanID,\n\t\tpeer.IdentityKey().SerializeCompressed())\n\n\t// If we are currently in the process of handling a funding locked\n\t// message for this channel, ignore.\n\tf.handleFundingLockedMtx.Lock()\n\t_, ok := f.handleFundingLockedBarriers[msg.ChanID]\n\tif ok {\n\t\tlog.Infof(\"Already handling fundingLocked for \"+\n\t\t\t\"ChannelID(%v), ignoring.\", msg.ChanID)\n\t\tf.handleFundingLockedMtx.Unlock()\n\t\treturn\n\t}\n\n\t// If not already handling fundingLocked for this channel, set up\n\t// barrier, and move on.\n\tf.handleFundingLockedBarriers[msg.ChanID] = struct{}{}\n\tf.handleFundingLockedMtx.Unlock()\n\n\tdefer func() {\n\t\tf.handleFundingLockedMtx.Lock()\n\t\tdelete(f.handleFundingLockedBarriers, msg.ChanID)\n\t\tf.handleFundingLockedMtx.Unlock()\n\t}()\n\n\tf.localDiscoveryMtx.Lock()\n\tlocalDiscoverySignal, ok := f.localDiscoverySignals[msg.ChanID]\n\tf.localDiscoveryMtx.Unlock()\n\n\tif ok {\n\t\t// Before we proceed with processing the funding locked\n\t\t// message, we'll wait for the local waitForFundingConfirmation\n\t\t// goroutine to signal that it has the necessary state in\n\t\t// place. Otherwise, we may be missing critical information\n\t\t// required to handle forwarded HTLC's.\n\t\tselect {\n\t\tcase <-localDiscoverySignal:\n\t\t\t// Fallthrough\n\t\tcase <-f.quit:\n\t\t\treturn\n\t\t}\n\n\t\t// With the signal received, we can now safely delete the entry\n\t\t// from the map.\n\t\tf.localDiscoveryMtx.Lock()\n\t\tdelete(f.localDiscoverySignals, msg.ChanID)\n\t\tf.localDiscoveryMtx.Unlock()\n\t}\n\n\t// First, we'll attempt to locate the channel whose funding workflow is\n\t// being finalized by this message. We go to the database rather than\n\t// our reservation map as we may have restarted, mid funding flow. Also\n\t// provide the node's public key to make the search faster.\n\tchanID := msg.ChanID\n\tchannel, err := f.cfg.FindChannel(peer.IdentityKey(), chanID)\n\tif err != nil {\n\t\tlog.Errorf(\"Unable to locate ChannelID(%v), cannot complete \"+\n\t\t\t\"funding\", chanID)\n\t\treturn\n\t}\n\n\t// We'll need to store the received TLV alias if the option_scid_alias\n\t// feature was negotiated. This will be used to provide route hints\n\t// during invoice creation. In the zero-conf case, it is also used to\n\t// provide a ChannelUpdate to the remote peer. This is done before the\n\t// call to InsertNextRevocation in case the call to PutPeerAlias fails.\n\t// If it were to fail on the first call to handleFundingLocked, we\n\t// wouldn't want the channel to be usable yet.\n\tif channel.NegotiatedAliasFeature() {\n\t\t// If the AliasScid field is nil, we must fail out. We will\n\t\t// most likely not be able to route through the peer.\n\t\tif msg.AliasScid == nil {\n\t\t\tlog.Debugf(\"Consider closing ChannelID(%v), peer \"+\n\t\t\t\t\"does not implement the option-scid-alias \"+\n\t\t\t\t\"feature properly\", chanID)\n\t\t\treturn\n\t\t}\n\n\t\t// We'll store the AliasScid so that invoice creation can use\n\t\t// it.\n\t\terr = f.cfg.AliasManager.PutPeerAlias(chanID, *msg.AliasScid)\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"unable to store peer's alias: %v\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// If we do not have an alias stored, we'll create one now.\n\t\t// This is only used in the upgrade case where a user toggles\n\t\t// the option-scid-alias feature-bit to on. We'll also send the\n\t\t// funding_locked message here in case the link is created\n\t\t// before sendFundingLocked is called.\n\t\taliases := f.cfg.AliasManager.GetAliases(\n\t\t\tchannel.ShortChannelID,\n\t\t)\n\t\tif len(aliases) == 0 {\n\t\t\t// No aliases were found so we'll request and store an\n\t\t\t// alias and use it in the funding_locked message.\n\t\t\talias, err := f.cfg.AliasManager.RequestAlias()\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"unable to request alias: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\terr = f.cfg.AliasManager.AddLocalAlias(\n\t\t\t\talias, channel.ShortChannelID, false,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"unable to add local alias: %v\",\n\t\t\t\t\terr)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tsecondPoint, err := channel.SecondCommitmentPoint()\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"unable to fetch second \"+\n\t\t\t\t\t\"commitment point: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tfundingLockedMsg := lnwire.NewFundingLocked(\n\t\t\t\tchanID, secondPoint,\n\t\t\t)\n\t\t\tfundingLockedMsg.AliasScid = &alias\n\n\t\t\terr = peer.SendMessage(true, fundingLockedMsg)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"unable to send funding locked: %v\",\n\t\t\t\t\terr)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\t// If the RemoteNextRevocation is non-nil, it means that we have\n\t// already processed fundingLocked for this channel, so ignore. This\n\t// check is after the alias logic so we store the peer's most recent\n\t// alias. The spec requires us to validate that subsequent\n\t// funding_locked messages use the same per commitment point (the\n\t// second), but it is not actually necessary since we'll just end up\n\t// ignoring it. We are, however, required to *send* the same per\n\t// commitment point, since another pedantic implementation might\n\t// verify it.\n\tif channel.RemoteNextRevocation != nil {\n\t\tlog.Infof(\"Received duplicate fundingLocked for \"+\n\t\t\t\"ChannelID(%v), ignoring.\", chanID)\n\t\treturn\n\t}\n\n\t// The funding locked message contains the next commitment point we'll\n\t// need to create the next commitment state for the remote party. So\n\t// we'll insert that into the channel now before passing it along to\n\t// other sub-systems.\n\terr = channel.InsertNextRevocation(msg.NextPerCommitmentPoint)\n\tif err != nil {\n\t\tlog.Errorf(\"unable to insert next commitment point: %v\", err)\n\t\treturn\n\t}\n\n\t// Launch a defer so we _ensure_ that the channel barrier is properly\n\t// closed even if the target peer is no longer online at this point.\n\tdefer func() {\n\t\t// Close the active channel barrier signaling the readHandler\n\t\t// that commitment related modifications to this channel can\n\t\t// now proceed.\n\t\tf.barrierMtx.Lock()\n\t\tchanBarrier, ok := f.newChanBarriers[chanID]\n\t\tif ok {\n\t\t\tlog.Tracef(\"Closing chan barrier for ChanID(%v)\",\n\t\t\t\tchanID)\n\t\t\tclose(chanBarrier)\n\t\t\tdelete(f.newChanBarriers, chanID)\n\t\t}\n\t\tf.barrierMtx.Unlock()\n\t}()\n\n\tif err := peer.AddNewChannel(channel, f.quit); err != nil {\n\t\tlog.Errorf(\"Unable to add new channel %v with peer %x: %v\",\n\t\t\tchannel.FundingOutpoint,\n\t\t\tpeer.IdentityKey().SerializeCompressed(), err,\n\t\t)\n\t}\n}\n\n// chanAnnouncement encapsulates the two authenticated announcements that we\n// send out to the network after a new channel has been created locally.",
      "length": 6145,
      "tokens": 850,
      "embedding": []
    },
    {
      "slug": "type chanAnnouncement struct {",
      "content": "type chanAnnouncement struct {\n\tchanAnn       *lnwire.ChannelAnnouncement\n\tchanUpdateAnn *lnwire.ChannelUpdate\n\tchanProof     *lnwire.AnnounceSignatures\n}\n\n// newChanAnnouncement creates the authenticated channel announcement messages\n// required to broadcast a newly created channel to the network. The\n// announcement is two part: the first part authenticates the existence of the\n// channel and contains four signatures binding the funding pub keys and\n// identity pub keys of both parties to the channel, and the second segment is\n// authenticated only by us and contains our directional routing policy for the\n// channel. ourPolicy may be set in order to re-use an existing, non-default\n// policy.",
      "length": 659,
      "tokens": 95,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) newChanAnnouncement(localPubKey,",
      "content": "func (f *Manager) newChanAnnouncement(localPubKey,\n\tremotePubKey *btcec.PublicKey, localFundingKey *keychain.KeyDescriptor,\n\tremoteFundingKey *btcec.PublicKey, shortChanID lnwire.ShortChannelID,\n\tchanID lnwire.ChannelID, fwdMinHTLC,\n\tfwdMaxHTLC lnwire.MilliSatoshi,\n\tourPolicy *channeldb.ChannelEdgePolicy) (*chanAnnouncement, error) {\n\n\tchainHash := *f.cfg.Wallet.Cfg.NetParams.GenesisHash\n\n\t// The unconditional section of the announcement is the ShortChannelID\n\t// itself which compactly encodes the location of the funding output\n\t// within the blockchain.\n\tchanAnn := &lnwire.ChannelAnnouncement{\n\t\tShortChannelID: shortChanID,\n\t\tFeatures:       lnwire.NewRawFeatureVector(),\n\t\tChainHash:      chainHash,\n\t}\n\n\t// The chanFlags field indicates which directed edge of the channel is\n\t// being updated within the ChannelUpdateAnnouncement announcement\n\t// below. A value of zero means it's the edge of the \"first\" node and 1\n\t// being the other node.\n\tvar chanFlags lnwire.ChanUpdateChanFlags\n\n\t// The lexicographical ordering of the two identity public keys of the\n\t// nodes indicates which of the nodes is \"first\". If our serialized\n\t// identity key is lower than theirs then we're the \"first\" node and\n\t// second otherwise.\n\tselfBytes := localPubKey.SerializeCompressed()\n\tremoteBytes := remotePubKey.SerializeCompressed()\n\tif bytes.Compare(selfBytes, remoteBytes) == -1 {\n\t\tcopy(chanAnn.NodeID1[:], localPubKey.SerializeCompressed())\n\t\tcopy(chanAnn.NodeID2[:], remotePubKey.SerializeCompressed())\n\t\tcopy(\n\t\t\tchanAnn.BitcoinKey1[:],\n\t\t\tlocalFundingKey.PubKey.SerializeCompressed(),\n\t\t)\n\t\tcopy(\n\t\t\tchanAnn.BitcoinKey2[:],\n\t\t\tremoteFundingKey.SerializeCompressed(),\n\t\t)\n\n\t\t// If we're the first node then update the chanFlags to\n\t\t// indicate the \"direction\" of the update.\n\t\tchanFlags = 0\n\t} else {\n\t\tcopy(chanAnn.NodeID1[:], remotePubKey.SerializeCompressed())\n\t\tcopy(chanAnn.NodeID2[:], localPubKey.SerializeCompressed())\n\t\tcopy(\n\t\t\tchanAnn.BitcoinKey1[:],\n\t\t\tremoteFundingKey.SerializeCompressed(),\n\t\t)\n\t\tcopy(\n\t\t\tchanAnn.BitcoinKey2[:],\n\t\t\tlocalFundingKey.PubKey.SerializeCompressed(),\n\t\t)\n\n\t\t// If we're the second node then update the chanFlags to\n\t\t// indicate the \"direction\" of the update.\n\t\tchanFlags = 1\n\t}\n\n\t// Our channel update message flags will signal that we support the\n\t// max_htlc field.\n\tmsgFlags := lnwire.ChanUpdateRequiredMaxHtlc\n\n\t// We announce the channel with the default values. Some of\n\t// these values can later be changed by crafting a new ChannelUpdate.\n\tchanUpdateAnn := &lnwire.ChannelUpdate{\n\t\tShortChannelID: shortChanID,\n\t\tChainHash:      chainHash,\n\t\tTimestamp:      uint32(time.Now().Unix()),\n\t\tMessageFlags:   msgFlags,\n\t\tChannelFlags:   chanFlags,\n\t\tTimeLockDelta: uint16(\n\t\t\tf.cfg.DefaultRoutingPolicy.TimeLockDelta,\n\t\t),\n\t\tHtlcMinimumMsat: fwdMinHTLC,\n\t\tHtlcMaximumMsat: fwdMaxHTLC,\n\t}\n\n\t// The caller of newChanAnnouncement is expected to provide the initial\n\t// forwarding policy to be announced. We abort the channel announcement\n\t// if they are not provided.\n\tstoredFwdingPolicy, err := f.getInitialFwdingPolicy(chanID)\n\tif err != nil {\n\t\treturn nil, errors.Errorf(\"unable to generate channel \"+\n\t\t\t\"update announcement: %v\", err)\n\t}\n\n\tswitch {\n\tcase ourPolicy != nil:\n\t\t// If ourPolicy is non-nil, modify the default parameters of the\n\t\t// ChannelUpdate.\n\t\tchanUpdateAnn.MessageFlags = ourPolicy.MessageFlags\n\t\tchanUpdateAnn.ChannelFlags = ourPolicy.ChannelFlags\n\t\tchanUpdateAnn.TimeLockDelta = ourPolicy.TimeLockDelta\n\t\tchanUpdateAnn.HtlcMinimumMsat = ourPolicy.MinHTLC\n\t\tchanUpdateAnn.HtlcMaximumMsat = ourPolicy.MaxHTLC\n\t\tchanUpdateAnn.BaseFee = uint32(ourPolicy.FeeBaseMSat)\n\t\tchanUpdateAnn.FeeRate = uint32(\n\t\t\tourPolicy.FeeProportionalMillionths,\n\t\t)\n\n\tcase storedFwdingPolicy != nil:\n\t\tchanUpdateAnn.BaseFee = uint32(storedFwdingPolicy.BaseFee)\n\t\tchanUpdateAnn.FeeRate = uint32(storedFwdingPolicy.FeeRate)\n\n\tdefault:\n\t\tlog.Infof(\"No channel forwaring policy specified for channel \"+\n\t\t\t\"announcement of ChannelID(%v). \"+\n\t\t\t\"Assuming default fee parameters.\", chanID)\n\t\tchanUpdateAnn.BaseFee = uint32(\n\t\t\tf.cfg.DefaultRoutingPolicy.BaseFee,\n\t\t)\n\t\tchanUpdateAnn.FeeRate = uint32(\n\t\t\tf.cfg.DefaultRoutingPolicy.FeeRate,\n\t\t)\n\t}\n\n\t// With the channel update announcement constructed, we'll generate a\n\t// signature that signs a double-sha digest of the announcement.\n\t// This'll serve to authenticate this announcement and any other future\n\t// updates we may send.\n\tchanUpdateMsg, err := chanUpdateAnn.DataToSign()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsig, err := f.cfg.SignMessage(f.cfg.IDKeyLoc, chanUpdateMsg, true)\n\tif err != nil {\n\t\treturn nil, errors.Errorf(\"unable to generate channel \"+\n\t\t\t\"update announcement signature: %v\", err)\n\t}\n\tchanUpdateAnn.Signature, err = lnwire.NewSigFromSignature(sig)\n\tif err != nil {\n\t\treturn nil, errors.Errorf(\"unable to generate channel \"+\n\t\t\t\"update announcement signature: %v\", err)\n\t}\n\n\t// The channel existence proofs itself is currently announced in\n\t// distinct message. In order to properly authenticate this message, we\n\t// need two signatures: one under the identity public key used which\n\t// signs the message itself and another signature of the identity\n\t// public key under the funding key itself.\n\t//\n\t// TODO(roasbeef): use SignAnnouncement here instead?\n\tchanAnnMsg, err := chanAnn.DataToSign()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tnodeSig, err := f.cfg.SignMessage(f.cfg.IDKeyLoc, chanAnnMsg, true)\n\tif err != nil {\n\t\treturn nil, errors.Errorf(\"unable to generate node \"+\n\t\t\t\"signature for channel announcement: %v\", err)\n\t}\n\tbitcoinSig, err := f.cfg.SignMessage(\n\t\tlocalFundingKey.KeyLocator, chanAnnMsg, true,\n\t)\n\tif err != nil {\n\t\treturn nil, errors.Errorf(\"unable to generate bitcoin \"+\n\t\t\t\"signature for node public key: %v\", err)\n\t}\n\n\t// Finally, we'll generate the announcement proof which we'll use to\n\t// provide the other side with the necessary signatures required to\n\t// allow them to reconstruct the full channel announcement.\n\tproof := &lnwire.AnnounceSignatures{\n\t\tChannelID:      chanID,\n\t\tShortChannelID: shortChanID,\n\t}\n\tproof.NodeSignature, err = lnwire.NewSigFromSignature(nodeSig)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tproof.BitcoinSignature, err = lnwire.NewSigFromSignature(bitcoinSig)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &chanAnnouncement{\n\t\tchanAnn:       chanAnn,\n\t\tchanUpdateAnn: chanUpdateAnn,\n\t\tchanProof:     proof,\n\t}, nil\n}\n\n// announceChannel announces a newly created channel to the rest of the network\n// by crafting the two authenticated announcements required for the peers on\n// the network to recognize the legitimacy of the channel. The crafted\n// announcements are then sent to the channel router to handle broadcasting to\n// the network during its next trickle.\n// This method is synchronous and will return when all the network requests\n// finish, either successfully or with an error.",
      "length": 6617,
      "tokens": 788,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) announceChannel(localIDKey, remoteIDKey *btcec.PublicKey,",
      "content": "func (f *Manager) announceChannel(localIDKey, remoteIDKey *btcec.PublicKey,\n\tlocalFundingKey *keychain.KeyDescriptor,\n\tremoteFundingKey *btcec.PublicKey, shortChanID lnwire.ShortChannelID,\n\tchanID lnwire.ChannelID) error {\n\n\t// First, we'll create the batch of announcements to be sent upon\n\t// initial channel creation. This includes the channel announcement\n\t// itself, the channel update announcement, and our half of the channel\n\t// proof needed to fully authenticate the channel.\n\t//\n\t// We can pass in zeroes for the min and max htlc policy, because we\n\t// only use the channel announcement message from the returned struct.\n\tann, err := f.newChanAnnouncement(localIDKey, remoteIDKey,\n\t\tlocalFundingKey, remoteFundingKey, shortChanID, chanID,\n\t\t0, 0, nil,\n\t)\n\tif err != nil {\n\t\tlog.Errorf(\"can't generate channel announcement: %v\", err)\n\t\treturn err\n\t}\n\n\t// After the fee parameters have been stored in the announcement\n\t// we can delete them from the database.\n\terr = f.deleteInitialFwdingPolicy(chanID)\n\tif err != nil {\n\t\tlog.Infof(\"Could not delete channel fees for chanId %x.\",\n\t\t\tchanID)\n\t}\n\n\t// We only send the channel proof announcement and the node announcement\n\t// because addToRouterGraph previously sent the ChannelAnnouncement and\n\t// the ChannelUpdate announcement messages. The channel proof and node\n\t// announcements are broadcast to the greater network.\n\terrChan := f.cfg.SendAnnouncement(ann.chanProof)\n\tselect {\n\tcase err := <-errChan:\n\t\tif err != nil {\n\t\t\tif routing.IsError(err, routing.ErrOutdated,\n\t\t\t\trouting.ErrIgnored) {\n\n\t\t\t\tlog.Debugf(\"Router rejected \"+\n\t\t\t\t\t\"AnnounceSignatures: %v\", err)\n\t\t\t} else {\n\t\t\t\tlog.Errorf(\"Unable to send channel \"+\n\t\t\t\t\t\"proof: %v\", err)\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\tcase <-f.quit:\n\t\treturn ErrFundingManagerShuttingDown\n\t}\n\n\t// Now that the channel is announced to the network, we will also\n\t// obtain and send a node announcement. This is done since a node\n\t// announcement is only accepted after a channel is known for that\n\t// particular node, and this might be our first channel.\n\tnodeAnn, err := f.cfg.CurrentNodeAnnouncement()\n\tif err != nil {\n\t\tlog.Errorf(\"can't generate node announcement: %v\", err)\n\t\treturn err\n\t}\n\n\terrChan = f.cfg.SendAnnouncement(&nodeAnn)\n\tselect {\n\tcase err := <-errChan:\n\t\tif err != nil {\n\t\t\tif routing.IsError(err, routing.ErrOutdated,\n\t\t\t\trouting.ErrIgnored) {\n\n\t\t\t\tlog.Debugf(\"Router rejected \"+\n\t\t\t\t\t\"NodeAnnouncement: %v\", err)\n\t\t\t} else {\n\t\t\t\tlog.Errorf(\"Unable to send node \"+\n\t\t\t\t\t\"announcement: %v\", err)\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\tcase <-f.quit:\n\t\treturn ErrFundingManagerShuttingDown\n\t}\n\n\treturn nil\n}\n\n// InitFundingWorkflow sends a message to the funding manager instructing it\n// to initiate a single funder workflow with the source peer.\n// TODO(roasbeef): re-visit blocking nature..",
      "length": 2629,
      "tokens": 366,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) InitFundingWorkflow(msg *InitFundingMsg) {",
      "content": "func (f *Manager) InitFundingWorkflow(msg *InitFundingMsg) {\n\tf.fundingRequests <- msg\n}\n\n// getUpfrontShutdownScript takes a user provided script and a getScript\n// function which can be used to generate an upfront shutdown script. If our\n// peer does not support the feature, this function will error if a non-zero\n// script was provided by the user, and return an empty script otherwise. If\n// our peer does support the feature, we will return the user provided script\n// if non-zero, or a freshly generated script if our node is configured to set\n// upfront shutdown scripts automatically.",
      "length": 523,
      "tokens": 90,
      "embedding": []
    },
    {
      "slug": "func getUpfrontShutdownScript(enableUpfrontShutdown bool, peer lnpeer.Peer,",
      "content": "func getUpfrontShutdownScript(enableUpfrontShutdown bool, peer lnpeer.Peer,\n\tscript lnwire.DeliveryAddress,\n\tgetScript func(bool) (lnwire.DeliveryAddress, error)) (lnwire.DeliveryAddress,\n\terror) {\n\n\t// Check whether the remote peer supports upfront shutdown scripts.\n\tremoteUpfrontShutdown := peer.RemoteFeatures().HasFeature(\n\t\tlnwire.UpfrontShutdownScriptOptional,\n\t)\n\n\t// If the peer does not support upfront shutdown scripts, and one has been\n\t// provided, return an error because the feature is not supported.\n\tif !remoteUpfrontShutdown && len(script) != 0 {\n\t\treturn nil, errUpfrontShutdownScriptNotSupported\n\t}\n\n\t// If the peer does not support upfront shutdown, return an empty address.\n\tif !remoteUpfrontShutdown {\n\t\treturn nil, nil\n\t}\n\n\t// If the user has provided an script and the peer supports the feature,\n\t// return it. Note that user set scripts override the enable upfront\n\t// shutdown flag.\n\tif len(script) > 0 {\n\t\treturn script, nil\n\t}\n\n\t// If we do not have setting of upfront shutdown script enabled, return\n\t// an empty script.\n\tif !enableUpfrontShutdown {\n\t\treturn nil, nil\n\t}\n\n\t// We can safely send a taproot address iff, both sides have negotiated\n\t// the shutdown-any-segwit feature.\n\ttaprootOK := peer.RemoteFeatures().HasFeature(lnwire.ShutdownAnySegwitOptional) &&\n\t\tpeer.LocalFeatures().HasFeature(lnwire.ShutdownAnySegwitOptional)\n\n\treturn getScript(taprootOK)\n}\n\n// handleInitFundingMsg creates a channel reservation within the daemon's\n// wallet, then sends a funding request to the remote peer kicking off the\n// funding workflow.",
      "length": 1447,
      "tokens": 193,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) handleInitFundingMsg(msg *InitFundingMsg) {",
      "content": "func (f *Manager) handleInitFundingMsg(msg *InitFundingMsg) {\n\tvar (\n\t\tpeerKey        = msg.Peer.IdentityKey()\n\t\tlocalAmt       = msg.LocalFundingAmt\n\t\tbaseFee        = msg.BaseFee\n\t\tfeeRate        = msg.FeeRate\n\t\tminHtlcIn      = msg.MinHtlcIn\n\t\tremoteCsvDelay = msg.RemoteCsvDelay\n\t\tmaxValue       = msg.MaxValueInFlight\n\t\tmaxHtlcs       = msg.MaxHtlcs\n\t\tmaxCSV         = msg.MaxLocalCsv\n\t\tchanReserve    = msg.RemoteChanReserve\n\t)\n\n\t// If no maximum CSV delay was set for this channel, we use our default\n\t// value.\n\tif maxCSV == 0 {\n\t\tmaxCSV = f.cfg.MaxLocalCSVDelay\n\t}\n\n\tlog.Infof(\"Initiating fundingRequest(local_amt=%v \"+\n\t\t\"(subtract_fees=%v), push_amt=%v, chain_hash=%v, peer=%x, \"+\n\t\t\"min_confs=%v)\", localAmt, msg.SubtractFees, msg.PushAmt,\n\t\tmsg.ChainHash, peerKey.SerializeCompressed(), msg.MinConfs)\n\n\t// We set the channel flags to indicate whether we want this channel to\n\t// be announced to the network.\n\tvar channelFlags lnwire.FundingFlag\n\tif !msg.Private {\n\t\t// This channel will be announced.\n\t\tchannelFlags = lnwire.FFAnnounceChannel\n\t}\n\n\t// If the caller specified their own channel ID, then we'll use that.\n\t// Otherwise we'll generate a fresh one as normal.  This will be used\n\t// to track this reservation throughout its lifetime.\n\tvar chanID [32]byte\n\tif msg.PendingChanID == zeroID {\n\t\tchanID = f.nextPendingChanID()\n\t} else {\n\t\t// If the user specified their own pending channel ID, then\n\t\t// we'll ensure it doesn't collide with any existing pending\n\t\t// channel ID.\n\t\tchanID = msg.PendingChanID\n\t\tif _, err := f.getReservationCtx(peerKey, chanID); err == nil {\n\t\t\tmsg.Err <- fmt.Errorf(\"pendingChannelID(%x) \"+\n\t\t\t\t\"already present\", chanID[:])\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Check whether the peer supports upfront shutdown, and get an address\n\t// which should be used (either a user specified address or a new\n\t// address from the wallet if our node is configured to set shutdown\n\t// address by default).\n\tshutdown, err := getUpfrontShutdownScript(\n\t\tf.cfg.EnableUpfrontShutdown, msg.Peer, msg.ShutdownScript,\n\t\tf.selectShutdownScript,\n\t)\n\tif err != nil {\n\t\tmsg.Err <- err\n\t\treturn\n\t}\n\n\t// Initialize a funding reservation with the local wallet. If the\n\t// wallet doesn't have enough funds to commit to this channel, then the\n\t// request will fail, and be aborted.\n\t//\n\t// Before we init the channel, we'll also check to see what commitment\n\t// format we can use with this peer. This is dependent on *both* us and\n\t// the remote peer are signaling the proper feature bit.\n\t_, chanType, commitType, err := negotiateCommitmentType(\n\t\tmsg.ChannelType, msg.Peer.LocalFeatures(),\n\t\tmsg.Peer.RemoteFeatures(), true,\n\t)\n\tif err != nil {\n\t\tlog.Errorf(\"channel type negotiation failed: %v\", err)\n\t\tmsg.Err <- err\n\t\treturn\n\t}\n\n\tvar (\n\t\tzeroConf bool\n\t\tscid     bool\n\t)\n\n\t// Check if the returned chanType includes either the zero-conf or\n\t// scid-alias bits.\n\tfeatureVec := lnwire.RawFeatureVector(*chanType)\n\tzeroConf = featureVec.IsSet(lnwire.ZeroConfRequired)\n\tscid = featureVec.IsSet(lnwire.ScidAliasRequired)\n\n\t// The option-scid-alias channel type for a public channel is\n\t// disallowed.\n\tif scid && !msg.Private {\n\t\terr = fmt.Errorf(\"option-scid-alias chantype for public \" +\n\t\t\t\"channel\")\n\t\tlog.Error(err)\n\t\tmsg.Err <- err\n\t\treturn\n\t}\n\n\t// First, we'll query the fee estimator for a fee that should get the\n\t// commitment transaction confirmed by the next few blocks (conf target\n\t// of 3). We target the near blocks here to ensure that we'll be able\n\t// to execute a timely unilateral channel closure if needed.\n\tcommitFeePerKw, err := f.cfg.FeeEstimator.EstimateFeePerKW(3)\n\tif err != nil {\n\t\tmsg.Err <- err\n\t\treturn\n\t}\n\n\t// For anchor channels cap the initial commit fee rate at our defined\n\t// maximum.\n\tif commitType.HasAnchors() &&\n\t\tcommitFeePerKw > f.cfg.MaxAnchorsCommitFeeRate {\n\n\t\tcommitFeePerKw = f.cfg.MaxAnchorsCommitFeeRate\n\t}\n\n\tvar scidFeatureVal bool\n\tif hasFeatures(\n\t\tmsg.Peer.LocalFeatures(), msg.Peer.RemoteFeatures(),\n\t\tlnwire.ScidAliasOptional,\n\t) {\n\n\t\tscidFeatureVal = true\n\t}\n\n\treq := &lnwallet.InitFundingReserveMsg{\n\t\tChainHash:        &msg.ChainHash,\n\t\tPendingChanID:    chanID,\n\t\tNodeID:           peerKey,\n\t\tNodeAddr:         msg.Peer.Address(),\n\t\tSubtractFees:     msg.SubtractFees,\n\t\tLocalFundingAmt:  localAmt,\n\t\tRemoteFundingAmt: 0,\n\t\tCommitFeePerKw:   commitFeePerKw,\n\t\tFundingFeePerKw:  msg.FundingFeePerKw,\n\t\tPushMSat:         msg.PushAmt,\n\t\tFlags:            channelFlags,\n\t\tMinConfs:         msg.MinConfs,\n\t\tCommitType:       commitType,\n\t\tChanFunder:       msg.ChanFunder,\n\t\tZeroConf:         zeroConf,\n\t\tOptionScidAlias:  scid,\n\t\tScidAliasFeature: scidFeatureVal,\n\t}\n\n\treservation, err := f.cfg.Wallet.InitChannelReservation(req)\n\tif err != nil {\n\t\tmsg.Err <- err\n\t\treturn\n\t}\n\n\tif zeroConf {\n\t\t// Store the alias for zero-conf channels in the underlying\n\t\t// partial channel state.\n\t\taliasScid, err := f.cfg.AliasManager.RequestAlias()\n\t\tif err != nil {\n\t\t\tmsg.Err <- err\n\t\t\treturn\n\t\t}\n\n\t\treservation.AddAlias(aliasScid)\n\t}\n\n\t// Set our upfront shutdown address in the existing reservation.\n\treservation.SetOurUpfrontShutdown(shutdown)\n\n\t// Now that we have successfully reserved funds for this channel in the\n\t// wallet, we can fetch the final channel capacity. This is done at\n\t// this point since the final capacity might change in case of\n\t// SubtractFees=true.\n\tcapacity := reservation.Capacity()\n\n\tlog.Infof(\"Target commit tx sat/kw for pendingID(%x): %v\", chanID,\n\t\tint64(commitFeePerKw))\n\n\t// If the remote CSV delay was not set in the open channel request,\n\t// we'll use the RequiredRemoteDelay closure to compute the delay we\n\t// require given the total amount of funds within the channel.\n\tif remoteCsvDelay == 0 {\n\t\tremoteCsvDelay = f.cfg.RequiredRemoteDelay(capacity)\n\t}\n\n\t// If no minimum HTLC value was specified, use the default one.\n\tif minHtlcIn == 0 {\n\t\tminHtlcIn = f.cfg.DefaultMinHtlcIn\n\t}\n\n\t// If no max value was specified, use the default one.\n\tif maxValue == 0 {\n\t\tmaxValue = f.cfg.RequiredRemoteMaxValue(capacity)\n\t}\n\n\tif maxHtlcs == 0 {\n\t\tmaxHtlcs = f.cfg.RequiredRemoteMaxHTLCs(capacity)\n\t}\n\n\t// Prepare the optional channel fee values from the initFundingMsg.\n\t// If useBaseFee or useFeeRate are false the client did not\n\t// provide fee values hence we assume default fee settings from\n\t// the config.\n\tforwardingPolicy := htlcswitch.ForwardingPolicy{\n\t\tBaseFee: f.cfg.DefaultRoutingPolicy.BaseFee,\n\t\tFeeRate: f.cfg.DefaultRoutingPolicy.FeeRate,\n\t}\n\tif baseFee != nil {\n\t\tforwardingPolicy.BaseFee = lnwire.MilliSatoshi(*baseFee)\n\t}\n\n\tif feeRate != nil {\n\t\tforwardingPolicy.FeeRate = lnwire.MilliSatoshi(*feeRate)\n\t}\n\n\t// Once the reservation has been created, and indexed, queue a funding\n\t// request to the remote peer, kicking off the funding workflow.\n\tourContribution := reservation.OurContribution()\n\n\t// Fetch our dust limit which is part of the default channel\n\t// constraints, and log it.\n\tourDustLimit := ourContribution.DustLimit\n\n\tlog.Infof(\"Dust limit for pendingID(%x): %v\", chanID, ourDustLimit)\n\n\t// If the channel reserve is not specified, then we calculate an\n\t// appropriate amount here.\n\tif chanReserve == 0 {\n\t\tchanReserve = f.cfg.RequiredRemoteChanReserve(\n\t\t\tcapacity, ourDustLimit,\n\t\t)\n\t}\n\n\t// If a pending channel map for this peer isn't already created, then\n\t// we create one, ultimately allowing us to track this pending\n\t// reservation within the target peer.\n\tpeerIDKey := newSerializedKey(peerKey)\n\tf.resMtx.Lock()\n\tif _, ok := f.activeReservations[peerIDKey]; !ok {\n\t\tf.activeReservations[peerIDKey] = make(pendingChannels)\n\t}\n\n\tresCtx := &reservationWithCtx{\n\t\tchanAmt:           capacity,\n\t\tforwardingPolicy:  forwardingPolicy,\n\t\tremoteCsvDelay:    remoteCsvDelay,\n\t\tremoteMinHtlc:     minHtlcIn,\n\t\tremoteMaxValue:    maxValue,\n\t\tremoteMaxHtlcs:    maxHtlcs,\n\t\tremoteChanReserve: chanReserve,\n\t\tmaxLocalCsv:       maxCSV,\n\t\tchannelType:       msg.ChannelType,\n\t\treservation:       reservation,\n\t\tpeer:              msg.Peer,\n\t\tupdates:           msg.Updates,\n\t\terr:               msg.Err,\n\t}\n\tf.activeReservations[peerIDKey][chanID] = resCtx\n\tf.resMtx.Unlock()\n\n\t// Update the timestamp once the InitFundingMsg has been handled.\n\tdefer resCtx.updateTimestamp()\n\n\t// Check the sanity of the selected channel constraints.\n\tchannelConstraints := &channeldb.ChannelConstraints{\n\t\tDustLimit:        ourDustLimit,\n\t\tChanReserve:      chanReserve,\n\t\tMaxPendingAmount: maxValue,\n\t\tMinHTLC:          minHtlcIn,\n\t\tMaxAcceptedHtlcs: maxHtlcs,\n\t\tCsvDelay:         remoteCsvDelay,\n\t}\n\terr = lnwallet.VerifyConstraints(\n\t\tchannelConstraints, resCtx.maxLocalCsv, capacity,\n\t)\n\tif err != nil {\n\t\t_, reserveErr := f.cancelReservationCtx(peerKey, chanID, false)\n\t\tif reserveErr != nil {\n\t\t\tlog.Errorf(\"unable to cancel reservation: %v\",\n\t\t\t\treserveErr)\n\t\t}\n\n\t\tmsg.Err <- err\n\t\treturn\n\t}\n\n\t// When opening a script enforced channel lease, include the required\n\t// expiry TLV record in our proposal.\n\tvar leaseExpiry *lnwire.LeaseExpiry\n\tif commitType == lnwallet.CommitmentTypeScriptEnforcedLease {\n\t\tleaseExpiry = new(lnwire.LeaseExpiry)\n\t\t*leaseExpiry = lnwire.LeaseExpiry(reservation.LeaseExpiry())\n\t}\n\n\tlog.Infof(\"Starting funding workflow with %v for pending_id(%x), \"+\n\t\t\"committype=%v\", msg.Peer.Address(), chanID, commitType)\n\n\tfundingOpen := lnwire.OpenChannel{\n\t\tChainHash:             *f.cfg.Wallet.Cfg.NetParams.GenesisHash,\n\t\tPendingChannelID:      chanID,\n\t\tFundingAmount:         capacity,\n\t\tPushAmount:            msg.PushAmt,\n\t\tDustLimit:             ourDustLimit,\n\t\tMaxValueInFlight:      maxValue,\n\t\tChannelReserve:        chanReserve,\n\t\tHtlcMinimum:           minHtlcIn,\n\t\tFeePerKiloWeight:      uint32(commitFeePerKw),\n\t\tCsvDelay:              remoteCsvDelay,\n\t\tMaxAcceptedHTLCs:      maxHtlcs,\n\t\tFundingKey:            ourContribution.MultiSigKey.PubKey,\n\t\tRevocationPoint:       ourContribution.RevocationBasePoint.PubKey,\n\t\tPaymentPoint:          ourContribution.PaymentBasePoint.PubKey,\n\t\tHtlcPoint:             ourContribution.HtlcBasePoint.PubKey,\n\t\tDelayedPaymentPoint:   ourContribution.DelayBasePoint.PubKey,\n\t\tFirstCommitmentPoint:  ourContribution.FirstCommitmentPoint,\n\t\tChannelFlags:          channelFlags,\n\t\tUpfrontShutdownScript: shutdown,\n\t\tChannelType:           chanType,\n\t\tLeaseExpiry:           leaseExpiry,\n\t}\n\tif err := msg.Peer.SendMessage(true, &fundingOpen); err != nil {\n\t\te := fmt.Errorf(\"unable to send funding request message: %v\",\n\t\t\terr)\n\t\tlog.Errorf(e.Error())\n\n\t\t// Since we were unable to send the initial message to the peer\n\t\t// and start the funding flow, we'll cancel this reservation.\n\t\t_, err := f.cancelReservationCtx(peerKey, chanID, false)\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"unable to cancel reservation: %v\", err)\n\t\t}\n\n\t\tmsg.Err <- e\n\t\treturn\n\t}\n}\n\n// handleWarningMsg processes the warning which was received from remote peer.",
      "length": 10397,
      "tokens": 1234,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) handleWarningMsg(peer lnpeer.Peer, msg *lnwire.Warning) {",
      "content": "func (f *Manager) handleWarningMsg(peer lnpeer.Peer, msg *lnwire.Warning) {\n\tlog.Warnf(\"received warning message from peer %x: %v\",\n\t\tpeer.IdentityKey().SerializeCompressed(), msg.Warning())\n}\n\n// handleErrorMsg processes the error which was received from remote peer,\n// depending on the type of error we should do different clean up steps and\n// inform the user about it.",
      "length": 291,
      "tokens": 42,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) handleErrorMsg(peer lnpeer.Peer, msg *lnwire.Error) {",
      "content": "func (f *Manager) handleErrorMsg(peer lnpeer.Peer, msg *lnwire.Error) {\n\tchanID := msg.ChanID\n\tpeerKey := peer.IdentityKey()\n\n\t// First, we'll attempt to retrieve and cancel the funding workflow\n\t// that this error was tied to. If we're unable to do so, then we'll\n\t// exit early as this was an unwarranted error.\n\tresCtx, err := f.cancelReservationCtx(peerKey, chanID, true)\n\tif err != nil {\n\t\tlog.Warnf(\"Received error for non-existent funding \"+\n\t\t\t\"flow: %v (%v)\", err, msg.Error())\n\t\treturn\n\t}\n\n\t// If we did indeed find the funding workflow, then we'll return the\n\t// error back to the caller (if any), and cancel the workflow itself.\n\tfundingErr := fmt.Errorf(\"received funding error from %x: %v\",\n\t\tpeerKey.SerializeCompressed(), msg.Error(),\n\t)\n\tlog.Errorf(fundingErr.Error())\n\n\t// If this was a PSBT funding flow, the remote likely timed out because\n\t// we waited too long. Return a nice error message to the user in that\n\t// case so the user knows what's the problem.\n\tif resCtx.reservation.IsPsbt() {\n\t\tfundingErr = fmt.Errorf(\"%w: %v\", chanfunding.ErrRemoteCanceled,\n\t\t\tfundingErr)\n\t}\n\n\tresCtx.err <- fundingErr\n}\n\n// pruneZombieReservations loops through all pending reservations and fails the\n// funding flow for any reservations that have not been updated since the\n// ReservationTimeout and are not locked waiting for the funding transaction.",
      "length": 1254,
      "tokens": 189,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) pruneZombieReservations() {",
      "content": "func (f *Manager) pruneZombieReservations() {\n\tzombieReservations := make(pendingChannels)\n\n\tf.resMtx.RLock()\n\tfor _, pendingReservations := range f.activeReservations {\n\t\tfor pendingChanID, resCtx := range pendingReservations {\n\t\t\tif resCtx.isLocked() {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// We don't want to expire PSBT funding reservations.\n\t\t\t// These reservations are always initiated by us and the\n\t\t\t// remote peer is likely going to cancel them after some\n\t\t\t// idle time anyway. So no need for us to also prune\n\t\t\t// them.\n\t\t\tsinceLastUpdate := time.Since(resCtx.lastUpdated)\n\t\t\tisExpired := sinceLastUpdate > f.cfg.ReservationTimeout\n\t\t\tif !resCtx.reservation.IsPsbt() && isExpired {\n\t\t\t\tzombieReservations[pendingChanID] = resCtx\n\t\t\t}\n\t\t}\n\t}\n\tf.resMtx.RUnlock()\n\n\tfor pendingChanID, resCtx := range zombieReservations {\n\t\terr := fmt.Errorf(\"reservation timed out waiting for peer \"+\n\t\t\t\"(peer_id:%x, chan_id:%x)\",\n\t\t\tresCtx.peer.IdentityKey().SerializeCompressed(),\n\t\t\tpendingChanID[:])\n\t\tlog.Warnf(err.Error())\n\t\tf.failFundingFlow(resCtx.peer, pendingChanID, err)\n\t}\n}\n\n// cancelReservationCtx does all needed work in order to securely cancel the\n// reservation.",
      "length": 1081,
      "tokens": 127,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) cancelReservationCtx(peerKey *btcec.PublicKey,",
      "content": "func (f *Manager) cancelReservationCtx(peerKey *btcec.PublicKey,\n\tpendingChanID [32]byte, byRemote bool) (*reservationWithCtx, error) {\n\n\tlog.Infof(\"Cancelling funding reservation for node_key=%x, \"+\n\t\t\"chan_id=%x\", peerKey.SerializeCompressed(), pendingChanID[:])\n\n\tpeerIDKey := newSerializedKey(peerKey)\n\tf.resMtx.Lock()\n\tdefer f.resMtx.Unlock()\n\n\tnodeReservations, ok := f.activeReservations[peerIDKey]\n\tif !ok {\n\t\t// No reservations for this node.\n\t\treturn nil, errors.Errorf(\"no active reservations for peer(%x)\",\n\t\t\tpeerIDKey[:])\n\t}\n\n\tctx, ok := nodeReservations[pendingChanID]\n\tif !ok {\n\t\treturn nil, errors.Errorf(\"unknown channel (id: %x) for \"+\n\t\t\t\"peer(%x)\", pendingChanID[:], peerIDKey[:])\n\t}\n\n\t// If the reservation was a PSBT funding flow and it was canceled by the\n\t// remote peer, then we need to thread through a different error message\n\t// to the subroutine that's waiting for the user input so it can return\n\t// a nice error message to the user.\n\tif ctx.reservation.IsPsbt() && byRemote {\n\t\tctx.reservation.RemoteCanceled()\n\t}\n\n\tif err := ctx.reservation.Cancel(); err != nil {\n\t\treturn nil, errors.Errorf(\"unable to cancel reservation: %v\",\n\t\t\terr)\n\t}\n\n\tdelete(nodeReservations, pendingChanID)\n\n\t// If this was the last active reservation for this peer, delete the\n\t// peer's entry altogether.\n\tif len(nodeReservations) == 0 {\n\t\tdelete(f.activeReservations, peerIDKey)\n\t}\n\treturn ctx, nil\n}\n\n// deleteReservationCtx deletes the reservation uniquely identified by the\n// target public key of the peer, and the specified pending channel ID.",
      "length": 1447,
      "tokens": 190,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) deleteReservationCtx(peerKey *btcec.PublicKey,",
      "content": "func (f *Manager) deleteReservationCtx(peerKey *btcec.PublicKey,\n\tpendingChanID [32]byte) {\n\n\t// TODO(roasbeef): possibly cancel funding barrier in peer's\n\t// channelManager?\n\tpeerIDKey := newSerializedKey(peerKey)\n\tf.resMtx.Lock()\n\tdefer f.resMtx.Unlock()\n\n\tnodeReservations, ok := f.activeReservations[peerIDKey]\n\tif !ok {\n\t\t// No reservations for this node.\n\t\treturn\n\t}\n\tdelete(nodeReservations, pendingChanID)\n\n\t// If this was the last active reservation for this peer, delete the\n\t// peer's entry altogether.\n\tif len(nodeReservations) == 0 {\n\t\tdelete(f.activeReservations, peerIDKey)\n\t}\n}\n\n// getReservationCtx returns the reservation context for a particular pending\n// channel ID for a target peer.",
      "length": 617,
      "tokens": 79,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) getReservationCtx(peerKey *btcec.PublicKey,",
      "content": "func (f *Manager) getReservationCtx(peerKey *btcec.PublicKey,\n\tpendingChanID [32]byte) (*reservationWithCtx, error) {\n\n\tpeerIDKey := newSerializedKey(peerKey)\n\tf.resMtx.RLock()\n\tresCtx, ok := f.activeReservations[peerIDKey][pendingChanID]\n\tf.resMtx.RUnlock()\n\n\tif !ok {\n\t\treturn nil, errors.Errorf(\"unknown channel (id: %x) for \"+\n\t\t\t\"peer(%x)\", pendingChanID[:], peerIDKey[:])\n\t}\n\n\treturn resCtx, nil\n}\n\n// IsPendingChannel returns a boolean indicating whether the channel identified\n// by the pendingChanID and given peer is pending, meaning it is in the process\n// of being funded. After the funding transaction has been confirmed, the\n// channel will receive a new, permanent channel ID, and will no longer be\n// considered pending.",
      "length": 655,
      "tokens": 87,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) IsPendingChannel(pendingChanID [32]byte,",
      "content": "func (f *Manager) IsPendingChannel(pendingChanID [32]byte,\n\tpeer lnpeer.Peer) bool {\n\n\tpeerIDKey := newSerializedKey(peer.IdentityKey())\n\tf.resMtx.RLock()\n\t_, ok := f.activeReservations[peerIDKey][pendingChanID]\n\tf.resMtx.RUnlock()\n\n\treturn ok\n}\n",
      "length": 178,
      "tokens": 16,
      "embedding": []
    },
    {
      "slug": "func copyPubKey(pub *btcec.PublicKey) *btcec.PublicKey {",
      "content": "func copyPubKey(pub *btcec.PublicKey) *btcec.PublicKey {\n\tvar tmp btcec.JacobianPoint\n\tpub.AsJacobian(&tmp)\n\ttmp.ToAffine()\n\treturn btcec.NewPublicKey(&tmp.X, &tmp.Y)\n}\n\n// saveInitialFwdingPolicy saves the forwarding policy for the provided\n// chanPoint in the channelOpeningStateBucket.",
      "length": 224,
      "tokens": 23,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) saveInitialFwdingPolicy(permChanID lnwire.ChannelID,",
      "content": "func (f *Manager) saveInitialFwdingPolicy(permChanID lnwire.ChannelID,\n\tforwardingPolicy *htlcswitch.ForwardingPolicy) error {\n\n\tchanID := make([]byte, 32)\n\tcopy(chanID, permChanID[:])\n\n\tscratch := make([]byte, 36)\n\tbyteOrder.PutUint64(scratch[:8], uint64(forwardingPolicy.MinHTLCOut))\n\tbyteOrder.PutUint64(scratch[8:16], uint64(forwardingPolicy.MaxHTLC))\n\tbyteOrder.PutUint64(scratch[16:24], uint64(forwardingPolicy.BaseFee))\n\tbyteOrder.PutUint64(scratch[24:32], uint64(forwardingPolicy.FeeRate))\n\tbyteOrder.PutUint32(scratch[32:], forwardingPolicy.TimeLockDelta)\n\n\treturn f.cfg.Wallet.Cfg.Database.SaveInitialFwdingPolicy(\n\t\tchanID, scratch,\n\t)\n}\n\n// getInitialFwdingPolicy fetches the initial forwarding policy for a given\n// channel id from the database which will be applied during the channel\n// announcement phase.",
      "length": 731,
      "tokens": 56,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) getInitialFwdingPolicy(permChanID lnwire.ChannelID) (",
      "content": "func (f *Manager) getInitialFwdingPolicy(permChanID lnwire.ChannelID) (\n\t*htlcswitch.ForwardingPolicy, error) {\n\n\tchanID := make([]byte, 32)\n\tcopy(chanID, permChanID[:])\n\n\tvalue, err := f.cfg.Wallet.Cfg.Database.GetInitialFwdingPolicy(chanID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar fwdingPolicy htlcswitch.ForwardingPolicy\n\tfwdingPolicy.MinHTLCOut = lnwire.MilliSatoshi(\n\t\tbyteOrder.Uint64(value[:8]),\n\t)\n\tfwdingPolicy.MaxHTLC = lnwire.MilliSatoshi(\n\t\tbyteOrder.Uint64(value[8:16]),\n\t)\n\tfwdingPolicy.BaseFee = lnwire.MilliSatoshi(\n\t\tbyteOrder.Uint64(value[16:24]),\n\t)\n\tfwdingPolicy.FeeRate = lnwire.MilliSatoshi(\n\t\tbyteOrder.Uint64(value[24:32]),\n\t)\n\tfwdingPolicy.TimeLockDelta = byteOrder.Uint32(value[32:36])\n\n\treturn &fwdingPolicy, nil\n}\n\n// deleteInitialFwdingPolicy removes channel fees for this chanID from\n// the database.",
      "length": 733,
      "tokens": 64,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) deleteInitialFwdingPolicy(permChanID lnwire.ChannelID) error {",
      "content": "func (f *Manager) deleteInitialFwdingPolicy(permChanID lnwire.ChannelID) error {\n\tchanID := make([]byte, 32)\n\tcopy(chanID, permChanID[:])\n\n\treturn f.cfg.Wallet.Cfg.Database.DeleteInitialFwdingPolicy(chanID)\n}\n\n// saveChannelOpeningState saves the channelOpeningState for the provided\n// chanPoint to the channelOpeningStateBucket.",
      "length": 242,
      "tokens": 22,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) saveChannelOpeningState(chanPoint *wire.OutPoint,",
      "content": "func (f *Manager) saveChannelOpeningState(chanPoint *wire.OutPoint,\n\tstate channelOpeningState, shortChanID *lnwire.ShortChannelID) error {\n\n\tvar outpointBytes bytes.Buffer\n\tif err := WriteOutpoint(&outpointBytes, chanPoint); err != nil {\n\t\treturn err\n\t}\n\n\t// Save state and the uint64 representation of the shortChanID\n\t// for later use.\n\tscratch := make([]byte, 10)\n\tbyteOrder.PutUint16(scratch[:2], uint16(state))\n\tbyteOrder.PutUint64(scratch[2:], shortChanID.ToUint64())\n\treturn f.cfg.Wallet.Cfg.Database.SaveChannelOpeningState(\n\t\toutpointBytes.Bytes(), scratch,\n\t)\n}\n\n// getChannelOpeningState fetches the channelOpeningState for the provided\n// chanPoint from the database, or returns ErrChannelNotFound if the channel\n// is not found.",
      "length": 655,
      "tokens": 72,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) getChannelOpeningState(chanPoint *wire.OutPoint) (",
      "content": "func (f *Manager) getChannelOpeningState(chanPoint *wire.OutPoint) (\n\tchannelOpeningState, *lnwire.ShortChannelID, error) {\n\n\tvar outpointBytes bytes.Buffer\n\tif err := WriteOutpoint(&outpointBytes, chanPoint); err != nil {\n\t\treturn 0, nil, err\n\t}\n\n\tvalue, err := f.cfg.Wallet.Cfg.Database.GetChannelOpeningState(\n\t\toutpointBytes.Bytes(),\n\t)\n\tif err != nil {\n\t\treturn 0, nil, err\n\t}\n\n\tstate := channelOpeningState(byteOrder.Uint16(value[:2]))\n\tshortChanID := lnwire.NewShortChanIDFromInt(byteOrder.Uint64(value[2:]))\n\treturn state, &shortChanID, nil\n}\n\n// deleteChannelOpeningState removes any state for chanPoint from the database.",
      "length": 543,
      "tokens": 58,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) deleteChannelOpeningState(chanPoint *wire.OutPoint) error {",
      "content": "func (f *Manager) deleteChannelOpeningState(chanPoint *wire.OutPoint) error {\n\tvar outpointBytes bytes.Buffer\n\tif err := WriteOutpoint(&outpointBytes, chanPoint); err != nil {\n\t\treturn err\n\t}\n\n\treturn f.cfg.Wallet.Cfg.Database.DeleteChannelOpeningState(\n\t\toutpointBytes.Bytes(),\n\t)\n}\n\n// selectShutdownScript selects the shutdown script we should send to the peer.\n// If we can use taproot, then we prefer that, otherwise we'll use a p2wkh\n// script.",
      "length": 360,
      "tokens": 49,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) selectShutdownScript(taprootOK bool,",
      "content": "func (f *Manager) selectShutdownScript(taprootOK bool,\n) (lnwire.DeliveryAddress, error) {\n\n\taddrType := lnwallet.WitnessPubKey\n\tif taprootOK {\n\t\taddrType = lnwallet.TaprootPubkey\n\t}\n\n\taddr, err := f.cfg.Wallet.NewAddress(\n\t\taddrType, false, lnwallet.DefaultAccountName,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn txscript.PayToAddrScript(addr)\n}\n\n// waitForPeerOnline blocks until the peer specified by peerPubkey comes online\n// and then returns the online peer.",
      "length": 398,
      "tokens": 52,
      "embedding": []
    },
    {
      "slug": "func (f *Manager) waitForPeerOnline(peerPubkey *btcec.PublicKey) (lnpeer.Peer,",
      "content": "func (f *Manager) waitForPeerOnline(peerPubkey *btcec.PublicKey) (lnpeer.Peer,\n\terror) {\n\n\tpeerChan := make(chan lnpeer.Peer, 1)\n\n\tvar peerKey [33]byte\n\tcopy(peerKey[:], peerPubkey.SerializeCompressed())\n\n\tf.cfg.NotifyWhenOnline(peerKey, peerChan)\n\n\tvar peer lnpeer.Peer\n\tselect {\n\tcase peer = <-peerChan:\n\tcase <-f.quit:\n\t\treturn peer, ErrFundingManagerShuttingDown\n\t}\n\treturn peer, nil\n}\n",
      "length": 294,
      "tokens": 33,
      "embedding": []
    }
  ]
}