{
  "filepath": "../implementations/go/lnd/htlcswitch/link.go",
  "package": "htlcswitch",
  "sections": [
    {
      "slug": "func init() {",
      "content": "func init() {\n\tprand.Seed(time.Now().UnixNano())\n}\n\nconst (\n\t// DefaultMaxOutgoingCltvExpiry is the maximum outgoing time lock that\n\t// the node accepts for forwarded payments. The value is relative to the\n\t// current block height. The reason to have a maximum is to prevent\n\t// funds getting locked up unreasonably long. Otherwise, an attacker\n\t// willing to lock its own funds too, could force the funds of this node\n\t// to be locked up for an indefinite (max int32) number of blocks.\n\t//\n\t// The value 2016 corresponds to on average two weeks worth of blocks\n\t// and is based on the maximum number of hops (20), the default CLTV\n\t// delta (40), and some extra margin to account for the other lightning\n\t// implementations and past lnd versions which used to have a default\n\t// CLTV delta of 144.\n\tDefaultMaxOutgoingCltvExpiry = 2016\n\n\t// DefaultMinLinkFeeUpdateTimeout represents the minimum interval in\n\t// which a link should propose to update its commitment fee rate.\n\tDefaultMinLinkFeeUpdateTimeout = 10 * time.Minute\n\n\t// DefaultMaxLinkFeeUpdateTimeout represents the maximum interval in\n\t// which a link should propose to update its commitment fee rate.\n\tDefaultMaxLinkFeeUpdateTimeout = 60 * time.Minute\n\n\t// DefaultMaxLinkFeeAllocation is the highest allocation we'll allow\n\t// a channel's commitment fee to be of its balance. This only applies to\n\t// the initiator of the channel.\n\tDefaultMaxLinkFeeAllocation float64 = 0.5\n)\n\n// ForwardingPolicy describes the set of constraints that a given ChannelLink\n// is to adhere to when forwarding HTLC's. For each incoming HTLC, this set of\n// constraints will be consulted in order to ensure that adequate fees are\n// paid, and our time-lock parameters are respected. In the event that an\n// incoming HTLC violates any of these constraints, it is to be _rejected_ with\n// the error possibly carrying along a ChannelUpdate message that includes the\n// latest policy.",
      "length": 1869,
      "tokens": 300,
      "embedding": []
    },
    {
      "slug": "type ForwardingPolicy struct {",
      "content": "type ForwardingPolicy struct {\n\t// MinHTLC is the smallest HTLC that is to be forwarded.\n\tMinHTLCOut lnwire.MilliSatoshi\n\n\t// MaxHTLC is the largest HTLC that is to be forwarded.\n\tMaxHTLC lnwire.MilliSatoshi\n\n\t// BaseFee is the base fee, expressed in milli-satoshi that must be\n\t// paid for each incoming HTLC. This field, combined with FeeRate is\n\t// used to compute the required fee for a given HTLC.\n\tBaseFee lnwire.MilliSatoshi\n\n\t// FeeRate is the fee rate, expressed in milli-satoshi that must be\n\t// paid for each incoming HTLC. This field combined with BaseFee is\n\t// used to compute the required fee for a given HTLC.\n\tFeeRate lnwire.MilliSatoshi\n\n\t// TimeLockDelta is the absolute time-lock value, expressed in blocks,\n\t// that will be subtracted from an incoming HTLC's timelock value to\n\t// create the time-lock value for the forwarded outgoing HTLC. The\n\t// following constraint MUST hold for an HTLC to be forwarded:\n\t//\n\t//  * incomingHtlc.timeLock - timeLockDelta = fwdInfo.OutgoingCTLV\n\t//\n\t//    where fwdInfo is the forwarding information extracted from the\n\t//    per-hop payload of the incoming HTLC's onion packet.\n\tTimeLockDelta uint32\n\n\t// TODO(roasbeef): add fee module inside of switch\n}\n\n// ExpectedFee computes the expected fee for a given htlc amount. The value\n// returned from this function is to be used as a sanity check when forwarding\n// HTLC's to ensure that an incoming HTLC properly adheres to our propagated\n// forwarding policy.\n//\n// TODO(roasbeef): also add in current available channel bandwidth, inverse\n// func",
      "length": 1487,
      "tokens": 240,
      "embedding": []
    },
    {
      "slug": "func ExpectedFee(f ForwardingPolicy,",
      "content": "func ExpectedFee(f ForwardingPolicy,\n\thtlcAmt lnwire.MilliSatoshi) lnwire.MilliSatoshi {\n\n\treturn f.BaseFee + (htlcAmt*f.FeeRate)/1000000\n}\n\n// ChannelLinkConfig defines the configuration for the channel link. ALL\n// elements within the configuration MUST be non-nil for channel link to carry\n// out its duties.",
      "length": 267,
      "tokens": 36,
      "embedding": []
    },
    {
      "slug": "type ChannelLinkConfig struct {",
      "content": "type ChannelLinkConfig struct {\n\t// FwrdingPolicy is the initial forwarding policy to be used when\n\t// deciding whether to forwarding incoming HTLC's or not. This value\n\t// can be updated with subsequent calls to UpdateForwardingPolicy\n\t// targeted at a given ChannelLink concrete interface implementation.\n\tFwrdingPolicy ForwardingPolicy\n\n\t// Circuits provides restricted access to the switch's circuit map,\n\t// allowing the link to open and close circuits.\n\tCircuits CircuitModifier\n\n\t// Switch provides a reference to the HTLC switch, we only use this in\n\t// testing to access circuit operations not typically exposed by the\n\t// CircuitModifier.\n\t//\n\t// TODO(conner): remove after refactoring htlcswitch testing framework.\n\tSwitch *Switch\n\n\t// BestHeight returns the best known height.\n\tBestHeight func() uint32\n\n\t// ForwardPackets attempts to forward the batch of htlcs through the\n\t// switch. The function returns and error in case it fails to send one or\n\t// more packets. The link's quit signal should be provided to allow\n\t// cancellation of forwarding during link shutdown.\n\tForwardPackets func(chan struct{}, bool, ...*htlcPacket) error\n\n\t// DecodeHopIterators facilitates batched decoding of HTLC Sphinx onion\n\t// blobs, which are then used to inform how to forward an HTLC.\n\t//\n\t// NOTE: This function assumes the same set of readers and preimages\n\t// are always presented for the same identifier.\n\tDecodeHopIterators func([]byte, []hop.DecodeHopIteratorRequest) (\n\t\t[]hop.DecodeHopIteratorResponse, error)\n\n\t// ExtractErrorEncrypter function is responsible for decoding HTLC\n\t// Sphinx onion blob, and creating onion failure obfuscator.\n\tExtractErrorEncrypter hop.ErrorEncrypterExtracter\n\n\t// FetchLastChannelUpdate retrieves the latest routing policy for a\n\t// target channel. This channel will typically be the outgoing channel\n\t// specified when we receive an incoming HTLC.  This will be used to\n\t// provide payment senders our latest policy when sending encrypted\n\t// error messages.\n\tFetchLastChannelUpdate func(lnwire.ShortChannelID) (*lnwire.ChannelUpdate, error)\n\n\t// Peer is a lightning network node with which we have the channel link\n\t// opened.\n\tPeer lnpeer.Peer\n\n\t// Registry is a sub-system which responsible for managing the invoices\n\t// in thread-safe manner.\n\tRegistry InvoiceDatabase\n\n\t// PreimageCache is a global witness beacon that houses any new\n\t// preimages discovered by other links. We'll use this to add new\n\t// witnesses that we discover which will notify any sub-systems\n\t// subscribed to new events.\n\tPreimageCache contractcourt.WitnessBeacon\n\n\t// OnChannelFailure is a function closure that we'll call if the\n\t// channel failed for some reason. Depending on the severity of the\n\t// error, the closure potentially must force close this channel and\n\t// disconnect the peer.\n\t//\n\t// NOTE: The method must return in order for the ChannelLink to be able\n\t// to shut down properly.\n\tOnChannelFailure func(lnwire.ChannelID, lnwire.ShortChannelID,\n\t\tLinkFailureError)\n\n\t// UpdateContractSignals is a function closure that we'll use to update\n\t// outside sub-systems with this channel's latest ShortChannelID.\n\tUpdateContractSignals func(*contractcourt.ContractSignals) error\n\n\t// NotifyContractUpdate is a function closure that we'll use to update\n\t// the contractcourt and more specifically the ChannelArbitrator of the\n\t// latest channel state.\n\tNotifyContractUpdate func(*contractcourt.ContractUpdate) error\n\n\t// ChainEvents is an active subscription to the chain watcher for this\n\t// channel to be notified of any on-chain activity related to this\n\t// channel.\n\tChainEvents *contractcourt.ChainEventSubscription\n\n\t// FeeEstimator is an instance of a live fee estimator which will be\n\t// used to dynamically regulate the current fee of the commitment\n\t// transaction to ensure timely confirmation.\n\tFeeEstimator chainfee.Estimator\n\n\t// hodl.Mask is a bitvector composed of hodl.Flags, specifying breakpoints\n\t// for HTLC forwarding internal to the switch.\n\t//\n\t// NOTE: This should only be used for testing.\n\tHodlMask hodl.Mask\n\n\t// SyncStates is used to indicate that we need send the channel\n\t// reestablishment message to the remote peer. It should be done if our\n\t// clients have been restarted, or remote peer have been reconnected.\n\tSyncStates bool\n\n\t// BatchTicker is the ticker that determines the interval that we'll\n\t// use to check the batch to see if there're any updates we should\n\t// flush out. By batching updates into a single commit, we attempt to\n\t// increase throughput by maximizing the number of updates coalesced\n\t// into a single commit.\n\tBatchTicker ticker.Ticker\n\n\t// FwdPkgGCTicker is the ticker determining the frequency at which\n\t// garbage collection of forwarding packages occurs. We use a\n\t// time-based approach, as opposed to block epochs, as to not hinder\n\t// syncing.\n\tFwdPkgGCTicker ticker.Ticker\n\n\t// PendingCommitTicker is a ticker that allows the link to determine if\n\t// a locally initiated commitment dance gets stuck waiting for the\n\t// remote party to revoke.\n\tPendingCommitTicker ticker.Ticker\n\n\t// BatchSize is the max size of a batch of updates done to the link\n\t// before we do a state update.\n\tBatchSize uint32\n\n\t// UnsafeReplay will cause a link to replay the adds in its latest\n\t// commitment txn after the link is restarted. This should only be used\n\t// in testing, it is here to ensure the sphinx replay detection on the\n\t// receiving node is persistent.\n\tUnsafeReplay bool\n\n\t// MinFeeUpdateTimeout represents the minimum interval in which a link\n\t// will propose to update its commitment fee rate. A random timeout will\n\t// be selected between this and MaxFeeUpdateTimeout.\n\tMinFeeUpdateTimeout time.Duration\n\n\t// MaxFeeUpdateTimeout represents the maximum interval in which a link\n\t// will propose to update its commitment fee rate. A random timeout will\n\t// be selected between this and MinFeeUpdateTimeout.\n\tMaxFeeUpdateTimeout time.Duration\n\n\t// OutgoingCltvRejectDelta defines the number of blocks before expiry of\n\t// an htlc where we don't offer an htlc anymore. This should be at least\n\t// the outgoing broadcast delta, because in any case we don't want to\n\t// risk offering an htlc that triggers channel closure.\n\tOutgoingCltvRejectDelta uint32\n\n\t// TowerClient is an optional engine that manages the signing,\n\t// encrypting, and uploading of justice transactions to the daemon's\n\t// configured set of watchtowers for legacy channels.\n\tTowerClient TowerClient\n\n\t// MaxOutgoingCltvExpiry is the maximum outgoing timelock that the link\n\t// should accept for a forwarded HTLC. The value is relative to the\n\t// current block height.\n\tMaxOutgoingCltvExpiry uint32\n\n\t// MaxFeeAllocation is the highest allocation we'll allow a channel's\n\t// commitment fee to be of its balance. This only applies to the\n\t// initiator of the channel.\n\tMaxFeeAllocation float64\n\n\t// MaxAnchorsCommitFeeRate is the max commitment fee rate we'll use as\n\t// the initiator for channels of the anchor type.\n\tMaxAnchorsCommitFeeRate chainfee.SatPerKWeight\n\n\t// NotifyActiveLink allows the link to tell the ChannelNotifier when a\n\t// link is first started.\n\tNotifyActiveLink func(wire.OutPoint)\n\n\t// NotifyActiveChannel allows the link to tell the ChannelNotifier when\n\t// channels becomes active.\n\tNotifyActiveChannel func(wire.OutPoint)\n\n\t// NotifyInactiveChannel allows the switch to tell the ChannelNotifier\n\t// when channels become inactive.\n\tNotifyInactiveChannel func(wire.OutPoint)\n\n\t// NotifyInactiveLinkEvent allows the switch to tell the\n\t// ChannelNotifier when a channel link become inactive.\n\tNotifyInactiveLinkEvent func(wire.OutPoint)\n\n\t// HtlcNotifier is an instance of a htlcNotifier which we will pipe htlc\n\t// events through.\n\tHtlcNotifier htlcNotifier\n\n\t// FailAliasUpdate is a function used to fail an HTLC for an\n\t// option_scid_alias channel.\n\tFailAliasUpdate func(sid lnwire.ShortChannelID,\n\t\tincoming bool) *lnwire.ChannelUpdate\n\n\t// GetAliases is used by the link and switch to fetch the set of\n\t// aliases for a given link.\n\tGetAliases func(base lnwire.ShortChannelID) []lnwire.ShortChannelID\n}\n\n// shutdownReq contains an error channel that will be used by the channelLink\n// to send an error if shutdown failed. If shutdown succeeded, the channel will\n// be closed.",
      "length": 8039,
      "tokens": 1173,
      "embedding": []
    },
    {
      "slug": "type shutdownReq struct {",
      "content": "type shutdownReq struct {\n\terr chan error\n}\n\n// channelLink is the service which drives a channel's commitment update\n// state-machine. In the event that an HTLC needs to be propagated to another\n// link, the forward handler from config is used which sends HTLC to the\n// switch. Additionally, the link encapsulate logic of commitment protocol\n// message ordering and updates.",
      "length": 343,
      "tokens": 58,
      "embedding": []
    },
    {
      "slug": "type channelLink struct {",
      "content": "type channelLink struct {\n\t// The following fields are only meant to be used *atomically*\n\tstarted       int32\n\treestablished int32\n\tshutdown      int32\n\n\t// failed should be set to true in case a link error happens, making\n\t// sure we don't process any more updates.\n\tfailed bool\n\n\t// keystoneBatch represents a volatile list of keystones that must be\n\t// written before attempting to sign the next commitment txn. These\n\t// represent all the HTLC's forwarded to the link from the switch. Once\n\t// we lock them into our outgoing commitment, then the circuit has a\n\t// keystone, and is fully opened.\n\tkeystoneBatch []Keystone\n\n\t// openedCircuits is the set of all payment circuits that will be open\n\t// once we make our next commitment. After making the commitment we'll\n\t// ACK all these from our mailbox to ensure that they don't get\n\t// re-delivered if we reconnect.\n\topenedCircuits []CircuitKey\n\n\t// closedCircuits is the set of all payment circuits that will be\n\t// closed once we make our next commitment. After taking the commitment\n\t// we'll ACK all these to ensure that they don't get re-delivered if we\n\t// reconnect.\n\tclosedCircuits []CircuitKey\n\n\t// channel is a lightning network channel to which we apply htlc\n\t// updates.\n\tchannel *lnwallet.LightningChannel\n\n\t// shortChanID is the most up to date short channel ID for the link.\n\tshortChanID lnwire.ShortChannelID\n\n\t// cfg is a structure which carries all dependable fields/handlers\n\t// which may affect behaviour of the service.\n\tcfg ChannelLinkConfig\n\n\t// mailBox is the main interface between the outside world and the\n\t// link. All incoming messages will be sent over this mailBox. Messages\n\t// include new updates from our connected peer, and new packets to be\n\t// forwarded sent by the switch.\n\tmailBox MailBox\n\n\t// upstream is a channel that new messages sent from the remote peer to\n\t// the local peer will be sent across.\n\tupstream chan lnwire.Message\n\n\t// downstream is a channel in which new multi-hop HTLC's to be\n\t// forwarded will be sent across. Messages from this channel are sent\n\t// by the HTLC switch.\n\tdownstream chan *htlcPacket\n\n\t// shutdownRequest is a channel that the channelLink will listen on to\n\t// service shutdown requests from ShutdownIfChannelClean calls.\n\tshutdownRequest chan *shutdownReq\n\n\t// updateFeeTimer is the timer responsible for updating the link's\n\t// commitment fee every time it fires.\n\tupdateFeeTimer *time.Timer\n\n\t// uncommittedPreimages stores a list of all preimages that have been\n\t// learned since receiving the last CommitSig from the remote peer. The\n\t// batch will be flushed just before accepting the subsequent CommitSig\n\t// or on shutdown to avoid doing a write for each preimage received.\n\tuncommittedPreimages []lntypes.Preimage\n\n\tsync.RWMutex\n\n\t// hodlQueue is used to receive exit hop htlc resolutions from invoice\n\t// registry.\n\thodlQueue *queue.ConcurrentQueue\n\n\t// hodlMap stores related htlc data for a circuit key. It allows\n\t// resolving those htlcs when we receive a message on hodlQueue.\n\thodlMap map[models.CircuitKey]hodlHtlc\n\n\t// log is a link-specific logging instance.\n\tlog btclog.Logger\n\n\twg   sync.WaitGroup\n\tquit chan struct{}\n}\n\n// hodlHtlc contains htlc data that is required for resolution.",
      "length": 3125,
      "tokens": 495,
      "embedding": []
    },
    {
      "slug": "type hodlHtlc struct {",
      "content": "type hodlHtlc struct {\n\tpd         *lnwallet.PaymentDescriptor\n\tobfuscator hop.ErrorEncrypter\n}\n\n// NewChannelLink creates a new instance of a ChannelLink given a configuration\n// and active channel that will be used to verify/apply updates to.",
      "length": 216,
      "tokens": 29,
      "embedding": []
    },
    {
      "slug": "func NewChannelLink(cfg ChannelLinkConfig,",
      "content": "func NewChannelLink(cfg ChannelLinkConfig,\n\tchannel *lnwallet.LightningChannel) ChannelLink {\n\n\tlogPrefix := fmt.Sprintf(\"ChannelLink(%v):\", channel.ChannelPoint())\n\n\treturn &channelLink{\n\t\tcfg:             cfg,\n\t\tchannel:         channel,\n\t\tshortChanID:     channel.ShortChanID(),\n\t\tshutdownRequest: make(chan *shutdownReq),\n\t\thodlMap:         make(map[models.CircuitKey]hodlHtlc),\n\t\thodlQueue:       queue.NewConcurrentQueue(10),\n\t\tlog:             build.NewPrefixLog(logPrefix, log),\n\t\tquit:            make(chan struct{}),\n\t}\n}\n\n// A compile time check to ensure channelLink implements the ChannelLink\n// interface.\nvar _ ChannelLink = (*channelLink)(nil)\n\n// Start starts all helper goroutines required for the operation of the channel\n// link.\n//\n// NOTE: Part of the ChannelLink interface.",
      "length": 730,
      "tokens": 72,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) Start() error {",
      "content": "func (l *channelLink) Start() error {\n\tif !atomic.CompareAndSwapInt32(&l.started, 0, 1) {\n\t\terr := errors.Errorf(\"channel link(%v): already started\", l)\n\t\tl.log.Warn(\"already started\")\n\t\treturn err\n\t}\n\n\tl.log.Info(\"starting\")\n\n\t// If the config supplied watchtower client, ensure the channel is\n\t// registered before trying to use it during operation.\n\tif l.cfg.TowerClient != nil {\n\t\terr := l.cfg.TowerClient.RegisterChannel(l.ChanID())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tl.mailBox.ResetMessages()\n\tl.hodlQueue.Start()\n\n\t// Before launching the htlcManager messages, revert any circuits that\n\t// were marked open in the switch's circuit map, but did not make it\n\t// into a commitment txn. We use the next local htlc index as the cut\n\t// off point, since all indexes below that are committed. This action\n\t// is only performed if the link's final short channel ID has been\n\t// assigned, otherwise we would try to trim the htlcs belonging to the\n\t// all-zero, hop.Source ID.\n\tif l.ShortChanID() != hop.Source {\n\t\tlocalHtlcIndex, err := l.channel.NextLocalHtlcIndex()\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to retrieve next local \"+\n\t\t\t\t\"htlc index: %v\", err)\n\t\t}\n\n\t\t// NOTE: This is automatically done by the switch when it\n\t\t// starts up, but is necessary to prevent inconsistencies in\n\t\t// the case that the link flaps. This is a result of a link's\n\t\t// life-cycle being shorter than that of the switch.\n\t\tchanID := l.ShortChanID()\n\t\terr = l.cfg.Circuits.TrimOpenCircuits(chanID, localHtlcIndex)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to trim circuits above \"+\n\t\t\t\t\"local htlc index %d: %v\", localHtlcIndex, err)\n\t\t}\n\n\t\t// Since the link is live, before we start the link we'll update\n\t\t// the ChainArbitrator with the set of new channel signals for\n\t\t// this channel.\n\t\t//\n\t\t// TODO(roasbeef): split goroutines within channel arb to avoid\n\t\tgo func() {\n\t\t\tsignals := &contractcourt.ContractSignals{\n\t\t\t\tShortChanID: l.channel.ShortChanID(),\n\t\t\t}\n\n\t\t\terr := l.cfg.UpdateContractSignals(signals)\n\t\t\tif err != nil {\n\t\t\t\tl.log.Errorf(\"unable to update signals\")\n\t\t\t}\n\t\t}()\n\t}\n\n\tl.updateFeeTimer = time.NewTimer(l.randomFeeUpdateTimeout())\n\n\tl.wg.Add(1)\n\tgo l.htlcManager()\n\n\treturn nil\n}\n\n// Stop gracefully stops all active helper goroutines, then waits until they've\n// exited.\n//\n// NOTE: Part of the ChannelLink interface.",
      "length": 2235,
      "tokens": 327,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) Stop() {",
      "content": "func (l *channelLink) Stop() {\n\tif !atomic.CompareAndSwapInt32(&l.shutdown, 0, 1) {\n\t\tl.log.Warn(\"already stopped\")\n\t\treturn\n\t}\n\n\tl.log.Info(\"stopping\")\n\n\t// As the link is stopping, we are no longer interested in htlc\n\t// resolutions coming from the invoice registry.\n\tl.cfg.Registry.HodlUnsubscribeAll(l.hodlQueue.ChanIn())\n\n\tif l.cfg.ChainEvents.Cancel != nil {\n\t\tl.cfg.ChainEvents.Cancel()\n\t}\n\n\t// Ensure the channel for the timer is drained.\n\tif !l.updateFeeTimer.Stop() {\n\t\tselect {\n\t\tcase <-l.updateFeeTimer.C:\n\t\tdefault:\n\t\t}\n\t}\n\n\tl.hodlQueue.Stop()\n\n\tclose(l.quit)\n\tl.wg.Wait()\n\n\t// Now that the htlcManager has completely exited, reset the packet\n\t// courier. This allows the mailbox to revaluate any lingering Adds that\n\t// were delivered but didn't make it on a commitment to be failed back\n\t// if the link is offline for an extended period of time. The error is\n\t// ignored since it can only fail when the daemon is exiting.\n\t_ = l.mailBox.ResetPackets()\n\n\t// As a final precaution, we will attempt to flush any uncommitted\n\t// preimages to the preimage cache. The preimages should be re-delivered\n\t// after channel reestablishment, however this adds an extra layer of\n\t// protection in case the peer never returns. Without this, we will be\n\t// unable to settle any contracts depending on the preimages even though\n\t// we had learned them at some point.\n\terr := l.cfg.PreimageCache.AddPreimages(l.uncommittedPreimages...)\n\tif err != nil {\n\t\tl.log.Errorf(\"unable to add preimages=%v to cache: %v\",\n\t\t\tl.uncommittedPreimages, err)\n\t}\n}\n\n// WaitForShutdown blocks until the link finishes shutting down, which includes\n// termination of all dependent goroutines.",
      "length": 1590,
      "tokens": 230,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) WaitForShutdown() {",
      "content": "func (l *channelLink) WaitForShutdown() {\n\tl.wg.Wait()\n}\n\n// EligibleToForward returns a bool indicating if the channel is able to\n// actively accept requests to forward HTLC's. We're able to forward HTLC's if\n// we know the remote party's next revocation point. Otherwise, we can't\n// initiate new channel state. We also require that the short channel ID not be\n// the all-zero source ID, meaning that the channel has had its ID finalized.",
      "length": 391,
      "tokens": 68,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) EligibleToForward() bool {",
      "content": "func (l *channelLink) EligibleToForward() bool {\n\treturn l.channel.RemoteNextRevocation() != nil &&\n\t\tl.ShortChanID() != hop.Source &&\n\t\tl.isReestablished()\n}\n\n// isReestablished returns true if the link has successfully completed the\n// channel reestablishment dance.",
      "length": 213,
      "tokens": 26,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) isReestablished() bool {",
      "content": "func (l *channelLink) isReestablished() bool {\n\treturn atomic.LoadInt32(&l.reestablished) == 1\n}\n\n// markReestablished signals that the remote peer has successfully exchanged\n// channel reestablish messages and that the channel is ready to process\n// subsequent messages.",
      "length": 219,
      "tokens": 30,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) markReestablished() {",
      "content": "func (l *channelLink) markReestablished() {\n\tatomic.StoreInt32(&l.reestablished, 1)\n}\n\n// IsUnadvertised returns true if the underlying channel is unadvertised.",
      "length": 113,
      "tokens": 13,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) IsUnadvertised() bool {",
      "content": "func (l *channelLink) IsUnadvertised() bool {\n\tstate := l.channel.State()\n\treturn state.ChannelFlags&lnwire.FFAnnounceChannel == 0\n}\n\n// sampleNetworkFee samples the current fee rate on the network to get into the\n// chain in a timely manner. The returned value is expressed in fee-per-kw, as\n// this is the native rate used when computing the fee for commitment\n// transactions, and the second-level HTLC transactions.",
      "length": 366,
      "tokens": 56,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) sampleNetworkFee() (chainfee.SatPerKWeight, error) {",
      "content": "func (l *channelLink) sampleNetworkFee() (chainfee.SatPerKWeight, error) {\n\t// We'll first query for the sat/kw recommended to be confirmed within 3\n\t// blocks.\n\tfeePerKw, err := l.cfg.FeeEstimator.EstimateFeePerKW(3)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tl.log.Debugf(\"sampled fee rate for 3 block conf: %v sat/kw\",\n\t\tint64(feePerKw))\n\n\treturn feePerKw, nil\n}\n\n// shouldAdjustCommitFee returns true if we should update our commitment fee to\n// match that of the network fee. We'll only update our commitment fee if the\n// network fee is +/- 10% to our commitment fee or if our current commitment\n// fee is below the minimum relay fee.",
      "length": 544,
      "tokens": 92,
      "embedding": []
    },
    {
      "slug": "func shouldAdjustCommitFee(netFee, chanFee,",
      "content": "func shouldAdjustCommitFee(netFee, chanFee,\n\tminRelayFee chainfee.SatPerKWeight) bool {\n\n\tswitch {\n\t// If the network fee is greater than our current commitment fee and\n\t// our current commitment fee is below the minimum relay fee then\n\t// we should switch to it no matter if it is less than a 10% increase.\n\tcase netFee > chanFee && chanFee < minRelayFee:\n\t\treturn true\n\n\t// If the network fee is greater than the commitment fee, then we'll\n\t// switch to it if it's at least 10% greater than the commit fee.\n\tcase netFee > chanFee && netFee >= (chanFee+(chanFee*10)/100):\n\t\treturn true\n\n\t// If the network fee is less than our commitment fee, then we'll\n\t// switch to it if it's at least 10% less than the commitment fee.\n\tcase netFee < chanFee && netFee <= (chanFee-(chanFee*10)/100):\n\t\treturn true\n\n\t// Otherwise, we won't modify our fee.\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// failCb is used to cut down on the argument verbosity.",
      "length": 860,
      "tokens": 154,
      "embedding": []
    },
    {
      "slug": "type failCb func(update *lnwire.ChannelUpdate) lnwire.FailureMessage",
      "content": "type failCb func(update *lnwire.ChannelUpdate) lnwire.FailureMessage\n\n// createFailureWithUpdate creates a ChannelUpdate when failing an incoming or\n// outgoing HTLC. It may return a FailureMessage that references a channel's\n// alias. If the channel does not have an alias, then the regular channel\n// update from disk will be returned.",
      "length": 264,
      "tokens": 43,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) createFailureWithUpdate(incoming bool,",
      "content": "func (l *channelLink) createFailureWithUpdate(incoming bool,\n\toutgoingScid lnwire.ShortChannelID, cb failCb) lnwire.FailureMessage {\n\n\t// Determine which SCID to use in case we need to use aliases in the\n\t// ChannelUpdate.\n\tscid := outgoingScid\n\tif incoming {\n\t\tscid = l.ShortChanID()\n\t}\n\n\t// Try using the FailAliasUpdate function. If it returns nil, fallback\n\t// to the non-alias behavior.\n\tupdate := l.cfg.FailAliasUpdate(scid, incoming)\n\tif update == nil {\n\t\t// Fallback to the non-alias behavior.\n\t\tvar err error\n\t\tupdate, err = l.cfg.FetchLastChannelUpdate(l.ShortChanID())\n\t\tif err != nil {\n\t\t\treturn &lnwire.FailTemporaryNodeFailure{}\n\t\t}\n\t}\n\n\treturn cb(update)\n}\n\n// syncChanState attempts to synchronize channel states with the remote party.\n// This method is to be called upon reconnection after the initial funding\n// flow. We'll compare out commitment chains with the remote party, and re-send\n// either a danging commit signature, a revocation, or both.",
      "length": 879,
      "tokens": 130,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) syncChanStates() error {",
      "content": "func (l *channelLink) syncChanStates() error {\n\tl.log.Info(\"attempting to re-synchronize\")\n\n\t// First, we'll generate our ChanSync message to send to the other\n\t// side. Based on this message, the remote party will decide if they\n\t// need to retransmit any data or not.\n\tchanState := l.channel.State()\n\tlocalChanSyncMsg, err := chanState.ChanSyncMsg()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to generate chan sync message for \"+\n\t\t\t\"ChannelPoint(%v)\", l.channel.ChannelPoint())\n\t}\n\n\tif err := l.cfg.Peer.SendMessage(true, localChanSyncMsg); err != nil {\n\t\treturn fmt.Errorf(\"unable to send chan sync message for \"+\n\t\t\t\"ChannelPoint(%v): %v\", l.channel.ChannelPoint(), err)\n\t}\n\n\tvar msgsToReSend []lnwire.Message\n\n\t// Next, we'll wait indefinitely to receive the ChanSync message. The\n\t// first message sent MUST be the ChanSync message.\n\tselect {\n\tcase msg := <-l.upstream:\n\t\tremoteChanSyncMsg, ok := msg.(*lnwire.ChannelReestablish)\n\t\tif !ok {\n\t\t\treturn fmt.Errorf(\"first message sent to sync \"+\n\t\t\t\t\"should be ChannelReestablish, instead \"+\n\t\t\t\t\"received: %T\", msg)\n\t\t}\n\n\t\t// If the remote party indicates that they think we haven't\n\t\t// done any state updates yet, then we'll retransmit the\n\t\t// funding locked message first. We do this, as at this point\n\t\t// we can't be sure if they've really received the\n\t\t// FundingLocked message.\n\t\tif remoteChanSyncMsg.NextLocalCommitHeight == 1 &&\n\t\t\tlocalChanSyncMsg.NextLocalCommitHeight == 1 &&\n\t\t\t!l.channel.IsPending() {\n\n\t\t\tl.log.Infof(\"resending FundingLocked message to peer\")\n\n\t\t\tnextRevocation, err := l.channel.NextRevocationKey()\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"unable to create next \"+\n\t\t\t\t\t\"revocation: %v\", err)\n\t\t\t}\n\n\t\t\tfundingLockedMsg := lnwire.NewFundingLocked(\n\t\t\t\tl.ChanID(), nextRevocation,\n\t\t\t)\n\n\t\t\t// For channels that negotiated the option-scid-alias\n\t\t\t// feature bit, ensure that we send over the alias in\n\t\t\t// the funding_locked message. We'll send the first\n\t\t\t// alias we find for the channel since it does not\n\t\t\t// matter which alias we send. We'll error out if no\n\t\t\t// aliases are found.\n\t\t\tif l.negotiatedAliasFeature() {\n\t\t\t\taliases := l.getAliases()\n\t\t\t\tif len(aliases) == 0 {\n\t\t\t\t\t// This shouldn't happen since we\n\t\t\t\t\t// always add at least one alias before\n\t\t\t\t\t// the channel reaches the link.\n\t\t\t\t\treturn fmt.Errorf(\"no aliases found\")\n\t\t\t\t}\n\n\t\t\t\t// getAliases returns a copy of the alias slice\n\t\t\t\t// so it is ok to use a pointer to the first\n\t\t\t\t// entry.\n\t\t\t\tfundingLockedMsg.AliasScid = &aliases[0]\n\t\t\t}\n\n\t\t\terr = l.cfg.Peer.SendMessage(false, fundingLockedMsg)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"unable to re-send \"+\n\t\t\t\t\t\"FundingLocked: %v\", err)\n\t\t\t}\n\t\t}\n\n\t\t// In any case, we'll then process their ChanSync message.\n\t\tl.log.Info(\"received re-establishment message from remote side\")\n\n\t\tvar (\n\t\t\topenedCircuits []CircuitKey\n\t\t\tclosedCircuits []CircuitKey\n\t\t)\n\n\t\t// We've just received a ChanSync message from the remote\n\t\t// party, so we'll process the message  in order to determine\n\t\t// if we need to re-transmit any messages to the remote party.\n\t\tmsgsToReSend, openedCircuits, closedCircuits, err =\n\t\t\tl.channel.ProcessChanSyncMsg(remoteChanSyncMsg)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Repopulate any identifiers for circuits that may have been\n\t\t// opened or unclosed. This may happen if we needed to\n\t\t// retransmit a commitment signature message.\n\t\tl.openedCircuits = openedCircuits\n\t\tl.closedCircuits = closedCircuits\n\n\t\t// Ensure that all packets have been have been removed from the\n\t\t// link's mailbox.\n\t\tif err := l.ackDownStreamPackets(); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif len(msgsToReSend) > 0 {\n\t\t\tl.log.Infof(\"sending %v updates to synchronize the \"+\n\t\t\t\t\"state\", len(msgsToReSend))\n\t\t}\n\n\t\t// If we have any messages to retransmit, we'll do so\n\t\t// immediately so we return to a synchronized state as soon as\n\t\t// possible.\n\t\tfor _, msg := range msgsToReSend {\n\t\t\tl.cfg.Peer.SendMessage(false, msg)\n\t\t}\n\n\tcase <-l.quit:\n\t\treturn ErrLinkShuttingDown\n\t}\n\n\treturn nil\n}\n\n// resolveFwdPkgs loads any forwarding packages for this link from disk, and\n// reprocesses them in order. The primary goal is to make sure that any HTLCs\n// we previously received are reinstated in memory, and forwarded to the switch\n// if necessary. After a restart, this will also delete any previously\n// completed packages.",
      "length": 4160,
      "tokens": 598,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) resolveFwdPkgs() error {",
      "content": "func (l *channelLink) resolveFwdPkgs() error {\n\tfwdPkgs, err := l.channel.LoadFwdPkgs()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tl.log.Debugf(\"loaded %d fwd pks\", len(fwdPkgs))\n\n\tfor _, fwdPkg := range fwdPkgs {\n\t\tif err := l.resolveFwdPkg(fwdPkg); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// If any of our reprocessing steps require an update to the commitment\n\t// txn, we initiate a state transition to capture all relevant changes.\n\tif l.channel.PendingLocalUpdateCount() > 0 {\n\t\treturn l.updateCommitTx()\n\t}\n\n\treturn nil\n}\n\n// resolveFwdPkg interprets the FwdState of the provided package, either\n// reprocesses any outstanding htlcs in the package, or performs garbage\n// collection on the package.",
      "length": 621,
      "tokens": 98,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) resolveFwdPkg(fwdPkg *channeldb.FwdPkg) error {",
      "content": "func (l *channelLink) resolveFwdPkg(fwdPkg *channeldb.FwdPkg) error {\n\t// Remove any completed packages to clear up space.\n\tif fwdPkg.State == channeldb.FwdStateCompleted {\n\t\tl.log.Debugf(\"removing completed fwd pkg for height=%d\",\n\t\t\tfwdPkg.Height)\n\n\t\terr := l.channel.RemoveFwdPkgs(fwdPkg.Height)\n\t\tif err != nil {\n\t\t\tl.log.Errorf(\"unable to remove fwd pkg for height=%d: \"+\n\t\t\t\t\"%v\", fwdPkg.Height, err)\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Otherwise this is either a new package or one has gone through\n\t// processing, but contains htlcs that need to be restored in memory.\n\t// We replay this forwarding package to make sure our local mem state\n\t// is resurrected, we mimic any original responses back to the remote\n\t// party, and re-forward the relevant HTLCs to the switch.\n\n\t// If the package is fully acked but not completed, it must still have\n\t// settles and fails to propagate.\n\tif !fwdPkg.SettleFailFilter.IsFull() {\n\t\tsettleFails, err := lnwallet.PayDescsFromRemoteLogUpdates(\n\t\t\tfwdPkg.Source, fwdPkg.Height, fwdPkg.SettleFails,\n\t\t)\n\t\tif err != nil {\n\t\t\tl.log.Errorf(\"unable to process remote log updates: %v\",\n\t\t\t\terr)\n\t\t\treturn err\n\t\t}\n\t\tl.processRemoteSettleFails(fwdPkg, settleFails)\n\t}\n\n\t// Finally, replay *ALL ADDS* in this forwarding package. The\n\t// downstream logic is able to filter out any duplicates, but we must\n\t// shove the entire, original set of adds down the pipeline so that the\n\t// batch of adds presented to the sphinx router does not ever change.\n\tif !fwdPkg.AckFilter.IsFull() {\n\t\tadds, err := lnwallet.PayDescsFromRemoteLogUpdates(\n\t\t\tfwdPkg.Source, fwdPkg.Height, fwdPkg.Adds,\n\t\t)\n\t\tif err != nil {\n\t\t\tl.log.Errorf(\"unable to process remote log updates: %v\",\n\t\t\t\terr)\n\t\t\treturn err\n\t\t}\n\t\tl.processRemoteAdds(fwdPkg, adds)\n\n\t\t// If the link failed during processing the adds, we must\n\t\t// return to ensure we won't attempted to update the state\n\t\t// further.\n\t\tif l.failed {\n\t\t\treturn fmt.Errorf(\"link failed while \" +\n\t\t\t\t\"processing remote adds\")\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// fwdPkgGarbager periodically reads all forwarding packages from disk and\n// removes those that can be discarded. It is safe to do this entirely in the\n// background, since all state is coordinated on disk. This also ensures the\n// link can continue to process messages and interleave database accesses.\n//\n// NOTE: This MUST be run as a goroutine.",
      "length": 2214,
      "tokens": 334,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) fwdPkgGarbager() {",
      "content": "func (l *channelLink) fwdPkgGarbager() {\n\tdefer l.wg.Done()\n\n\tl.cfg.FwdPkgGCTicker.Resume()\n\tdefer l.cfg.FwdPkgGCTicker.Stop()\n\n\tif err := l.loadAndRemove(); err != nil {\n\t\tl.log.Warnf(\"unable to run initial fwd pkgs gc: %v\", err)\n\t}\n\n\tfor {\n\t\tselect {\n\t\tcase <-l.cfg.FwdPkgGCTicker.Ticks():\n\t\t\tif err := l.loadAndRemove(); err != nil {\n\t\t\t\tl.log.Warnf(\"unable to remove fwd pkgs: %v\",\n\t\t\t\t\terr)\n\t\t\t\tcontinue\n\t\t\t}\n\t\tcase <-l.quit:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// loadAndRemove loads all the channels forwarding packages and determines if\n// they can be removed. It is called once before the FwdPkgGCTicker ticks so that\n// a longer tick interval can be used.",
      "length": 583,
      "tokens": 86,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) loadAndRemove() error {",
      "content": "func (l *channelLink) loadAndRemove() error {\n\tfwdPkgs, err := l.channel.LoadFwdPkgs()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tvar removeHeights []uint64\n\tfor _, fwdPkg := range fwdPkgs {\n\t\tif fwdPkg.State != channeldb.FwdStateCompleted {\n\t\t\tcontinue\n\t\t}\n\n\t\tremoveHeights = append(removeHeights, fwdPkg.Height)\n\t}\n\n\t// If removeHeights is empty, return early so we don't use a db\n\t// transaction.\n\tif len(removeHeights) == 0 {\n\t\treturn nil\n\t}\n\n\treturn l.channel.RemoveFwdPkgs(removeHeights...)\n}\n\n// htlcManager is the primary goroutine which drives a channel's commitment\n// update state-machine in response to messages received via several channels.\n// This goroutine reads messages from the upstream (remote) peer, and also from\n// downstream channel managed by the channel link. In the event that an htlc\n// needs to be forwarded, then send-only forward handler is used which sends\n// htlc packets to the switch. Additionally, this goroutine handles acting upon\n// all timeouts for any active HTLCs, manages the channel's revocation window,\n// and also the htlc trickle queue+timer for this active channels.\n//\n// NOTE: This MUST be run as a goroutine.",
      "length": 1073,
      "tokens": 167,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) htlcManager() {",
      "content": "func (l *channelLink) htlcManager() {\n\tdefer func() {\n\t\tl.cfg.BatchTicker.Stop()\n\t\tl.wg.Done()\n\t\tl.log.Infof(\"exited\")\n\t}()\n\n\tl.log.Infof(\"HTLC manager started, bandwidth=%v\", l.Bandwidth())\n\n\t// Notify any clients that the link is now in the switch via an\n\t// ActiveLinkEvent. We'll also defer an inactive link notification for\n\t// when the link exits to ensure that every active notification is\n\t// matched by an inactive one.\n\tl.cfg.NotifyActiveLink(*l.ChannelPoint())\n\tdefer l.cfg.NotifyInactiveLinkEvent(*l.ChannelPoint())\n\n\t// TODO(roasbeef): need to call wipe chan whenever D/C?\n\n\t// If this isn't the first time that this channel link has been\n\t// created, then we'll need to check to see if we need to\n\t// re-synchronize state with the remote peer. settledHtlcs is a map of\n\t// HTLC's that we re-settled as part of the channel state sync.\n\tif l.cfg.SyncStates {\n\t\terr := l.syncChanStates()\n\t\tif err != nil {\n\t\t\tl.log.Warnf(\"error when syncing channel states: %v\", err)\n\n\t\t\terrDataLoss, localDataLoss :=\n\t\t\t\terr.(*lnwallet.ErrCommitSyncLocalDataLoss)\n\n\t\t\tswitch {\n\t\t\tcase err == ErrLinkShuttingDown:\n\t\t\t\tl.log.Debugf(\"unable to sync channel states, \" +\n\t\t\t\t\t\"link is shutting down\")\n\t\t\t\treturn\n\n\t\t\t// We failed syncing the commit chains, probably\n\t\t\t// because the remote has lost state. We should force\n\t\t\t// close the channel.\n\t\t\tcase err == lnwallet.ErrCommitSyncRemoteDataLoss:\n\t\t\t\tfallthrough\n\n\t\t\t// The remote sent us an invalid last commit secret, we\n\t\t\t// should force close the channel.\n\t\t\t// TODO(halseth): and permanently ban the peer?\n\t\t\tcase err == lnwallet.ErrInvalidLastCommitSecret:\n\t\t\t\tfallthrough\n\n\t\t\t// The remote sent us a commit point different from\n\t\t\t// what they sent us before.\n\t\t\t// TODO(halseth): ban peer?\n\t\t\tcase err == lnwallet.ErrInvalidLocalUnrevokedCommitPoint:\n\t\t\t\t// We'll fail the link and tell the peer to\n\t\t\t\t// force close the channel. Note that the\n\t\t\t\t// database state is not updated here, but will\n\t\t\t\t// be updated when the close transaction is\n\t\t\t\t// ready to avoid that we go down before\n\t\t\t\t// storing the transaction in the db.\n\t\t\t\tl.fail(\n\t\t\t\t\tLinkFailureError{\n\t\t\t\t\t\tcode:       ErrSyncError,\n\t\t\t\t\t\tForceClose: true,\n\t\t\t\t\t},\n\t\t\t\t\t\"unable to synchronize channel \"+\n\t\t\t\t\t\t\"states: %v\", err,\n\t\t\t\t)\n\t\t\t\treturn\n\n\t\t\t// We have lost state and cannot safely force close the\n\t\t\t// channel. Fail the channel and wait for the remote to\n\t\t\t// hopefully force close it. The remote has sent us its\n\t\t\t// latest unrevoked commitment point, and we'll store\n\t\t\t// it in the database, such that we can attempt to\n\t\t\t// recover the funds if the remote force closes the\n\t\t\t// channel.\n\t\t\tcase localDataLoss:\n\t\t\t\terr := l.channel.MarkDataLoss(\n\t\t\t\t\terrDataLoss.CommitPoint,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\tl.log.Errorf(\"unable to mark channel \"+\n\t\t\t\t\t\t\"data loss: %v\", err)\n\t\t\t\t}\n\n\t\t\t// We determined the commit chains were not possible to\n\t\t\t// sync. We cautiously fail the channel, but don't\n\t\t\t// force close.\n\t\t\t// TODO(halseth): can we safely force close in any\n\t\t\t// cases where this error is returned?\n\t\t\tcase err == lnwallet.ErrCannotSyncCommitChains:\n\t\t\t\tif err := l.channel.MarkBorked(); err != nil {\n\t\t\t\t\tl.log.Errorf(\"unable to mark channel \"+\n\t\t\t\t\t\t\"borked: %v\", err)\n\t\t\t\t}\n\n\t\t\t// Other, unspecified error.\n\t\t\tdefault:\n\t\t\t}\n\n\t\t\tl.fail(\n\t\t\t\tLinkFailureError{\n\t\t\t\t\tcode:       ErrRecoveryError,\n\t\t\t\t\tForceClose: false,\n\t\t\t\t},\n\t\t\t\t\"unable to synchronize channel \"+\n\t\t\t\t\t\"states: %v\", err,\n\t\t\t)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// We've successfully reestablished the channel, mark it as such to\n\t// allow the switch to forward HTLCs in the outbound direction.\n\tl.markReestablished()\n\n\t// Now that we've received both funding locked and channel reestablish,\n\t// we can go ahead and send the active channel notification. We'll also\n\t// defer the inactive notification for when the link exits to ensure\n\t// that every active notification is matched by an inactive one.\n\tl.cfg.NotifyActiveChannel(*l.ChannelPoint())\n\tdefer l.cfg.NotifyInactiveChannel(*l.ChannelPoint())\n\n\t// With the channel states synced, we now reset the mailbox to ensure\n\t// we start processing all unacked packets in order. This is done here\n\t// to ensure that all acknowledgments that occur during channel\n\t// resynchronization have taken affect, causing us only to pull unacked\n\t// packets after starting to read from the downstream mailbox.\n\tl.mailBox.ResetPackets()\n\n\t// After cleaning up any memory pertaining to incoming packets, we now\n\t// replay our forwarding packages to handle any htlcs that can be\n\t// processed locally, or need to be forwarded out to the switch. We will\n\t// only attempt to resolve packages if our short chan id indicates that\n\t// the channel is not pending, otherwise we should have no htlcs to\n\t// reforward.\n\tif l.ShortChanID() != hop.Source {\n\t\terr := l.resolveFwdPkgs()\n\t\tswitch err {\n\t\t// No error was encountered, success.\n\t\tcase nil:\n\n\t\t// If the duplicate keystone error was encountered, we'll fail\n\t\t// without sending an Error message to the peer.\n\t\tcase ErrDuplicateKeystone:\n\t\t\tl.fail(LinkFailureError{code: ErrCircuitError},\n\t\t\t\t\"temporary circuit error: %v\", err)\n\t\t\treturn\n\n\t\t// A non-nil error was encountered, send an Error message to\n\t\t// the peer.\n\t\tdefault:\n\t\t\tl.fail(LinkFailureError{code: ErrInternalError},\n\t\t\t\t\"unable to resolve fwd pkgs: %v\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// With our link's in-memory state fully reconstructed, spawn a\n\t\t// goroutine to manage the reclamation of disk space occupied by\n\t\t// completed forwarding packages.\n\t\tl.wg.Add(1)\n\t\tgo l.fwdPkgGarbager()\n\t}\n\n\tfor {\n\t\t// We must always check if we failed at some point processing\n\t\t// the last update before processing the next.\n\t\tif l.failed {\n\t\t\tl.log.Errorf(\"link failed, exiting htlcManager\")\n\t\t\treturn\n\t\t}\n\n\t\t// If the previous event resulted in a non-empty batch, resume\n\t\t// the batch ticker so that it can be cleared. Otherwise pause\n\t\t// the ticker to prevent waking up the htlcManager while the\n\t\t// batch is empty.\n\t\tif l.channel.PendingLocalUpdateCount() > 0 {\n\t\t\tl.cfg.BatchTicker.Resume()\n\t\t\tl.log.Tracef(\"BatchTicker resumed, \"+\n\t\t\t\t\"PendingLocalUpdateCount=%d\",\n\t\t\t\tl.channel.PendingLocalUpdateCount())\n\t\t} else {\n\t\t\tl.cfg.BatchTicker.Pause()\n\t\t\tl.log.Trace(\"BatchTicker paused due to zero \" +\n\t\t\t\t\"PendingLocalUpdateCount\")\n\t\t}\n\n\t\tselect {\n\t\t// Our update fee timer has fired, so we'll check the network\n\t\t// fee to see if we should adjust our commitment fee.\n\t\tcase <-l.updateFeeTimer.C:\n\t\t\tl.updateFeeTimer.Reset(l.randomFeeUpdateTimeout())\n\n\t\t\t// If we're not the initiator of the channel, don't we\n\t\t\t// don't control the fees, so we can ignore this.\n\t\t\tif !l.channel.IsInitiator() {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// If we are the initiator, then we'll sample the\n\t\t\t// current fee rate to get into the chain within 3\n\t\t\t// blocks.\n\t\t\tnetFee, err := l.sampleNetworkFee()\n\t\t\tif err != nil {\n\t\t\t\tl.log.Errorf(\"unable to sample network fee: %v\",\n\t\t\t\t\terr)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tminRelayFee := l.cfg.FeeEstimator.RelayFeePerKW()\n\n\t\t\tnewCommitFee := l.channel.IdealCommitFeeRate(\n\t\t\t\tnetFee, minRelayFee,\n\t\t\t\tl.cfg.MaxAnchorsCommitFeeRate,\n\t\t\t\tl.cfg.MaxFeeAllocation,\n\t\t\t)\n\n\t\t\t// We determine if we should adjust the commitment fee\n\t\t\t// based on the current commitment fee, the suggested\n\t\t\t// new commitment fee and the current minimum relay fee\n\t\t\t// rate.\n\t\t\tcommitFee := l.channel.CommitFeeRate()\n\t\t\tif !shouldAdjustCommitFee(\n\t\t\t\tnewCommitFee, commitFee, minRelayFee,\n\t\t\t) {\n\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// If we do, then we'll send a new UpdateFee message to\n\t\t\t// the remote party, to be locked in with a new update.\n\t\t\tif err := l.updateChannelFee(newCommitFee); err != nil {\n\t\t\t\tl.log.Errorf(\"unable to update fee rate: %v\",\n\t\t\t\t\terr)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t// The underlying channel has notified us of a unilateral close\n\t\t// carried out by the remote peer. In the case of such an\n\t\t// event, we'll wipe the channel state from the peer, and mark\n\t\t// the contract as fully settled. Afterwards we can exit.\n\t\t//\n\t\t// TODO(roasbeef): add force closure? also breach?\n\t\tcase <-l.cfg.ChainEvents.RemoteUnilateralClosure:\n\t\t\tl.log.Warnf(\"remote peer has closed on-chain\")\n\n\t\t\t// TODO(roasbeef): remove all together\n\t\t\tgo func() {\n\t\t\t\tchanPoint := l.channel.ChannelPoint()\n\t\t\t\tl.cfg.Peer.WipeChannel(chanPoint)\n\t\t\t}()\n\n\t\t\treturn\n\n\t\tcase <-l.cfg.BatchTicker.Ticks():\n\t\t\t// Attempt to extend the remote commitment chain\n\t\t\t// including all the currently pending entries. If the\n\t\t\t// send was unsuccessful, then abandon the update,\n\t\t\t// waiting for the revocation window to open up.\n\t\t\tif !l.updateCommitTxOrFail() {\n\t\t\t\treturn\n\t\t\t}\n\n\t\tcase <-l.cfg.PendingCommitTicker.Ticks():\n\t\t\tl.fail(LinkFailureError{code: ErrRemoteUnresponsive},\n\t\t\t\t\"unable to complete dance\")\n\t\t\treturn\n\n\t\t// A message from the switch was just received. This indicates\n\t\t// that the link is an intermediate hop in a multi-hop HTLC\n\t\t// circuit.\n\t\tcase pkt := <-l.downstream:\n\t\t\tl.handleDownstreamPkt(pkt)\n\n\t\t// A message from the connected peer was just received. This\n\t\t// indicates that we have a new incoming HTLC, either directly\n\t\t// for us, or part of a multi-hop HTLC circuit.\n\t\tcase msg := <-l.upstream:\n\t\t\tl.handleUpstreamMsg(msg)\n\n\t\t// A htlc resolution is received. This means that we now have a\n\t\t// resolution for a previously accepted htlc.\n\t\tcase hodlItem := <-l.hodlQueue.ChanOut():\n\t\t\thtlcResolution := hodlItem.(invoices.HtlcResolution)\n\t\t\terr := l.processHodlQueue(htlcResolution)\n\t\t\tswitch err {\n\t\t\t// No error, success.\n\t\t\tcase nil:\n\n\t\t\t// If the duplicate keystone error was encountered,\n\t\t\t// fail back gracefully.\n\t\t\tcase ErrDuplicateKeystone:\n\t\t\t\tl.fail(LinkFailureError{code: ErrCircuitError},\n\t\t\t\t\tfmt.Sprintf(\"process hodl queue: \"+\n\t\t\t\t\t\t\"temporary circuit error: %v\",\n\t\t\t\t\t\terr,\n\t\t\t\t\t),\n\t\t\t\t)\n\n\t\t\t// Send an Error message to the peer.\n\t\t\tdefault:\n\t\t\t\tl.fail(LinkFailureError{code: ErrInternalError},\n\t\t\t\t\tfmt.Sprintf(\"process hodl queue: \"+\n\t\t\t\t\t\t\"unable to update commitment:\"+\n\t\t\t\t\t\t\" %v\", err),\n\t\t\t\t)\n\t\t\t}\n\n\t\tcase req := <-l.shutdownRequest:\n\t\t\t// If the channel is clean, we send nil on the err chan\n\t\t\t// and return to prevent the htlcManager goroutine from\n\t\t\t// processing any more updates. The full link shutdown\n\t\t\t// will be triggered by RemoveLink in the peer.\n\t\t\tif l.channel.IsChannelClean() {\n\t\t\t\treq.err <- nil\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Otherwise, the channel has lingering updates, send\n\t\t\t// an error and continue.\n\t\t\treq.err <- ErrLinkFailedShutdown\n\n\t\tcase <-l.quit:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// processHodlQueue processes a received htlc resolution and continues reading\n// from the hodl queue until no more resolutions remain. When this function\n// returns without an error, the commit tx should be updated.",
      "length": 10281,
      "tokens": 1447,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) processHodlQueue(",
      "content": "func (l *channelLink) processHodlQueue(\n\tfirstResolution invoices.HtlcResolution) error {\n\n\t// Try to read all waiting resolution messages, so that they can all be\n\t// processed in a single commitment tx update.\n\thtlcResolution := firstResolution\nloop:\n\tfor {\n\t\t// Lookup all hodl htlcs that can be failed or settled with this event.\n\t\t// The hodl htlc must be present in the map.\n\t\tcircuitKey := htlcResolution.CircuitKey()\n\t\thodlHtlc, ok := l.hodlMap[circuitKey]\n\t\tif !ok {\n\t\t\treturn fmt.Errorf(\"hodl htlc not found: %v\", circuitKey)\n\t\t}\n\n\t\tif err := l.processHtlcResolution(htlcResolution, hodlHtlc); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Clean up hodl map.\n\t\tdelete(l.hodlMap, circuitKey)\n\n\t\tselect {\n\t\tcase item := <-l.hodlQueue.ChanOut():\n\t\t\thtlcResolution = item.(invoices.HtlcResolution)\n\t\tdefault:\n\t\t\tbreak loop\n\t\t}\n\t}\n\n\t// Update the commitment tx.\n\tif err := l.updateCommitTx(); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// processHtlcResolution applies a received htlc resolution to the provided\n// htlc. When this function returns without an error, the commit tx should be\n// updated.",
      "length": 1019,
      "tokens": 152,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) processHtlcResolution(resolution invoices.HtlcResolution,",
      "content": "func (l *channelLink) processHtlcResolution(resolution invoices.HtlcResolution,\n\thtlc hodlHtlc) error {\n\n\tcircuitKey := resolution.CircuitKey()\n\n\t// Determine required action for the resolution based on the type of\n\t// resolution we have received.\n\tswitch res := resolution.(type) {\n\t// Settle htlcs that returned a settle resolution using the preimage\n\t// in the resolution.\n\tcase *invoices.HtlcSettleResolution:\n\t\tl.log.Debugf(\"received settle resolution for %v \"+\n\t\t\t\"with outcome: %v\", circuitKey, res.Outcome)\n\n\t\treturn l.settleHTLC(res.Preimage, htlc.pd)\n\n\t// For htlc failures, we get the relevant failure message based\n\t// on the failure resolution and then fail the htlc.\n\tcase *invoices.HtlcFailResolution:\n\t\tl.log.Debugf(\"received cancel resolution for \"+\n\t\t\t\"%v with outcome: %v\", circuitKey, res.Outcome)\n\n\t\t// Get the lnwire failure message based on the resolution\n\t\t// result.\n\t\tfailure := getResolutionFailure(res, htlc.pd.Amount)\n\n\t\tl.sendHTLCError(\n\t\t\thtlc.pd, failure, htlc.obfuscator, true,\n\t\t)\n\t\treturn nil\n\n\t// Fail if we do not get a settle of fail resolution, since we\n\t// are only expecting to handle settles and fails.\n\tdefault:\n\t\treturn fmt.Errorf(\"unknown htlc resolution type: %T\",\n\t\t\tresolution)\n\t}\n}\n\n// getResolutionFailure returns the wire message that a htlc resolution should\n// be failed with.",
      "length": 1210,
      "tokens": 166,
      "embedding": []
    },
    {
      "slug": "func getResolutionFailure(resolution *invoices.HtlcFailResolution,",
      "content": "func getResolutionFailure(resolution *invoices.HtlcFailResolution,\n\tamount lnwire.MilliSatoshi) *LinkError {\n\n\t// If the resolution has been resolved as part of a MPP timeout,\n\t// we need to fail the htlc with lnwire.FailMppTimeout.\n\tif resolution.Outcome == invoices.ResultMppTimeout {\n\t\treturn NewDetailedLinkError(\n\t\t\t&lnwire.FailMPPTimeout{}, resolution.Outcome,\n\t\t)\n\t}\n\n\t// If the htlc is not a MPP timeout, we fail it with\n\t// FailIncorrectDetails. This error is sent for invoice payment\n\t// failures such as underpayment/ expiry too soon and hodl invoices\n\t// (which return FailIncorrectDetails to avoid leaking information).\n\tincorrectDetails := lnwire.NewFailIncorrectDetails(\n\t\tamount, uint32(resolution.AcceptHeight),\n\t)\n\n\treturn NewDetailedLinkError(incorrectDetails, resolution.Outcome)\n}\n\n// randomFeeUpdateTimeout returns a random timeout between the bounds defined\n// within the link's configuration that will be used to determine when the link\n// should propose an update to its commitment fee rate.",
      "length": 926,
      "tokens": 122,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) randomFeeUpdateTimeout() time.Duration {",
      "content": "func (l *channelLink) randomFeeUpdateTimeout() time.Duration {\n\tlower := int64(l.cfg.MinFeeUpdateTimeout)\n\tupper := int64(l.cfg.MaxFeeUpdateTimeout)\n\treturn time.Duration(prand.Int63n(upper-lower) + lower)\n}\n\n// handleDownstreamUpdateAdd processes an UpdateAddHTLC packet sent from the\n// downstream HTLC Switch.",
      "length": 243,
      "tokens": 24,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) handleDownstreamUpdateAdd(pkt *htlcPacket) error {",
      "content": "func (l *channelLink) handleDownstreamUpdateAdd(pkt *htlcPacket) error {\n\thtlc, ok := pkt.htlc.(*lnwire.UpdateAddHTLC)\n\tif !ok {\n\t\treturn errors.New(\"not an UpdateAddHTLC packet\")\n\t}\n\n\t// If hodl.AddOutgoing mode is active, we exit early to simulate\n\t// arbitrary delays between the switch adding an ADD to the\n\t// mailbox, and the HTLC being added to the commitment state.\n\tif l.cfg.HodlMask.Active(hodl.AddOutgoing) {\n\t\tl.log.Warnf(hodl.AddOutgoing.Warning())\n\t\tl.mailBox.AckPacket(pkt.inKey())\n\t\treturn nil\n\t}\n\n\t// A new payment has been initiated via the downstream channel,\n\t// so we add the new HTLC to our local log, then update the\n\t// commitment chains.\n\thtlc.ChanID = l.ChanID()\n\topenCircuitRef := pkt.inKey()\n\tindex, err := l.channel.AddHTLC(htlc, &openCircuitRef)\n\tif err != nil {\n\t\t// The HTLC was unable to be added to the state machine,\n\t\t// as a result, we'll signal the switch to cancel the\n\t\t// pending payment.\n\t\tl.log.Warnf(\"Unable to handle downstream add HTLC: %v\",\n\t\t\terr)\n\n\t\t// Remove this packet from the link's mailbox, this\n\t\t// prevents it from being reprocessed if the link\n\t\t// restarts and resets it mailbox. If this response\n\t\t// doesn't make it back to the originating link, it will\n\t\t// be rejected upon attempting to reforward the Add to\n\t\t// the switch, since the circuit was never fully opened,\n\t\t// and the forwarding package shows it as\n\t\t// unacknowledged.\n\t\tl.mailBox.FailAdd(pkt)\n\n\t\treturn NewDetailedLinkError(\n\t\t\tlnwire.NewTemporaryChannelFailure(nil),\n\t\t\tOutgoingFailureDownstreamHtlcAdd,\n\t\t)\n\t}\n\n\tl.log.Tracef(\"received downstream htlc: payment_hash=%x, \"+\n\t\t\"local_log_index=%v, pend_updates=%v\",\n\t\thtlc.PaymentHash[:], index,\n\t\tl.channel.PendingLocalUpdateCount())\n\n\tpkt.outgoingChanID = l.ShortChanID()\n\tpkt.outgoingHTLCID = index\n\thtlc.ID = index\n\n\tl.log.Debugf(\"queueing keystone of ADD open circuit: %s->%s\",\n\t\tpkt.inKey(), pkt.outKey())\n\n\tl.openedCircuits = append(l.openedCircuits, pkt.inKey())\n\tl.keystoneBatch = append(l.keystoneBatch, pkt.keystone())\n\n\t_ = l.cfg.Peer.SendMessage(false, htlc)\n\n\t// Send a forward event notification to htlcNotifier.\n\tl.cfg.HtlcNotifier.NotifyForwardingEvent(\n\t\tnewHtlcKey(pkt),\n\t\tHtlcInfo{\n\t\t\tIncomingTimeLock: pkt.incomingTimeout,\n\t\t\tIncomingAmt:      pkt.incomingAmount,\n\t\t\tOutgoingTimeLock: htlc.Expiry,\n\t\t\tOutgoingAmt:      htlc.Amount,\n\t\t},\n\t\tgetEventType(pkt),\n\t)\n\n\tl.tryBatchUpdateCommitTx()\n\n\treturn nil\n}\n\n// handleDownstreamPkt processes an HTLC packet sent from the downstream HTLC\n// Switch. Possible messages sent by the switch include requests to forward new\n// HTLCs, timeout previously cleared HTLCs, and finally to settle currently\n// cleared HTLCs with the upstream peer.\n//\n// TODO(roasbeef): add sync ntfn to ensure switch always has consistent view?",
      "length": 2605,
      "tokens": 328,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) handleDownstreamPkt(pkt *htlcPacket) {",
      "content": "func (l *channelLink) handleDownstreamPkt(pkt *htlcPacket) {\n\tswitch htlc := pkt.htlc.(type) {\n\tcase *lnwire.UpdateAddHTLC:\n\t\t// Handle add message. The returned error can be ignored,\n\t\t// because it is also sent through the mailbox.\n\t\t_ = l.handleDownstreamUpdateAdd(pkt)\n\n\tcase *lnwire.UpdateFulfillHTLC:\n\t\t// If hodl.SettleOutgoing mode is active, we exit early to\n\t\t// simulate arbitrary delays between the switch adding the\n\t\t// SETTLE to the mailbox, and the HTLC being added to the\n\t\t// commitment state.\n\t\tif l.cfg.HodlMask.Active(hodl.SettleOutgoing) {\n\t\t\tl.log.Warnf(hodl.SettleOutgoing.Warning())\n\t\t\tl.mailBox.AckPacket(pkt.inKey())\n\t\t\treturn\n\t\t}\n\n\t\t// An HTLC we forward to the switch has just settled somewhere\n\t\t// upstream. Therefore we settle the HTLC within the our local\n\t\t// state machine.\n\t\tinKey := pkt.inKey()\n\t\terr := l.channel.SettleHTLC(\n\t\t\thtlc.PaymentPreimage,\n\t\t\tpkt.incomingHTLCID,\n\t\t\tpkt.sourceRef,\n\t\t\tpkt.destRef,\n\t\t\t&inKey,\n\t\t)\n\t\tif err != nil {\n\t\t\tl.log.Errorf(\"unable to settle incoming HTLC for \"+\n\t\t\t\t\"circuit-key=%v: %v\", inKey, err)\n\n\t\t\t// If the HTLC index for Settle response was not known\n\t\t\t// to our commitment state, it has already been\n\t\t\t// cleaned up by a prior response. We'll thus try to\n\t\t\t// clean up any lingering state to ensure we don't\n\t\t\t// continue reforwarding.\n\t\t\tif _, ok := err.(lnwallet.ErrUnknownHtlcIndex); ok {\n\t\t\t\tl.cleanupSpuriousResponse(pkt)\n\t\t\t}\n\n\t\t\t// Remove the packet from the link's mailbox to ensure\n\t\t\t// it doesn't get replayed after a reconnection.\n\t\t\tl.mailBox.AckPacket(inKey)\n\n\t\t\treturn\n\t\t}\n\n\t\tl.log.Debugf(\"queueing removal of SETTLE closed circuit: \"+\n\t\t\t\"%s->%s\", pkt.inKey(), pkt.outKey())\n\n\t\tl.closedCircuits = append(l.closedCircuits, pkt.inKey())\n\n\t\t// With the HTLC settled, we'll need to populate the wire\n\t\t// message to target the specific channel and HTLC to be\n\t\t// canceled.\n\t\thtlc.ChanID = l.ChanID()\n\t\thtlc.ID = pkt.incomingHTLCID\n\n\t\t// Then we send the HTLC settle message to the connected peer\n\t\t// so we can continue the propagation of the settle message.\n\t\tl.cfg.Peer.SendMessage(false, htlc)\n\n\t\t// Send a settle event notification to htlcNotifier.\n\t\tl.cfg.HtlcNotifier.NotifySettleEvent(\n\t\t\tnewHtlcKey(pkt),\n\t\t\thtlc.PaymentPreimage,\n\t\t\tgetEventType(pkt),\n\t\t)\n\n\t\t// Immediately update the commitment tx to minimize latency.\n\t\tl.updateCommitTxOrFail()\n\n\tcase *lnwire.UpdateFailHTLC:\n\t\t// If hodl.FailOutgoing mode is active, we exit early to\n\t\t// simulate arbitrary delays between the switch adding a FAIL to\n\t\t// the mailbox, and the HTLC being added to the commitment\n\t\t// state.\n\t\tif l.cfg.HodlMask.Active(hodl.FailOutgoing) {\n\t\t\tl.log.Warnf(hodl.FailOutgoing.Warning())\n\t\t\tl.mailBox.AckPacket(pkt.inKey())\n\t\t\treturn\n\t\t}\n\n\t\t// An HTLC cancellation has been triggered somewhere upstream,\n\t\t// we'll remove then HTLC from our local state machine.\n\t\tinKey := pkt.inKey()\n\t\terr := l.channel.FailHTLC(\n\t\t\tpkt.incomingHTLCID,\n\t\t\thtlc.Reason,\n\t\t\tpkt.sourceRef,\n\t\t\tpkt.destRef,\n\t\t\t&inKey,\n\t\t)\n\t\tif err != nil {\n\t\t\tl.log.Errorf(\"unable to cancel incoming HTLC for \"+\n\t\t\t\t\"circuit-key=%v: %v\", inKey, err)\n\n\t\t\t// If the HTLC index for Fail response was not known to\n\t\t\t// our commitment state, it has already been cleaned up\n\t\t\t// by a prior response. We'll thus try to clean up any\n\t\t\t// lingering state to ensure we don't continue\n\t\t\t// reforwarding.\n\t\t\tif _, ok := err.(lnwallet.ErrUnknownHtlcIndex); ok {\n\t\t\t\tl.cleanupSpuriousResponse(pkt)\n\t\t\t}\n\n\t\t\t// Remove the packet from the link's mailbox to ensure\n\t\t\t// it doesn't get replayed after a reconnection.\n\t\t\tl.mailBox.AckPacket(inKey)\n\n\t\t\treturn\n\t\t}\n\n\t\tl.log.Debugf(\"queueing removal of FAIL closed circuit: %s->%s\",\n\t\t\tpkt.inKey(), pkt.outKey())\n\n\t\tl.closedCircuits = append(l.closedCircuits, pkt.inKey())\n\n\t\t// With the HTLC removed, we'll need to populate the wire\n\t\t// message to target the specific channel and HTLC to be\n\t\t// canceled. The \"Reason\" field will have already been set\n\t\t// within the switch.\n\t\thtlc.ChanID = l.ChanID()\n\t\thtlc.ID = pkt.incomingHTLCID\n\n\t\t// We send the HTLC message to the peer which initially created\n\t\t// the HTLC.\n\t\tl.cfg.Peer.SendMessage(false, htlc)\n\n\t\t// If the packet does not have a link failure set, it failed\n\t\t// further down the route so we notify a forwarding failure.\n\t\t// Otherwise, we notify a link failure because it failed at our\n\t\t// node.\n\t\tif pkt.linkFailure != nil {\n\t\t\tl.cfg.HtlcNotifier.NotifyLinkFailEvent(\n\t\t\t\tnewHtlcKey(pkt),\n\t\t\t\tnewHtlcInfo(pkt),\n\t\t\t\tgetEventType(pkt),\n\t\t\t\tpkt.linkFailure,\n\t\t\t\tfalse,\n\t\t\t)\n\t\t} else {\n\t\t\tl.cfg.HtlcNotifier.NotifyForwardingFailEvent(\n\t\t\t\tnewHtlcKey(pkt), getEventType(pkt),\n\t\t\t)\n\t\t}\n\n\t\t// Immediately update the commitment tx to minimize latency.\n\t\tl.updateCommitTxOrFail()\n\t}\n}\n\n// tryBatchUpdateCommitTx updates the commitment transaction if the batch is\n// full.",
      "length": 4594,
      "tokens": 610,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) tryBatchUpdateCommitTx() {",
      "content": "func (l *channelLink) tryBatchUpdateCommitTx() {\n\tif l.channel.PendingLocalUpdateCount() < uint64(l.cfg.BatchSize) {\n\t\treturn\n\t}\n\n\tl.updateCommitTxOrFail()\n}\n\n// cleanupSpuriousResponse attempts to ack any AddRef or SettleFailRef\n// associated with this packet. If successful in doing so, it will also purge\n// the open circuit from the circuit map and remove the packet from the link's\n// mailbox.",
      "length": 339,
      "tokens": 49,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) cleanupSpuriousResponse(pkt *htlcPacket) {",
      "content": "func (l *channelLink) cleanupSpuriousResponse(pkt *htlcPacket) {\n\tinKey := pkt.inKey()\n\n\tl.log.Debugf(\"cleaning up spurious response for incoming \"+\n\t\t\"circuit-key=%v\", inKey)\n\n\t// If the htlc packet doesn't have a source reference, it is unsafe to\n\t// proceed, as skipping this ack may cause the htlc to be reforwarded.\n\tif pkt.sourceRef == nil {\n\t\tl.log.Errorf(\"unable to cleanup response for incoming \"+\n\t\t\t\"circuit-key=%v, does not contain source reference\",\n\t\t\tinKey)\n\t\treturn\n\t}\n\n\t// If the source reference is present,  we will try to prevent this link\n\t// from resending the packet to the switch. To do so, we ack the AddRef\n\t// of the incoming HTLC belonging to this link.\n\terr := l.channel.AckAddHtlcs(*pkt.sourceRef)\n\tif err != nil {\n\t\tl.log.Errorf(\"unable to ack AddRef for incoming \"+\n\t\t\t\"circuit-key=%v: %v\", inKey, err)\n\n\t\t// If this operation failed, it is unsafe to attempt removal of\n\t\t// the destination reference or circuit, so we exit early. The\n\t\t// cleanup may proceed with a different packet in the future\n\t\t// that succeeds on this step.\n\t\treturn\n\t}\n\n\t// Now that we know this link will stop retransmitting Adds to the\n\t// switch, we can begin to teardown the response reference and circuit\n\t// map.\n\t//\n\t// If the packet includes a destination reference, then a response for\n\t// this HTLC was locked into the outgoing channel. Attempt to remove\n\t// this reference, so we stop retransmitting the response internally.\n\t// Even if this fails, we will proceed in trying to delete the circuit.\n\t// When retransmitting responses, the destination references will be\n\t// cleaned up if an open circuit is not found in the circuit map.\n\tif pkt.destRef != nil {\n\t\terr := l.channel.AckSettleFails(*pkt.destRef)\n\t\tif err != nil {\n\t\t\tl.log.Errorf(\"unable to ack SettleFailRef \"+\n\t\t\t\t\"for incoming circuit-key=%v: %v\",\n\t\t\t\tinKey, err)\n\t\t}\n\t}\n\n\tl.log.Debugf(\"deleting circuit for incoming circuit-key=%x\", inKey)\n\n\t// With all known references acked, we can now safely delete the circuit\n\t// from the switch's circuit map, as the state is no longer needed.\n\terr = l.cfg.Circuits.DeleteCircuits(inKey)\n\tif err != nil {\n\t\tl.log.Errorf(\"unable to delete circuit for \"+\n\t\t\t\"circuit-key=%v: %v\", inKey, err)\n\t}\n}\n\n// handleUpstreamMsg processes wire messages related to commitment state\n// updates from the upstream peer. The upstream peer is the peer whom we have a\n// direct channel with, updating our respective commitment chains.",
      "length": 2312,
      "tokens": 370,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) handleUpstreamMsg(msg lnwire.Message) {",
      "content": "func (l *channelLink) handleUpstreamMsg(msg lnwire.Message) {\n\tswitch msg := msg.(type) {\n\n\tcase *lnwire.UpdateAddHTLC:\n\t\t// We just received an add request from an upstream peer, so we\n\t\t// add it to our state machine, then add the HTLC to our\n\t\t// \"settle\" list in the event that we know the preimage.\n\t\tindex, err := l.channel.ReceiveHTLC(msg)\n\t\tif err != nil {\n\t\t\tl.fail(LinkFailureError{code: ErrInvalidUpdate},\n\t\t\t\t\"unable to handle upstream add HTLC: %v\", err)\n\t\t\treturn\n\t\t}\n\n\t\tl.log.Tracef(\"receive upstream htlc with payment hash(%x), \"+\n\t\t\t\"assigning index: %v\", msg.PaymentHash[:], index)\n\n\tcase *lnwire.UpdateFulfillHTLC:\n\t\tpre := msg.PaymentPreimage\n\t\tidx := msg.ID\n\n\t\t// Before we pipeline the settle, we'll check the set of active\n\t\t// htlc's to see if the related UpdateAddHTLC has been fully\n\t\t// locked-in.\n\t\tvar lockedin bool\n\t\thtlcs := l.channel.ActiveHtlcs()\n\t\tfor _, add := range htlcs {\n\t\t\t// The HTLC will be outgoing and match idx.\n\t\t\tif !add.Incoming && add.HtlcIndex == idx {\n\t\t\t\tlockedin = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\tif !lockedin {\n\t\t\tl.fail(\n\t\t\t\tLinkFailureError{code: ErrInvalidUpdate},\n\t\t\t\t\"unable to handle upstream settle\",\n\t\t\t)\n\t\t\treturn\n\t\t}\n\n\t\tif err := l.channel.ReceiveHTLCSettle(pre, idx); err != nil {\n\t\t\tl.fail(\n\t\t\t\tLinkFailureError{\n\t\t\t\t\tcode:       ErrInvalidUpdate,\n\t\t\t\t\tForceClose: true,\n\t\t\t\t},\n\t\t\t\t\"unable to handle upstream settle HTLC: %v\", err,\n\t\t\t)\n\t\t\treturn\n\t\t}\n\n\t\tsettlePacket := &htlcPacket{\n\t\t\toutgoingChanID: l.ShortChanID(),\n\t\t\toutgoingHTLCID: idx,\n\t\t\thtlc: &lnwire.UpdateFulfillHTLC{\n\t\t\t\tPaymentPreimage: pre,\n\t\t\t},\n\t\t}\n\n\t\t// Add the newly discovered preimage to our growing list of\n\t\t// uncommitted preimage. These will be written to the witness\n\t\t// cache just before accepting the next commitment signature\n\t\t// from the remote peer.\n\t\tl.uncommittedPreimages = append(l.uncommittedPreimages, pre)\n\n\t\t// Pipeline this settle, send it to the switch.\n\t\tgo l.forwardBatch(false, settlePacket)\n\n\tcase *lnwire.UpdateFailMalformedHTLC:\n\t\t// Convert the failure type encoded within the HTLC fail\n\t\t// message to the proper generic lnwire error code.\n\t\tvar failure lnwire.FailureMessage\n\t\tswitch msg.FailureCode {\n\t\tcase lnwire.CodeInvalidOnionVersion:\n\t\t\tfailure = &lnwire.FailInvalidOnionVersion{\n\t\t\t\tOnionSHA256: msg.ShaOnionBlob,\n\t\t\t}\n\t\tcase lnwire.CodeInvalidOnionHmac:\n\t\t\tfailure = &lnwire.FailInvalidOnionHmac{\n\t\t\t\tOnionSHA256: msg.ShaOnionBlob,\n\t\t\t}\n\n\t\tcase lnwire.CodeInvalidOnionKey:\n\t\t\tfailure = &lnwire.FailInvalidOnionKey{\n\t\t\t\tOnionSHA256: msg.ShaOnionBlob,\n\t\t\t}\n\t\tdefault:\n\t\t\tl.log.Warnf(\"unexpected failure code received in \"+\n\t\t\t\t\"UpdateFailMailformedHTLC: %v\", msg.FailureCode)\n\n\t\t\t// We don't just pass back the error we received from\n\t\t\t// our successor. Otherwise we might report a failure\n\t\t\t// that penalizes us more than needed. If the onion that\n\t\t\t// we forwarded was correct, the node should have been\n\t\t\t// able to send back its own failure. The node did not\n\t\t\t// send back its own failure, so we assume there was a\n\t\t\t// problem with the onion and report that back. We reuse\n\t\t\t// the invalid onion key failure because there is no\n\t\t\t// specific error for this case.\n\t\t\tfailure = &lnwire.FailInvalidOnionKey{\n\t\t\t\tOnionSHA256: msg.ShaOnionBlob,\n\t\t\t}\n\t\t}\n\n\t\t// With the error parsed, we'll convert the into it's opaque\n\t\t// form.\n\t\tvar b bytes.Buffer\n\t\tif err := lnwire.EncodeFailure(&b, failure, 0); err != nil {\n\t\t\tl.log.Errorf(\"unable to encode malformed error: %v\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// If remote side have been unable to parse the onion blob we\n\t\t// have sent to it, than we should transform the malformed HTLC\n\t\t// message to the usual HTLC fail message.\n\t\terr := l.channel.ReceiveFailHTLC(msg.ID, b.Bytes())\n\t\tif err != nil {\n\t\t\tl.fail(LinkFailureError{code: ErrInvalidUpdate},\n\t\t\t\t\"unable to handle upstream fail HTLC: %v\", err)\n\t\t\treturn\n\t\t}\n\n\tcase *lnwire.UpdateFailHTLC:\n\t\t// Verify that the failure reason is at least 256 bytes plus\n\t\t// overhead.\n\t\tconst minimumFailReasonLength = lnwire.FailureMessageLength +\n\t\t\t2 + 2 + 32\n\n\t\tif len(msg.Reason) < minimumFailReasonLength {\n\t\t\t// We've received a reason with a non-compliant length.\n\t\t\t// Older nodes happily relay back these failures that\n\t\t\t// may originate from a node further downstream.\n\t\t\t// Therefore we can't just fail the channel.\n\t\t\t//\n\t\t\t// We want to be compliant ourselves, so we also can't\n\t\t\t// pass back the reason unmodified. And we must make\n\t\t\t// sure that we don't hit the magic length check of 260\n\t\t\t// bytes in processRemoteSettleFails either.\n\t\t\t//\n\t\t\t// Because the reason is unreadable for the payer\n\t\t\t// anyway, we just replace it by a compliant-length\n\t\t\t// series of random bytes.\n\t\t\tmsg.Reason = make([]byte, minimumFailReasonLength)\n\t\t\t_, err := crand.Read(msg.Reason[:])\n\t\t\tif err != nil {\n\t\t\t\tl.log.Errorf(\"Random generation error: %v\", err)\n\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\t// Add fail to the update log.\n\t\tidx := msg.ID\n\t\terr := l.channel.ReceiveFailHTLC(idx, msg.Reason[:])\n\t\tif err != nil {\n\t\t\tl.fail(LinkFailureError{code: ErrInvalidUpdate},\n\t\t\t\t\"unable to handle upstream fail HTLC: %v\", err)\n\t\t\treturn\n\t\t}\n\n\tcase *lnwire.CommitSig:\n\t\t// Since we may have learned new preimages for the first time,\n\t\t// we'll add them to our preimage cache. By doing this, we\n\t\t// ensure any contested contracts watched by any on-chain\n\t\t// arbitrators can now sweep this HTLC on-chain. We delay\n\t\t// committing the preimages until just before accepting the new\n\t\t// remote commitment, as afterwards the peer won't resend the\n\t\t// Settle messages on the next channel reestablishment. Doing so\n\t\t// allows us to more effectively batch this operation, instead\n\t\t// of doing a single write per preimage.\n\t\terr := l.cfg.PreimageCache.AddPreimages(\n\t\t\tl.uncommittedPreimages...,\n\t\t)\n\t\tif err != nil {\n\t\t\tl.fail(\n\t\t\t\tLinkFailureError{code: ErrInternalError},\n\t\t\t\t\"unable to add preimages=%v to cache: %v\",\n\t\t\t\tl.uncommittedPreimages, err,\n\t\t\t)\n\t\t\treturn\n\t\t}\n\n\t\t// Instead of truncating the slice to conserve memory\n\t\t// allocations, we simply set the uncommitted preimage slice to\n\t\t// nil so that a new one will be initialized if any more\n\t\t// witnesses are discovered. We do this maximum size of the\n\t\t// slice can occupy 15KB, and want to ensure we release that\n\t\t// memory back to the runtime.\n\t\tl.uncommittedPreimages = nil\n\n\t\t// We just received a new updates to our local commitment\n\t\t// chain, validate this new commitment, closing the link if\n\t\t// invalid.\n\t\terr = l.channel.ReceiveNewCommitment(msg.CommitSig, msg.HtlcSigs)\n\t\tif err != nil {\n\t\t\t// If we were unable to reconstruct their proposed\n\t\t\t// commitment, then we'll examine the type of error. If\n\t\t\t// it's an InvalidCommitSigError, then we'll send a\n\t\t\t// direct error.\n\t\t\tvar sendData []byte\n\t\t\tswitch err.(type) {\n\t\t\tcase *lnwallet.InvalidCommitSigError:\n\t\t\t\tsendData = []byte(err.Error())\n\t\t\tcase *lnwallet.InvalidHtlcSigError:\n\t\t\t\tsendData = []byte(err.Error())\n\t\t\t}\n\t\t\tl.fail(\n\t\t\t\tLinkFailureError{\n\t\t\t\t\tcode:       ErrInvalidCommitment,\n\t\t\t\t\tForceClose: true,\n\t\t\t\t\tSendData:   sendData,\n\t\t\t\t},\n\t\t\t\t\"ChannelPoint(%v): unable to accept new \"+\n\t\t\t\t\t\"commitment: %v\",\n\t\t\t\tl.channel.ChannelPoint(), err,\n\t\t\t)\n\t\t\treturn\n\t\t}\n\n\t\t// As we've just accepted a new state, we'll now\n\t\t// immediately send the remote peer a revocation for our prior\n\t\t// state.\n\t\tnextRevocation, currentHtlcs, finalHTLCs, err :=\n\t\t\tl.channel.RevokeCurrentCommitment()\n\t\tif err != nil {\n\t\t\tl.log.Errorf(\"unable to revoke commitment: %v\", err)\n\t\t\treturn\n\t\t}\n\t\tl.cfg.Peer.SendMessage(false, nextRevocation)\n\n\t\t// Notify the incoming htlcs of which the resolutions were\n\t\t// locked in.\n\t\tfor id, settled := range finalHTLCs {\n\t\t\tl.cfg.HtlcNotifier.NotifyFinalHtlcEvent(\n\t\t\t\tmodels.CircuitKey{\n\t\t\t\t\tChanID: l.shortChanID,\n\t\t\t\t\tHtlcID: id,\n\t\t\t\t},\n\t\t\t\tchanneldb.FinalHtlcInfo{\n\t\t\t\t\tSettled:  settled,\n\t\t\t\t\tOffchain: true,\n\t\t\t\t},\n\t\t\t)\n\t\t}\n\n\t\t// Since we just revoked our commitment, we may have a new set\n\t\t// of HTLC's on our commitment, so we'll send them using our\n\t\t// function closure NotifyContractUpdate.\n\t\tnewUpdate := &contractcourt.ContractUpdate{\n\t\t\tHtlcKey: contractcourt.LocalHtlcSet,\n\t\t\tHtlcs:   currentHtlcs,\n\t\t}\n\t\terr = l.cfg.NotifyContractUpdate(newUpdate)\n\t\tif err != nil {\n\t\t\tl.log.Errorf(\"unable to notify contract update: %v\",\n\t\t\t\terr)\n\t\t\treturn\n\t\t}\n\n\t\tselect {\n\t\tcase <-l.quit:\n\t\t\treturn\n\t\tdefault:\n\t\t}\n\n\t\t// If both commitment chains are fully synced from our PoV,\n\t\t// then we don't need to reply with a signature as both sides\n\t\t// already have a commitment with the latest accepted.\n\t\tif !l.channel.OweCommitment() {\n\t\t\treturn\n\t\t}\n\n\t\t// Otherwise, the remote party initiated the state transition,\n\t\t// so we'll reply with a signature to provide them with their\n\t\t// version of the latest commitment.\n\t\tif !l.updateCommitTxOrFail() {\n\t\t\treturn\n\t\t}\n\n\tcase *lnwire.RevokeAndAck:\n\t\t// We've received a revocation from the remote chain, if valid,\n\t\t// this moves the remote chain forward, and expands our\n\t\t// revocation window.\n\t\t//\n\t\t// Before advancing our remote chain, we will record the\n\t\t// current commit tx, which is used by the TowerClient to\n\t\t// create backups.\n\t\toldCommitTx := l.channel.State().RemoteCommitment.CommitTx\n\n\t\t// We now process the message and advance our remote commit\n\t\t// chain.\n\t\tfwdPkg, adds, settleFails, remoteHTLCs, err := l.channel.\n\t\t\tReceiveRevocation(msg)\n\t\tif err != nil {\n\t\t\t// TODO(halseth): force close?\n\t\t\tl.fail(LinkFailureError{code: ErrInvalidRevocation},\n\t\t\t\t\"unable to accept revocation: %v\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// The remote party now has a new primary commitment, so we'll\n\t\t// update the contract court to be aware of this new set (the\n\t\t// prior old remote pending).\n\t\tnewUpdate := &contractcourt.ContractUpdate{\n\t\t\tHtlcKey: contractcourt.RemoteHtlcSet,\n\t\t\tHtlcs:   remoteHTLCs,\n\t\t}\n\t\terr = l.cfg.NotifyContractUpdate(newUpdate)\n\t\tif err != nil {\n\t\t\tl.log.Errorf(\"unable to notify contract update: %v\",\n\t\t\t\terr)\n\t\t\treturn\n\t\t}\n\n\t\tselect {\n\t\tcase <-l.quit:\n\t\t\treturn\n\t\tdefault:\n\t\t}\n\n\t\t// If we have a tower client for this channel type, we'll\n\t\t// create a backup for the current state.\n\t\tif l.cfg.TowerClient != nil {\n\t\t\tstate := l.channel.State()\n\t\t\tbreachInfo, err := lnwallet.NewBreachRetribution(\n\t\t\t\tstate, state.RemoteCommitment.CommitHeight-1, 0,\n\t\t\t\t// OldCommitTx is the breaching tx at height-1.\n\t\t\t\toldCommitTx,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tl.fail(LinkFailureError{code: ErrInternalError},\n\t\t\t\t\t\"failed to load breach info: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tchanID := l.ChanID()\n\t\t\terr = l.cfg.TowerClient.BackupState(\n\t\t\t\t&chanID, breachInfo, state.ChanType,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tl.fail(LinkFailureError{code: ErrInternalError},\n\t\t\t\t\t\"unable to queue breach backup: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\tl.processRemoteSettleFails(fwdPkg, settleFails)\n\t\tl.processRemoteAdds(fwdPkg, adds)\n\n\t\t// If the link failed during processing the adds, we must\n\t\t// return to ensure we won't attempted to update the state\n\t\t// further.\n\t\tif l.failed {\n\t\t\treturn\n\t\t}\n\n\t\t// The revocation window opened up. If there are pending local\n\t\t// updates, try to update the commit tx. Pending updates could\n\t\t// already have been present because of a previously failed\n\t\t// update to the commit tx or freshly added in by\n\t\t// processRemoteAdds. Also in case there are no local updates,\n\t\t// but there are still remote updates that are not in the remote\n\t\t// commit tx yet, send out an update.\n\t\tif l.channel.OweCommitment() {\n\t\t\tif !l.updateCommitTxOrFail() {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\tcase *lnwire.UpdateFee:\n\t\t// We received fee update from peer. If we are the initiator we\n\t\t// will fail the channel, if not we will apply the update.\n\t\tfee := chainfee.SatPerKWeight(msg.FeePerKw)\n\t\tif err := l.channel.ReceiveUpdateFee(fee); err != nil {\n\t\t\tl.fail(LinkFailureError{code: ErrInvalidUpdate},\n\t\t\t\t\"error receiving fee update: %v\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Update the mailbox's feerate as well.\n\t\tl.mailBox.SetFeeRate(fee)\n\n\t// In the case where we receive a warning message from our peer, just\n\t// log it and move on. We choose not to disconnect from our peer,\n\t// although we \"MAY\" do so according to the specification.\n\tcase *lnwire.Warning:\n\t\tl.log.Warnf(\"received warning message from peer: %v\",\n\t\t\tmsg.Warning())\n\n\tcase *lnwire.Error:\n\t\t// Error received from remote, MUST fail channel, but should\n\t\t// only print the contents of the error message if all\n\t\t// characters are printable ASCII.\n\t\tl.fail(\n\t\t\tLinkFailureError{\n\t\t\t\tcode: ErrRemoteError,\n\n\t\t\t\t// TODO(halseth): we currently don't fail the\n\t\t\t\t// channel permanently, as there are some sync\n\t\t\t\t// issues with other implementations that will\n\t\t\t\t// lead to them sending an error message, but\n\t\t\t\t// we can recover from on next connection. See\n\t\t\t\t// https://github.com/ElementsProject/lightning/issues/4212\n\t\t\t\tPermanentFailure: false,\n\t\t\t},\n\t\t\t\"ChannelPoint(%v): received error from peer: %v\",\n\t\t\tl.channel.ChannelPoint(), msg.Error(),\n\t\t)\n\tdefault:\n\t\tl.log.Warnf(\"received unknown message of type %T\", msg)\n\t}\n\n}\n\n// ackDownStreamPackets is responsible for removing htlcs from a link's mailbox\n// for packets delivered from server, and cleaning up any circuits closed by\n// signing a previous commitment txn. This method ensures that the circuits are\n// removed from the circuit map before removing them from the link's mailbox,\n// otherwise it could be possible for some circuit to be missed if this link\n// flaps.",
      "length": 12850,
      "tokens": 1798,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) ackDownStreamPackets() error {",
      "content": "func (l *channelLink) ackDownStreamPackets() error {\n\t// First, remove the downstream Add packets that were included in the\n\t// previous commitment signature. This will prevent the Adds from being\n\t// replayed if this link disconnects.\n\tfor _, inKey := range l.openedCircuits {\n\t\t// In order to test the sphinx replay logic of the remote\n\t\t// party, unsafe replay does not acknowledge the packets from\n\t\t// the mailbox. We can then force a replay of any Add packets\n\t\t// held in memory by disconnecting and reconnecting the link.\n\t\tif l.cfg.UnsafeReplay {\n\t\t\tcontinue\n\t\t}\n\n\t\tl.log.Debugf(\"removing Add packet %s from mailbox\", inKey)\n\t\tl.mailBox.AckPacket(inKey)\n\t}\n\n\t// Now, we will delete all circuits closed by the previous commitment\n\t// signature, which is the result of downstream Settle/Fail packets. We\n\t// batch them here to ensure circuits are closed atomically and for\n\t// performance.\n\terr := l.cfg.Circuits.DeleteCircuits(l.closedCircuits...)\n\tswitch err {\n\tcase nil:\n\t\t// Successful deletion.\n\n\tdefault:\n\t\tl.log.Errorf(\"unable to delete %d circuits: %v\",\n\t\t\tlen(l.closedCircuits), err)\n\t\treturn err\n\t}\n\n\t// With the circuits removed from memory and disk, we now ack any\n\t// Settle/Fails in the mailbox to ensure they do not get redelivered\n\t// after startup. If forgive is enabled and we've reached this point,\n\t// the circuits must have been removed at some point, so it is now safe\n\t// to un-queue the corresponding Settle/Fails.\n\tfor _, inKey := range l.closedCircuits {\n\t\tl.log.Debugf(\"removing Fail/Settle packet %s from mailbox\",\n\t\t\tinKey)\n\t\tl.mailBox.AckPacket(inKey)\n\t}\n\n\t// Lastly, reset our buffers to be empty while keeping any acquired\n\t// growth in the backing array.\n\tl.openedCircuits = l.openedCircuits[:0]\n\tl.closedCircuits = l.closedCircuits[:0]\n\n\treturn nil\n}\n\n// updateCommitTxOrFail updates the commitment tx and if that fails, it fails\n// the link.",
      "length": 1779,
      "tokens": 271,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) updateCommitTxOrFail() bool {",
      "content": "func (l *channelLink) updateCommitTxOrFail() bool {\n\terr := l.updateCommitTx()\n\tswitch err {\n\t// No error encountered, success.\n\tcase nil:\n\n\t// A duplicate keystone error should be resolved and is not fatal, so\n\t// we won't send an Error message to the peer.\n\tcase ErrDuplicateKeystone:\n\t\tl.fail(LinkFailureError{code: ErrCircuitError},\n\t\t\t\"temporary circuit error: %v\", err)\n\t\treturn false\n\n\t// Any other error is treated results in an Error message being sent to\n\t// the peer.\n\tdefault:\n\t\tl.fail(LinkFailureError{code: ErrInternalError},\n\t\t\t\"unable to update commitment: %v\", err)\n\t\treturn false\n\t}\n\n\treturn true\n}\n\n// updateCommitTx signs, then sends an update to the remote peer adding a new\n// commitment to their commitment chain which includes all the latest updates\n// we've received+processed up to this point.",
      "length": 742,
      "tokens": 112,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) updateCommitTx() error {",
      "content": "func (l *channelLink) updateCommitTx() error {\n\t// Preemptively write all pending keystones to disk, just in case the\n\t// HTLCs we have in memory are included in the subsequent attempt to\n\t// sign a commitment state.\n\terr := l.cfg.Circuits.OpenCircuits(l.keystoneBatch...)\n\tif err != nil {\n\t\t// If ErrDuplicateKeystone is returned, the caller will catch\n\t\t// it.\n\t\treturn err\n\t}\n\n\t// Reset the batch, but keep the backing buffer to avoid reallocating.\n\tl.keystoneBatch = l.keystoneBatch[:0]\n\n\t// If hodl.Commit mode is active, we will refrain from attempting to\n\t// commit any in-memory modifications to the channel state. Exiting here\n\t// permits testing of either the switch or link's ability to trim\n\t// circuits that have been opened, but unsuccessfully committed.\n\tif l.cfg.HodlMask.Active(hodl.Commit) {\n\t\tl.log.Warnf(hodl.Commit.Warning())\n\t\treturn nil\n\t}\n\n\ttheirCommitSig, htlcSigs, pendingHTLCs, err := l.channel.SignNextCommitment()\n\tif err == lnwallet.ErrNoWindow {\n\t\tl.cfg.PendingCommitTicker.Resume()\n\t\tl.log.Trace(\"PendingCommitTicker resumed\")\n\n\t\tl.log.Tracef(\"revocation window exhausted, unable to send: \"+\n\t\t\t\"%v, pend_updates=%v, dangling_closes%v\",\n\t\t\tl.channel.PendingLocalUpdateCount(),\n\t\t\tnewLogClosure(func() string {\n\t\t\t\treturn spew.Sdump(l.openedCircuits)\n\t\t\t}),\n\t\t\tnewLogClosure(func() string {\n\t\t\t\treturn spew.Sdump(l.closedCircuits)\n\t\t\t}),\n\t\t)\n\t\treturn nil\n\t} else if err != nil {\n\t\treturn err\n\t}\n\n\tif err := l.ackDownStreamPackets(); err != nil {\n\t\treturn err\n\t}\n\n\tl.cfg.PendingCommitTicker.Pause()\n\tl.log.Trace(\"PendingCommitTicker paused after ackDownStreamPackets\")\n\n\t// The remote party now has a new pending commitment, so we'll update\n\t// the contract court to be aware of this new set (the prior old remote\n\t// pending).\n\tnewUpdate := &contractcourt.ContractUpdate{\n\t\tHtlcKey: contractcourt.RemotePendingHtlcSet,\n\t\tHtlcs:   pendingHTLCs,\n\t}\n\terr = l.cfg.NotifyContractUpdate(newUpdate)\n\tif err != nil {\n\t\tl.log.Errorf(\"unable to notify contract update: %v\", err)\n\t\treturn err\n\t}\n\n\tselect {\n\tcase <-l.quit:\n\t\treturn ErrLinkShuttingDown\n\tdefault:\n\t}\n\n\tcommitSig := &lnwire.CommitSig{\n\t\tChanID:    l.ChanID(),\n\t\tCommitSig: theirCommitSig,\n\t\tHtlcSigs:  htlcSigs,\n\t}\n\tl.cfg.Peer.SendMessage(false, commitSig)\n\n\treturn nil\n}\n\n// Peer returns the representation of remote peer with which we have the\n// channel link opened.\n//\n// NOTE: Part of the ChannelLink interface.",
      "length": 2272,
      "tokens": 288,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) Peer() lnpeer.Peer {",
      "content": "func (l *channelLink) Peer() lnpeer.Peer {\n\treturn l.cfg.Peer\n}\n\n// ChannelPoint returns the channel outpoint for the channel link.\n// NOTE: Part of the ChannelLink interface.",
      "length": 128,
      "tokens": 20,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) ChannelPoint() *wire.OutPoint {",
      "content": "func (l *channelLink) ChannelPoint() *wire.OutPoint {\n\treturn l.channel.ChannelPoint()\n}\n\n// ShortChanID returns the short channel ID for the channel link. The short\n// channel ID encodes the exact location in the main chain that the original\n// funding output can be found.\n//\n// NOTE: Part of the ChannelLink interface.",
      "length": 260,
      "tokens": 44,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) ShortChanID() lnwire.ShortChannelID {",
      "content": "func (l *channelLink) ShortChanID() lnwire.ShortChannelID {\n\tl.RLock()\n\tdefer l.RUnlock()\n\n\treturn l.shortChanID\n}\n\n// UpdateShortChanID updates the short channel ID for a link. This may be\n// required in the event that a link is created before the short chan ID for it\n// is known, or a re-org occurs, and the funding transaction changes location\n// within the chain.\n//\n// NOTE: Part of the ChannelLink interface.",
      "length": 344,
      "tokens": 61,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) UpdateShortChanID() (lnwire.ShortChannelID, error) {",
      "content": "func (l *channelLink) UpdateShortChanID() (lnwire.ShortChannelID, error) {\n\tchanID := l.ChanID()\n\n\t// Refresh the channel state's short channel ID by loading it from disk.\n\t// This ensures that the channel state accurately reflects the updated\n\t// short channel ID.\n\terr := l.channel.State().Refresh()\n\tif err != nil {\n\t\tl.log.Errorf(\"unable to refresh short_chan_id for chan_id=%v: \"+\n\t\t\t\"%v\", chanID, err)\n\t\treturn hop.Source, err\n\t}\n\n\treturn hop.Source, nil\n}\n\n// ChanID returns the channel ID for the channel link. The channel ID is a more\n// compact representation of a channel's full outpoint.\n//\n// NOTE: Part of the ChannelLink interface.",
      "length": 553,
      "tokens": 89,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) ChanID() lnwire.ChannelID {",
      "content": "func (l *channelLink) ChanID() lnwire.ChannelID {\n\treturn lnwire.NewChanIDFromOutPoint(l.channel.ChannelPoint())\n}\n\n// Bandwidth returns the total amount that can flow through the channel link at\n// this given instance. The value returned is expressed in millisatoshi and can\n// be used by callers when making forwarding decisions to determine if a link\n// can accept an HTLC.\n//\n// NOTE: Part of the ChannelLink interface.",
      "length": 365,
      "tokens": 57,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) Bandwidth() lnwire.MilliSatoshi {",
      "content": "func (l *channelLink) Bandwidth() lnwire.MilliSatoshi {\n\t// Get the balance available on the channel for new HTLCs. This takes\n\t// the channel reserve into account so HTLCs up to this value won't\n\t// violate it.\n\treturn l.channel.AvailableBalance()\n}\n\n// MayAddOutgoingHtlc indicates whether we can add an outgoing htlc with the\n// amount provided to the link. This check does not reserve a space, since\n// forwards or other payments may use the available slot, so it should be\n// considered best-effort.",
      "length": 439,
      "tokens": 75,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) MayAddOutgoingHtlc(amt lnwire.MilliSatoshi) error {",
      "content": "func (l *channelLink) MayAddOutgoingHtlc(amt lnwire.MilliSatoshi) error {\n\treturn l.channel.MayAddOutgoingHtlc(amt)\n}\n\n// getDustSum is a wrapper method that calls the underlying channel's dust sum\n// method.\n//\n// NOTE: Part of the dustHandler interface.",
      "length": 175,
      "tokens": 26,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) getDustSum(remote bool) lnwire.MilliSatoshi {",
      "content": "func (l *channelLink) getDustSum(remote bool) lnwire.MilliSatoshi {\n\treturn l.channel.GetDustSum(remote)\n}\n\n// getFeeRate is a wrapper method that retrieves the underlying channel's\n// feerate.\n//\n// NOTE: Part of the dustHandler interface.",
      "length": 166,
      "tokens": 24,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) getFeeRate() chainfee.SatPerKWeight {",
      "content": "func (l *channelLink) getFeeRate() chainfee.SatPerKWeight {\n\treturn l.channel.CommitFeeRate()\n}\n\n// getDustClosure returns a closure that can be used by the switch or mailbox\n// to evaluate whether a given HTLC is dust.\n//\n// NOTE: Part of the dustHandler interface.",
      "length": 200,
      "tokens": 34,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) getDustClosure() dustClosure {",
      "content": "func (l *channelLink) getDustClosure() dustClosure {\n\tlocalDustLimit := l.channel.State().LocalChanCfg.DustLimit\n\tremoteDustLimit := l.channel.State().RemoteChanCfg.DustLimit\n\tchanType := l.channel.State().ChanType\n\n\treturn dustHelper(chanType, localDustLimit, remoteDustLimit)\n}\n\n// dustClosure is a function that evaluates whether an HTLC is dust. It returns\n// true if the HTLC is dust. It takes in a feerate, a boolean denoting whether\n// the HTLC is incoming (i.e. one that the remote sent), a boolean denoting\n// whether to evaluate on the local or remote commit, and finally an HTLC\n// amount to test.",
      "length": 544,
      "tokens": 76,
      "embedding": []
    },
    {
      "slug": "type dustClosure func(chainfee.SatPerKWeight, bool, bool, btcutil.Amount) bool",
      "content": "type dustClosure func(chainfee.SatPerKWeight, bool, bool, btcutil.Amount) bool\n\n// dustHelper is used to construct the dustClosure.",
      "length": 51,
      "tokens": 8,
      "embedding": []
    },
    {
      "slug": "func dustHelper(chantype channeldb.ChannelType, localDustLimit,",
      "content": "func dustHelper(chantype channeldb.ChannelType, localDustLimit,\n\tremoteDustLimit btcutil.Amount) dustClosure {\n\n\tisDust := func(feerate chainfee.SatPerKWeight, incoming,\n\t\tlocalCommit bool, amt btcutil.Amount) bool {\n\n\t\tif localCommit {\n\t\t\treturn lnwallet.HtlcIsDust(\n\t\t\t\tchantype, incoming, true, feerate, amt,\n\t\t\t\tlocalDustLimit,\n\t\t\t)\n\t\t}\n\n\t\treturn lnwallet.HtlcIsDust(\n\t\t\tchantype, incoming, false, feerate, amt,\n\t\t\tremoteDustLimit,\n\t\t)\n\t}\n\n\treturn isDust\n}\n\n// zeroConfConfirmed returns whether or not the zero-conf channel has\n// confirmed on-chain.\n//\n// Part of the scidAliasHandler interface.",
      "length": 512,
      "tokens": 61,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) zeroConfConfirmed() bool {",
      "content": "func (l *channelLink) zeroConfConfirmed() bool {\n\treturn l.channel.State().ZeroConfConfirmed()\n}\n\n// confirmedScid returns the confirmed SCID for a zero-conf channel. This\n// should not be called for non-zero-conf channels.\n//\n// Part of the scidAliasHandler interface.",
      "length": 214,
      "tokens": 29,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) confirmedScid() lnwire.ShortChannelID {",
      "content": "func (l *channelLink) confirmedScid() lnwire.ShortChannelID {\n\treturn l.channel.State().ZeroConfRealScid()\n}\n\n// isZeroConf returns whether or not the underlying channel is a zero-conf\n// channel.\n//\n// Part of the scidAliasHandler interface.",
      "length": 174,
      "tokens": 24,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) isZeroConf() bool {",
      "content": "func (l *channelLink) isZeroConf() bool {\n\treturn l.channel.State().IsZeroConf()\n}\n\n// negotiatedAliasFeature returns whether or not the underlying channel has\n// negotiated the option-scid-alias feature bit. This will be true for both\n// option-scid-alias and zero-conf channel-types. It will also be true for\n// channels with the feature bit but without the above channel-types.\n//\n// Part of the scidAliasFeature interface.",
      "length": 376,
      "tokens": 54,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) negotiatedAliasFeature() bool {",
      "content": "func (l *channelLink) negotiatedAliasFeature() bool {\n\treturn l.channel.State().NegotiatedAliasFeature()\n}\n\n// getAliases returns the set of aliases for the underlying channel.\n//\n// Part of the scidAliasHandler interface.",
      "length": 163,
      "tokens": 21,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) getAliases() []lnwire.ShortChannelID {",
      "content": "func (l *channelLink) getAliases() []lnwire.ShortChannelID {\n\treturn l.cfg.GetAliases(l.ShortChanID())\n}\n\n// attachFailAliasUpdate sets the link's FailAliasUpdate function.\n//\n// Part of the scidAliasHandler interface.",
      "length": 152,
      "tokens": 17,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) attachFailAliasUpdate(closure func(",
      "content": "func (l *channelLink) attachFailAliasUpdate(closure func(\n\tsid lnwire.ShortChannelID, incoming bool) *lnwire.ChannelUpdate) {\n\n\tl.Lock()\n\tl.cfg.FailAliasUpdate = closure\n\tl.Unlock()\n}\n\n// AttachMailBox updates the current mailbox used by this link, and hooks up\n// the mailbox's message and packet outboxes to the link's upstream and\n// downstream chans, respectively.",
      "length": 301,
      "tokens": 41,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) AttachMailBox(mailbox MailBox) {",
      "content": "func (l *channelLink) AttachMailBox(mailbox MailBox) {\n\tl.Lock()\n\tl.mailBox = mailbox\n\tl.upstream = mailbox.MessageOutBox()\n\tl.downstream = mailbox.PacketOutBox()\n\tl.Unlock()\n\n\t// Set the mailbox's fee rate. This may be refreshing a feerate that was\n\t// never committed.\n\tl.mailBox.SetFeeRate(l.getFeeRate())\n\n\t// Also set the mailbox's dust closure so that it can query whether HTLC's\n\t// are dust given the current feerate.\n\tl.mailBox.SetDustClosure(l.getDustClosure())\n}\n\n// UpdateForwardingPolicy updates the forwarding policy for the target\n// ChannelLink. Once updated, the link will use the new forwarding policy to\n// govern if it an incoming HTLC should be forwarded or not. We assume that\n// fields that are zero are intentionally set to zero, so we'll use newPolicy to\n// update all of the link's FwrdingPolicy's values.\n//\n// NOTE: Part of the ChannelLink interface.",
      "length": 802,
      "tokens": 120,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) UpdateForwardingPolicy(newPolicy ForwardingPolicy) {",
      "content": "func (l *channelLink) UpdateForwardingPolicy(newPolicy ForwardingPolicy) {\n\tl.Lock()\n\tdefer l.Unlock()\n\n\tl.cfg.FwrdingPolicy = newPolicy\n}\n\n// CheckHtlcForward should return a nil error if the passed HTLC details\n// satisfy the current forwarding policy fo the target link. Otherwise,\n// a LinkError with a valid protocol failure message should be returned\n// in order to signal to the source of the HTLC, the policy consistency\n// issue.\n//\n// NOTE: Part of the ChannelLink interface.",
      "length": 398,
      "tokens": 66,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) CheckHtlcForward(payHash [32]byte,",
      "content": "func (l *channelLink) CheckHtlcForward(payHash [32]byte,\n\tincomingHtlcAmt, amtToForward lnwire.MilliSatoshi,\n\tincomingTimeout, outgoingTimeout uint32,\n\theightNow uint32, originalScid lnwire.ShortChannelID) *LinkError {\n\n\tl.RLock()\n\tpolicy := l.cfg.FwrdingPolicy\n\tl.RUnlock()\n\n\t// First check whether the outgoing htlc satisfies the channel policy.\n\terr := l.canSendHtlc(\n\t\tpolicy, payHash, amtToForward, outgoingTimeout, heightNow,\n\t\toriginalScid,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Next, using the amount of the incoming HTLC, we'll calculate the\n\t// expected fee this incoming HTLC must carry in order to satisfy the\n\t// constraints of the outgoing link.\n\texpectedFee := ExpectedFee(policy, amtToForward)\n\n\t// If the actual fee is less than our expected fee, then we'll reject\n\t// this HTLC as it didn't provide a sufficient amount of fees, or the\n\t// values have been tampered with, or the send used incorrect/dated\n\t// information to construct the forwarding information for this hop. In\n\t// any case, we'll cancel this HTLC.\n\tactualFee := incomingHtlcAmt - amtToForward\n\tif incomingHtlcAmt < amtToForward || actualFee < expectedFee {\n\t\tl.log.Warnf(\"outgoing htlc(%x) has insufficient fee: \"+\n\t\t\t\"expected %v, got %v\",\n\t\t\tpayHash[:], int64(expectedFee), int64(actualFee))\n\n\t\t// As part of the returned error, we'll send our latest routing\n\t\t// policy so the sending node obtains the most up to date data.\n\t\tcb := func(upd *lnwire.ChannelUpdate) lnwire.FailureMessage {\n\t\t\treturn lnwire.NewFeeInsufficient(amtToForward, *upd)\n\t\t}\n\t\tfailure := l.createFailureWithUpdate(false, originalScid, cb)\n\t\treturn NewLinkError(failure)\n\t}\n\n\t// Finally, we'll ensure that the time-lock on the outgoing HTLC meets\n\t// the following constraint: the incoming time-lock minus our time-lock\n\t// delta should equal the outgoing time lock. Otherwise, whether the\n\t// sender messed up, or an intermediate node tampered with the HTLC.\n\ttimeDelta := policy.TimeLockDelta\n\tif incomingTimeout < outgoingTimeout+timeDelta {\n\t\tl.log.Warnf(\"incoming htlc(%x) has incorrect time-lock value: \"+\n\t\t\t\"expected at least %v block delta, got %v block delta\",\n\t\t\tpayHash[:], timeDelta, incomingTimeout-outgoingTimeout)\n\n\t\t// Grab the latest routing policy so the sending node is up to\n\t\t// date with our current policy.\n\t\tcb := func(upd *lnwire.ChannelUpdate) lnwire.FailureMessage {\n\t\t\treturn lnwire.NewIncorrectCltvExpiry(\n\t\t\t\tincomingTimeout, *upd,\n\t\t\t)\n\t\t}\n\t\tfailure := l.createFailureWithUpdate(false, originalScid, cb)\n\t\treturn NewLinkError(failure)\n\t}\n\n\treturn nil\n}\n\n// CheckHtlcTransit should return a nil error if the passed HTLC details\n// satisfy the current channel policy.  Otherwise, a LinkError with a\n// valid protocol failure message should be returned in order to signal\n// the violation. This call is intended to be used for locally initiated\n// payments for which there is no corresponding incoming htlc.",
      "length": 2772,
      "tokens": 381,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) CheckHtlcTransit(payHash [32]byte,",
      "content": "func (l *channelLink) CheckHtlcTransit(payHash [32]byte,\n\tamt lnwire.MilliSatoshi, timeout uint32,\n\theightNow uint32) *LinkError {\n\n\tl.RLock()\n\tpolicy := l.cfg.FwrdingPolicy\n\tl.RUnlock()\n\n\t// We pass in hop.Source here as this is only used in the Switch when\n\t// trying to send over a local link. This causes the fallback mechanism\n\t// to occur.\n\treturn l.canSendHtlc(\n\t\tpolicy, payHash, amt, timeout, heightNow, hop.Source,\n\t)\n}\n\n// canSendHtlc checks whether the given htlc parameters satisfy\n// the channel's amount and time lock constraints.",
      "length": 472,
      "tokens": 71,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) canSendHtlc(policy ForwardingPolicy,",
      "content": "func (l *channelLink) canSendHtlc(policy ForwardingPolicy,\n\tpayHash [32]byte, amt lnwire.MilliSatoshi, timeout uint32,\n\theightNow uint32, originalScid lnwire.ShortChannelID) *LinkError {\n\n\t// As our first sanity check, we'll ensure that the passed HTLC isn't\n\t// too small for the next hop. If so, then we'll cancel the HTLC\n\t// directly.\n\tif amt < policy.MinHTLCOut {\n\t\tl.log.Warnf(\"outgoing htlc(%x) is too small: min_htlc=%v, \"+\n\t\t\t\"htlc_value=%v\", payHash[:], policy.MinHTLCOut,\n\t\t\tamt)\n\n\t\t// As part of the returned error, we'll send our latest routing\n\t\t// policy so the sending node obtains the most up to date data.\n\t\tcb := func(upd *lnwire.ChannelUpdate) lnwire.FailureMessage {\n\t\t\treturn lnwire.NewAmountBelowMinimum(amt, *upd)\n\t\t}\n\t\tfailure := l.createFailureWithUpdate(false, originalScid, cb)\n\t\treturn NewLinkError(failure)\n\t}\n\n\t// Next, ensure that the passed HTLC isn't too large. If so, we'll\n\t// cancel the HTLC directly.\n\tif policy.MaxHTLC != 0 && amt > policy.MaxHTLC {\n\t\tl.log.Warnf(\"outgoing htlc(%x) is too large: max_htlc=%v, \"+\n\t\t\t\"htlc_value=%v\", payHash[:], policy.MaxHTLC, amt)\n\n\t\t// As part of the returned error, we'll send our latest routing\n\t\t// policy so the sending node obtains the most up-to-date data.\n\t\tcb := func(upd *lnwire.ChannelUpdate) lnwire.FailureMessage {\n\t\t\treturn lnwire.NewTemporaryChannelFailure(upd)\n\t\t}\n\t\tfailure := l.createFailureWithUpdate(false, originalScid, cb)\n\t\treturn NewDetailedLinkError(failure, OutgoingFailureHTLCExceedsMax)\n\t}\n\n\t// We want to avoid offering an HTLC which will expire in the near\n\t// future, so we'll reject an HTLC if the outgoing expiration time is\n\t// too close to the current height.\n\tif timeout <= heightNow+l.cfg.OutgoingCltvRejectDelta {\n\t\tl.log.Warnf(\"htlc(%x) has an expiry that's too soon: \"+\n\t\t\t\"outgoing_expiry=%v, best_height=%v\", payHash[:],\n\t\t\ttimeout, heightNow)\n\n\t\tcb := func(upd *lnwire.ChannelUpdate) lnwire.FailureMessage {\n\t\t\treturn lnwire.NewExpiryTooSoon(*upd)\n\t\t}\n\t\tfailure := l.createFailureWithUpdate(false, originalScid, cb)\n\t\treturn NewLinkError(failure)\n\t}\n\n\t// Check absolute max delta.\n\tif timeout > l.cfg.MaxOutgoingCltvExpiry+heightNow {\n\t\tl.log.Warnf(\"outgoing htlc(%x) has a time lock too far in \"+\n\t\t\t\"the future: got %v, but maximum is %v\", payHash[:],\n\t\t\ttimeout-heightNow, l.cfg.MaxOutgoingCltvExpiry)\n\n\t\treturn NewLinkError(&lnwire.FailExpiryTooFar{})\n\t}\n\n\t// Check to see if there is enough balance in this channel.\n\tif amt > l.Bandwidth() {\n\t\tl.log.Warnf(\"insufficient bandwidth to route htlc: %v is \"+\n\t\t\t\"larger than %v\", amt, l.Bandwidth())\n\t\tcb := func(upd *lnwire.ChannelUpdate) lnwire.FailureMessage {\n\t\t\treturn lnwire.NewTemporaryChannelFailure(upd)\n\t\t}\n\t\tfailure := l.createFailureWithUpdate(false, originalScid, cb)\n\t\treturn NewDetailedLinkError(\n\t\t\tfailure, OutgoingFailureInsufficientBalance,\n\t\t)\n\t}\n\n\treturn nil\n}\n\n// Stats returns the statistics of channel link.\n//\n// NOTE: Part of the ChannelLink interface.",
      "length": 2809,
      "tokens": 351,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) Stats() (uint64, lnwire.MilliSatoshi, lnwire.MilliSatoshi) {",
      "content": "func (l *channelLink) Stats() (uint64, lnwire.MilliSatoshi, lnwire.MilliSatoshi) {\n\tsnapshot := l.channel.StateSnapshot()\n\n\treturn snapshot.ChannelCommitment.CommitHeight,\n\t\tsnapshot.TotalMSatSent,\n\t\tsnapshot.TotalMSatReceived\n}\n\n// String returns the string representation of channel link.\n//\n// NOTE: Part of the ChannelLink interface.",
      "length": 245,
      "tokens": 25,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) String() string {",
      "content": "func (l *channelLink) String() string {\n\treturn l.channel.ChannelPoint().String()\n}\n\n// handleSwitchPacket handles the switch packets. This packets which might be\n// forwarded to us from another channel link in case the htlc update came from\n// another peer or if the update was created by user\n//\n// NOTE: Part of the packetHandler interface.",
      "length": 296,
      "tokens": 48,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) handleSwitchPacket(pkt *htlcPacket) error {",
      "content": "func (l *channelLink) handleSwitchPacket(pkt *htlcPacket) error {\n\tl.log.Tracef(\"received switch packet inkey=%v, outkey=%v\",\n\t\tpkt.inKey(), pkt.outKey())\n\n\treturn l.mailBox.AddPacket(pkt)\n}\n\n// HandleChannelUpdate handles the htlc requests as settle/add/fail which sent\n// to us from remote peer we have a channel with.\n//\n// NOTE: Part of the ChannelLink interface.",
      "length": 292,
      "tokens": 39,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) HandleChannelUpdate(message lnwire.Message) {",
      "content": "func (l *channelLink) HandleChannelUpdate(message lnwire.Message) {\n\tselect {\n\tcase <-l.quit:\n\t\t// Return early if the link is already in the process of\n\t\t// quitting. It doesn't make sense to hand the message to the\n\t\t// mailbox here.\n\t\treturn\n\tdefault:\n\t}\n\n\tl.mailBox.AddMessage(message)\n}\n\n// ShutdownIfChannelClean triggers a link shutdown if the channel is in a clean\n// state and errors if the channel has lingering updates.\n//\n// NOTE: Part of the ChannelUpdateHandler interface.",
      "length": 403,
      "tokens": 67,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) ShutdownIfChannelClean() error {",
      "content": "func (l *channelLink) ShutdownIfChannelClean() error {\n\terrChan := make(chan error, 1)\n\n\tselect {\n\tcase l.shutdownRequest <- &shutdownReq{\n\t\terr: errChan,\n\t}:\n\tcase <-l.quit:\n\t\treturn ErrLinkShuttingDown\n\t}\n\n\tselect {\n\tcase err := <-errChan:\n\t\treturn err\n\tcase <-l.quit:\n\t\treturn ErrLinkShuttingDown\n\t}\n}\n\n// updateChannelFee updates the commitment fee-per-kw on this channel by\n// committing to an update_fee message.",
      "length": 344,
      "tokens": 49,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) updateChannelFee(feePerKw chainfee.SatPerKWeight) error {",
      "content": "func (l *channelLink) updateChannelFee(feePerKw chainfee.SatPerKWeight) error {\n\tl.log.Infof(\"updating commit fee to %v sat/kw\", feePerKw)\n\n\t// We skip sending the UpdateFee message if the channel is not\n\t// currently eligible to forward messages.\n\tif !l.EligibleToForward() {\n\t\tl.log.Debugf(\"skipping fee update for inactive channel\")\n\t\treturn nil\n\t}\n\n\t// First, we'll update the local fee on our commitment.\n\tif err := l.channel.UpdateFee(feePerKw); err != nil {\n\t\treturn err\n\t}\n\n\t// The fee passed the channel's validation checks, so we update the\n\t// mailbox feerate.\n\tl.mailBox.SetFeeRate(feePerKw)\n\n\t// We'll then attempt to send a new UpdateFee message, and also lock it\n\t// in immediately by triggering a commitment update.\n\tmsg := lnwire.NewUpdateFee(l.ChanID(), uint32(feePerKw))\n\tif err := l.cfg.Peer.SendMessage(false, msg); err != nil {\n\t\treturn err\n\t}\n\treturn l.updateCommitTx()\n}\n\n// processRemoteSettleFails accepts a batch of settle/fail payment descriptors\n// after receiving a revocation from the remote party, and reprocesses them in\n// the context of the provided forwarding package. Any settles or fails that\n// have already been acknowledged in the forwarding package will not be sent to\n// the switch.",
      "length": 1114,
      "tokens": 167,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) processRemoteSettleFails(fwdPkg *channeldb.FwdPkg,",
      "content": "func (l *channelLink) processRemoteSettleFails(fwdPkg *channeldb.FwdPkg,\n\tsettleFails []*lnwallet.PaymentDescriptor) {\n\n\tif len(settleFails) == 0 {\n\t\treturn\n\t}\n\n\tl.log.Debugf(\"settle-fail-filter %v\", fwdPkg.SettleFailFilter)\n\n\tvar switchPackets []*htlcPacket\n\tfor i, pd := range settleFails {\n\t\t// Skip any settles or fails that have already been\n\t\t// acknowledged by the incoming link that originated the\n\t\t// forwarded Add.\n\t\tif fwdPkg.SettleFailFilter.Contains(uint16(i)) {\n\t\t\tcontinue\n\t\t}\n\n\t\t// TODO(roasbeef): rework log entries to a shared\n\t\t// interface.\n\n\t\tswitch pd.EntryType {\n\n\t\t// A settle for an HTLC we previously forwarded HTLC has been\n\t\t// received. So we'll forward the HTLC to the switch which will\n\t\t// handle propagating the settle to the prior hop.\n\t\tcase lnwallet.Settle:\n\t\t\t// If hodl.SettleIncoming is requested, we will not\n\t\t\t// forward the SETTLE to the switch and will not signal\n\t\t\t// a free slot on the commitment transaction.\n\t\t\tif l.cfg.HodlMask.Active(hodl.SettleIncoming) {\n\t\t\t\tl.log.Warnf(hodl.SettleIncoming.Warning())\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tsettlePacket := &htlcPacket{\n\t\t\t\toutgoingChanID: l.ShortChanID(),\n\t\t\t\toutgoingHTLCID: pd.ParentIndex,\n\t\t\t\tdestRef:        pd.DestRef,\n\t\t\t\thtlc: &lnwire.UpdateFulfillHTLC{\n\t\t\t\t\tPaymentPreimage: pd.RPreimage,\n\t\t\t\t},\n\t\t\t}\n\n\t\t\t// Add the packet to the batch to be forwarded, and\n\t\t\t// notify the overflow queue that a spare spot has been\n\t\t\t// freed up within the commitment state.\n\t\t\tswitchPackets = append(switchPackets, settlePacket)\n\n\t\t// A failureCode message for a previously forwarded HTLC has\n\t\t// been received. As a result a new slot will be freed up in\n\t\t// our commitment state, so we'll forward this to the switch so\n\t\t// the backwards undo can continue.\n\t\tcase lnwallet.Fail:\n\t\t\t// If hodl.SettleIncoming is requested, we will not\n\t\t\t// forward the FAIL to the switch and will not signal a\n\t\t\t// free slot on the commitment transaction.\n\t\t\tif l.cfg.HodlMask.Active(hodl.FailIncoming) {\n\t\t\t\tl.log.Warnf(hodl.FailIncoming.Warning())\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Fetch the reason the HTLC was canceled so we can\n\t\t\t// continue to propagate it. This failure originated\n\t\t\t// from another node, so the linkFailure field is not\n\t\t\t// set on the packet.\n\t\t\tfailPacket := &htlcPacket{\n\t\t\t\toutgoingChanID: l.ShortChanID(),\n\t\t\t\toutgoingHTLCID: pd.ParentIndex,\n\t\t\t\tdestRef:        pd.DestRef,\n\t\t\t\thtlc: &lnwire.UpdateFailHTLC{\n\t\t\t\t\tReason: lnwire.OpaqueReason(\n\t\t\t\t\t\tpd.FailReason,\n\t\t\t\t\t),\n\t\t\t\t},\n\t\t\t}\n\n\t\t\tl.log.Debugf(\"Failed to send %s\", pd.Amount)\n\n\t\t\t// If the failure message lacks an HMAC (but includes\n\t\t\t// the 4 bytes for encoding the message and padding\n\t\t\t// lengths, then this means that we received it as an\n\t\t\t// UpdateFailMalformedHTLC. As a result, we'll signal\n\t\t\t// that we need to convert this error within the switch\n\t\t\t// to an actual error, by encrypting it as if we were\n\t\t\t// the originating hop.\n\t\t\tconvertedErrorSize := lnwire.FailureMessageLength + 4\n\t\t\tif len(pd.FailReason) == convertedErrorSize {\n\t\t\t\tfailPacket.convertedError = true\n\t\t\t}\n\n\t\t\t// Add the packet to the batch to be forwarded, and\n\t\t\t// notify the overflow queue that a spare spot has been\n\t\t\t// freed up within the commitment state.\n\t\t\tswitchPackets = append(switchPackets, failPacket)\n\t\t}\n\t}\n\n\t// Only spawn the task forward packets we have a non-zero number.\n\tif len(switchPackets) > 0 {\n\t\tgo l.forwardBatch(false, switchPackets...)\n\t}\n}\n\n// processRemoteAdds serially processes each of the Add payment descriptors\n// which have been \"locked-in\" by receiving a revocation from the remote party.\n// The forwarding package provided instructs how to process this batch,\n// indicating whether this is the first time these Adds are being processed, or\n// whether we are reprocessing as a result of a failure or restart. Adds that\n// have already been acknowledged in the forwarding package will be ignored.",
      "length": 3690,
      "tokens": 523,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) processRemoteAdds(fwdPkg *channeldb.FwdPkg,",
      "content": "func (l *channelLink) processRemoteAdds(fwdPkg *channeldb.FwdPkg,\n\tlockedInHtlcs []*lnwallet.PaymentDescriptor) {\n\n\tl.log.Tracef(\"processing %d remote adds for height %d\",\n\t\tlen(lockedInHtlcs), fwdPkg.Height)\n\n\tdecodeReqs := make(\n\t\t[]hop.DecodeHopIteratorRequest, 0, len(lockedInHtlcs),\n\t)\n\tfor _, pd := range lockedInHtlcs {\n\t\tswitch pd.EntryType {\n\n\t\t// TODO(conner): remove type switch?\n\t\tcase lnwallet.Add:\n\t\t\t// Before adding the new htlc to the state machine,\n\t\t\t// parse the onion object in order to obtain the\n\t\t\t// routing information with DecodeHopIterator function\n\t\t\t// which process the Sphinx packet.\n\t\t\tonionReader := bytes.NewReader(pd.OnionBlob)\n\n\t\t\treq := hop.DecodeHopIteratorRequest{\n\t\t\t\tOnionReader:  onionReader,\n\t\t\t\tRHash:        pd.RHash[:],\n\t\t\t\tIncomingCltv: pd.Timeout,\n\t\t\t}\n\n\t\t\tdecodeReqs = append(decodeReqs, req)\n\t\t}\n\t}\n\n\t// Atomically decode the incoming htlcs, simultaneously checking for\n\t// replay attempts. A particular index in the returned, spare list of\n\t// channel iterators should only be used if the failure code at the\n\t// same index is lnwire.FailCodeNone.\n\tdecodeResps, sphinxErr := l.cfg.DecodeHopIterators(\n\t\tfwdPkg.ID(), decodeReqs,\n\t)\n\tif sphinxErr != nil {\n\t\tl.fail(LinkFailureError{code: ErrInternalError},\n\t\t\t\"unable to decode hop iterators: %v\", sphinxErr)\n\t\treturn\n\t}\n\n\tvar switchPackets []*htlcPacket\n\n\tfor i, pd := range lockedInHtlcs {\n\t\tidx := uint16(i)\n\n\t\tif fwdPkg.State == channeldb.FwdStateProcessed &&\n\t\t\tfwdPkg.AckFilter.Contains(idx) {\n\n\t\t\t// If this index is already found in the ack filter,\n\t\t\t// the response to this forwarding decision has already\n\t\t\t// been committed by one of our commitment txns. ADDs\n\t\t\t// in this state are waiting for the rest of the fwding\n\t\t\t// package to get acked before being garbage collected.\n\t\t\tcontinue\n\t\t}\n\n\t\t// An incoming HTLC add has been full-locked in. As a result we\n\t\t// can now examine the forwarding details of the HTLC, and the\n\t\t// HTLC itself to decide if: we should forward it, cancel it,\n\t\t// or are able to settle it (and it adheres to our fee related\n\t\t// constraints).\n\n\t\t// Fetch the onion blob that was included within this processed\n\t\t// payment descriptor.\n\t\tvar onionBlob [lnwire.OnionPacketSize]byte\n\t\tcopy(onionBlob[:], pd.OnionBlob)\n\n\t\t// Before adding the new htlc to the state machine, parse the\n\t\t// onion object in order to obtain the routing information with\n\t\t// DecodeHopIterator function which process the Sphinx packet.\n\t\tchanIterator, failureCode := decodeResps[i].Result()\n\t\tif failureCode != lnwire.CodeNone {\n\t\t\t// If we're unable to process the onion blob than we\n\t\t\t// should send the malformed htlc error to payment\n\t\t\t// sender.\n\t\t\tl.sendMalformedHTLCError(pd.HtlcIndex, failureCode,\n\t\t\t\tonionBlob[:], pd.SourceRef)\n\n\t\t\tl.log.Errorf(\"unable to decode onion hop \"+\n\t\t\t\t\"iterator: %v\", failureCode)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Retrieve onion obfuscator from onion blob in order to\n\t\t// produce initial obfuscation of the onion failureCode.\n\t\tobfuscator, failureCode := chanIterator.ExtractErrorEncrypter(\n\t\t\tl.cfg.ExtractErrorEncrypter,\n\t\t)\n\t\tif failureCode != lnwire.CodeNone {\n\t\t\t// If we're unable to process the onion blob than we\n\t\t\t// should send the malformed htlc error to payment\n\t\t\t// sender.\n\t\t\tl.sendMalformedHTLCError(\n\t\t\t\tpd.HtlcIndex, failureCode, onionBlob[:], pd.SourceRef,\n\t\t\t)\n\n\t\t\tl.log.Errorf(\"unable to decode onion \"+\n\t\t\t\t\"obfuscator: %v\", failureCode)\n\t\t\tcontinue\n\t\t}\n\n\t\theightNow := l.cfg.BestHeight()\n\n\t\tpld, err := chanIterator.HopPayload()\n\t\tif err != nil {\n\t\t\t// If we're unable to process the onion payload, or we\n\t\t\t// received invalid onion payload failure, then we\n\t\t\t// should send an error back to the caller so the HTLC\n\t\t\t// can be canceled.\n\t\t\tvar failedType uint64\n\t\t\tif e, ok := err.(hop.ErrInvalidPayload); ok {\n\t\t\t\tfailedType = uint64(e.Type)\n\t\t\t}\n\n\t\t\t// TODO: currently none of the test unit infrastructure\n\t\t\t// is setup to handle TLV payloads, so testing this\n\t\t\t// would require implementing a separate mock iterator\n\t\t\t// for TLV payloads that also supports injecting invalid\n\t\t\t// payloads. Deferring this non-trival effort till a\n\t\t\t// later date\n\t\t\tfailure := lnwire.NewInvalidOnionPayload(failedType, 0)\n\t\t\tl.sendHTLCError(\n\t\t\t\tpd, NewLinkError(failure), obfuscator, false,\n\t\t\t)\n\n\t\t\tl.log.Errorf(\"unable to decode forwarding \"+\n\t\t\t\t\"instructions: %v\", err)\n\t\t\tcontinue\n\t\t}\n\n\t\tfwdInfo := pld.ForwardingInfo()\n\n\t\tswitch fwdInfo.NextHop {\n\t\tcase hop.Exit:\n\t\t\terr := l.processExitHop(\n\t\t\t\tpd, obfuscator, fwdInfo, heightNow, pld,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tl.fail(LinkFailureError{code: ErrInternalError},\n\t\t\t\t\terr.Error(),\n\t\t\t\t)\n\n\t\t\t\treturn\n\t\t\t}\n\n\t\t// There are additional channels left within this route. So\n\t\t// we'll simply do some forwarding package book-keeping.\n\t\tdefault:\n\t\t\t// If hodl.AddIncoming is requested, we will not\n\t\t\t// validate the forwarded ADD, nor will we send the\n\t\t\t// packet to the htlc switch.\n\t\t\tif l.cfg.HodlMask.Active(hodl.AddIncoming) {\n\t\t\t\tl.log.Warnf(hodl.AddIncoming.Warning())\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tswitch fwdPkg.State {\n\t\t\tcase channeldb.FwdStateProcessed:\n\t\t\t\t// This add was not forwarded on the previous\n\t\t\t\t// processing phase, run it through our\n\t\t\t\t// validation pipeline to reproduce an error.\n\t\t\t\t// This may trigger a different error due to\n\t\t\t\t// expiring timelocks, but we expect that an\n\t\t\t\t// error will be reproduced.\n\t\t\t\tif !fwdPkg.FwdFilter.Contains(idx) {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\t// Otherwise, it was already processed, we can\n\t\t\t\t// can collect it and continue.\n\t\t\t\taddMsg := &lnwire.UpdateAddHTLC{\n\t\t\t\t\tExpiry:      fwdInfo.OutgoingCTLV,\n\t\t\t\t\tAmount:      fwdInfo.AmountToForward,\n\t\t\t\t\tPaymentHash: pd.RHash,\n\t\t\t\t}\n\n\t\t\t\t// Finally, we'll encode the onion packet for\n\t\t\t\t// the _next_ hop using the hop iterator\n\t\t\t\t// decoded for the current hop.\n\t\t\t\tbuf := bytes.NewBuffer(addMsg.OnionBlob[0:0])\n\n\t\t\t\t// We know this cannot fail, as this ADD\n\t\t\t\t// was marked forwarded in a previous\n\t\t\t\t// round of processing.\n\t\t\t\tchanIterator.EncodeNextHop(buf)\n\n\t\t\t\tupdatePacket := &htlcPacket{\n\t\t\t\t\tincomingChanID:  l.ShortChanID(),\n\t\t\t\t\tincomingHTLCID:  pd.HtlcIndex,\n\t\t\t\t\toutgoingChanID:  fwdInfo.NextHop,\n\t\t\t\t\tsourceRef:       pd.SourceRef,\n\t\t\t\t\tincomingAmount:  pd.Amount,\n\t\t\t\t\tamount:          addMsg.Amount,\n\t\t\t\t\thtlc:            addMsg,\n\t\t\t\t\tobfuscator:      obfuscator,\n\t\t\t\t\tincomingTimeout: pd.Timeout,\n\t\t\t\t\toutgoingTimeout: fwdInfo.OutgoingCTLV,\n\t\t\t\t\tcustomRecords:   pld.CustomRecords(),\n\t\t\t\t}\n\t\t\t\tswitchPackets = append(\n\t\t\t\t\tswitchPackets, updatePacket,\n\t\t\t\t)\n\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// TODO(roasbeef): ensure don't accept outrageous\n\t\t\t// timeout for htlc\n\n\t\t\t// With all our forwarding constraints met, we'll\n\t\t\t// create the outgoing HTLC using the parameters as\n\t\t\t// specified in the forwarding info.\n\t\t\taddMsg := &lnwire.UpdateAddHTLC{\n\t\t\t\tExpiry:      fwdInfo.OutgoingCTLV,\n\t\t\t\tAmount:      fwdInfo.AmountToForward,\n\t\t\t\tPaymentHash: pd.RHash,\n\t\t\t}\n\n\t\t\t// Finally, we'll encode the onion packet for the\n\t\t\t// _next_ hop using the hop iterator decoded for the\n\t\t\t// current hop.\n\t\t\tbuf := bytes.NewBuffer(addMsg.OnionBlob[0:0])\n\t\t\terr := chanIterator.EncodeNextHop(buf)\n\t\t\tif err != nil {\n\t\t\t\tl.log.Errorf(\"unable to encode the \"+\n\t\t\t\t\t\"remaining route %v\", err)\n\n\t\t\t\tcb := func(upd *lnwire.ChannelUpdate) lnwire.FailureMessage {\n\t\t\t\t\treturn lnwire.NewTemporaryChannelFailure(upd)\n\t\t\t\t}\n\n\t\t\t\tfailure := l.createFailureWithUpdate(\n\t\t\t\t\ttrue, hop.Source, cb,\n\t\t\t\t)\n\n\t\t\t\tl.sendHTLCError(\n\t\t\t\t\tpd, NewLinkError(failure), obfuscator, false,\n\t\t\t\t)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Now that this add has been reprocessed, only append\n\t\t\t// it to our list of packets to forward to the switch\n\t\t\t// this is the first time processing the add. If the\n\t\t\t// fwd pkg has already been processed, then we entered\n\t\t\t// the above section to recreate a previous error.  If\n\t\t\t// the packet had previously been forwarded, it would\n\t\t\t// have been added to switchPackets at the top of this\n\t\t\t// section.\n\t\t\tif fwdPkg.State == channeldb.FwdStateLockedIn {\n\t\t\t\tupdatePacket := &htlcPacket{\n\t\t\t\t\tincomingChanID:  l.ShortChanID(),\n\t\t\t\t\tincomingHTLCID:  pd.HtlcIndex,\n\t\t\t\t\toutgoingChanID:  fwdInfo.NextHop,\n\t\t\t\t\tsourceRef:       pd.SourceRef,\n\t\t\t\t\tincomingAmount:  pd.Amount,\n\t\t\t\t\tamount:          addMsg.Amount,\n\t\t\t\t\thtlc:            addMsg,\n\t\t\t\t\tobfuscator:      obfuscator,\n\t\t\t\t\tincomingTimeout: pd.Timeout,\n\t\t\t\t\toutgoingTimeout: fwdInfo.OutgoingCTLV,\n\t\t\t\t\tcustomRecords:   pld.CustomRecords(),\n\t\t\t\t}\n\n\t\t\t\tfwdPkg.FwdFilter.Set(idx)\n\t\t\t\tswitchPackets = append(switchPackets,\n\t\t\t\t\tupdatePacket)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Commit the htlcs we are intending to forward if this package has not\n\t// been fully processed.\n\tif fwdPkg.State == channeldb.FwdStateLockedIn {\n\t\terr := l.channel.SetFwdFilter(fwdPkg.Height, fwdPkg.FwdFilter)\n\t\tif err != nil {\n\t\t\tl.fail(LinkFailureError{code: ErrInternalError},\n\t\t\t\t\"unable to set fwd filter: %v\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\tif len(switchPackets) == 0 {\n\t\treturn\n\t}\n\n\treplay := fwdPkg.State != channeldb.FwdStateLockedIn\n\n\tl.log.Debugf(\"forwarding %d packets to switch: replay=%v\",\n\t\tlen(switchPackets), replay)\n\n\t// NOTE: This call is made synchronous so that we ensure all circuits\n\t// are committed in the exact order that they are processed in the link.\n\t// Failing to do this could cause reorderings/gaps in the range of\n\t// opened circuits, which violates assumptions made by the circuit\n\t// trimming.\n\tl.forwardBatch(replay, switchPackets...)\n}\n\n// processExitHop handles an htlc for which this link is the exit hop. It\n// returns a boolean indicating whether the commitment tx needs an update.",
      "length": 9170,
      "tokens": 1169,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) processExitHop(pd *lnwallet.PaymentDescriptor,",
      "content": "func (l *channelLink) processExitHop(pd *lnwallet.PaymentDescriptor,\n\tobfuscator hop.ErrorEncrypter, fwdInfo hop.ForwardingInfo,\n\theightNow uint32, payload invoices.Payload) error {\n\n\t// If hodl.ExitSettle is requested, we will not validate the final hop's\n\t// ADD, nor will we settle the corresponding invoice or respond with the\n\t// preimage.\n\tif l.cfg.HodlMask.Active(hodl.ExitSettle) {\n\t\tl.log.Warnf(hodl.ExitSettle.Warning())\n\n\t\treturn nil\n\t}\n\n\t// As we're the exit hop, we'll double check the hop-payload included in\n\t// the HTLC to ensure that it was crafted correctly by the sender and\n\t// matches the HTLC we were extended.\n\tif pd.Amount != fwdInfo.AmountToForward {\n\n\t\tl.log.Errorf(\"onion payload of incoming htlc(%x) has incorrect \"+\n\t\t\t\"value: expected %v, got %v\", pd.RHash,\n\t\t\tpd.Amount, fwdInfo.AmountToForward)\n\n\t\tfailure := NewLinkError(\n\t\t\tlnwire.NewFinalIncorrectHtlcAmount(pd.Amount),\n\t\t)\n\t\tl.sendHTLCError(pd, failure, obfuscator, true)\n\n\t\treturn nil\n\t}\n\n\t// We'll also ensure that our time-lock value has been computed\n\t// correctly.\n\tif pd.Timeout != fwdInfo.OutgoingCTLV {\n\t\tl.log.Errorf(\"onion payload of incoming htlc(%x) has incorrect \"+\n\t\t\t\"time-lock: expected %v, got %v\",\n\t\t\tpd.RHash[:], pd.Timeout, fwdInfo.OutgoingCTLV)\n\n\t\tfailure := NewLinkError(\n\t\t\tlnwire.NewFinalIncorrectCltvExpiry(pd.Timeout),\n\t\t)\n\t\tl.sendHTLCError(pd, failure, obfuscator, true)\n\n\t\treturn nil\n\t}\n\n\t// Notify the invoiceRegistry of the exit hop htlc. If we crash right\n\t// after this, this code will be re-executed after restart. We will\n\t// receive back a resolution event.\n\tinvoiceHash := lntypes.Hash(pd.RHash)\n\n\tcircuitKey := models.CircuitKey{\n\t\tChanID: l.ShortChanID(),\n\t\tHtlcID: pd.HtlcIndex,\n\t}\n\n\tevent, err := l.cfg.Registry.NotifyExitHopHtlc(\n\t\tinvoiceHash, pd.Amount, pd.Timeout, int32(heightNow),\n\t\tcircuitKey, l.hodlQueue.ChanIn(), payload,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Create a hodlHtlc struct and decide either resolved now or later.\n\thtlc := hodlHtlc{\n\t\tpd:         pd,\n\t\tobfuscator: obfuscator,\n\t}\n\n\t// If the event is nil, the invoice is being held, so we save payment\n\t// descriptor for future reference.\n\tif event == nil {\n\t\tl.hodlMap[circuitKey] = htlc\n\t\treturn nil\n\t}\n\n\t// Process the received resolution.\n\treturn l.processHtlcResolution(event, htlc)\n}\n\n// settleHTLC settles the HTLC on the channel.",
      "length": 2190,
      "tokens": 287,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) settleHTLC(preimage lntypes.Preimage,",
      "content": "func (l *channelLink) settleHTLC(preimage lntypes.Preimage,\n\tpd *lnwallet.PaymentDescriptor) error {\n\n\thash := preimage.Hash()\n\n\tl.log.Infof(\"settling htlc %v as exit hop\", hash)\n\n\terr := l.channel.SettleHTLC(\n\t\tpreimage, pd.HtlcIndex, pd.SourceRef, nil, nil,\n\t)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to settle htlc: %v\", err)\n\t}\n\n\t// If the link is in hodl.BogusSettle mode, replace the preimage with a\n\t// fake one before sending it to the peer.\n\tif l.cfg.HodlMask.Active(hodl.BogusSettle) {\n\t\tl.log.Warnf(hodl.BogusSettle.Warning())\n\t\tpreimage = [32]byte{}\n\t\tcopy(preimage[:], bytes.Repeat([]byte{2}, 32))\n\t}\n\n\t// HTLC was successfully settled locally send notification about it\n\t// remote peer.\n\tl.cfg.Peer.SendMessage(false, &lnwire.UpdateFulfillHTLC{\n\t\tChanID:          l.ChanID(),\n\t\tID:              pd.HtlcIndex,\n\t\tPaymentPreimage: preimage,\n\t})\n\n\t// Once we have successfully settled the htlc, notify a settle event.\n\tl.cfg.HtlcNotifier.NotifySettleEvent(\n\t\tHtlcKey{\n\t\t\tIncomingCircuit: models.CircuitKey{\n\t\t\t\tChanID: l.ShortChanID(),\n\t\t\t\tHtlcID: pd.HtlcIndex,\n\t\t\t},\n\t\t},\n\t\tpreimage,\n\t\tHtlcEventTypeReceive,\n\t)\n\n\treturn nil\n}\n\n// forwardBatch forwards the given htlcPackets to the switch, and waits on the\n// err chan for the individual responses. This method is intended to be spawned\n// as a goroutine so the responses can be handled in the background.",
      "length": 1264,
      "tokens": 159,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) forwardBatch(replay bool, packets ...*htlcPacket) {",
      "content": "func (l *channelLink) forwardBatch(replay bool, packets ...*htlcPacket) {\n\t// Don't forward packets for which we already have a response in our\n\t// mailbox. This could happen if a packet fails and is buffered in the\n\t// mailbox, and the incoming link flaps.\n\tvar filteredPkts = make([]*htlcPacket, 0, len(packets))\n\tfor _, pkt := range packets {\n\t\tif l.mailBox.HasPacket(pkt.inKey()) {\n\t\t\tcontinue\n\t\t}\n\n\t\tfilteredPkts = append(filteredPkts, pkt)\n\t}\n\n\terr := l.cfg.ForwardPackets(l.quit, replay, filteredPkts...)\n\tif err != nil {\n\t\tlog.Errorf(\"Unhandled error while reforwarding htlc \"+\n\t\t\t\"settle/fail over htlcswitch: %v\", err)\n\t}\n}\n\n// sendHTLCError functions cancels HTLC and send cancel message back to the\n// peer from which HTLC was received.",
      "length": 654,
      "tokens": 99,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) sendHTLCError(pd *lnwallet.PaymentDescriptor,",
      "content": "func (l *channelLink) sendHTLCError(pd *lnwallet.PaymentDescriptor,\n\tfailure *LinkError, e hop.ErrorEncrypter, isReceive bool) {\n\n\treason, err := e.EncryptFirstHop(failure.WireMessage())\n\tif err != nil {\n\t\tl.log.Errorf(\"unable to obfuscate error: %v\", err)\n\t\treturn\n\t}\n\n\terr = l.channel.FailHTLC(pd.HtlcIndex, reason, pd.SourceRef, nil, nil)\n\tif err != nil {\n\t\tl.log.Errorf(\"unable cancel htlc: %v\", err)\n\t\treturn\n\t}\n\n\tl.cfg.Peer.SendMessage(false, &lnwire.UpdateFailHTLC{\n\t\tChanID: l.ChanID(),\n\t\tID:     pd.HtlcIndex,\n\t\tReason: reason,\n\t})\n\n\t// Notify a link failure on our incoming link. Outgoing htlc information\n\t// is not available at this point, because we have not decrypted the\n\t// onion, so it is excluded.\n\tvar eventType HtlcEventType\n\tif isReceive {\n\t\teventType = HtlcEventTypeReceive\n\t} else {\n\t\teventType = HtlcEventTypeForward\n\t}\n\n\tl.cfg.HtlcNotifier.NotifyLinkFailEvent(\n\t\tHtlcKey{\n\t\t\tIncomingCircuit: models.CircuitKey{\n\t\t\t\tChanID: l.ShortChanID(),\n\t\t\t\tHtlcID: pd.HtlcIndex,\n\t\t\t},\n\t\t},\n\t\tHtlcInfo{\n\t\t\tIncomingTimeLock: pd.Timeout,\n\t\t\tIncomingAmt:      pd.Amount,\n\t\t},\n\t\teventType,\n\t\tfailure,\n\t\ttrue,\n\t)\n}\n\n// sendMalformedHTLCError helper function which sends the malformed HTLC update\n// to the payment sender.",
      "length": 1111,
      "tokens": 135,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) sendMalformedHTLCError(htlcIndex uint64,",
      "content": "func (l *channelLink) sendMalformedHTLCError(htlcIndex uint64,\n\tcode lnwire.FailCode, onionBlob []byte, sourceRef *channeldb.AddRef) {\n\n\tshaOnionBlob := sha256.Sum256(onionBlob)\n\terr := l.channel.MalformedFailHTLC(htlcIndex, code, shaOnionBlob, sourceRef)\n\tif err != nil {\n\t\tl.log.Errorf(\"unable cancel htlc: %v\", err)\n\t\treturn\n\t}\n\n\tl.cfg.Peer.SendMessage(false, &lnwire.UpdateFailMalformedHTLC{\n\t\tChanID:       l.ChanID(),\n\t\tID:           htlcIndex,\n\t\tShaOnionBlob: shaOnionBlob,\n\t\tFailureCode:  code,\n\t})\n}\n\n// fail is a function which is used to encapsulate the action necessary for\n// properly failing the link. It takes a LinkFailureError, which will be passed\n// to the OnChannelFailure closure, in order for it to determine if we should\n// force close the channel, and if we should send an error message to the\n// remote peer.",
      "length": 749,
      "tokens": 99,
      "embedding": []
    },
    {
      "slug": "func (l *channelLink) fail(linkErr LinkFailureError,",
      "content": "func (l *channelLink) fail(linkErr LinkFailureError,\n\tformat string, a ...interface{}) {\n\treason := errors.Errorf(format, a...)\n\n\t// Return if we have already notified about a failure.\n\tif l.failed {\n\t\tl.log.Warnf(\"ignoring link failure (%v), as link already \"+\n\t\t\t\"failed\", reason)\n\t\treturn\n\t}\n\n\tl.log.Errorf(\"failing link: %s with error: %v\", reason, linkErr)\n\n\t// Set failed, such that we won't process any more updates, and notify\n\t// the peer about the failure.\n\tl.failed = true\n\tl.cfg.OnChannelFailure(l.ChanID(), l.ShortChanID(), linkErr)\n}\n",
      "length": 478,
      "tokens": 68,
      "embedding": []
    }
  ]
}