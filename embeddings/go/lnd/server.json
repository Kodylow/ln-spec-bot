{
  "filepath": "../implementations/go/lnd/server.go",
  "package": "lnd",
  "sections": [
    {
      "slug": "type errPeerAlreadyConnected struct {",
      "content": "type errPeerAlreadyConnected struct {\n\tpeer *peer.Brontide\n}\n\n// Error returns the human readable version of this error type.\n//\n// NOTE: Part of the error interface.",
      "length": 123,
      "tokens": 22,
      "embedding": []
    },
    {
      "slug": "func (e *errPeerAlreadyConnected) Error() string {",
      "content": "func (e *errPeerAlreadyConnected) Error() string {\n\treturn fmt.Sprintf(\"already connected to peer: %v\", e.peer)\n}\n\n// server is the main server of the Lightning Network Daemon. The server houses\n// global state pertaining to the wallet, database, and the rpcserver.\n// Additionally, the server is also used as a central messaging bus to interact\n// with any of its companion objects.",
      "length": 326,
      "tokens": 54,
      "embedding": []
    },
    {
      "slug": "type server struct {",
      "content": "type server struct {\n\tactive   int32 // atomic\n\tstopping int32 // atomic\n\n\tstart sync.Once\n\tstop  sync.Once\n\n\tcfg *Config\n\n\t// identityECDH is an ECDH capable wrapper for the private key used\n\t// to authenticate any incoming connections.\n\tidentityECDH keychain.SingleKeyECDH\n\n\t// identityKeyLoc is the key locator for the above wrapped identity key.\n\tidentityKeyLoc keychain.KeyLocator\n\n\t// nodeSigner is an implementation of the MessageSigner implementation\n\t// that's backed by the identity private key of the running lnd node.\n\tnodeSigner *netann.NodeSigner\n\n\tchanStatusMgr *netann.ChanStatusManager\n\n\t// listenAddrs is the list of addresses the server is currently\n\t// listening on.\n\tlistenAddrs []net.Addr\n\n\t// torController is a client that will communicate with a locally\n\t// running Tor server. This client will handle initiating and\n\t// authenticating the connection to the Tor server, automatically\n\t// creating and setting up onion services, etc.\n\ttorController *tor.Controller\n\n\t// natTraversal is the specific NAT traversal technique used to\n\t// automatically set up port forwarding rules in order to advertise to\n\t// the network that the node is accepting inbound connections.\n\tnatTraversal nat.Traversal\n\n\t// lastDetectedIP is the last IP detected by the NAT traversal technique\n\t// above. This IP will be watched periodically in a goroutine in order\n\t// to handle dynamic IP changes.\n\tlastDetectedIP net.IP\n\n\tmu         sync.RWMutex\n\tpeersByPub map[string]*peer.Brontide\n\n\tinboundPeers  map[string]*peer.Brontide\n\toutboundPeers map[string]*peer.Brontide\n\n\tpeerConnectedListeners    map[string][]chan<- lnpeer.Peer\n\tpeerDisconnectedListeners map[string][]chan<- struct{}\n\n\t// TODO(yy): the Brontide.Start doesn't know this value, which means it\n\t// will continue to send messages even if there are no active channels\n\t// and the value below is false. Once it's pruned, all its connections\n\t// will be closed, thus the Brontide.Start will return an error.\n\tpersistentPeers        map[string]bool\n\tpersistentPeersBackoff map[string]time.Duration\n\tpersistentPeerAddrs    map[string][]*lnwire.NetAddress\n\tpersistentConnReqs     map[string][]*connmgr.ConnReq\n\tpersistentRetryCancels map[string]chan struct{}\n\n\t// peerErrors keeps a set of peer error buffers for peers that have\n\t// disconnected from us. This allows us to track historic peer errors\n\t// over connections. The string of the peer's compressed pubkey is used\n\t// as a key for this map.\n\tpeerErrors map[string]*queue.CircularBuffer\n\n\t// ignorePeerTermination tracks peers for which the server has initiated\n\t// a disconnect. Adding a peer to this map causes the peer termination\n\t// watcher to short circuit in the event that peers are purposefully\n\t// disconnected.\n\tignorePeerTermination map[*peer.Brontide]struct{}\n\n\t// scheduledPeerConnection maps a pubkey string to a callback that\n\t// should be executed in the peerTerminationWatcher the prior peer with\n\t// the same pubkey exits.  This allows the server to wait until the\n\t// prior peer has cleaned up successfully, before adding the new peer\n\t// intended to replace it.\n\tscheduledPeerConnection map[string]func()\n\n\t// pongBuf is a shared pong reply buffer we'll use across all active\n\t// peer goroutines. We know the max size of a pong message\n\t// (lnwire.MaxPongBytes), so we can allocate this ahead of time, and\n\t// avoid allocations each time we need to send a pong message.\n\tpongBuf []byte\n\n\tcc *chainreg.ChainControl\n\n\tfundingMgr *funding.Manager\n\n\tgraphDB *channeldb.ChannelGraph\n\n\tchanStateDB *channeldb.ChannelStateDB\n\n\taddrSource chanbackup.AddressSource\n\n\t// miscDB is the DB that contains all \"other\" databases within the main\n\t// channel DB that haven't been separated out yet.\n\tmiscDB *channeldb.DB\n\n\taliasMgr *aliasmgr.Manager\n\n\thtlcSwitch *htlcswitch.Switch\n\n\tinterceptableSwitch *htlcswitch.InterceptableSwitch\n\n\tinvoices *invoices.InvoiceRegistry\n\n\tchannelNotifier *channelnotifier.ChannelNotifier\n\n\tpeerNotifier *peernotifier.PeerNotifier\n\n\thtlcNotifier *htlcswitch.HtlcNotifier\n\n\twitnessBeacon contractcourt.WitnessBeacon\n\n\tbreachArbiter *contractcourt.BreachArbiter\n\n\tmissionControl *routing.MissionControl\n\n\tchanRouter *routing.ChannelRouter\n\n\tcontrolTower routing.ControlTower\n\n\tauthGossiper *discovery.AuthenticatedGossiper\n\n\tlocalChanMgr *localchans.Manager\n\n\tutxoNursery *contractcourt.UtxoNursery\n\n\tsweeper *sweep.UtxoSweeper\n\n\tchainArb *contractcourt.ChainArbitrator\n\n\tsphinx *hop.OnionProcessor\n\n\ttowerClient wtclient.Client\n\n\tanchorTowerClient wtclient.Client\n\n\tconnMgr *connmgr.ConnManager\n\n\tsigPool *lnwallet.SigPool\n\n\twritePool *pool.Write\n\n\treadPool *pool.Read\n\n\ttlsManager *TLSManager\n\n\t// featureMgr dispatches feature vectors for various contexts within the\n\t// daemon.\n\tfeatureMgr *feature.Manager\n\n\t// currentNodeAnn is the node announcement that has been broadcast to\n\t// the network upon startup, if the attributes of the node (us) has\n\t// changed since last start.\n\tcurrentNodeAnn *lnwire.NodeAnnouncement\n\n\t// chansToRestore is the set of channels that upon starting, the server\n\t// should attempt to restore/recover.\n\tchansToRestore walletunlocker.ChannelsToRecover\n\n\t// chanSubSwapper is a sub-system that will ensure our on-disk channel\n\t// backups are consistent at all times. It interacts with the\n\t// channelNotifier to be notified of newly opened and closed channels.\n\tchanSubSwapper *chanbackup.SubSwapper\n\n\t// chanEventStore tracks the behaviour of channels and their remote peers to\n\t// provide insights into their health and performance.\n\tchanEventStore *chanfitness.ChannelEventStore\n\n\thostAnn *netann.HostAnnouncer\n\n\t// livenessMonitor monitors that lnd has access to critical resources.\n\tlivenessMonitor *healthcheck.Monitor\n\n\tcustomMessageServer *subscribe.Server\n\n\tquit chan struct{}\n\n\twg sync.WaitGroup\n}\n\n// updatePersistentPeerAddrs subscribes to topology changes and stores\n// advertised addresses for any NodeAnnouncements from our persisted peers.",
      "length": 5736,
      "tokens": 703,
      "embedding": []
    },
    {
      "slug": "func (s *server) updatePersistentPeerAddrs() error {",
      "content": "func (s *server) updatePersistentPeerAddrs() error {\n\tgraphSub, err := s.chanRouter.SubscribeTopology()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\ts.wg.Add(1)\n\tgo func() {\n\t\tdefer func() {\n\t\t\tgraphSub.Cancel()\n\t\t\ts.wg.Done()\n\t\t}()\n\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-s.quit:\n\t\t\t\treturn\n\n\t\t\tcase topChange, ok := <-graphSub.TopologyChanges:\n\t\t\t\t// If the router is shutting down, then we will\n\t\t\t\t// as well.\n\t\t\t\tif !ok {\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tfor _, update := range topChange.NodeUpdates {\n\t\t\t\t\tpubKeyStr := string(\n\t\t\t\t\t\tupdate.IdentityKey.\n\t\t\t\t\t\t\tSerializeCompressed(),\n\t\t\t\t\t)\n\n\t\t\t\t\t// We only care about updates from\n\t\t\t\t\t// our persistentPeers.\n\t\t\t\t\ts.mu.RLock()\n\t\t\t\t\t_, ok := s.persistentPeers[pubKeyStr]\n\t\t\t\t\ts.mu.RUnlock()\n\t\t\t\t\tif !ok {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\n\t\t\t\t\taddrs := make([]*lnwire.NetAddress, 0,\n\t\t\t\t\t\tlen(update.Addresses))\n\n\t\t\t\t\tfor _, addr := range update.Addresses {\n\t\t\t\t\t\taddrs = append(addrs,\n\t\t\t\t\t\t\t&lnwire.NetAddress{\n\t\t\t\t\t\t\t\tIdentityKey: update.IdentityKey,\n\t\t\t\t\t\t\t\tAddress:     addr,\n\t\t\t\t\t\t\t\tChainNet:    s.cfg.ActiveNetParams.Net,\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t)\n\t\t\t\t\t}\n\n\t\t\t\t\ts.mu.Lock()\n\n\t\t\t\t\t// Update the stored addresses for this\n\t\t\t\t\t// to peer to reflect the new set.\n\t\t\t\t\ts.persistentPeerAddrs[pubKeyStr] = addrs\n\n\t\t\t\t\t// If there are no outstanding\n\t\t\t\t\t// connection requests for this peer\n\t\t\t\t\t// then our work is done since we are\n\t\t\t\t\t// not currently trying to connect to\n\t\t\t\t\t// them.\n\t\t\t\t\tif len(s.persistentConnReqs[pubKeyStr]) == 0 {\n\t\t\t\t\t\ts.mu.Unlock()\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\n\t\t\t\t\ts.mu.Unlock()\n\n\t\t\t\t\ts.connectToPersistentPeer(pubKeyStr)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn nil\n}\n\n// CustomMessage is a custom message that is received from a peer.",
      "length": 1526,
      "tokens": 189,
      "embedding": []
    },
    {
      "slug": "type CustomMessage struct {",
      "content": "type CustomMessage struct {\n\t// Peer is the peer pubkey\n\tPeer [33]byte\n\n\t// Msg is the custom wire message.\n\tMsg *lnwire.Custom\n}\n\n// parseAddr parses an address from its string format to a net.Addr.",
      "length": 164,
      "tokens": 30,
      "embedding": []
    },
    {
      "slug": "func parseAddr(address string, netCfg tor.Net) (net.Addr, error) {",
      "content": "func parseAddr(address string, netCfg tor.Net) (net.Addr, error) {\n\tvar (\n\t\thost string\n\t\tport int\n\t)\n\n\t// Split the address into its host and port components.\n\th, p, err := net.SplitHostPort(address)\n\tif err != nil {\n\t\t// If a port wasn't specified, we'll assume the address only\n\t\t// contains the host so we'll use the default port.\n\t\thost = address\n\t\tport = defaultPeerPort\n\t} else {\n\t\t// Otherwise, we'll note both the host and ports.\n\t\thost = h\n\t\tportNum, err := strconv.Atoi(p)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tport = portNum\n\t}\n\n\tif tor.IsOnionHost(host) {\n\t\treturn &tor.OnionAddr{OnionService: host, Port: port}, nil\n\t}\n\n\t// If the host is part of a TCP address, we'll use the network\n\t// specific ResolveTCPAddr function in order to resolve these\n\t// addresses over Tor in order to prevent leaking your real IP\n\t// address.\n\thostPort := net.JoinHostPort(host, strconv.Itoa(port))\n\treturn netCfg.ResolveTCPAddr(\"tcp\", hostPort)\n}\n\n// noiseDial is a factory function which creates a connmgr compliant dialing\n// function by returning a closure which includes the server's identity key.",
      "length": 1000,
      "tokens": 165,
      "embedding": []
    },
    {
      "slug": "func noiseDial(idKey keychain.SingleKeyECDH,",
      "content": "func noiseDial(idKey keychain.SingleKeyECDH,\n\tnetCfg tor.Net, timeout time.Duration) func(net.Addr) (net.Conn, error) {\n\n\treturn func(a net.Addr) (net.Conn, error) {\n\t\tlnAddr := a.(*lnwire.NetAddress)\n\t\treturn brontide.Dial(idKey, lnAddr, timeout, netCfg.Dial)\n\t}\n}\n\n// newServer creates a new instance of the server which is to listen using the\n// passed listener address.",
      "length": 319,
      "tokens": 43,
      "embedding": []
    },
    {
      "slug": "func newServer(cfg *Config, listenAddrs []net.Addr,",
      "content": "func newServer(cfg *Config, listenAddrs []net.Addr,\n\tdbs *DatabaseInstances, cc *chainreg.ChainControl,\n\tnodeKeyDesc *keychain.KeyDescriptor,\n\tchansToRestore walletunlocker.ChannelsToRecover,\n\tchanPredicate chanacceptor.ChannelAcceptor,\n\ttorController *tor.Controller, tlsManager *TLSManager) (*server,\n\terror) {\n\n\tvar (\n\t\terr         error\n\t\tnodeKeyECDH = keychain.NewPubKeyECDH(*nodeKeyDesc, cc.KeyRing)\n\n\t\t// We just derived the full descriptor, so we know the public\n\t\t// key is set on it.\n\t\tnodeKeySigner = keychain.NewPubKeyMessageSigner(\n\t\t\tnodeKeyDesc.PubKey, nodeKeyDesc.KeyLocator, cc.KeyRing,\n\t\t)\n\t)\n\n\tlisteners := make([]net.Listener, len(listenAddrs))\n\tfor i, listenAddr := range listenAddrs {\n\t\t// Note: though brontide.NewListener uses ResolveTCPAddr, it\n\t\t// doesn't need to call the general lndResolveTCP function\n\t\t// since we are resolving a local address.\n\t\tlisteners[i], err = brontide.NewListener(\n\t\t\tnodeKeyECDH, listenAddr.String(),\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tvar serializedPubKey [33]byte\n\tcopy(serializedPubKey[:], nodeKeyDesc.PubKey.SerializeCompressed())\n\n\t// Initialize the sphinx router.\n\treplayLog := htlcswitch.NewDecayedLog(\n\t\tdbs.DecayedLogDB, cc.ChainNotifier,\n\t)\n\tsphinxRouter := sphinx.NewRouter(\n\t\tnodeKeyECDH, cfg.ActiveNetParams.Params, replayLog,\n\t)\n\n\twriteBufferPool := pool.NewWriteBuffer(\n\t\tpool.DefaultWriteBufferGCInterval,\n\t\tpool.DefaultWriteBufferExpiryInterval,\n\t)\n\n\twritePool := pool.NewWrite(\n\t\twriteBufferPool, cfg.Workers.Write, pool.DefaultWorkerTimeout,\n\t)\n\n\treadBufferPool := pool.NewReadBuffer(\n\t\tpool.DefaultReadBufferGCInterval,\n\t\tpool.DefaultReadBufferExpiryInterval,\n\t)\n\n\treadPool := pool.NewRead(\n\t\treadBufferPool, cfg.Workers.Read, pool.DefaultWorkerTimeout,\n\t)\n\n\tfeatureMgr, err := feature.NewManager(feature.Config{\n\t\tNoTLVOnion:               cfg.ProtocolOptions.LegacyOnion(),\n\t\tNoStaticRemoteKey:        cfg.ProtocolOptions.NoStaticRemoteKey(),\n\t\tNoAnchors:                cfg.ProtocolOptions.NoAnchorCommitments(),\n\t\tNoWumbo:                  !cfg.ProtocolOptions.Wumbo(),\n\t\tNoScriptEnforcementLease: cfg.ProtocolOptions.NoScriptEnforcementLease(),\n\t\tNoKeysend:                !cfg.AcceptKeySend,\n\t\tNoOptionScidAlias:        !cfg.ProtocolOptions.ScidAlias(),\n\t\tNoZeroConf:               !cfg.ProtocolOptions.ZeroConf(),\n\t\tNoAnySegwit:              cfg.ProtocolOptions.NoAnySegwit(),\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tregistryConfig := invoices.RegistryConfig{\n\t\tFinalCltvRejectDelta:        lncfg.DefaultFinalCltvRejectDelta,\n\t\tHtlcHoldDuration:            invoices.DefaultHtlcHoldDuration,\n\t\tClock:                       clock.NewDefaultClock(),\n\t\tAcceptKeySend:               cfg.AcceptKeySend,\n\t\tAcceptAMP:                   cfg.AcceptAMP,\n\t\tGcCanceledInvoicesOnStartup: cfg.GcCanceledInvoicesOnStartup,\n\t\tGcCanceledInvoicesOnTheFly:  cfg.GcCanceledInvoicesOnTheFly,\n\t\tKeysendHoldTime:             cfg.KeysendHoldTime,\n\t}\n\n\ts := &server{\n\t\tcfg:            cfg,\n\t\tgraphDB:        dbs.GraphDB.ChannelGraph(),\n\t\tchanStateDB:    dbs.ChanStateDB.ChannelStateDB(),\n\t\taddrSource:     dbs.ChanStateDB,\n\t\tmiscDB:         dbs.ChanStateDB,\n\t\tcc:             cc,\n\t\tsigPool:        lnwallet.NewSigPool(cfg.Workers.Sig, cc.Signer),\n\t\twritePool:      writePool,\n\t\treadPool:       readPool,\n\t\tchansToRestore: chansToRestore,\n\n\t\tchannelNotifier: channelnotifier.New(\n\t\t\tdbs.ChanStateDB.ChannelStateDB(),\n\t\t),\n\n\t\tidentityECDH:   nodeKeyECDH,\n\t\tidentityKeyLoc: nodeKeyDesc.KeyLocator,\n\t\tnodeSigner:     netann.NewNodeSigner(nodeKeySigner),\n\n\t\tlistenAddrs: listenAddrs,\n\n\t\t// TODO(roasbeef): derive proper onion key based on rotation\n\t\t// schedule\n\t\tsphinx: hop.NewOnionProcessor(sphinxRouter),\n\n\t\ttorController: torController,\n\n\t\tpersistentPeers:         make(map[string]bool),\n\t\tpersistentPeersBackoff:  make(map[string]time.Duration),\n\t\tpersistentConnReqs:      make(map[string][]*connmgr.ConnReq),\n\t\tpersistentPeerAddrs:     make(map[string][]*lnwire.NetAddress),\n\t\tpersistentRetryCancels:  make(map[string]chan struct{}),\n\t\tpeerErrors:              make(map[string]*queue.CircularBuffer),\n\t\tignorePeerTermination:   make(map[*peer.Brontide]struct{}),\n\t\tscheduledPeerConnection: make(map[string]func()),\n\t\tpongBuf:                 make([]byte, lnwire.MaxPongBytes),\n\n\t\tpeersByPub:                make(map[string]*peer.Brontide),\n\t\tinboundPeers:              make(map[string]*peer.Brontide),\n\t\toutboundPeers:             make(map[string]*peer.Brontide),\n\t\tpeerConnectedListeners:    make(map[string][]chan<- lnpeer.Peer),\n\t\tpeerDisconnectedListeners: make(map[string][]chan<- struct{}),\n\n\t\tcustomMessageServer: subscribe.NewServer(),\n\n\t\ttlsManager: tlsManager,\n\n\t\tfeatureMgr: featureMgr,\n\t\tquit:       make(chan struct{}),\n\t}\n\n\tcurrentHash, currentHeight, err := s.cc.ChainIO.GetBestBlock()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\texpiryWatcher := invoices.NewInvoiceExpiryWatcher(\n\t\tclock.NewDefaultClock(), cfg.Invoices.HoldExpiryDelta,\n\t\tuint32(currentHeight), currentHash, cc.ChainNotifier,\n\t)\n\ts.invoices = invoices.NewRegistry(\n\t\tdbs.InvoiceDB, expiryWatcher, &registryConfig,\n\t)\n\n\ts.htlcNotifier = htlcswitch.NewHtlcNotifier(time.Now)\n\n\tthresholdSats := btcutil.Amount(cfg.DustThreshold)\n\tthresholdMSats := lnwire.NewMSatFromSatoshis(thresholdSats)\n\n\ts.aliasMgr, err = aliasmgr.NewManager(dbs.ChanStateDB)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ts.htlcSwitch, err = htlcswitch.New(htlcswitch.Config{\n\t\tDB:                   dbs.ChanStateDB,\n\t\tFetchAllOpenChannels: s.chanStateDB.FetchAllOpenChannels,\n\t\tFetchAllChannels:     s.chanStateDB.FetchAllChannels,\n\t\tFetchClosedChannels:  s.chanStateDB.FetchClosedChannels,\n\t\tLocalChannelClose: func(pubKey []byte,\n\t\t\trequest *htlcswitch.ChanClose) {\n\n\t\t\tpeer, err := s.FindPeerByPubStr(string(pubKey))\n\t\t\tif err != nil {\n\t\t\t\tsrvrLog.Errorf(\"unable to close channel, peer\"+\n\t\t\t\t\t\" with %v id can't be found: %v\",\n\t\t\t\t\tpubKey, err,\n\t\t\t\t)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tpeer.HandleLocalCloseChanReqs(request)\n\t\t},\n\t\tFwdingLog:              dbs.ChanStateDB.ForwardingLog(),\n\t\tSwitchPackager:         channeldb.NewSwitchPackager(),\n\t\tExtractErrorEncrypter:  s.sphinx.ExtractErrorEncrypter,\n\t\tFetchLastChannelUpdate: s.fetchLastChanUpdate(),\n\t\tNotifier:               s.cc.ChainNotifier,\n\t\tHtlcNotifier:           s.htlcNotifier,\n\t\tFwdEventTicker:         ticker.New(htlcswitch.DefaultFwdEventInterval),\n\t\tLogEventTicker:         ticker.New(htlcswitch.DefaultLogInterval),\n\t\tAckEventTicker:         ticker.New(htlcswitch.DefaultAckInterval),\n\t\tAllowCircularRoute:     cfg.AllowCircularRoute,\n\t\tRejectHTLC:             cfg.RejectHTLC,\n\t\tClock:                  clock.NewDefaultClock(),\n\t\tMailboxDeliveryTimeout: cfg.Htlcswitch.MailboxDeliveryTimeout,\n\t\tDustThreshold:          thresholdMSats,\n\t\tSignAliasUpdate:        s.signAliasUpdate,\n\t\tIsAlias:                aliasmgr.IsAlias,\n\t}, uint32(currentHeight))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ts.interceptableSwitch, err = htlcswitch.NewInterceptableSwitch(\n\t\t&htlcswitch.InterceptableSwitchConfig{\n\t\t\tSwitch:             s.htlcSwitch,\n\t\t\tCltvRejectDelta:    lncfg.DefaultFinalCltvRejectDelta,\n\t\t\tCltvInterceptDelta: lncfg.DefaultCltvInterceptDelta,\n\t\t\tRequireInterceptor: s.cfg.RequireInterceptor,\n\t\t\tNotifier:           s.cc.ChainNotifier,\n\t\t},\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ts.witnessBeacon = newPreimageBeacon(\n\t\tdbs.ChanStateDB.NewWitnessCache(),\n\t\ts.interceptableSwitch.ForwardPacket,\n\t)\n\n\tchanStatusMgrCfg := &netann.ChanStatusConfig{\n\t\tChanStatusSampleInterval: cfg.ChanStatusSampleInterval,\n\t\tChanEnableTimeout:        cfg.ChanEnableTimeout,\n\t\tChanDisableTimeout:       cfg.ChanDisableTimeout,\n\t\tOurPubKey:                nodeKeyDesc.PubKey,\n\t\tOurKeyLoc:                nodeKeyDesc.KeyLocator,\n\t\tMessageSigner:            s.nodeSigner,\n\t\tIsChannelActive:          s.htlcSwitch.HasActiveLink,\n\t\tApplyChannelUpdate:       s.applyChannelUpdate,\n\t\tDB:                       s.chanStateDB,\n\t\tGraph:                    dbs.GraphDB.ChannelGraph(),\n\t}\n\n\tchanStatusMgr, err := netann.NewChanStatusManager(chanStatusMgrCfg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ts.chanStatusMgr = chanStatusMgr\n\n\t// If enabled, use either UPnP or NAT-PMP to automatically configure\n\t// port forwarding for users behind a NAT.\n\tif cfg.NAT {\n\t\tsrvrLog.Info(\"Scanning local network for a UPnP enabled device\")\n\n\t\tdiscoveryTimeout := time.Duration(10 * time.Second)\n\n\t\tctx, cancel := context.WithTimeout(\n\t\t\tcontext.Background(), discoveryTimeout,\n\t\t)\n\t\tdefer cancel()\n\t\tupnp, err := nat.DiscoverUPnP(ctx)\n\t\tif err == nil {\n\t\t\ts.natTraversal = upnp\n\t\t} else {\n\t\t\t// If we were not able to discover a UPnP enabled device\n\t\t\t// on the local network, we'll fall back to attempting\n\t\t\t// to discover a NAT-PMP enabled device.\n\t\t\tsrvrLog.Errorf(\"Unable to discover a UPnP enabled \"+\n\t\t\t\t\"device on the local network: %v\", err)\n\n\t\t\tsrvrLog.Info(\"Scanning local network for a NAT-PMP \" +\n\t\t\t\t\"enabled device\")\n\n\t\t\tpmp, err := nat.DiscoverPMP(discoveryTimeout)\n\t\t\tif err != nil {\n\t\t\t\terr := fmt.Errorf(\"unable to discover a \"+\n\t\t\t\t\t\"NAT-PMP enabled device on the local \"+\n\t\t\t\t\t\"network: %v\", err)\n\t\t\t\tsrvrLog.Error(err)\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\ts.natTraversal = pmp\n\t\t}\n\t}\n\n\t// If we were requested to automatically configure port forwarding,\n\t// we'll use the ports that the server will be listening on.\n\texternalIPStrings := make([]string, len(cfg.ExternalIPs))\n\tfor idx, ip := range cfg.ExternalIPs {\n\t\texternalIPStrings[idx] = ip.String()\n\t}\n\tif s.natTraversal != nil {\n\t\tlistenPorts := make([]uint16, 0, len(listenAddrs))\n\t\tfor _, listenAddr := range listenAddrs {\n\t\t\t// At this point, the listen addresses should have\n\t\t\t// already been normalized, so it's safe to ignore the\n\t\t\t// errors.\n\t\t\t_, portStr, _ := net.SplitHostPort(listenAddr.String())\n\t\t\tport, _ := strconv.Atoi(portStr)\n\n\t\t\tlistenPorts = append(listenPorts, uint16(port))\n\t\t}\n\n\t\tips, err := s.configurePortForwarding(listenPorts...)\n\t\tif err != nil {\n\t\t\tsrvrLog.Errorf(\"Unable to automatically set up port \"+\n\t\t\t\t\"forwarding using %s: %v\",\n\t\t\t\ts.natTraversal.Name(), err)\n\t\t} else {\n\t\t\tsrvrLog.Infof(\"Automatically set up port forwarding \"+\n\t\t\t\t\"using %s to advertise external IP\",\n\t\t\t\ts.natTraversal.Name())\n\t\t\texternalIPStrings = append(externalIPStrings, ips...)\n\t\t}\n\t}\n\n\t// If external IP addresses have been specified, add those to the list\n\t// of this server's addresses.\n\texternalIPs, err := lncfg.NormalizeAddresses(\n\t\texternalIPStrings, strconv.Itoa(defaultPeerPort),\n\t\tcfg.net.ResolveTCPAddr,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tselfAddrs := make([]net.Addr, 0, len(externalIPs))\n\tselfAddrs = append(selfAddrs, externalIPs...)\n\n\t// As the graph can be obtained at anytime from the network, we won't\n\t// replicate it, and instead it'll only be stored locally.\n\tchanGraph := dbs.GraphDB.ChannelGraph()\n\n\t// We'll now reconstruct a node announcement based on our current\n\t// configuration so we can send it out as a sort of heart beat within\n\t// the network.\n\t//\n\t// We'll start by parsing the node color from configuration.\n\tcolor, err := lncfg.ParseHexColor(cfg.Color)\n\tif err != nil {\n\t\tsrvrLog.Errorf(\"unable to parse color: %v\\n\", err)\n\t\treturn nil, err\n\t}\n\n\t// If no alias is provided, default to first 10 characters of public\n\t// key.\n\talias := cfg.Alias\n\tif alias == \"\" {\n\t\talias = hex.EncodeToString(serializedPubKey[:10])\n\t}\n\tnodeAlias, err := lnwire.NewNodeAlias(alias)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tselfNode := &channeldb.LightningNode{\n\t\tHaveNodeAnnouncement: true,\n\t\tLastUpdate:           time.Now(),\n\t\tAddresses:            selfAddrs,\n\t\tAlias:                nodeAlias.String(),\n\t\tFeatures:             s.featureMgr.Get(feature.SetNodeAnn),\n\t\tColor:                color,\n\t}\n\tcopy(selfNode.PubKeyBytes[:], nodeKeyDesc.PubKey.SerializeCompressed())\n\n\t// Based on the disk representation of the node announcement generated\n\t// above, we'll generate a node announcement that can go out on the\n\t// network so we can properly sign it.\n\tnodeAnn, err := selfNode.NodeAnnouncement(false)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to gen self node ann: %v\", err)\n\t}\n\n\t// With the announcement generated, we'll sign it to properly\n\t// authenticate the message on the network.\n\tauthSig, err := netann.SignAnnouncement(\n\t\ts.nodeSigner, nodeKeyDesc.KeyLocator, nodeAnn,\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to generate signature for \"+\n\t\t\t\"self node announcement: %v\", err)\n\t}\n\tselfNode.AuthSigBytes = authSig.Serialize()\n\tnodeAnn.Signature, err = lnwire.NewSigFromRawSignature(\n\t\tselfNode.AuthSigBytes,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Finally, we'll update the representation on disk, and update our\n\t// cached in-memory version as well.\n\tif err := chanGraph.SetSourceNode(selfNode); err != nil {\n\t\treturn nil, fmt.Errorf(\"can't set self node: %v\", err)\n\t}\n\ts.currentNodeAnn = nodeAnn\n\n\t// The router will get access to the payment ID sequencer, such that it\n\t// can generate unique payment IDs.\n\tsequencer, err := htlcswitch.NewPersistentSequencer(dbs.ChanStateDB)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Instantiate mission control with config from the sub server.\n\t//\n\t// TODO(joostjager): When we are further in the process of moving to sub\n\t// servers, the mission control instance itself can be moved there too.\n\troutingConfig := routerrpc.GetRoutingConfig(cfg.SubRPCServers.RouterRPC)\n\n\t// We only initialize a probability estimator if there's no custom one.\n\tvar estimator routing.Estimator\n\tif cfg.Estimator != nil {\n\t\testimator = cfg.Estimator\n\t} else {\n\t\tswitch routingConfig.ProbabilityEstimatorType {\n\t\tcase routing.AprioriEstimatorName:\n\t\t\taCfg := routingConfig.AprioriConfig\n\t\t\taprioriConfig := routing.AprioriConfig{\n\t\t\t\tAprioriHopProbability: aCfg.HopProbability,\n\t\t\t\tPenaltyHalfLife:       aCfg.PenaltyHalfLife,\n\t\t\t\tAprioriWeight:         aCfg.Weight,\n\t\t\t\tCapacityFraction:      aCfg.CapacityFraction,\n\t\t\t}\n\n\t\t\testimator, err = routing.NewAprioriEstimator(\n\t\t\t\taprioriConfig,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\tcase routing.BimodalEstimatorName:\n\t\t\tbCfg := routingConfig.BimodalConfig\n\t\t\tbimodalConfig := routing.BimodalConfig{\n\t\t\t\tBimodalNodeWeight: bCfg.NodeWeight,\n\t\t\t\tBimodalScaleMsat: lnwire.MilliSatoshi(\n\t\t\t\t\tbCfg.Scale,\n\t\t\t\t),\n\t\t\t\tBimodalDecayTime: bCfg.DecayTime,\n\t\t\t}\n\n\t\t\testimator, err = routing.NewBimodalEstimator(\n\t\t\t\tbimodalConfig,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"unknown estimator type %v\",\n\t\t\t\troutingConfig.ProbabilityEstimatorType)\n\t\t}\n\t}\n\n\tmcCfg := &routing.MissionControlConfig{\n\t\tEstimator:               estimator,\n\t\tMaxMcHistory:            routingConfig.MaxMcHistory,\n\t\tMcFlushInterval:         routingConfig.McFlushInterval,\n\t\tMinFailureRelaxInterval: routing.DefaultMinFailureRelaxInterval,\n\t}\n\ts.missionControl, err = routing.NewMissionControl(\n\t\tdbs.ChanStateDB, selfNode.PubKeyBytes, mcCfg,\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"can't create mission control: %v\", err)\n\t}\n\n\tsrvrLog.Debugf(\"Instantiating payment session source with config: \"+\n\t\t\"AttemptCost=%v + %v%%, MinRouteProbability=%v\",\n\t\tint64(routingConfig.AttemptCost),\n\t\tfloat64(routingConfig.AttemptCostPPM)/10000,\n\t\troutingConfig.MinRouteProbability)\n\n\tpathFindingConfig := routing.PathFindingConfig{\n\t\tAttemptCost: lnwire.NewMSatFromSatoshis(\n\t\t\troutingConfig.AttemptCost,\n\t\t),\n\t\tAttemptCostPPM: routingConfig.AttemptCostPPM,\n\t\tMinProbability: routingConfig.MinRouteProbability,\n\t}\n\n\tsourceNode, err := chanGraph.SourceNode()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error getting source node: %v\", err)\n\t}\n\tpaymentSessionSource := &routing.SessionSource{\n\t\tGraph:             chanGraph,\n\t\tSourceNode:        sourceNode,\n\t\tMissionControl:    s.missionControl,\n\t\tGetLink:           s.htlcSwitch.GetLinkByShortID,\n\t\tPathFindingConfig: pathFindingConfig,\n\t}\n\n\tpaymentControl := channeldb.NewPaymentControl(dbs.ChanStateDB)\n\n\ts.controlTower = routing.NewControlTower(paymentControl)\n\n\tstrictPruning := (cfg.Bitcoin.Node == \"neutrino\" ||\n\t\tcfg.Routing.StrictZombiePruning)\n\ts.chanRouter, err = routing.New(routing.Config{\n\t\tGraph:               chanGraph,\n\t\tChain:               cc.ChainIO,\n\t\tChainView:           cc.ChainView,\n\t\tNotifier:            cc.ChainNotifier,\n\t\tPayer:               s.htlcSwitch,\n\t\tControl:             s.controlTower,\n\t\tMissionControl:      s.missionControl,\n\t\tSessionSource:       paymentSessionSource,\n\t\tChannelPruneExpiry:  routing.DefaultChannelPruneExpiry,\n\t\tGraphPruneInterval:  time.Hour,\n\t\tFirstTimePruneDelay: routing.DefaultFirstTimePruneDelay,\n\t\tGetLink:             s.htlcSwitch.GetLinkByShortID,\n\t\tAssumeChannelValid:  cfg.Routing.AssumeChannelValid,\n\t\tNextPaymentID:       sequencer.NextID,\n\t\tPathFindingConfig:   pathFindingConfig,\n\t\tClock:               clock.NewDefaultClock(),\n\t\tStrictZombiePruning: strictPruning,\n\t\tIsAlias:             aliasmgr.IsAlias,\n\t})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"can't create router: %v\", err)\n\t}\n\n\tchanSeries := discovery.NewChanSeries(s.graphDB)\n\tgossipMessageStore, err := discovery.NewMessageStore(dbs.ChanStateDB)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\twaitingProofStore, err := channeldb.NewWaitingProofStore(dbs.ChanStateDB)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ts.authGossiper = discovery.New(discovery.Config{\n\t\tRouter:            s.chanRouter,\n\t\tNotifier:          s.cc.ChainNotifier,\n\t\tChainHash:         *s.cfg.ActiveNetParams.GenesisHash,\n\t\tBroadcast:         s.BroadcastMessage,\n\t\tChanSeries:        chanSeries,\n\t\tNotifyWhenOnline:  s.NotifyWhenOnline,\n\t\tNotifyWhenOffline: s.NotifyWhenOffline,\n\t\tSelfNodeAnnouncement: func(refresh bool) (lnwire.NodeAnnouncement, error) {\n\t\t\treturn s.genNodeAnnouncement(refresh)\n\t\t},\n\t\tProofMatureDelta:        0,\n\t\tTrickleDelay:            time.Millisecond * time.Duration(cfg.TrickleDelay),\n\t\tRetransmitTicker:        ticker.New(time.Minute * 30),\n\t\tRebroadcastInterval:     time.Hour * 24,\n\t\tWaitingProofStore:       waitingProofStore,\n\t\tMessageStore:            gossipMessageStore,\n\t\tAnnSigner:               s.nodeSigner,\n\t\tRotateTicker:            ticker.New(discovery.DefaultSyncerRotationInterval),\n\t\tHistoricalSyncTicker:    ticker.New(cfg.HistoricalSyncInterval),\n\t\tNumActiveSyncers:        cfg.NumGraphSyncPeers,\n\t\tMinimumBatchSize:        10,\n\t\tSubBatchDelay:           cfg.Gossip.SubBatchDelay,\n\t\tIgnoreHistoricalFilters: cfg.IgnoreHistoricalGossipFilters,\n\t\tPinnedSyncers:           cfg.Gossip.PinnedSyncers,\n\t\tMaxChannelUpdateBurst:   cfg.Gossip.MaxChannelUpdateBurst,\n\t\tChannelUpdateInterval:   cfg.Gossip.ChannelUpdateInterval,\n\t\tIsAlias:                 aliasmgr.IsAlias,\n\t\tSignAliasUpdate:         s.signAliasUpdate,\n\t\tFindBaseByAlias:         s.aliasMgr.FindBaseSCID,\n\t\tGetAlias:                s.aliasMgr.GetPeerAlias,\n\t\tFindChannel:             s.findChannel,\n\t}, nodeKeyDesc)\n\n\ts.localChanMgr = &localchans.Manager{\n\t\tForAllOutgoingChannels:    s.chanRouter.ForAllOutgoingChannels,\n\t\tPropagateChanPolicyUpdate: s.authGossiper.PropagateChanPolicyUpdate,\n\t\tUpdateForwardingPolicies:  s.htlcSwitch.UpdateForwardingPolicies,\n\t\tFetchChannel:              s.chanStateDB.FetchChannel,\n\t}\n\n\tutxnStore, err := contractcourt.NewNurseryStore(\n\t\ts.cfg.ActiveNetParams.GenesisHash, dbs.ChanStateDB,\n\t)\n\tif err != nil {\n\t\tsrvrLog.Errorf(\"unable to create nursery store: %v\", err)\n\t\treturn nil, err\n\t}\n\n\tsrvrLog.Debugf(\"Sweeper batch window duration: %v\",\n\t\tcfg.Sweeper.BatchWindowDuration)\n\n\tsweeperStore, err := sweep.NewSweeperStore(\n\t\tdbs.ChanStateDB, s.cfg.ActiveNetParams.GenesisHash,\n\t)\n\tif err != nil {\n\t\tsrvrLog.Errorf(\"unable to create sweeper store: %v\", err)\n\t\treturn nil, err\n\t}\n\n\ts.sweeper = sweep.New(&sweep.UtxoSweeperConfig{\n\t\tFeeEstimator:   cc.FeeEstimator,\n\t\tGenSweepScript: newSweepPkScriptGen(cc.Wallet),\n\t\tSigner:         cc.Wallet.Cfg.Signer,\n\t\tWallet:         cc.Wallet,\n\t\tNewBatchTimer: func() <-chan time.Time {\n\t\t\treturn time.NewTimer(cfg.Sweeper.BatchWindowDuration).C\n\t\t},\n\t\tNotifier:             cc.ChainNotifier,\n\t\tStore:                sweeperStore,\n\t\tMaxInputsPerTx:       sweep.DefaultMaxInputsPerTx,\n\t\tMaxSweepAttempts:     sweep.DefaultMaxSweepAttempts,\n\t\tNextAttemptDeltaFunc: sweep.DefaultNextAttemptDeltaFunc,\n\t\tMaxFeeRate:           sweep.DefaultMaxFeeRate,\n\t\tFeeRateBucketSize:    sweep.DefaultFeeRateBucketSize,\n\t})\n\n\ts.utxoNursery = contractcourt.NewUtxoNursery(&contractcourt.NurseryConfig{\n\t\tChainIO:             cc.ChainIO,\n\t\tConfDepth:           1,\n\t\tFetchClosedChannels: s.chanStateDB.FetchClosedChannels,\n\t\tFetchClosedChannel:  s.chanStateDB.FetchClosedChannel,\n\t\tNotifier:            cc.ChainNotifier,\n\t\tPublishTransaction:  cc.Wallet.PublishTransaction,\n\t\tStore:               utxnStore,\n\t\tSweepInput:          s.sweeper.SweepInput,\n\t})\n\n\t// Construct a closure that wraps the htlcswitch's CloseLink method.\n\tcloseLink := func(chanPoint *wire.OutPoint,\n\t\tclosureType contractcourt.ChannelCloseType) {\n\t\t// TODO(conner): Properly respect the update and error channels\n\t\t// returned by CloseLink.\n\n\t\t// Instruct the switch to close the channel.  Provide no close out\n\t\t// delivery script or target fee per kw because user input is not\n\t\t// available when the remote peer closes the channel.\n\t\ts.htlcSwitch.CloseLink(chanPoint, closureType, 0, 0, nil)\n\t}\n\n\t// We will use the following channel to reliably hand off contract\n\t// breach events from the ChannelArbitrator to the breachArbiter,\n\tcontractBreaches := make(chan *contractcourt.ContractBreachEvent, 1)\n\n\ts.breachArbiter = contractcourt.NewBreachArbiter(&contractcourt.BreachConfig{\n\t\tCloseLink:          closeLink,\n\t\tDB:                 s.chanStateDB,\n\t\tEstimator:          s.cc.FeeEstimator,\n\t\tGenSweepScript:     newSweepPkScriptGen(cc.Wallet),\n\t\tNotifier:           cc.ChainNotifier,\n\t\tPublishTransaction: cc.Wallet.PublishTransaction,\n\t\tContractBreaches:   contractBreaches,\n\t\tSigner:             cc.Wallet.Cfg.Signer,\n\t\tStore: contractcourt.NewRetributionStore(\n\t\t\tdbs.ChanStateDB,\n\t\t),\n\t})\n\n\ts.chainArb = contractcourt.NewChainArbitrator(contractcourt.ChainArbitratorConfig{\n\t\tChainHash:              *s.cfg.ActiveNetParams.GenesisHash,\n\t\tIncomingBroadcastDelta: lncfg.DefaultIncomingBroadcastDelta,\n\t\tOutgoingBroadcastDelta: lncfg.DefaultOutgoingBroadcastDelta,\n\t\tNewSweepAddr:           newSweepPkScriptGen(cc.Wallet),\n\t\tPublishTx:              cc.Wallet.PublishTransaction,\n\t\tDeliverResolutionMsg: func(msgs ...contractcourt.ResolutionMsg) error {\n\t\t\tfor _, msg := range msgs {\n\t\t\t\terr := s.htlcSwitch.ProcessContractResolution(msg)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\t},\n\t\tIncubateOutputs: func(chanPoint wire.OutPoint,\n\t\t\toutHtlcRes *lnwallet.OutgoingHtlcResolution,\n\t\t\tinHtlcRes *lnwallet.IncomingHtlcResolution,\n\t\t\tbroadcastHeight uint32) error {\n\n\t\t\tvar (\n\t\t\t\tinRes  []lnwallet.IncomingHtlcResolution\n\t\t\t\toutRes []lnwallet.OutgoingHtlcResolution\n\t\t\t)\n\t\t\tif inHtlcRes != nil {\n\t\t\t\tinRes = append(inRes, *inHtlcRes)\n\t\t\t}\n\t\t\tif outHtlcRes != nil {\n\t\t\t\toutRes = append(outRes, *outHtlcRes)\n\t\t\t}\n\n\t\t\treturn s.utxoNursery.IncubateOutputs(\n\t\t\t\tchanPoint, outRes, inRes,\n\t\t\t\tbroadcastHeight,\n\t\t\t)\n\t\t},\n\t\tPreimageDB:   s.witnessBeacon,\n\t\tNotifier:     cc.ChainNotifier,\n\t\tSigner:       cc.Wallet.Cfg.Signer,\n\t\tFeeEstimator: cc.FeeEstimator,\n\t\tChainIO:      cc.ChainIO,\n\t\tMarkLinkInactive: func(chanPoint wire.OutPoint) error {\n\t\t\tchanID := lnwire.NewChanIDFromOutPoint(&chanPoint)\n\t\t\ts.htlcSwitch.RemoveLink(chanID)\n\t\t\treturn nil\n\t\t},\n\t\tIsOurAddress: cc.Wallet.IsOurAddress,\n\t\tContractBreach: func(chanPoint wire.OutPoint,\n\t\t\tbreachRet *lnwallet.BreachRetribution) error {\n\n\t\t\t// processACK will handle the breachArbiter ACKing the\n\t\t\t// event.\n\t\t\tfinalErr := make(chan error, 1)\n\t\t\tprocessACK := func(brarErr error) {\n\t\t\t\tif brarErr != nil {\n\t\t\t\t\tfinalErr <- brarErr\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\t// If the breachArbiter successfully handled\n\t\t\t\t// the event, we can signal that the handoff\n\t\t\t\t// was successful.\n\t\t\t\tfinalErr <- nil\n\t\t\t}\n\n\t\t\tevent := &contractcourt.ContractBreachEvent{\n\t\t\t\tChanPoint:         chanPoint,\n\t\t\t\tProcessACK:        processACK,\n\t\t\t\tBreachRetribution: breachRet,\n\t\t\t}\n\n\t\t\t// Send the contract breach event to the breachArbiter.\n\t\t\tselect {\n\t\t\tcase contractBreaches <- event:\n\t\t\tcase <-s.quit:\n\t\t\t\treturn ErrServerShuttingDown\n\t\t\t}\n\n\t\t\t// We'll wait for a final error to be available from\n\t\t\t// the breachArbiter.\n\t\t\tselect {\n\t\t\tcase err := <-finalErr:\n\t\t\t\treturn err\n\t\t\tcase <-s.quit:\n\t\t\t\treturn ErrServerShuttingDown\n\t\t\t}\n\t\t},\n\t\tDisableChannel: func(chanPoint wire.OutPoint) error {\n\t\t\treturn s.chanStatusMgr.RequestDisable(chanPoint, false)\n\t\t},\n\t\tSweeper:                       s.sweeper,\n\t\tRegistry:                      s.invoices,\n\t\tNotifyClosedChannel:           s.channelNotifier.NotifyClosedChannelEvent,\n\t\tNotifyFullyResolvedChannel:    s.channelNotifier.NotifyFullyResolvedChannelEvent,\n\t\tOnionProcessor:                s.sphinx,\n\t\tPaymentsExpirationGracePeriod: cfg.PaymentsExpirationGracePeriod,\n\t\tIsForwardedHTLC:               s.htlcSwitch.IsForwardedHTLC,\n\t\tClock:                         clock.NewDefaultClock(),\n\t\tSubscribeBreachComplete:       s.breachArbiter.SubscribeBreachComplete,\n\t\tPutFinalHtlcOutcome:           s.chanStateDB.PutOnchainFinalHtlcOutcome, //nolint: lll\n\t\tHtlcNotifier:                  s.htlcNotifier,\n\t}, dbs.ChanStateDB)\n\n\t// Select the configuration and furnding parameters for Bitcoin or\n\t// Litecoin, depending on the primary registered chain.\n\tprimaryChain := cfg.registeredChains.PrimaryChain()\n\tchainCfg := cfg.Bitcoin\n\tminRemoteDelay := funding.MinBtcRemoteDelay\n\tmaxRemoteDelay := funding.MaxBtcRemoteDelay\n\tif primaryChain == chainreg.LitecoinChain {\n\t\tchainCfg = cfg.Litecoin\n\t\tminRemoteDelay = funding.MinLtcRemoteDelay\n\t\tmaxRemoteDelay = funding.MaxLtcRemoteDelay\n\t}\n\n\tvar chanIDSeed [32]byte\n\tif _, err := rand.Read(chanIDSeed[:]); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Wrap the DeleteChannelEdges method so that the funding manager can\n\t// use it without depending on several layers of indirection.\n\tdeleteAliasEdge := func(scid lnwire.ShortChannelID) (\n\t\t*channeldb.ChannelEdgePolicy, error) {\n\n\t\tinfo, e1, e2, err := s.graphDB.FetchChannelEdgesByID(\n\t\t\tscid.ToUint64(),\n\t\t)\n\t\tif err == channeldb.ErrEdgeNotFound {\n\t\t\t// This is unlikely but there is a slim chance of this\n\t\t\t// being hit if lnd was killed via SIGKILL and the\n\t\t\t// funding manager was stepping through the delete\n\t\t\t// alias edge logic.\n\t\t\treturn nil, nil\n\t\t} else if err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// Grab our key to find our policy.\n\t\tvar ourKey [33]byte\n\t\tcopy(ourKey[:], nodeKeyDesc.PubKey.SerializeCompressed())\n\n\t\tvar ourPolicy *channeldb.ChannelEdgePolicy\n\t\tif info != nil && info.NodeKey1Bytes == ourKey {\n\t\t\tourPolicy = e1\n\t\t} else {\n\t\t\tourPolicy = e2\n\t\t}\n\n\t\tif ourPolicy == nil {\n\t\t\t// Something is wrong, so return an error.\n\t\t\treturn nil, fmt.Errorf(\"we don't have an edge\")\n\t\t}\n\n\t\terr = s.graphDB.DeleteChannelEdges(\n\t\t\tfalse, false, scid.ToUint64(),\n\t\t)\n\t\treturn ourPolicy, err\n\t}\n\n\ts.fundingMgr, err = funding.NewFundingManager(funding.Config{\n\t\tNoWumboChans:       !cfg.ProtocolOptions.Wumbo(),\n\t\tIDKey:              nodeKeyDesc.PubKey,\n\t\tIDKeyLoc:           nodeKeyDesc.KeyLocator,\n\t\tWallet:             cc.Wallet,\n\t\tPublishTransaction: cc.Wallet.PublishTransaction,\n\t\tUpdateLabel: func(hash chainhash.Hash, label string) error {\n\t\t\treturn cc.Wallet.LabelTransaction(hash, label, true)\n\t\t},\n\t\tNotifier:     cc.ChainNotifier,\n\t\tFeeEstimator: cc.FeeEstimator,\n\t\tSignMessage:  cc.MsgSigner.SignMessage,\n\t\tCurrentNodeAnnouncement: func() (lnwire.NodeAnnouncement, error) {\n\t\t\treturn s.genNodeAnnouncement(true)\n\t\t},\n\t\tSendAnnouncement:     s.authGossiper.ProcessLocalAnnouncement,\n\t\tNotifyWhenOnline:     s.NotifyWhenOnline,\n\t\tTempChanIDSeed:       chanIDSeed,\n\t\tFindChannel:          s.findChannel,\n\t\tDefaultRoutingPolicy: cc.RoutingPolicy,\n\t\tDefaultMinHtlcIn:     cc.MinHtlcIn,\n\t\tNumRequiredConfs: func(chanAmt btcutil.Amount,\n\t\t\tpushAmt lnwire.MilliSatoshi) uint16 {\n\t\t\t// For large channels we increase the number\n\t\t\t// of confirmations we require for the\n\t\t\t// channel to be considered open. As it is\n\t\t\t// always the responder that gets to choose\n\t\t\t// value, the pushAmt is value being pushed\n\t\t\t// to us. This means we have more to lose\n\t\t\t// in the case this gets re-orged out, and\n\t\t\t// we will require more confirmations before\n\t\t\t// we consider it open.\n\t\t\t// TODO(halseth): Use Litecoin params in case\n\t\t\t// of LTC channels.\n\n\t\t\t// In case the user has explicitly specified\n\t\t\t// a default value for the number of\n\t\t\t// confirmations, we use it.\n\t\t\tdefaultConf := uint16(chainCfg.DefaultNumChanConfs)\n\t\t\tif defaultConf != 0 {\n\t\t\t\treturn defaultConf\n\t\t\t}\n\n\t\t\tminConf := uint64(3)\n\t\t\tmaxConf := uint64(6)\n\n\t\t\t// If this is a wumbo channel, then we'll require the\n\t\t\t// max amount of confirmations.\n\t\t\tif chanAmt > MaxFundingAmount {\n\t\t\t\treturn uint16(maxConf)\n\t\t\t}\n\n\t\t\t// If not we return a value scaled linearly\n\t\t\t// between 3 and 6, depending on channel size.\n\t\t\t// TODO(halseth): Use 1 as minimum?\n\t\t\tmaxChannelSize := uint64(\n\t\t\t\tlnwire.NewMSatFromSatoshis(MaxFundingAmount))\n\t\t\tstake := lnwire.NewMSatFromSatoshis(chanAmt) + pushAmt\n\t\t\tconf := maxConf * uint64(stake) / maxChannelSize\n\t\t\tif conf < minConf {\n\t\t\t\tconf = minConf\n\t\t\t}\n\t\t\tif conf > maxConf {\n\t\t\t\tconf = maxConf\n\t\t\t}\n\t\t\treturn uint16(conf)\n\t\t},\n\t\tRequiredRemoteDelay: func(chanAmt btcutil.Amount) uint16 {\n\t\t\t// We scale the remote CSV delay (the time the\n\t\t\t// remote have to claim funds in case of a unilateral\n\t\t\t// close) linearly from minRemoteDelay blocks\n\t\t\t// for small channels, to maxRemoteDelay blocks\n\t\t\t// for channels of size MaxFundingAmount.\n\t\t\t// TODO(halseth): Litecoin parameter for LTC.\n\n\t\t\t// In case the user has explicitly specified\n\t\t\t// a default value for the remote delay, we\n\t\t\t// use it.\n\t\t\tdefaultDelay := uint16(chainCfg.DefaultRemoteDelay)\n\t\t\tif defaultDelay > 0 {\n\t\t\t\treturn defaultDelay\n\t\t\t}\n\n\t\t\t// If this is a wumbo channel, then we'll require the\n\t\t\t// max value.\n\t\t\tif chanAmt > MaxFundingAmount {\n\t\t\t\treturn maxRemoteDelay\n\t\t\t}\n\n\t\t\t// If not we scale according to channel size.\n\t\t\tdelay := uint16(btcutil.Amount(maxRemoteDelay) *\n\t\t\t\tchanAmt / MaxFundingAmount)\n\t\t\tif delay < minRemoteDelay {\n\t\t\t\tdelay = minRemoteDelay\n\t\t\t}\n\t\t\tif delay > maxRemoteDelay {\n\t\t\t\tdelay = maxRemoteDelay\n\t\t\t}\n\t\t\treturn delay\n\t\t},\n\t\tWatchNewChannel: func(channel *channeldb.OpenChannel,\n\t\t\tpeerKey *btcec.PublicKey) error {\n\n\t\t\t// First, we'll mark this new peer as a persistent peer\n\t\t\t// for re-connection purposes. If the peer is not yet\n\t\t\t// tracked or the user hasn't requested it to be perm,\n\t\t\t// we'll set false to prevent the server from continuing\n\t\t\t// to connect to this peer even if the number of\n\t\t\t// channels with this peer is zero.\n\t\t\ts.mu.Lock()\n\t\t\tpubStr := string(peerKey.SerializeCompressed())\n\t\t\tif _, ok := s.persistentPeers[pubStr]; !ok {\n\t\t\t\ts.persistentPeers[pubStr] = false\n\t\t\t}\n\t\t\ts.mu.Unlock()\n\n\t\t\t// With that taken care of, we'll send this channel to\n\t\t\t// the chain arb so it can react to on-chain events.\n\t\t\treturn s.chainArb.WatchNewChannel(channel)\n\t\t},\n\t\tReportShortChanID: func(chanPoint wire.OutPoint) error {\n\t\t\tcid := lnwire.NewChanIDFromOutPoint(&chanPoint)\n\t\t\treturn s.htlcSwitch.UpdateShortChanID(cid)\n\t\t},\n\t\tRequiredRemoteChanReserve: func(chanAmt,\n\t\t\tdustLimit btcutil.Amount) btcutil.Amount {\n\n\t\t\t// By default, we'll require the remote peer to maintain\n\t\t\t// at least 1% of the total channel capacity at all\n\t\t\t// times. If this value ends up dipping below the dust\n\t\t\t// limit, then we'll use the dust limit itself as the\n\t\t\t// reserve as required by BOLT #2.\n\t\t\treserve := chanAmt / 100\n\t\t\tif reserve < dustLimit {\n\t\t\t\treserve = dustLimit\n\t\t\t}\n\n\t\t\treturn reserve\n\t\t},\n\t\tRequiredRemoteMaxValue: func(chanAmt btcutil.Amount) lnwire.MilliSatoshi {\n\t\t\t// By default, we'll allow the remote peer to fully\n\t\t\t// utilize the full bandwidth of the channel, minus our\n\t\t\t// required reserve.\n\t\t\treserve := lnwire.NewMSatFromSatoshis(chanAmt / 100)\n\t\t\treturn lnwire.NewMSatFromSatoshis(chanAmt) - reserve\n\t\t},\n\t\tRequiredRemoteMaxHTLCs: func(chanAmt btcutil.Amount) uint16 {\n\t\t\tif cfg.DefaultRemoteMaxHtlcs > 0 {\n\t\t\t\treturn cfg.DefaultRemoteMaxHtlcs\n\t\t\t}\n\n\t\t\t// By default, we'll permit them to utilize the full\n\t\t\t// channel bandwidth.\n\t\t\treturn uint16(input.MaxHTLCNumber / 2)\n\t\t},\n\t\tZombieSweeperInterval:         1 * time.Minute,\n\t\tReservationTimeout:            10 * time.Minute,\n\t\tMinChanSize:                   btcutil.Amount(cfg.MinChanSize),\n\t\tMaxChanSize:                   btcutil.Amount(cfg.MaxChanSize),\n\t\tMaxPendingChannels:            cfg.MaxPendingChannels,\n\t\tRejectPush:                    cfg.RejectPush,\n\t\tMaxLocalCSVDelay:              chainCfg.MaxLocalDelay,\n\t\tNotifyOpenChannelEvent:        s.channelNotifier.NotifyOpenChannelEvent,\n\t\tOpenChannelPredicate:          chanPredicate,\n\t\tNotifyPendingOpenChannelEvent: s.channelNotifier.NotifyPendingOpenChannelEvent,\n\t\tEnableUpfrontShutdown:         cfg.EnableUpfrontShutdown,\n\t\tRegisteredChains:              cfg.registeredChains,\n\t\tMaxAnchorsCommitFeeRate: chainfee.SatPerKVByte(\n\t\t\ts.cfg.MaxCommitFeeRateAnchors * 1000).FeePerKWeight(),\n\t\tDeleteAliasEdge: deleteAliasEdge,\n\t\tAliasManager:    s.aliasMgr,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Next, we'll assemble the sub-system that will maintain an on-disk\n\t// static backup of the latest channel state.\n\tchanNotifier := &channelNotifier{\n\t\tchanNotifier: s.channelNotifier,\n\t\taddrs:        dbs.ChanStateDB,\n\t}\n\tbackupFile := chanbackup.NewMultiFile(cfg.BackupFilePath)\n\tstartingChans, err := chanbackup.FetchStaticChanBackups(\n\t\ts.chanStateDB, s.addrSource,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ts.chanSubSwapper, err = chanbackup.NewSubSwapper(\n\t\tstartingChans, chanNotifier, s.cc.KeyRing, backupFile,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Assemble a peer notifier which will provide clients with subscriptions\n\t// to peer online and offline events.\n\ts.peerNotifier = peernotifier.New()\n\n\t// Create a channel event store which monitors all open channels.\n\ts.chanEventStore = chanfitness.NewChannelEventStore(&chanfitness.Config{\n\t\tSubscribeChannelEvents: func() (subscribe.Subscription, error) {\n\t\t\treturn s.channelNotifier.SubscribeChannelEvents()\n\t\t},\n\t\tSubscribePeerEvents: func() (subscribe.Subscription, error) {\n\t\t\treturn s.peerNotifier.SubscribePeerEvents()\n\t\t},\n\t\tGetOpenChannels: s.chanStateDB.FetchAllOpenChannels,\n\t\tClock:           clock.NewDefaultClock(),\n\t\tReadFlapCount:   s.miscDB.ReadFlapCount,\n\t\tWriteFlapCount:  s.miscDB.WriteFlapCounts,\n\t\tFlapCountTicker: ticker.New(chanfitness.FlapCountFlushRate),\n\t})\n\n\tif cfg.WtClient.Active {\n\t\tpolicy := wtpolicy.DefaultPolicy()\n\n\t\tif cfg.WtClient.SweepFeeRate != 0 {\n\t\t\t// We expose the sweep fee rate in sat/vbyte, but the\n\t\t\t// tower protocol operations on sat/kw.\n\t\t\tsweepRateSatPerVByte := chainfee.SatPerKVByte(\n\t\t\t\t1000 * cfg.WtClient.SweepFeeRate,\n\t\t\t)\n\t\t\tpolicy.SweepFeeRate = sweepRateSatPerVByte.FeePerKWeight()\n\t\t}\n\n\t\tif err := policy.Validate(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// authDial is the wrapper around the btrontide.Dial for the\n\t\t// watchtower.\n\t\tauthDial := func(localKey keychain.SingleKeyECDH,\n\t\t\tnetAddr *lnwire.NetAddress,\n\t\t\tdialer tor.DialFunc) (wtserver.Peer, error) {\n\n\t\t\treturn brontide.Dial(\n\t\t\t\tlocalKey, netAddr, cfg.ConnectionTimeout, dialer,\n\t\t\t)\n\t\t}\n\n\t\ts.towerClient, err = wtclient.New(&wtclient.Config{\n\t\t\tSigner:         cc.Wallet.Cfg.Signer,\n\t\t\tNewAddress:     newSweepPkScriptGen(cc.Wallet),\n\t\t\tSecretKeyRing:  s.cc.KeyRing,\n\t\t\tDial:           cfg.net.Dial,\n\t\t\tAuthDial:       authDial,\n\t\t\tDB:             dbs.TowerClientDB,\n\t\t\tPolicy:         policy,\n\t\t\tChainHash:      *s.cfg.ActiveNetParams.GenesisHash,\n\t\t\tMinBackoff:     10 * time.Second,\n\t\t\tMaxBackoff:     5 * time.Minute,\n\t\t\tForceQuitDelay: wtclient.DefaultForceQuitDelay,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// Copy the policy for legacy channels and set the blob flag\n\t\t// signalling support for anchor channels.\n\t\tanchorPolicy := policy\n\t\tanchorPolicy.TxPolicy.BlobType |=\n\t\t\tblob.Type(blob.FlagAnchorChannel)\n\n\t\ts.anchorTowerClient, err = wtclient.New(&wtclient.Config{\n\t\t\tSigner:         cc.Wallet.Cfg.Signer,\n\t\t\tNewAddress:     newSweepPkScriptGen(cc.Wallet),\n\t\t\tSecretKeyRing:  s.cc.KeyRing,\n\t\t\tDial:           cfg.net.Dial,\n\t\t\tAuthDial:       authDial,\n\t\t\tDB:             dbs.TowerClientDB,\n\t\t\tPolicy:         anchorPolicy,\n\t\t\tChainHash:      *s.cfg.ActiveNetParams.GenesisHash,\n\t\t\tMinBackoff:     10 * time.Second,\n\t\t\tMaxBackoff:     5 * time.Minute,\n\t\t\tForceQuitDelay: wtclient.DefaultForceQuitDelay,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tif len(cfg.ExternalHosts) != 0 {\n\t\tadvertisedIPs := make(map[string]struct{})\n\t\tfor _, addr := range s.currentNodeAnn.Addresses {\n\t\t\tadvertisedIPs[addr.String()] = struct{}{}\n\t\t}\n\n\t\ts.hostAnn = netann.NewHostAnnouncer(netann.HostAnnouncerConfig{\n\t\t\tHosts:         cfg.ExternalHosts,\n\t\t\tRefreshTicker: ticker.New(defaultHostSampleInterval),\n\t\t\tLookupHost: func(host string) (net.Addr, error) {\n\t\t\t\treturn lncfg.ParseAddressString(\n\t\t\t\t\thost, strconv.Itoa(defaultPeerPort),\n\t\t\t\t\tcfg.net.ResolveTCPAddr,\n\t\t\t\t)\n\t\t\t},\n\t\t\tAdvertisedIPs:  advertisedIPs,\n\t\t\tAnnounceNewIPs: netann.IPAnnouncer(s.genNodeAnnouncement),\n\t\t})\n\t}\n\n\t// Create liveness monitor.\n\ts.createLivenessMonitor(cfg, cc)\n\n\t// Create the connection manager which will be responsible for\n\t// maintaining persistent outbound connections and also accepting new\n\t// incoming connections\n\tcmgr, err := connmgr.New(&connmgr.Config{\n\t\tListeners:      listeners,\n\t\tOnAccept:       s.InboundPeerConnected,\n\t\tRetryDuration:  time.Second * 5,\n\t\tTargetOutbound: 100,\n\t\tDial: noiseDial(\n\t\t\tnodeKeyECDH, s.cfg.net, s.cfg.ConnectionTimeout,\n\t\t),\n\t\tOnConnection: s.OutboundPeerConnected,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ts.connMgr = cmgr\n\n\treturn s, nil\n}\n\n// signAliasUpdate takes a ChannelUpdate and returns the signature. This is\n// used for option_scid_alias channels where the ChannelUpdate to be sent back\n// may differ from what is on disk.",
      "length": 36684,
      "tokens": 3458,
      "embedding": []
    },
    {
      "slug": "func (s *server) signAliasUpdate(u *lnwire.ChannelUpdate) (*ecdsa.Signature,",
      "content": "func (s *server) signAliasUpdate(u *lnwire.ChannelUpdate) (*ecdsa.Signature,\n\terror) {\n\n\tdata, err := u.DataToSign()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn s.cc.MsgSigner.SignMessage(s.identityKeyLoc, data, true)\n}\n\n// createLivenessMonitor creates a set of health checks using our configured\n// values and uses these checks to create a liveness monitor. Available\n// health checks,\n//   - chainHealthCheck (will be disabled for --nochainbackend mode)\n//   - diskCheck\n//   - tlsHealthCheck\n//   - torController, only created when tor is enabled.\n//\n// If a health check has been disabled by setting attempts to 0, our monitor\n// will not run it.",
      "length": 558,
      "tokens": 91,
      "embedding": []
    },
    {
      "slug": "func (s *server) createLivenessMonitor(cfg *Config, cc *chainreg.ChainControl) {",
      "content": "func (s *server) createLivenessMonitor(cfg *Config, cc *chainreg.ChainControl) {\n\tchainBackendAttempts := cfg.HealthChecks.ChainCheck.Attempts\n\tif cfg.Bitcoin.Node == \"nochainbackend\" {\n\t\tsrvrLog.Info(\"Disabling chain backend checks for \" +\n\t\t\t\"nochainbackend mode\")\n\n\t\tchainBackendAttempts = 0\n\t}\n\n\tchainHealthCheck := healthcheck.NewObservation(\n\t\t\"chain backend\",\n\t\tcc.HealthCheck,\n\t\tcfg.HealthChecks.ChainCheck.Interval,\n\t\tcfg.HealthChecks.ChainCheck.Timeout,\n\t\tcfg.HealthChecks.ChainCheck.Backoff,\n\t\tchainBackendAttempts,\n\t)\n\n\tdiskCheck := healthcheck.NewObservation(\n\t\t\"disk space\",\n\t\tfunc() error {\n\t\t\tfree, err := healthcheck.AvailableDiskSpaceRatio(\n\t\t\t\tcfg.LndDir,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t// If we have more free space than we require,\n\t\t\t// we return a nil error.\n\t\t\tif free > cfg.HealthChecks.DiskCheck.RequiredRemaining {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\treturn fmt.Errorf(\"require: %v free space, got: %v\",\n\t\t\t\tcfg.HealthChecks.DiskCheck.RequiredRemaining,\n\t\t\t\tfree)\n\t\t},\n\t\tcfg.HealthChecks.DiskCheck.Interval,\n\t\tcfg.HealthChecks.DiskCheck.Timeout,\n\t\tcfg.HealthChecks.DiskCheck.Backoff,\n\t\tcfg.HealthChecks.DiskCheck.Attempts,\n\t)\n\n\ttlsHealthCheck := healthcheck.NewObservation(\n\t\t\"tls\",\n\t\tfunc() error {\n\t\t\texpired, expTime, err := s.tlsManager.IsCertExpired(\n\t\t\t\ts.cc.KeyRing,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif expired {\n\t\t\t\treturn fmt.Errorf(\"TLS certificate is \"+\n\t\t\t\t\t\"expired as of %v\", expTime)\n\t\t\t}\n\n\t\t\t// If the certificate is not outdated, no error needs\n\t\t\t// to be returned\n\t\t\treturn nil\n\t\t},\n\t\tcfg.HealthChecks.TLSCheck.Interval,\n\t\tcfg.HealthChecks.TLSCheck.Timeout,\n\t\tcfg.HealthChecks.TLSCheck.Backoff,\n\t\tcfg.HealthChecks.TLSCheck.Attempts,\n\t)\n\n\tchecks := []*healthcheck.Observation{\n\t\tchainHealthCheck, diskCheck, tlsHealthCheck,\n\t}\n\n\t// If Tor is enabled, add the healthcheck for tor connection.\n\tif s.torController != nil {\n\t\ttorConnectionCheck := healthcheck.NewObservation(\n\t\t\t\"tor connection\",\n\t\t\tfunc() error {\n\t\t\t\treturn healthcheck.CheckTorServiceStatus(\n\t\t\t\t\ts.torController,\n\t\t\t\t\ts.createNewHiddenService,\n\t\t\t\t)\n\t\t\t},\n\t\t\tcfg.HealthChecks.TorConnection.Interval,\n\t\t\tcfg.HealthChecks.TorConnection.Timeout,\n\t\t\tcfg.HealthChecks.TorConnection.Backoff,\n\t\t\tcfg.HealthChecks.TorConnection.Attempts,\n\t\t)\n\t\tchecks = append(checks, torConnectionCheck)\n\t}\n\n\t// If remote signing is enabled, add the healthcheck for the remote\n\t// signing RPC interface.\n\tif s.cfg.RemoteSigner != nil && s.cfg.RemoteSigner.Enable {\n\t\t// Because we have two cascading timeouts here, we need to add\n\t\t// some slack to the \"outer\" one of them in case the \"inner\"\n\t\t// returns exactly on time.\n\t\toverhead := time.Millisecond * 10\n\n\t\tremoteSignerConnectionCheck := healthcheck.NewObservation(\n\t\t\t\"remote signer connection\",\n\t\t\trpcwallet.HealthCheck(\n\t\t\t\ts.cfg.RemoteSigner,\n\n\t\t\t\t// For the health check we might to be even\n\t\t\t\t// stricter than the initial/normal connect, so\n\t\t\t\t// we use the health check timeout here.\n\t\t\t\tcfg.HealthChecks.RemoteSigner.Timeout,\n\t\t\t),\n\t\t\tcfg.HealthChecks.RemoteSigner.Interval,\n\t\t\tcfg.HealthChecks.RemoteSigner.Timeout+overhead,\n\t\t\tcfg.HealthChecks.RemoteSigner.Backoff,\n\t\t\tcfg.HealthChecks.RemoteSigner.Attempts,\n\t\t)\n\t\tchecks = append(checks, remoteSignerConnectionCheck)\n\t}\n\n\t// If we have not disabled all of our health checks, we create a\n\t// liveness monitor with our configured checks.\n\ts.livenessMonitor = healthcheck.NewMonitor(\n\t\t&healthcheck.Config{\n\t\t\tChecks:   checks,\n\t\t\tShutdown: srvrLog.Criticalf,\n\t\t},\n\t)\n}\n\n// Started returns true if the server has been started, and false otherwise.\n// NOTE: This function is safe for concurrent access.",
      "length": 3415,
      "tokens": 355,
      "embedding": []
    },
    {
      "slug": "func (s *server) Started() bool {",
      "content": "func (s *server) Started() bool {\n\treturn atomic.LoadInt32(&s.active) != 0\n}\n\n// cleaner is used to aggregate \"cleanup\" functions during an operation that\n// starts several subsystems. In case one of the subsystem fails to start\n// and a proper resource cleanup is required, the \"run\" method achieves this\n// by running all these added \"cleanup\" functions.",
      "length": 316,
      "tokens": 51,
      "embedding": []
    },
    {
      "slug": "type cleaner []func() error",
      "content": "type cleaner []func() error\n\n// add is used to add a cleanup function to be called when\n// the run function is executed.",
      "length": 90,
      "tokens": 19,
      "embedding": []
    },
    {
      "slug": "func (c cleaner) add(cleanup func() error) cleaner {",
      "content": "func (c cleaner) add(cleanup func() error) cleaner {\n\treturn append(c, cleanup)\n}\n\n// run is used to run all the previousely added cleanup functions.",
      "length": 93,
      "tokens": 16,
      "embedding": []
    },
    {
      "slug": "func (c cleaner) run() {",
      "content": "func (c cleaner) run() {\n\tfor i := len(c) - 1; i >= 0; i-- {\n\t\tif err := c[i](); err != nil {\n\t\t\tsrvrLog.Infof(\"Cleanup failed: %v\", err)\n\t\t}\n\t}\n}\n\n// Start starts the main daemon server, all requested listeners, and any helper\n// goroutines.\n// NOTE: This function is safe for concurrent access.",
      "length": 262,
      "tokens": 50,
      "embedding": []
    },
    {
      "slug": "func (s *server) Start() error {",
      "content": "func (s *server) Start() error {\n\tvar startErr error\n\n\t// If one sub system fails to start, the following code ensures that the\n\t// previous started ones are stopped. It also ensures a proper wallet\n\t// shutdown which is important for releasing its resources (boltdb, etc...)\n\tcleanup := cleaner{}\n\n\ts.start.Do(func() {\n\t\tif err := s.customMessageServer.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tcleanup = cleanup.add(s.customMessageServer.Stop)\n\n\t\tif s.hostAnn != nil {\n\t\t\tif err := s.hostAnn.Start(); err != nil {\n\t\t\t\tstartErr = err\n\t\t\t\treturn\n\t\t\t}\n\t\t\tcleanup = cleanup.add(s.hostAnn.Stop)\n\t\t}\n\n\t\tif s.livenessMonitor != nil {\n\t\t\tif err := s.livenessMonitor.Start(); err != nil {\n\t\t\t\tstartErr = err\n\t\t\t\treturn\n\t\t\t}\n\t\t\tcleanup = cleanup.add(s.livenessMonitor.Stop)\n\t\t}\n\n\t\t// Start the notification server. This is used so channel\n\t\t// management goroutines can be notified when a funding\n\t\t// transaction reaches a sufficient number of confirmations, or\n\t\t// when the input for the funding transaction is spent in an\n\t\t// attempt at an uncooperative close by the counterparty.\n\t\tif err := s.sigPool.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tcleanup = cleanup.add(s.sigPool.Stop)\n\n\t\tif err := s.writePool.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tcleanup = cleanup.add(s.writePool.Stop)\n\n\t\tif err := s.readPool.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tcleanup = cleanup.add(s.readPool.Stop)\n\n\t\tif err := s.cc.ChainNotifier.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tcleanup = cleanup.add(s.cc.ChainNotifier.Stop)\n\n\t\tif err := s.channelNotifier.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tcleanup = cleanup.add(s.channelNotifier.Stop)\n\n\t\tif err := s.peerNotifier.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tcleanup = cleanup.add(func() error {\n\t\t\treturn s.peerNotifier.Stop()\n\t\t})\n\t\tif err := s.htlcNotifier.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tcleanup = cleanup.add(s.htlcNotifier.Stop)\n\n\t\tif s.towerClient != nil {\n\t\t\tif err := s.towerClient.Start(); err != nil {\n\t\t\t\tstartErr = err\n\t\t\t\treturn\n\t\t\t}\n\t\t\tcleanup = cleanup.add(s.towerClient.Stop)\n\t\t}\n\t\tif s.anchorTowerClient != nil {\n\t\t\tif err := s.anchorTowerClient.Start(); err != nil {\n\t\t\t\tstartErr = err\n\t\t\t\treturn\n\t\t\t}\n\t\t\tcleanup = cleanup.add(s.anchorTowerClient.Stop)\n\t\t}\n\n\t\tif err := s.sweeper.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tcleanup = cleanup.add(s.sweeper.Stop)\n\n\t\tif err := s.utxoNursery.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tcleanup = cleanup.add(s.utxoNursery.Stop)\n\n\t\tif err := s.breachArbiter.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tcleanup = cleanup.add(s.breachArbiter.Stop)\n\n\t\tif err := s.fundingMgr.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tcleanup = cleanup.add(s.fundingMgr.Stop)\n\n\t\t// htlcSwitch must be started before chainArb since the latter\n\t\t// relies on htlcSwitch to deliver resolution message upon\n\t\t// start.\n\t\tif err := s.htlcSwitch.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tcleanup = cleanup.add(s.htlcSwitch.Stop)\n\n\t\tif err := s.interceptableSwitch.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tcleanup = cleanup.add(s.interceptableSwitch.Stop)\n\n\t\tif err := s.chainArb.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tcleanup = cleanup.add(s.chainArb.Stop)\n\n\t\tif err := s.authGossiper.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tcleanup = cleanup.add(s.authGossiper.Stop)\n\n\t\tif err := s.chanRouter.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tcleanup = cleanup.add(s.chanRouter.Stop)\n\n\t\tif err := s.invoices.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tcleanup = cleanup.add(s.invoices.Stop)\n\n\t\tif err := s.sphinx.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tcleanup = cleanup.add(s.sphinx.Stop)\n\n\t\tif err := s.chanStatusMgr.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tcleanup = cleanup.add(s.chanStatusMgr.Stop)\n\n\t\tif err := s.chanEventStore.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tcleanup = cleanup.add(func() error {\n\t\t\ts.chanEventStore.Stop()\n\t\t\treturn nil\n\t\t})\n\n\t\ts.missionControl.RunStoreTicker()\n\t\tcleanup.add(func() error {\n\t\t\ts.missionControl.StopStoreTicker()\n\t\t\treturn nil\n\t\t})\n\n\t\t// Before we start the connMgr, we'll check to see if we have\n\t\t// any backups to recover. We do this now as we want to ensure\n\t\t// that have all the information we need to handle channel\n\t\t// recovery _before_ we even accept connections from any peers.\n\t\tchanRestorer := &chanDBRestorer{\n\t\t\tdb:         s.chanStateDB,\n\t\t\tsecretKeys: s.cc.KeyRing,\n\t\t\tchainArb:   s.chainArb,\n\t\t}\n\t\tif len(s.chansToRestore.PackedSingleChanBackups) != 0 {\n\t\t\terr := chanbackup.UnpackAndRecoverSingles(\n\t\t\t\ts.chansToRestore.PackedSingleChanBackups,\n\t\t\t\ts.cc.KeyRing, chanRestorer, s,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tstartErr = fmt.Errorf(\"unable to unpack single \"+\n\t\t\t\t\t\"backups: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tif len(s.chansToRestore.PackedMultiChanBackup) != 0 {\n\t\t\terr := chanbackup.UnpackAndRecoverMulti(\n\t\t\t\ts.chansToRestore.PackedMultiChanBackup,\n\t\t\t\ts.cc.KeyRing, chanRestorer, s,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tstartErr = fmt.Errorf(\"unable to unpack chan \"+\n\t\t\t\t\t\"backup: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\tif err := s.chanSubSwapper.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tcleanup = cleanup.add(s.chanSubSwapper.Stop)\n\n\t\tif s.torController != nil {\n\t\t\tif err := s.createNewHiddenService(); err != nil {\n\t\t\t\tstartErr = err\n\t\t\t\treturn\n\t\t\t}\n\t\t\tcleanup = cleanup.add(s.torController.Stop)\n\t\t}\n\n\t\tif s.natTraversal != nil {\n\t\t\ts.wg.Add(1)\n\t\t\tgo s.watchExternalIP()\n\t\t}\n\n\t\t// Start connmgr last to prevent connections before init.\n\t\ts.connMgr.Start()\n\t\tcleanup = cleanup.add(func() error {\n\t\t\ts.connMgr.Stop()\n\t\t\treturn nil\n\t\t})\n\n\t\t// If peers are specified as a config option, we'll add those\n\t\t// peers first.\n\t\tfor _, peerAddrCfg := range s.cfg.AddPeers {\n\t\t\tparsedPubkey, parsedHost, err := lncfg.ParseLNAddressPubkey(\n\t\t\t\tpeerAddrCfg,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tstartErr = fmt.Errorf(\"unable to parse peer \"+\n\t\t\t\t\t\"pubkey from config: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\taddr, err := parseAddr(parsedHost, s.cfg.net)\n\t\t\tif err != nil {\n\t\t\t\tstartErr = fmt.Errorf(\"unable to parse peer \"+\n\t\t\t\t\t\"address provided as a config option: \"+\n\t\t\t\t\t\"%v\", err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tpeerAddr := &lnwire.NetAddress{\n\t\t\t\tIdentityKey: parsedPubkey,\n\t\t\t\tAddress:     addr,\n\t\t\t\tChainNet:    s.cfg.ActiveNetParams.Net,\n\t\t\t}\n\n\t\t\terr = s.ConnectToPeer(\n\t\t\t\tpeerAddr, true,\n\t\t\t\ts.cfg.ConnectionTimeout,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tstartErr = fmt.Errorf(\"unable to connect to \"+\n\t\t\t\t\t\"peer address provided as a config \"+\n\t\t\t\t\t\"option: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\t// Subscribe to NodeAnnouncements that advertise new addresses\n\t\t// our persistent peers.\n\t\tif err := s.updatePersistentPeerAddrs(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\t// With all the relevant sub-systems started, we'll now attempt\n\t\t// to establish persistent connections to our direct channel\n\t\t// collaborators within the network. Before doing so however,\n\t\t// we'll prune our set of link nodes found within the database\n\t\t// to ensure we don't reconnect to any nodes we no longer have\n\t\t// open channels with.\n\t\tif err := s.chanStateDB.PruneLinkNodes(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tif err := s.establishPersistentConnections(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\t// setSeedList is a helper function that turns multiple DNS seed\n\t\t// server tuples from the command line or config file into the\n\t\t// data structure we need and does a basic formal sanity check\n\t\t// in the process.\n\t\tsetSeedList := func(tuples []string, genesisHash chainhash.Hash) {\n\t\t\tif len(tuples) == 0 {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tresult := make([][2]string, len(tuples))\n\t\t\tfor idx, tuple := range tuples {\n\t\t\t\ttuple = strings.TrimSpace(tuple)\n\t\t\t\tif len(tuple) == 0 {\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tservers := strings.Split(tuple, \",\")\n\t\t\t\tif len(servers) > 2 || len(servers) == 0 {\n\t\t\t\t\tsrvrLog.Warnf(\"Ignoring invalid DNS \"+\n\t\t\t\t\t\t\"seed tuple: %v\", servers)\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tcopy(result[idx][:], servers)\n\t\t\t}\n\n\t\t\tchainreg.ChainDNSSeeds[genesisHash] = result\n\t\t}\n\n\t\t// Let users overwrite the DNS seed nodes. We only allow them\n\t\t// for bitcoin mainnet/testnet and litecoin mainnet, all other\n\t\t// combinations will just be ignored.\n\t\tif s.cfg.Bitcoin.Active && s.cfg.Bitcoin.MainNet {\n\t\t\tsetSeedList(\n\t\t\t\ts.cfg.Bitcoin.DNSSeeds,\n\t\t\t\tchainreg.BitcoinMainnetGenesis,\n\t\t\t)\n\t\t}\n\t\tif s.cfg.Bitcoin.Active && s.cfg.Bitcoin.TestNet3 {\n\t\t\tsetSeedList(\n\t\t\t\ts.cfg.Bitcoin.DNSSeeds,\n\t\t\t\tchainreg.BitcoinTestnetGenesis,\n\t\t\t)\n\t\t}\n\t\tif s.cfg.Bitcoin.Active && s.cfg.Bitcoin.SigNet {\n\t\t\tsetSeedList(\n\t\t\t\ts.cfg.Bitcoin.DNSSeeds,\n\t\t\t\tchainreg.BitcoinSignetGenesis,\n\t\t\t)\n\t\t}\n\t\tif s.cfg.Litecoin.Active && s.cfg.Litecoin.MainNet {\n\t\t\tsetSeedList(\n\t\t\t\ts.cfg.Litecoin.DNSSeeds,\n\t\t\t\tchainreg.LitecoinMainnetGenesis,\n\t\t\t)\n\t\t}\n\n\t\t// If network bootstrapping hasn't been disabled, then we'll\n\t\t// configure the set of active bootstrappers, and launch a\n\t\t// dedicated goroutine to maintain a set of persistent\n\t\t// connections.\n\t\tif shouldPeerBootstrap(s.cfg) {\n\t\t\tbootstrappers, err := initNetworkBootstrappers(s)\n\t\t\tif err != nil {\n\t\t\t\tstartErr = err\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\ts.wg.Add(1)\n\t\t\tgo s.peerBootstrapper(defaultMinPeers, bootstrappers)\n\t\t} else {\n\t\t\tsrvrLog.Infof(\"Auto peer bootstrapping is disabled\")\n\t\t}\n\n\t\t// Set the active flag now that we've completed the full\n\t\t// startup.\n\t\tatomic.StoreInt32(&s.active, 1)\n\t})\n\n\tif startErr != nil {\n\t\tcleanup.run()\n\t}\n\treturn startErr\n}\n\n// Stop gracefully shutsdown the main daemon server. This function will signal\n// any active goroutines, or helper objects to exit, then blocks until they've\n// all successfully exited. Additionally, any/all listeners are closed.\n// NOTE: This function is safe for concurrent access.",
      "length": 9439,
      "tokens": 1270,
      "embedding": []
    },
    {
      "slug": "func (s *server) Stop() error {",
      "content": "func (s *server) Stop() error {\n\ts.stop.Do(func() {\n\t\tatomic.StoreInt32(&s.stopping, 1)\n\n\t\tclose(s.quit)\n\n\t\t// Shutdown connMgr first to prevent conns during shutdown.\n\t\ts.connMgr.Stop()\n\n\t\t// Shutdown the wallet, funding manager, and the rpc server.\n\t\tif err := s.chanStatusMgr.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop chanStatusMgr: %v\", err)\n\t\t}\n\t\tif err := s.htlcSwitch.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop htlcSwitch: %v\", err)\n\t\t}\n\t\tif err := s.sphinx.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop sphinx: %v\", err)\n\t\t}\n\t\tif err := s.invoices.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop invoices: %v\", err)\n\t\t}\n\t\tif err := s.chanRouter.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop chanRouter: %v\", err)\n\t\t}\n\t\tif err := s.chainArb.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop chainArb: %v\", err)\n\t\t}\n\t\tif err := s.fundingMgr.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop fundingMgr: %v\", err)\n\t\t}\n\t\tif err := s.breachArbiter.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop breachArbiter: %v\", err)\n\t\t}\n\t\tif err := s.utxoNursery.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop utxoNursery: %v\", err)\n\t\t}\n\t\tif err := s.authGossiper.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop authGossiper: %v\", err)\n\t\t}\n\t\tif err := s.sweeper.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop sweeper: %v\", err)\n\t\t}\n\t\tif err := s.channelNotifier.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop channelNotifier: %v\", err)\n\t\t}\n\t\tif err := s.peerNotifier.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop peerNotifier: %v\", err)\n\t\t}\n\t\tif err := s.htlcNotifier.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop htlcNotifier: %v\", err)\n\t\t}\n\t\tif err := s.chanSubSwapper.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop chanSubSwapper: %v\", err)\n\t\t}\n\t\tif err := s.cc.ChainNotifier.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"Unable to stop ChainNotifier: %v\", err)\n\t\t}\n\t\ts.chanEventStore.Stop()\n\t\ts.missionControl.StopStoreTicker()\n\n\t\t// Disconnect from each active peers to ensure that\n\t\t// peerTerminationWatchers signal completion to each peer.\n\t\tfor _, peer := range s.Peers() {\n\t\t\terr := s.DisconnectPeer(peer.IdentityKey())\n\t\t\tif err != nil {\n\t\t\t\tsrvrLog.Warnf(\"could not disconnect peer: %v\"+\n\t\t\t\t\t\"received error: %v\", peer.IdentityKey(),\n\t\t\t\t\terr,\n\t\t\t\t)\n\t\t\t}\n\t\t}\n\n\t\t// Now that all connections have been torn down, stop the tower\n\t\t// client which will reliably flush all queued states to the\n\t\t// tower. If this is halted for any reason, the force quit timer\n\t\t// will kick in and abort to allow this method to return.\n\t\tif s.towerClient != nil {\n\t\t\tif err := s.towerClient.Stop(); err != nil {\n\t\t\t\tsrvrLog.Warnf(\"Unable to shut down tower \"+\n\t\t\t\t\t\"client: %v\", err)\n\t\t\t}\n\t\t}\n\t\tif s.anchorTowerClient != nil {\n\t\t\tif err := s.anchorTowerClient.Stop(); err != nil {\n\t\t\t\tsrvrLog.Warnf(\"Unable to shut down anchor \"+\n\t\t\t\t\t\"tower client: %v\", err)\n\t\t\t}\n\t\t}\n\n\t\tif s.hostAnn != nil {\n\t\t\tif err := s.hostAnn.Stop(); err != nil {\n\t\t\t\tsrvrLog.Warnf(\"unable to shut down host \"+\n\t\t\t\t\t\"annoucner: %v\", err)\n\t\t\t}\n\t\t}\n\n\t\tif s.livenessMonitor != nil {\n\t\t\tif err := s.livenessMonitor.Stop(); err != nil {\n\t\t\t\tsrvrLog.Warnf(\"unable to shutdown liveness \"+\n\t\t\t\t\t\"monitor: %v\", err)\n\t\t\t}\n\t\t}\n\n\t\t// Wait for all lingering goroutines to quit.\n\t\ts.wg.Wait()\n\n\t\ts.sigPool.Stop()\n\t\ts.writePool.Stop()\n\t\ts.readPool.Stop()\n\t})\n\n\treturn nil\n}\n\n// Stopped returns true if the server has been instructed to shutdown.\n// NOTE: This function is safe for concurrent access.",
      "length": 3397,
      "tokens": 492,
      "embedding": []
    },
    {
      "slug": "func (s *server) Stopped() bool {",
      "content": "func (s *server) Stopped() bool {\n\treturn atomic.LoadInt32(&s.stopping) != 0\n}\n\n// configurePortForwarding attempts to set up port forwarding for the different\n// ports that the server will be listening on.\n//\n// NOTE: This should only be used when using some kind of NAT traversal to\n// automatically set up forwarding rules.",
      "length": 285,
      "tokens": 47,
      "embedding": []
    },
    {
      "slug": "func (s *server) configurePortForwarding(ports ...uint16) ([]string, error) {",
      "content": "func (s *server) configurePortForwarding(ports ...uint16) ([]string, error) {\n\tip, err := s.natTraversal.ExternalIP()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ts.lastDetectedIP = ip\n\n\texternalIPs := make([]string, 0, len(ports))\n\tfor _, port := range ports {\n\t\tif err := s.natTraversal.AddPortMapping(port); err != nil {\n\t\t\tsrvrLog.Debugf(\"Unable to forward port %d: %v\", port, err)\n\t\t\tcontinue\n\t\t}\n\n\t\thostIP := fmt.Sprintf(\"%v:%d\", ip, port)\n\t\texternalIPs = append(externalIPs, hostIP)\n\t}\n\n\treturn externalIPs, nil\n}\n\n// removePortForwarding attempts to clear the forwarding rules for the different\n// ports the server is currently listening on.\n//\n// NOTE: This should only be used when using some kind of NAT traversal to\n// automatically set up forwarding rules.",
      "length": 661,
      "tokens": 101,
      "embedding": []
    },
    {
      "slug": "func (s *server) removePortForwarding() {",
      "content": "func (s *server) removePortForwarding() {\n\tforwardedPorts := s.natTraversal.ForwardedPorts()\n\tfor _, port := range forwardedPorts {\n\t\tif err := s.natTraversal.DeletePortMapping(port); err != nil {\n\t\t\tsrvrLog.Errorf(\"Unable to remove forwarding rules for \"+\n\t\t\t\t\"port %d: %v\", port, err)\n\t\t}\n\t}\n}\n\n// watchExternalIP continuously checks for an updated external IP address every\n// 15 minutes. Once a new IP address has been detected, it will automatically\n// handle port forwarding rules and send updated node announcements to the\n// currently connected peers.\n//\n// NOTE: This MUST be run as a goroutine.",
      "length": 548,
      "tokens": 84,
      "embedding": []
    },
    {
      "slug": "func (s *server) watchExternalIP() {",
      "content": "func (s *server) watchExternalIP() {\n\tdefer s.wg.Done()\n\n\t// Before exiting, we'll make sure to remove the forwarding rules set\n\t// up by the server.\n\tdefer s.removePortForwarding()\n\n\t// Keep track of the external IPs set by the user to avoid replacing\n\t// them when detecting a new IP.\n\tipsSetByUser := make(map[string]struct{})\n\tfor _, ip := range s.cfg.ExternalIPs {\n\t\tipsSetByUser[ip.String()] = struct{}{}\n\t}\n\n\tforwardedPorts := s.natTraversal.ForwardedPorts()\n\n\tticker := time.NewTicker(15 * time.Minute)\n\tdefer ticker.Stop()\nout:\n\tfor {\n\t\tselect {\n\t\tcase <-ticker.C:\n\t\t\t// We'll start off by making sure a new IP address has\n\t\t\t// been detected.\n\t\t\tip, err := s.natTraversal.ExternalIP()\n\t\t\tif err != nil {\n\t\t\t\tsrvrLog.Debugf(\"Unable to retrieve the \"+\n\t\t\t\t\t\"external IP address: %v\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Periodically renew the NAT port forwarding.\n\t\t\tfor _, port := range forwardedPorts {\n\t\t\t\terr := s.natTraversal.AddPortMapping(port)\n\t\t\t\tif err != nil {\n\t\t\t\t\tsrvrLog.Warnf(\"Unable to automatically \"+\n\t\t\t\t\t\t\"re-create port forwarding using %s: %v\",\n\t\t\t\t\t\ts.natTraversal.Name(), err)\n\t\t\t\t} else {\n\t\t\t\t\tsrvrLog.Debugf(\"Automatically re-created \"+\n\t\t\t\t\t\t\"forwarding for port %d using %s to \"+\n\t\t\t\t\t\t\"advertise external IP\",\n\t\t\t\t\t\tport, s.natTraversal.Name())\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif ip.Equal(s.lastDetectedIP) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tsrvrLog.Infof(\"Detected new external IP address %s\", ip)\n\n\t\t\t// Next, we'll craft the new addresses that will be\n\t\t\t// included in the new node announcement and advertised\n\t\t\t// to the network. Each address will consist of the new\n\t\t\t// IP detected and one of the currently advertised\n\t\t\t// ports.\n\t\t\tvar newAddrs []net.Addr\n\t\t\tfor _, port := range forwardedPorts {\n\t\t\t\thostIP := fmt.Sprintf(\"%v:%d\", ip, port)\n\t\t\t\taddr, err := net.ResolveTCPAddr(\"tcp\", hostIP)\n\t\t\t\tif err != nil {\n\t\t\t\t\tsrvrLog.Debugf(\"Unable to resolve \"+\n\t\t\t\t\t\t\"host %v: %v\", addr, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tnewAddrs = append(newAddrs, addr)\n\t\t\t}\n\n\t\t\t// Skip the update if we weren't able to resolve any of\n\t\t\t// the new addresses.\n\t\t\tif len(newAddrs) == 0 {\n\t\t\t\tsrvrLog.Debug(\"Skipping node announcement \" +\n\t\t\t\t\t\"update due to not being able to \" +\n\t\t\t\t\t\"resolve any new addresses\")\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Now, we'll need to update the addresses in our node's\n\t\t\t// announcement in order to propagate the update\n\t\t\t// throughout the network. We'll only include addresses\n\t\t\t// that have a different IP from the previous one, as\n\t\t\t// the previous IP is no longer valid.\n\t\t\tcurrentNodeAnn, err := s.genNodeAnnouncement(false)\n\t\t\tif err != nil {\n\t\t\t\tsrvrLog.Debugf(\"Unable to retrieve current \"+\n\t\t\t\t\t\"node announcement: %v\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfor _, addr := range currentNodeAnn.Addresses {\n\t\t\t\thost, _, err := net.SplitHostPort(addr.String())\n\t\t\t\tif err != nil {\n\t\t\t\t\tsrvrLog.Debugf(\"Unable to determine \"+\n\t\t\t\t\t\t\"host from address %v: %v\",\n\t\t\t\t\t\taddr, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\t// We'll also make sure to include external IPs\n\t\t\t\t// set manually by the user.\n\t\t\t\t_, setByUser := ipsSetByUser[addr.String()]\n\t\t\t\tif setByUser || host != s.lastDetectedIP.String() {\n\t\t\t\t\tnewAddrs = append(newAddrs, addr)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Then, we'll generate a new timestamped node\n\t\t\t// announcement with the updated addresses and broadcast\n\t\t\t// it to our peers.\n\t\t\tnewNodeAnn, err := s.genNodeAnnouncement(\n\t\t\t\ttrue, netann.NodeAnnSetAddrs(newAddrs),\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tsrvrLog.Debugf(\"Unable to generate new node \"+\n\t\t\t\t\t\"announcement: %v\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\terr = s.BroadcastMessage(nil, &newNodeAnn)\n\t\t\tif err != nil {\n\t\t\t\tsrvrLog.Debugf(\"Unable to broadcast new node \"+\n\t\t\t\t\t\"announcement to peers: %v\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Finally, update the last IP seen to the current one.\n\t\t\ts.lastDetectedIP = ip\n\t\tcase <-s.quit:\n\t\t\tbreak out\n\t\t}\n\t}\n}\n\n// initNetworkBootstrappers initializes a set of network peer bootstrappers\n// based on the server, and currently active bootstrap mechanisms as defined\n// within the current configuration.",
      "length": 3811,
      "tokens": 540,
      "embedding": []
    },
    {
      "slug": "func initNetworkBootstrappers(s *server) ([]discovery.NetworkPeerBootstrapper, error) {",
      "content": "func initNetworkBootstrappers(s *server) ([]discovery.NetworkPeerBootstrapper, error) {\n\tsrvrLog.Infof(\"Initializing peer network bootstrappers!\")\n\n\tvar bootStrappers []discovery.NetworkPeerBootstrapper\n\n\t// First, we'll create an instance of the ChannelGraphBootstrapper as\n\t// this can be used by default if we've already partially seeded the\n\t// network.\n\tchanGraph := autopilot.ChannelGraphFromDatabase(s.graphDB)\n\tgraphBootstrapper, err := discovery.NewGraphBootstrapper(chanGraph)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbootStrappers = append(bootStrappers, graphBootstrapper)\n\n\t// If this isn't simnet mode, then one of our additional bootstrapping\n\t// sources will be the set of running DNS seeds.\n\tif !s.cfg.Bitcoin.SimNet || !s.cfg.Litecoin.SimNet {\n\t\tdnsSeeds, ok := chainreg.ChainDNSSeeds[*s.cfg.ActiveNetParams.GenesisHash]\n\n\t\t// If we have a set of DNS seeds for this chain, then we'll add\n\t\t// it as an additional bootstrapping source.\n\t\tif ok {\n\t\t\tsrvrLog.Infof(\"Creating DNS peer bootstrapper with \"+\n\t\t\t\t\"seeds: %v\", dnsSeeds)\n\n\t\t\tdnsBootStrapper := discovery.NewDNSSeedBootstrapper(\n\t\t\t\tdnsSeeds, s.cfg.net, s.cfg.ConnectionTimeout,\n\t\t\t)\n\t\t\tbootStrappers = append(bootStrappers, dnsBootStrapper)\n\t\t}\n\t}\n\n\treturn bootStrappers, nil\n}\n\n// createBootstrapIgnorePeers creates a map of peers that the bootstrap process\n// needs to ignore, which is made of three parts,\n//   - the node itself needs to be skipped as it doesn't make sense to connect\n//     to itself.\n//   - the peers that already have connections with, as in s.peersByPub.\n//   - the peers that we are attempting to connect, as in s.persistentPeers.",
      "length": 1501,
      "tokens": 199,
      "embedding": []
    },
    {
      "slug": "func (s *server) createBootstrapIgnorePeers() map[autopilot.NodeID]struct{} {",
      "content": "func (s *server) createBootstrapIgnorePeers() map[autopilot.NodeID]struct{} {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\n\tignore := make(map[autopilot.NodeID]struct{})\n\n\t// We should ignore ourselves from bootstrapping.\n\tselfKey := autopilot.NewNodeID(s.identityECDH.PubKey())\n\tignore[selfKey] = struct{}{}\n\n\t// Ignore all connected peers.\n\tfor _, peer := range s.peersByPub {\n\t\tnID := autopilot.NewNodeID(peer.IdentityKey())\n\t\tignore[nID] = struct{}{}\n\t}\n\n\t// Ignore all persistent peers as they have a dedicated reconnecting\n\t// process.\n\tfor pubKeyStr := range s.persistentPeers {\n\t\tvar nID autopilot.NodeID\n\t\tcopy(nID[:], []byte(pubKeyStr))\n\t\tignore[nID] = struct{}{}\n\t}\n\n\treturn ignore\n}\n\n// peerBootstrapper is a goroutine which is tasked with attempting to establish\n// and maintain a target minimum number of outbound connections. With this\n// invariant, we ensure that our node is connected to a diverse set of peers\n// and that nodes newly joining the network receive an up to date network view\n// as soon as possible.",
      "length": 914,
      "tokens": 128,
      "embedding": []
    },
    {
      "slug": "func (s *server) peerBootstrapper(numTargetPeers uint32,",
      "content": "func (s *server) peerBootstrapper(numTargetPeers uint32,\n\tbootstrappers []discovery.NetworkPeerBootstrapper) {\n\n\tdefer s.wg.Done()\n\n\t// Before we continue, init the ignore peers map.\n\tignoreList := s.createBootstrapIgnorePeers()\n\n\t// We'll start off by aggressively attempting connections to peers in\n\t// order to be a part of the network as soon as possible.\n\ts.initialPeerBootstrap(ignoreList, numTargetPeers, bootstrappers)\n\n\t// Once done, we'll attempt to maintain our target minimum number of\n\t// peers.\n\t//\n\t// We'll use a 15 second backoff, and double the time every time an\n\t// epoch fails up to a ceiling.\n\tbackOff := time.Second * 15\n\n\t// We'll create a new ticker to wake us up every 15 seconds so we can\n\t// see if we've reached our minimum number of peers.\n\tsampleTicker := time.NewTicker(backOff)\n\tdefer sampleTicker.Stop()\n\n\t// We'll use the number of attempts and errors to determine if we need\n\t// to increase the time between discovery epochs.\n\tvar epochErrors uint32 // To be used atomically.\n\tvar epochAttempts uint32\n\n\tfor {\n\t\tselect {\n\t\t// The ticker has just woken us up, so we'll need to check if\n\t\t// we need to attempt to connect our to any more peers.\n\t\tcase <-sampleTicker.C:\n\t\t\t// Obtain the current number of peers, so we can gauge\n\t\t\t// if we need to sample more peers or not.\n\t\t\ts.mu.RLock()\n\t\t\tnumActivePeers := uint32(len(s.peersByPub))\n\t\t\ts.mu.RUnlock()\n\n\t\t\t// If we have enough peers, then we can loop back\n\t\t\t// around to the next round as we're done here.\n\t\t\tif numActivePeers >= numTargetPeers {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// If all of our attempts failed during this last back\n\t\t\t// off period, then will increase our backoff to 5\n\t\t\t// minute ceiling to avoid an excessive number of\n\t\t\t// queries\n\t\t\t//\n\t\t\t// TODO(roasbeef): add reverse policy too?\n\n\t\t\tif epochAttempts > 0 &&\n\t\t\t\tatomic.LoadUint32(&epochErrors) >= epochAttempts {\n\n\t\t\t\tsampleTicker.Stop()\n\n\t\t\t\tbackOff *= 2\n\t\t\t\tif backOff > bootstrapBackOffCeiling {\n\t\t\t\t\tbackOff = bootstrapBackOffCeiling\n\t\t\t\t}\n\n\t\t\t\tsrvrLog.Debugf(\"Backing off peer bootstrapper to \"+\n\t\t\t\t\t\"%v\", backOff)\n\t\t\t\tsampleTicker = time.NewTicker(backOff)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tatomic.StoreUint32(&epochErrors, 0)\n\t\t\tepochAttempts = 0\n\n\t\t\t// Since we know need more peers, we'll compute the\n\t\t\t// exact number we need to reach our threshold.\n\t\t\tnumNeeded := numTargetPeers - numActivePeers\n\n\t\t\tsrvrLog.Debugf(\"Attempting to obtain %v more network \"+\n\t\t\t\t\"peers\", numNeeded)\n\n\t\t\t// With the number of peers we need calculated, we'll\n\t\t\t// query the network bootstrappers to sample a set of\n\t\t\t// random addrs for us.\n\t\t\t//\n\t\t\t// Before we continue, get a copy of the ignore peers\n\t\t\t// map.\n\t\t\tignoreList = s.createBootstrapIgnorePeers()\n\n\t\t\tpeerAddrs, err := discovery.MultiSourceBootstrap(\n\t\t\t\tignoreList, numNeeded*2, bootstrappers...,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tsrvrLog.Errorf(\"Unable to retrieve bootstrap \"+\n\t\t\t\t\t\"peers: %v\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Finally, we'll launch a new goroutine for each\n\t\t\t// prospective peer candidates.\n\t\t\tfor _, addr := range peerAddrs {\n\t\t\t\tepochAttempts++\n\n\t\t\t\tgo func(a *lnwire.NetAddress) {\n\t\t\t\t\t// TODO(roasbeef): can do AS, subnet,\n\t\t\t\t\t// country diversity, etc\n\t\t\t\t\terrChan := make(chan error, 1)\n\t\t\t\t\ts.connectToPeer(\n\t\t\t\t\t\ta, errChan,\n\t\t\t\t\t\ts.cfg.ConnectionTimeout,\n\t\t\t\t\t)\n\t\t\t\t\tselect {\n\t\t\t\t\tcase err := <-errChan:\n\t\t\t\t\t\tif err == nil {\n\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tsrvrLog.Errorf(\"Unable to \"+\n\t\t\t\t\t\t\t\"connect to %v: %v\",\n\t\t\t\t\t\t\ta, err)\n\t\t\t\t\t\tatomic.AddUint32(&epochErrors, 1)\n\t\t\t\t\tcase <-s.quit:\n\t\t\t\t\t}\n\t\t\t\t}(addr)\n\t\t\t}\n\t\tcase <-s.quit:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// bootstrapBackOffCeiling is the maximum amount of time we'll wait between\n// failed attempts to locate a set of bootstrap peers. We'll slowly double our\n// query back off each time we encounter a failure.\nconst bootstrapBackOffCeiling = time.Minute * 5\n\n// initialPeerBootstrap attempts to continuously connect to peers on startup\n// until the target number of peers has been reached. This ensures that nodes\n// receive an up to date network view as soon as possible.",
      "length": 3851,
      "tokens": 569,
      "embedding": []
    },
    {
      "slug": "func (s *server) initialPeerBootstrap(ignore map[autopilot.NodeID]struct{},",
      "content": "func (s *server) initialPeerBootstrap(ignore map[autopilot.NodeID]struct{},\n\tnumTargetPeers uint32,\n\tbootstrappers []discovery.NetworkPeerBootstrapper) {\n\n\tsrvrLog.Debugf(\"Init bootstrap with targetPeers=%v, bootstrappers=%v, \"+\n\t\t\"ignore=%v\", numTargetPeers, len(bootstrappers), len(ignore))\n\n\t// We'll start off by waiting 2 seconds between failed attempts, then\n\t// double each time we fail until we hit the bootstrapBackOffCeiling.\n\tvar delaySignal <-chan time.Time\n\tdelayTime := time.Second * 2\n\n\t// As want to be more aggressive, we'll use a lower back off celling\n\t// then the main peer bootstrap logic.\n\tbackOffCeiling := bootstrapBackOffCeiling / 5\n\n\tfor attempts := 0; ; attempts++ {\n\t\t// Check if the server has been requested to shut down in order\n\t\t// to prevent blocking.\n\t\tif s.Stopped() {\n\t\t\treturn\n\t\t}\n\n\t\t// We can exit our aggressive initial peer bootstrapping stage\n\t\t// if we've reached out target number of peers.\n\t\ts.mu.RLock()\n\t\tnumActivePeers := uint32(len(s.peersByPub))\n\t\ts.mu.RUnlock()\n\n\t\tif numActivePeers >= numTargetPeers {\n\t\t\treturn\n\t\t}\n\n\t\tif attempts > 0 {\n\t\t\tsrvrLog.Debugf(\"Waiting %v before trying to locate \"+\n\t\t\t\t\"bootstrap peers (attempt #%v)\", delayTime,\n\t\t\t\tattempts)\n\n\t\t\t// We've completed at least one iterating and haven't\n\t\t\t// finished, so we'll start to insert a delay period\n\t\t\t// between each attempt.\n\t\t\tdelaySignal = time.After(delayTime)\n\t\t\tselect {\n\t\t\tcase <-delaySignal:\n\t\t\tcase <-s.quit:\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// After our delay, we'll double the time we wait up to\n\t\t\t// the max back off period.\n\t\t\tdelayTime *= 2\n\t\t\tif delayTime > backOffCeiling {\n\t\t\t\tdelayTime = backOffCeiling\n\t\t\t}\n\t\t}\n\n\t\t// Otherwise, we'll request for the remaining number of peers\n\t\t// in order to reach our target.\n\t\tpeersNeeded := numTargetPeers - numActivePeers\n\t\tbootstrapAddrs, err := discovery.MultiSourceBootstrap(\n\t\t\tignore, peersNeeded, bootstrappers...,\n\t\t)\n\t\tif err != nil {\n\t\t\tsrvrLog.Errorf(\"Unable to retrieve initial bootstrap \"+\n\t\t\t\t\"peers: %v\", err)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Then, we'll attempt to establish a connection to the\n\t\t// different peer addresses retrieved by our bootstrappers.\n\t\tvar wg sync.WaitGroup\n\t\tfor _, bootstrapAddr := range bootstrapAddrs {\n\t\t\twg.Add(1)\n\t\t\tgo func(addr *lnwire.NetAddress) {\n\t\t\t\tdefer wg.Done()\n\n\t\t\t\terrChan := make(chan error, 1)\n\t\t\t\tgo s.connectToPeer(\n\t\t\t\t\taddr, errChan, s.cfg.ConnectionTimeout,\n\t\t\t\t)\n\n\t\t\t\t// We'll only allow this connection attempt to\n\t\t\t\t// take up to 3 seconds. This allows us to move\n\t\t\t\t// quickly by discarding peers that are slowing\n\t\t\t\t// us down.\n\t\t\t\tselect {\n\t\t\t\tcase err := <-errChan:\n\t\t\t\t\tif err == nil {\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tsrvrLog.Errorf(\"Unable to connect to \"+\n\t\t\t\t\t\t\"%v: %v\", addr, err)\n\t\t\t\t// TODO: tune timeout? 3 seconds might be *too*\n\t\t\t\t// aggressive but works well.\n\t\t\t\tcase <-time.After(3 * time.Second):\n\t\t\t\t\tsrvrLog.Tracef(\"Skipping peer %v due \"+\n\t\t\t\t\t\t\"to not establishing a \"+\n\t\t\t\t\t\t\"connection within 3 seconds\",\n\t\t\t\t\t\taddr)\n\t\t\t\tcase <-s.quit:\n\t\t\t\t}\n\t\t\t}(bootstrapAddr)\n\t\t}\n\n\t\twg.Wait()\n\t}\n}\n\n// createNewHiddenService automatically sets up a v2 or v3 onion service in\n// order to listen for inbound connections over Tor.",
      "length": 2974,
      "tokens": 422,
      "embedding": []
    },
    {
      "slug": "func (s *server) createNewHiddenService() error {",
      "content": "func (s *server) createNewHiddenService() error {\n\t// Determine the different ports the server is listening on. The onion\n\t// service's virtual port will map to these ports and one will be picked\n\t// at random when the onion service is being accessed.\n\tlistenPorts := make([]int, 0, len(s.listenAddrs))\n\tfor _, listenAddr := range s.listenAddrs {\n\t\tport := listenAddr.(*net.TCPAddr).Port\n\t\tlistenPorts = append(listenPorts, port)\n\t}\n\n\tencrypter, err := lnencrypt.KeyRingEncrypter(s.cc.KeyRing)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Once the port mapping has been set, we can go ahead and automatically\n\t// create our onion service. The service's private key will be saved to\n\t// disk in order to regain access to this service when restarting `lnd`.\n\tonionCfg := tor.AddOnionConfig{\n\t\tVirtualPort: defaultPeerPort,\n\t\tTargetPorts: listenPorts,\n\t\tStore: tor.NewOnionFile(\n\t\t\ts.cfg.Tor.PrivateKeyPath, 0600, s.cfg.Tor.EncryptKey,\n\t\t\tencrypter,\n\t\t),\n\t}\n\n\tswitch {\n\tcase s.cfg.Tor.V2:\n\t\tonionCfg.Type = tor.V2\n\tcase s.cfg.Tor.V3:\n\t\tonionCfg.Type = tor.V3\n\t}\n\n\taddr, err := s.torController.AddOnion(onionCfg)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Now that the onion service has been created, we'll add the onion\n\t// address it can be reached at to our list of advertised addresses.\n\tnewNodeAnn, err := s.genNodeAnnouncement(\n\t\ttrue, func(currentAnn *lnwire.NodeAnnouncement) {\n\t\t\tcurrentAnn.Addresses = append(currentAnn.Addresses, addr)\n\t\t},\n\t)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to generate new node \"+\n\t\t\t\"announcement: %v\", err)\n\t}\n\n\t// Finally, we'll update the on-disk version of our announcement so it\n\t// will eventually propagate to nodes in the network.\n\tselfNode := &channeldb.LightningNode{\n\t\tHaveNodeAnnouncement: true,\n\t\tLastUpdate:           time.Unix(int64(newNodeAnn.Timestamp), 0),\n\t\tAddresses:            newNodeAnn.Addresses,\n\t\tAlias:                newNodeAnn.Alias.String(),\n\t\tFeatures: lnwire.NewFeatureVector(\n\t\t\tnewNodeAnn.Features, lnwire.Features,\n\t\t),\n\t\tColor:        newNodeAnn.RGBColor,\n\t\tAuthSigBytes: newNodeAnn.Signature.ToSignatureBytes(),\n\t}\n\tcopy(selfNode.PubKeyBytes[:], s.identityECDH.PubKey().SerializeCompressed())\n\tif err := s.graphDB.SetSourceNode(selfNode); err != nil {\n\t\treturn fmt.Errorf(\"can't set self node: %v\", err)\n\t}\n\n\treturn nil\n}\n\n// findChannel finds a channel given a public key and ChannelID. It is an\n// optimization that is quicker than seeking for a channel given only the\n// ChannelID.",
      "length": 2326,
      "tokens": 297,
      "embedding": []
    },
    {
      "slug": "func (s *server) findChannel(node *btcec.PublicKey, chanID lnwire.ChannelID) (",
      "content": "func (s *server) findChannel(node *btcec.PublicKey, chanID lnwire.ChannelID) (\n\t*channeldb.OpenChannel, error) {\n\n\tnodeChans, err := s.chanStateDB.FetchOpenChannels(node)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor _, channel := range nodeChans {\n\t\tif chanID.IsChanPoint(&channel.FundingOutpoint) {\n\t\t\treturn channel, nil\n\t\t}\n\t}\n\n\treturn nil, fmt.Errorf(\"unable to find channel\")\n}\n\n// genNodeAnnouncement generates and returns the current fully signed node\n// announcement. If refresh is true, then the time stamp of the announcement\n// will be updated in order to ensure it propagates through the network.",
      "length": 510,
      "tokens": 74,
      "embedding": []
    },
    {
      "slug": "func (s *server) genNodeAnnouncement(refresh bool,",
      "content": "func (s *server) genNodeAnnouncement(refresh bool,\n\tmodifiers ...netann.NodeAnnModifier) (lnwire.NodeAnnouncement, error) {\n\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\t// If we don't need to refresh the announcement, then we can return a\n\t// copy of our cached version.\n\tif !refresh {\n\t\treturn *s.currentNodeAnn, nil\n\t}\n\n\t// Always update the timestamp when refreshing to ensure the update\n\t// propagates.\n\tmodifiers = append(modifiers, netann.NodeAnnSetTimestamp)\n\n\t// Otherwise, we'll sign a new update after applying all of the passed\n\t// modifiers.\n\terr := netann.SignNodeAnnouncement(\n\t\ts.nodeSigner, s.identityKeyLoc, s.currentNodeAnn,\n\t\tmodifiers...,\n\t)\n\tif err != nil {\n\t\treturn lnwire.NodeAnnouncement{}, err\n\t}\n\n\treturn *s.currentNodeAnn, nil\n}\n\n// updateAndBrodcastSelfNode generates a new node announcement\n// applying the giving modifiers and updating the time stamp\n// to ensure it propagates through the network. Then it brodcasts\n// it to the network.",
      "length": 878,
      "tokens": 121,
      "embedding": []
    },
    {
      "slug": "func (s *server) updateAndBrodcastSelfNode(",
      "content": "func (s *server) updateAndBrodcastSelfNode(\n\tmodifiers ...netann.NodeAnnModifier) error {\n\n\tnewNodeAnn, err := s.genNodeAnnouncement(true, modifiers...)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to generate new node \"+\n\t\t\t\"announcement: %v\", err)\n\t}\n\n\t// Update the on-disk version of our announcement.\n\t// Load and modify self node istead of creating anew instance so we\n\t// don't risk overwriting any existing values.\n\tselfNode, err := s.graphDB.SourceNode()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to get current source node: %v\", err)\n\t}\n\n\tselfNode.HaveNodeAnnouncement = true\n\tselfNode.LastUpdate = time.Unix(int64(newNodeAnn.Timestamp), 0)\n\tselfNode.Addresses = newNodeAnn.Addresses\n\tselfNode.Alias = newNodeAnn.Alias.String()\n\tselfNode.Features = lnwire.NewFeatureVector(\n\t\tnewNodeAnn.Features, lnwire.Features,\n\t)\n\tselfNode.Color = newNodeAnn.RGBColor\n\tselfNode.AuthSigBytes = newNodeAnn.Signature.ToSignatureBytes()\n\n\tcopy(selfNode.PubKeyBytes[:], s.identityECDH.PubKey().SerializeCompressed())\n\n\tif err := s.graphDB.SetSourceNode(selfNode); err != nil {\n\t\treturn fmt.Errorf(\"can't set self node: %v\", err)\n\t}\n\n\t// Update the feature bits for the SetNodeAnn in case they changed.\n\ts.featureMgr.SetRaw(\n\t\tfeature.SetNodeAnn, selfNode.Features.RawFeatureVector,\n\t)\n\n\t// Finally, propagate it to the nodes in the network.\n\terr = s.BroadcastMessage(nil, &newNodeAnn)\n\tif err != nil {\n\t\trpcsLog.Debugf(\"Unable to broadcast new node \"+\n\t\t\t\"announcement to peers: %v\", err)\n\t\treturn err\n\t}\n\n\treturn nil\n}\n",
      "length": 1424,
      "tokens": 167,
      "embedding": []
    },
    {
      "slug": "type nodeAddresses struct {",
      "content": "type nodeAddresses struct {\n\tpubKey    *btcec.PublicKey\n\taddresses []net.Addr\n}\n\n// establishPersistentConnections attempts to establish persistent connections\n// to all our direct channel collaborators. In order to promote liveness of our\n// active channels, we instruct the connection manager to attempt to establish\n// and maintain persistent connections to all our direct channel counterparties.",
      "length": 364,
      "tokens": 49,
      "embedding": []
    },
    {
      "slug": "func (s *server) establishPersistentConnections() error {",
      "content": "func (s *server) establishPersistentConnections() error {\n\t// nodeAddrsMap stores the combination of node public keys and addresses\n\t// that we'll attempt to reconnect to. PubKey strings are used as keys\n\t// since other PubKey forms can't be compared.\n\tnodeAddrsMap := map[string]*nodeAddresses{}\n\n\t// Iterate through the list of LinkNodes to find addresses we should\n\t// attempt to connect to based on our set of previous connections. Set\n\t// the reconnection port to the default peer port.\n\tlinkNodes, err := s.chanStateDB.LinkNodeDB().FetchAllLinkNodes()\n\tif err != nil && err != channeldb.ErrLinkNodesNotFound {\n\t\treturn err\n\t}\n\tfor _, node := range linkNodes {\n\t\tpubStr := string(node.IdentityPub.SerializeCompressed())\n\t\tnodeAddrs := &nodeAddresses{\n\t\t\tpubKey:    node.IdentityPub,\n\t\t\taddresses: node.Addresses,\n\t\t}\n\t\tnodeAddrsMap[pubStr] = nodeAddrs\n\t}\n\n\t// After checking our previous connections for addresses to connect to,\n\t// iterate through the nodes in our channel graph to find addresses\n\t// that have been added via NodeAnnouncement messages.\n\tsourceNode, err := s.graphDB.SourceNode()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// TODO(roasbeef): instead iterate over link nodes and query graph for\n\t// each of the nodes.\n\tselfPub := s.identityECDH.PubKey().SerializeCompressed()\n\terr = sourceNode.ForEachChannel(nil, func(\n\t\ttx kvdb.RTx,\n\t\tchanInfo *channeldb.ChannelEdgeInfo,\n\t\tpolicy, _ *channeldb.ChannelEdgePolicy) error {\n\n\t\t// If the remote party has announced the channel to us, but we\n\t\t// haven't yet, then we won't have a policy. However, we don't\n\t\t// need this to connect to the peer, so we'll log it and move on.\n\t\tif policy == nil {\n\t\t\tsrvrLog.Warnf(\"No channel policy found for \"+\n\t\t\t\t\"ChannelPoint(%v): \", chanInfo.ChannelPoint)\n\t\t}\n\n\t\t// We'll now fetch the peer opposite from us within this\n\t\t// channel so we can queue up a direct connection to them.\n\t\tchannelPeer, err := chanInfo.FetchOtherNode(tx, selfPub)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to fetch channel peer for \"+\n\t\t\t\t\"ChannelPoint(%v): %v\", chanInfo.ChannelPoint,\n\t\t\t\terr)\n\t\t}\n\n\t\tpubStr := string(channelPeer.PubKeyBytes[:])\n\n\t\t// Add all unique addresses from channel\n\t\t// graph/NodeAnnouncements to the list of addresses we'll\n\t\t// connect to for this peer.\n\t\taddrSet := make(map[string]net.Addr)\n\t\tfor _, addr := range channelPeer.Addresses {\n\t\t\tswitch addr.(type) {\n\t\t\tcase *net.TCPAddr:\n\t\t\t\taddrSet[addr.String()] = addr\n\n\t\t\t// We'll only attempt to connect to Tor addresses if Tor\n\t\t\t// outbound support is enabled.\n\t\t\tcase *tor.OnionAddr:\n\t\t\t\tif s.cfg.Tor.Active {\n\t\t\t\t\taddrSet[addr.String()] = addr\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// If this peer is also recorded as a link node, we'll add any\n\t\t// additional addresses that have not already been selected.\n\t\tlinkNodeAddrs, ok := nodeAddrsMap[pubStr]\n\t\tif ok {\n\t\t\tfor _, lnAddress := range linkNodeAddrs.addresses {\n\t\t\t\tswitch lnAddress.(type) {\n\t\t\t\tcase *net.TCPAddr:\n\t\t\t\t\taddrSet[lnAddress.String()] = lnAddress\n\n\t\t\t\t// We'll only attempt to connect to Tor\n\t\t\t\t// addresses if Tor outbound support is enabled.\n\t\t\t\tcase *tor.OnionAddr:\n\t\t\t\t\tif s.cfg.Tor.Active {\n\t\t\t\t\t\taddrSet[lnAddress.String()] = lnAddress\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Construct a slice of the deduped addresses.\n\t\tvar addrs []net.Addr\n\t\tfor _, addr := range addrSet {\n\t\t\taddrs = append(addrs, addr)\n\t\t}\n\n\t\tn := &nodeAddresses{\n\t\t\taddresses: addrs,\n\t\t}\n\t\tn.pubKey, err = channelPeer.PubKey()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tnodeAddrsMap[pubStr] = n\n\t\treturn nil\n\t})\n\tif err != nil && err != channeldb.ErrGraphNoEdgesFound {\n\t\treturn err\n\t}\n\n\tsrvrLog.Debugf(\"Establishing %v persistent connections on start\",\n\t\tlen(nodeAddrsMap))\n\n\t// Acquire and hold server lock until all persistent connection requests\n\t// have been recorded and sent to the connection manager.\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\t// Iterate through the combined list of addresses from prior links and\n\t// node announcements and attempt to reconnect to each node.\n\tvar numOutboundConns int\n\tfor pubStr, nodeAddr := range nodeAddrsMap {\n\t\t// Add this peer to the set of peers we should maintain a\n\t\t// persistent connection with. We set the value to false to\n\t\t// indicate that we should not continue to reconnect if the\n\t\t// number of channels returns to zero, since this peer has not\n\t\t// been requested as perm by the user.\n\t\ts.persistentPeers[pubStr] = false\n\t\tif _, ok := s.persistentPeersBackoff[pubStr]; !ok {\n\t\t\ts.persistentPeersBackoff[pubStr] = s.cfg.MinBackoff\n\t\t}\n\n\t\tfor _, address := range nodeAddr.addresses {\n\t\t\t// Create a wrapper address which couples the IP and\n\t\t\t// the pubkey so the brontide authenticated connection\n\t\t\t// can be established.\n\t\t\tlnAddr := &lnwire.NetAddress{\n\t\t\t\tIdentityKey: nodeAddr.pubKey,\n\t\t\t\tAddress:     address,\n\t\t\t}\n\n\t\t\ts.persistentPeerAddrs[pubStr] = append(\n\t\t\t\ts.persistentPeerAddrs[pubStr], lnAddr)\n\t\t}\n\n\t\t// We'll connect to the first 10 peers immediately, then\n\t\t// randomly stagger any remaining connections if the\n\t\t// stagger initial reconnect flag is set. This ensures\n\t\t// that mobile nodes or nodes with a small number of\n\t\t// channels obtain connectivity quickly, but larger\n\t\t// nodes are able to disperse the costs of connecting to\n\t\t// all peers at once.\n\t\tif numOutboundConns < numInstantInitReconnect ||\n\t\t\t!s.cfg.StaggerInitialReconnect {\n\n\t\t\tgo s.connectToPersistentPeer(pubStr)\n\t\t} else {\n\t\t\tgo s.delayInitialReconnect(pubStr)\n\t\t}\n\n\t\tnumOutboundConns++\n\t}\n\n\treturn nil\n}\n\n// delayInitialReconnect will attempt a reconnection to the given peer after\n// sampling a value for the delay between 0s and the maxInitReconnectDelay.\n//\n// NOTE: This method MUST be run as a goroutine.",
      "length": 5401,
      "tokens": 774,
      "embedding": []
    },
    {
      "slug": "func (s *server) delayInitialReconnect(pubStr string) {",
      "content": "func (s *server) delayInitialReconnect(pubStr string) {\n\tdelay := time.Duration(prand.Intn(maxInitReconnectDelay)) * time.Second\n\tselect {\n\tcase <-time.After(delay):\n\t\ts.connectToPersistentPeer(pubStr)\n\tcase <-s.quit:\n\t}\n}\n\n// prunePersistentPeerConnection removes all internal state related to\n// persistent connections to a peer within the server. This is used to avoid\n// persistent connection retries to peers we do not have any open channels with.",
      "length": 386,
      "tokens": 50,
      "embedding": []
    },
    {
      "slug": "func (s *server) prunePersistentPeerConnection(compressedPubKey [33]byte) {",
      "content": "func (s *server) prunePersistentPeerConnection(compressedPubKey [33]byte) {\n\tpubKeyStr := string(compressedPubKey[:])\n\n\ts.mu.Lock()\n\tif perm, ok := s.persistentPeers[pubKeyStr]; ok && !perm {\n\t\tdelete(s.persistentPeers, pubKeyStr)\n\t\tdelete(s.persistentPeersBackoff, pubKeyStr)\n\t\tdelete(s.persistentPeerAddrs, pubKeyStr)\n\t\ts.cancelConnReqs(pubKeyStr, nil)\n\t\ts.mu.Unlock()\n\n\t\tsrvrLog.Infof(\"Pruned peer %x from persistent connections, \"+\n\t\t\t\"peer has no open channels\", compressedPubKey)\n\n\t\treturn\n\t}\n\ts.mu.Unlock()\n}\n\n// BroadcastMessage sends a request to the server to broadcast a set of\n// messages to all peers other than the one specified by the `skips` parameter.\n// All messages sent via BroadcastMessage will be queued for lazy delivery to\n// the target peers.\n//\n// NOTE: This function is safe for concurrent access.",
      "length": 725,
      "tokens": 93,
      "embedding": []
    },
    {
      "slug": "func (s *server) BroadcastMessage(skips map[route.Vertex]struct{},",
      "content": "func (s *server) BroadcastMessage(skips map[route.Vertex]struct{},\n\tmsgs ...lnwire.Message) error {\n\n\t// Filter out peers found in the skips map. We synchronize access to\n\t// peersByPub throughout this process to ensure we deliver messages to\n\t// exact set of peers present at the time of invocation.\n\ts.mu.RLock()\n\tpeers := make([]*peer.Brontide, 0, len(s.peersByPub))\n\tfor pubStr, sPeer := range s.peersByPub {\n\t\tif skips != nil {\n\t\t\tif _, ok := skips[sPeer.PubKey()]; ok {\n\t\t\t\tsrvrLog.Debugf(\"Skipping %x in broadcast with \"+\n\t\t\t\t\t\"pubStr=%x\", sPeer.PubKey(), pubStr)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tpeers = append(peers, sPeer)\n\t}\n\ts.mu.RUnlock()\n\n\t// Iterate over all known peers, dispatching a go routine to enqueue\n\t// all messages to each of peers.\n\tvar wg sync.WaitGroup\n\tfor _, sPeer := range peers {\n\t\tsrvrLog.Debugf(\"Sending %v messages to peer %x\", len(msgs),\n\t\t\tsPeer.PubKey())\n\n\t\t// Dispatch a go routine to enqueue all messages to this peer.\n\t\twg.Add(1)\n\t\ts.wg.Add(1)\n\t\tgo func(p lnpeer.Peer) {\n\t\t\tdefer s.wg.Done()\n\t\t\tdefer wg.Done()\n\n\t\t\tp.SendMessageLazy(false, msgs...)\n\t\t}(sPeer)\n\t}\n\n\t// Wait for all messages to have been dispatched before returning to\n\t// caller.\n\twg.Wait()\n\n\treturn nil\n}\n\n// NotifyWhenOnline can be called by other subsystems to get notified when a\n// particular peer comes online. The peer itself is sent across the peerChan.\n//\n// NOTE: This function is safe for concurrent access.",
      "length": 1302,
      "tokens": 199,
      "embedding": []
    },
    {
      "slug": "func (s *server) NotifyWhenOnline(peerKey [33]byte,",
      "content": "func (s *server) NotifyWhenOnline(peerKey [33]byte,\n\tpeerChan chan<- lnpeer.Peer) {\n\n\ts.mu.Lock()\n\n\t// Compute the target peer's identifier.\n\tpubStr := string(peerKey[:])\n\n\t// Check if peer is connected.\n\tpeer, ok := s.peersByPub[pubStr]\n\tif ok {\n\t\t// Unlock here so that the mutex isn't held while we are\n\t\t// waiting for the peer to become active.\n\t\ts.mu.Unlock()\n\n\t\t// Wait until the peer signals that it is actually active\n\t\t// rather than only in the server's maps.\n\t\tselect {\n\t\tcase <-peer.ActiveSignal():\n\t\tcase <-peer.QuitSignal():\n\t\t\t// The peer quit, so we'll add the channel to the slice\n\t\t\t// and return.\n\t\t\ts.mu.Lock()\n\t\t\ts.peerConnectedListeners[pubStr] = append(\n\t\t\t\ts.peerConnectedListeners[pubStr], peerChan,\n\t\t\t)\n\t\t\ts.mu.Unlock()\n\t\t\treturn\n\t\t}\n\n\t\t// Connected, can return early.\n\t\tsrvrLog.Debugf(\"Notifying that peer %x is online\", peerKey)\n\n\t\tselect {\n\t\tcase peerChan <- peer:\n\t\tcase <-s.quit:\n\t\t}\n\n\t\treturn\n\t}\n\n\t// Not connected, store this listener such that it can be notified when\n\t// the peer comes online.\n\ts.peerConnectedListeners[pubStr] = append(\n\t\ts.peerConnectedListeners[pubStr], peerChan,\n\t)\n\ts.mu.Unlock()\n}\n\n// NotifyWhenOffline delivers a notification to the caller of when the peer with\n// the given public key has been disconnected. The notification is signaled by\n// closing the channel returned.",
      "length": 1232,
      "tokens": 178,
      "embedding": []
    },
    {
      "slug": "func (s *server) NotifyWhenOffline(peerPubKey [33]byte) <-chan struct{} {",
      "content": "func (s *server) NotifyWhenOffline(peerPubKey [33]byte) <-chan struct{} {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tc := make(chan struct{})\n\n\t// If the peer is already offline, we can immediately trigger the\n\t// notification.\n\tpeerPubKeyStr := string(peerPubKey[:])\n\tif _, ok := s.peersByPub[peerPubKeyStr]; !ok {\n\t\tsrvrLog.Debugf(\"Notifying that peer %x is offline\", peerPubKey)\n\t\tclose(c)\n\t\treturn c\n\t}\n\n\t// Otherwise, the peer is online, so we'll keep track of the channel to\n\t// trigger the notification once the server detects the peer\n\t// disconnects.\n\ts.peerDisconnectedListeners[peerPubKeyStr] = append(\n\t\ts.peerDisconnectedListeners[peerPubKeyStr], c,\n\t)\n\n\treturn c\n}\n\n// FindPeer will return the peer that corresponds to the passed in public key.\n// This function is used by the funding manager, allowing it to update the\n// daemon's local representation of the remote peer.\n//\n// NOTE: This function is safe for concurrent access.",
      "length": 834,
      "tokens": 123,
      "embedding": []
    },
    {
      "slug": "func (s *server) FindPeer(peerKey *btcec.PublicKey) (*peer.Brontide, error) {",
      "content": "func (s *server) FindPeer(peerKey *btcec.PublicKey) (*peer.Brontide, error) {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\n\tpubStr := string(peerKey.SerializeCompressed())\n\n\treturn s.findPeerByPubStr(pubStr)\n}\n\n// FindPeerByPubStr will return the peer that corresponds to the passed peerID,\n// which should be a string representation of the peer's serialized, compressed\n// public key.\n//\n// NOTE: This function is safe for concurrent access.",
      "length": 344,
      "tokens": 46,
      "embedding": []
    },
    {
      "slug": "func (s *server) FindPeerByPubStr(pubStr string) (*peer.Brontide, error) {",
      "content": "func (s *server) FindPeerByPubStr(pubStr string) (*peer.Brontide, error) {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\n\treturn s.findPeerByPubStr(pubStr)\n}\n\n// findPeerByPubStr is an internal method that retrieves the specified peer from\n// the server's internal state using.",
      "length": 186,
      "tokens": 24,
      "embedding": []
    },
    {
      "slug": "func (s *server) findPeerByPubStr(pubStr string) (*peer.Brontide, error) {",
      "content": "func (s *server) findPeerByPubStr(pubStr string) (*peer.Brontide, error) {\n\tpeer, ok := s.peersByPub[pubStr]\n\tif !ok {\n\t\treturn nil, ErrPeerNotConnected\n\t}\n\n\treturn peer, nil\n}\n\n// nextPeerBackoff computes the next backoff duration for a peer's pubkey using\n// exponential backoff. If no previous backoff was known, the default is\n// returned.",
      "length": 258,
      "tokens": 41,
      "embedding": []
    },
    {
      "slug": "func (s *server) nextPeerBackoff(pubStr string,",
      "content": "func (s *server) nextPeerBackoff(pubStr string,\n\tstartTime time.Time) time.Duration {\n\n\t// Now, determine the appropriate backoff to use for the retry.\n\tbackoff, ok := s.persistentPeersBackoff[pubStr]\n\tif !ok {\n\t\t// If an existing backoff was unknown, use the default.\n\t\treturn s.cfg.MinBackoff\n\t}\n\n\t// If the peer failed to start properly, we'll just use the previous\n\t// backoff to compute the subsequent randomized exponential backoff\n\t// duration. This will roughly double on average.\n\tif startTime.IsZero() {\n\t\treturn computeNextBackoff(backoff, s.cfg.MaxBackoff)\n\t}\n\n\t// The peer succeeded in starting. If the connection didn't last long\n\t// enough to be considered stable, we'll continue to back off retries\n\t// with this peer.\n\tconnDuration := time.Since(startTime)\n\tif connDuration < defaultStableConnDuration {\n\t\treturn computeNextBackoff(backoff, s.cfg.MaxBackoff)\n\t}\n\n\t// The peer succeed in starting and this was stable peer, so we'll\n\t// reduce the timeout duration by the length of the connection after\n\t// applying randomized exponential backoff. We'll only apply this in the\n\t// case that:\n\t//   reb(curBackoff) - connDuration > cfg.MinBackoff\n\trelaxedBackoff := computeNextBackoff(backoff, s.cfg.MaxBackoff) - connDuration\n\tif relaxedBackoff > s.cfg.MinBackoff {\n\t\treturn relaxedBackoff\n\t}\n\n\t// Lastly, if reb(currBackoff) - connDuration <= cfg.MinBackoff, meaning\n\t// the stable connection lasted much longer than our previous backoff.\n\t// To reward such good behavior, we'll reconnect after the default\n\t// timeout.\n\treturn s.cfg.MinBackoff\n}\n\n// shouldDropConnection determines if our local connection to a remote peer\n// should be dropped in the case of concurrent connection establishment. In\n// order to deterministically decide which connection should be dropped, we'll\n// utilize the ordering of the local and remote public key. If we didn't use\n// such a tie breaker, then we risk _both_ connections erroneously being\n// dropped.",
      "length": 1862,
      "tokens": 270,
      "embedding": []
    },
    {
      "slug": "func shouldDropLocalConnection(local, remote *btcec.PublicKey) bool {",
      "content": "func shouldDropLocalConnection(local, remote *btcec.PublicKey) bool {\n\tlocalPubBytes := local.SerializeCompressed()\n\tremotePubPbytes := remote.SerializeCompressed()\n\n\t// The connection that comes from the node with a \"smaller\" pubkey\n\t// should be kept. Therefore, if our pubkey is \"greater\" than theirs, we\n\t// should drop our established connection.\n\treturn bytes.Compare(localPubBytes, remotePubPbytes) > 0\n}\n\n// InboundPeerConnected initializes a new peer in response to a new inbound\n// connection.\n//\n// NOTE: This function is safe for concurrent access.",
      "length": 478,
      "tokens": 67,
      "embedding": []
    },
    {
      "slug": "func (s *server) InboundPeerConnected(conn net.Conn) {",
      "content": "func (s *server) InboundPeerConnected(conn net.Conn) {\n\t// Exit early if we have already been instructed to shutdown, this\n\t// prevents any delayed callbacks from accidentally registering peers.\n\tif s.Stopped() {\n\t\treturn\n\t}\n\n\tnodePub := conn.(*brontide.Conn).RemotePub()\n\tpubStr := string(nodePub.SerializeCompressed())\n\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\t// If we already have an outbound connection to this peer, then ignore\n\t// this new connection.\n\tif p, ok := s.outboundPeers[pubStr]; ok {\n\t\tsrvrLog.Debugf(\"Already have outbound connection for %v, \"+\n\t\t\t\"ignoring inbound connection from local=%v, remote=%v\",\n\t\t\tp, conn.LocalAddr(), conn.RemoteAddr())\n\n\t\tconn.Close()\n\t\treturn\n\t}\n\n\t// If we already have a valid connection that is scheduled to take\n\t// precedence once the prior peer has finished disconnecting, we'll\n\t// ignore this connection.\n\tif p, ok := s.scheduledPeerConnection[pubStr]; ok {\n\t\tsrvrLog.Debugf(\"Ignoring connection from %v, peer %v already \"+\n\t\t\t\"scheduled\", conn.RemoteAddr(), p)\n\t\tconn.Close()\n\t\treturn\n\t}\n\n\tsrvrLog.Infof(\"New inbound connection from %v\", conn.RemoteAddr())\n\n\t// Check to see if we already have a connection with this peer. If so,\n\t// we may need to drop our existing connection. This prevents us from\n\t// having duplicate connections to the same peer. We forgo adding a\n\t// default case as we expect these to be the only error values returned\n\t// from findPeerByPubStr.\n\tconnectedPeer, err := s.findPeerByPubStr(pubStr)\n\tswitch err {\n\tcase ErrPeerNotConnected:\n\t\t// We were unable to locate an existing connection with the\n\t\t// target peer, proceed to connect.\n\t\ts.cancelConnReqs(pubStr, nil)\n\t\ts.peerConnected(conn, nil, true)\n\n\tcase nil:\n\t\t// We already have a connection with the incoming peer. If the\n\t\t// connection we've already established should be kept and is\n\t\t// not of the same type of the new connection (inbound), then\n\t\t// we'll close out the new connection s.t there's only a single\n\t\t// connection between us.\n\t\tlocalPub := s.identityECDH.PubKey()\n\t\tif !connectedPeer.Inbound() &&\n\t\t\t!shouldDropLocalConnection(localPub, nodePub) {\n\n\t\t\tsrvrLog.Warnf(\"Received inbound connection from \"+\n\t\t\t\t\"peer %v, but already have outbound \"+\n\t\t\t\t\"connection, dropping conn\", connectedPeer)\n\t\t\tconn.Close()\n\t\t\treturn\n\t\t}\n\n\t\t// Otherwise, if we should drop the connection, then we'll\n\t\t// disconnect our already connected peer.\n\t\tsrvrLog.Debugf(\"Disconnecting stale connection to %v\",\n\t\t\tconnectedPeer)\n\n\t\ts.cancelConnReqs(pubStr, nil)\n\n\t\t// Remove the current peer from the server's internal state and\n\t\t// signal that the peer termination watcher does not need to\n\t\t// execute for this peer.\n\t\ts.removePeer(connectedPeer)\n\t\ts.ignorePeerTermination[connectedPeer] = struct{}{}\n\t\ts.scheduledPeerConnection[pubStr] = func() {\n\t\t\ts.peerConnected(conn, nil, true)\n\t\t}\n\t}\n}\n\n// OutboundPeerConnected initializes a new peer in response to a new outbound\n// connection.\n// NOTE: This function is safe for concurrent access.",
      "length": 2831,
      "tokens": 388,
      "embedding": []
    },
    {
      "slug": "func (s *server) OutboundPeerConnected(connReq *connmgr.ConnReq, conn net.Conn) {",
      "content": "func (s *server) OutboundPeerConnected(connReq *connmgr.ConnReq, conn net.Conn) {\n\t// Exit early if we have already been instructed to shutdown, this\n\t// prevents any delayed callbacks from accidentally registering peers.\n\tif s.Stopped() {\n\t\treturn\n\t}\n\n\tnodePub := conn.(*brontide.Conn).RemotePub()\n\tpubStr := string(nodePub.SerializeCompressed())\n\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\t// If we already have an inbound connection to this peer, then ignore\n\t// this new connection.\n\tif p, ok := s.inboundPeers[pubStr]; ok {\n\t\tsrvrLog.Debugf(\"Already have inbound connection for %v, \"+\n\t\t\t\"ignoring outbound connection from local=%v, remote=%v\",\n\t\t\tp, conn.LocalAddr(), conn.RemoteAddr())\n\n\t\tif connReq != nil {\n\t\t\ts.connMgr.Remove(connReq.ID())\n\t\t}\n\t\tconn.Close()\n\t\treturn\n\t}\n\tif _, ok := s.persistentConnReqs[pubStr]; !ok && connReq != nil {\n\t\tsrvrLog.Debugf(\"Ignoring canceled outbound connection\")\n\t\ts.connMgr.Remove(connReq.ID())\n\t\tconn.Close()\n\t\treturn\n\t}\n\n\t// If we already have a valid connection that is scheduled to take\n\t// precedence once the prior peer has finished disconnecting, we'll\n\t// ignore this connection.\n\tif _, ok := s.scheduledPeerConnection[pubStr]; ok {\n\t\tsrvrLog.Debugf(\"Ignoring connection, peer already scheduled\")\n\n\t\tif connReq != nil {\n\t\t\ts.connMgr.Remove(connReq.ID())\n\t\t}\n\n\t\tconn.Close()\n\t\treturn\n\t}\n\n\tsrvrLog.Infof(\"Established connection to: %x@%v\", pubStr,\n\t\tconn.RemoteAddr())\n\n\tif connReq != nil {\n\t\t// A successful connection was returned by the connmgr.\n\t\t// Immediately cancel all pending requests, excluding the\n\t\t// outbound connection we just established.\n\t\tignore := connReq.ID()\n\t\ts.cancelConnReqs(pubStr, &ignore)\n\t} else {\n\t\t// This was a successful connection made by some other\n\t\t// subsystem. Remove all requests being managed by the connmgr.\n\t\ts.cancelConnReqs(pubStr, nil)\n\t}\n\n\t// If we already have a connection with this peer, decide whether or not\n\t// we need to drop the stale connection. We forgo adding a default case\n\t// as we expect these to be the only error values returned from\n\t// findPeerByPubStr.\n\tconnectedPeer, err := s.findPeerByPubStr(pubStr)\n\tswitch err {\n\tcase ErrPeerNotConnected:\n\t\t// We were unable to locate an existing connection with the\n\t\t// target peer, proceed to connect.\n\t\ts.peerConnected(conn, connReq, false)\n\n\tcase nil:\n\t\t// We already have a connection with the incoming peer. If the\n\t\t// connection we've already established should be kept and is\n\t\t// not of the same type of the new connection (outbound), then\n\t\t// we'll close out the new connection s.t there's only a single\n\t\t// connection between us.\n\t\tlocalPub := s.identityECDH.PubKey()\n\t\tif connectedPeer.Inbound() &&\n\t\t\tshouldDropLocalConnection(localPub, nodePub) {\n\n\t\t\tsrvrLog.Warnf(\"Established outbound connection to \"+\n\t\t\t\t\"peer %v, but already have inbound \"+\n\t\t\t\t\"connection, dropping conn\", connectedPeer)\n\t\t\tif connReq != nil {\n\t\t\t\ts.connMgr.Remove(connReq.ID())\n\t\t\t}\n\t\t\tconn.Close()\n\t\t\treturn\n\t\t}\n\n\t\t// Otherwise, _their_ connection should be dropped. So we'll\n\t\t// disconnect the peer and send the now obsolete peer to the\n\t\t// server for garbage collection.\n\t\tsrvrLog.Debugf(\"Disconnecting stale connection to %v\",\n\t\t\tconnectedPeer)\n\n\t\t// Remove the current peer from the server's internal state and\n\t\t// signal that the peer termination watcher does not need to\n\t\t// execute for this peer.\n\t\ts.removePeer(connectedPeer)\n\t\ts.ignorePeerTermination[connectedPeer] = struct{}{}\n\t\ts.scheduledPeerConnection[pubStr] = func() {\n\t\t\ts.peerConnected(conn, connReq, false)\n\t\t}\n\t}\n}\n\n// UnassignedConnID is the default connection ID that a request can have before\n// it actually is submitted to the connmgr.\n// TODO(conner): move into connmgr package, or better, add connmgr method for\n// generating atomic IDs\nconst UnassignedConnID uint64 = 0\n\n// cancelConnReqs stops all persistent connection requests for a given pubkey.\n// Any attempts initiated by the peerTerminationWatcher are canceled first.\n// Afterwards, each connection request removed from the connmgr. The caller can\n// optionally specify a connection ID to ignore, which prevents us from\n// canceling a successful request. All persistent connreqs for the provided\n// pubkey are discarded after the operationjw.",
      "length": 4020,
      "tokens": 555,
      "embedding": []
    },
    {
      "slug": "func (s *server) cancelConnReqs(pubStr string, skip *uint64) {",
      "content": "func (s *server) cancelConnReqs(pubStr string, skip *uint64) {\n\t// First, cancel any lingering persistent retry attempts, which will\n\t// prevent retries for any with backoffs that are still maturing.\n\tif cancelChan, ok := s.persistentRetryCancels[pubStr]; ok {\n\t\tclose(cancelChan)\n\t\tdelete(s.persistentRetryCancels, pubStr)\n\t}\n\n\t// Next, check to see if we have any outstanding persistent connection\n\t// requests to this peer. If so, then we'll remove all of these\n\t// connection requests, and also delete the entry from the map.\n\tconnReqs, ok := s.persistentConnReqs[pubStr]\n\tif !ok {\n\t\treturn\n\t}\n\n\tfor _, connReq := range connReqs {\n\t\tsrvrLog.Tracef(\"Canceling %s:\", connReqs)\n\n\t\t// Atomically capture the current request identifier.\n\t\tconnID := connReq.ID()\n\n\t\t// Skip any zero IDs, this indicates the request has not\n\t\t// yet been schedule.\n\t\tif connID == UnassignedConnID {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Skip a particular connection ID if instructed.\n\t\tif skip != nil && connID == *skip {\n\t\t\tcontinue\n\t\t}\n\n\t\ts.connMgr.Remove(connID)\n\t}\n\n\tdelete(s.persistentConnReqs, pubStr)\n}\n\n// handleCustomMessage dispatches an incoming custom peers message to\n// subscribers.",
      "length": 1058,
      "tokens": 154,
      "embedding": []
    },
    {
      "slug": "func (s *server) handleCustomMessage(peer [33]byte, msg *lnwire.Custom) error {",
      "content": "func (s *server) handleCustomMessage(peer [33]byte, msg *lnwire.Custom) error {\n\tsrvrLog.Debugf(\"Custom message received: peer=%x, type=%d\",\n\t\tpeer, msg.Type)\n\n\treturn s.customMessageServer.SendUpdate(&CustomMessage{\n\t\tPeer: peer,\n\t\tMsg:  msg,\n\t})\n}\n\n// SubscribeCustomMessages subscribes to a stream of incoming custom peer\n// messages.",
      "length": 247,
      "tokens": 27,
      "embedding": []
    },
    {
      "slug": "func (s *server) SubscribeCustomMessages() (*subscribe.Client, error) {",
      "content": "func (s *server) SubscribeCustomMessages() (*subscribe.Client, error) {\n\treturn s.customMessageServer.Subscribe()\n}\n\n// peerConnected is a function that handles initialization a newly connected\n// peer by adding it to the server's global list of all active peers, and\n// starting all the goroutines the peer needs to function properly. The inbound\n// boolean should be true if the peer initiated the connection to us.",
      "length": 339,
      "tokens": 55,
      "embedding": []
    },
    {
      "slug": "func (s *server) peerConnected(conn net.Conn, connReq *connmgr.ConnReq,",
      "content": "func (s *server) peerConnected(conn net.Conn, connReq *connmgr.ConnReq,\n\tinbound bool) {\n\n\tbrontideConn := conn.(*brontide.Conn)\n\taddr := conn.RemoteAddr()\n\tpubKey := brontideConn.RemotePub()\n\n\tsrvrLog.Infof(\"Finalizing connection to %x@%s, inbound=%v\",\n\t\tpubKey.SerializeCompressed(), addr, inbound)\n\n\tpeerAddr := &lnwire.NetAddress{\n\t\tIdentityKey: pubKey,\n\t\tAddress:     addr,\n\t\tChainNet:    s.cfg.ActiveNetParams.Net,\n\t}\n\n\t// With the brontide connection established, we'll now craft the feature\n\t// vectors to advertise to the remote node.\n\tinitFeatures := s.featureMgr.Get(feature.SetInit)\n\tlegacyFeatures := s.featureMgr.Get(feature.SetLegacyGlobal)\n\n\t// Lookup past error caches for the peer in the server. If no buffer is\n\t// found, create a fresh buffer.\n\tpkStr := string(peerAddr.IdentityKey.SerializeCompressed())\n\terrBuffer, ok := s.peerErrors[pkStr]\n\tif !ok {\n\t\tvar err error\n\t\terrBuffer, err = queue.NewCircularBuffer(peer.ErrorBufferSize)\n\t\tif err != nil {\n\t\t\tsrvrLog.Errorf(\"unable to create peer %v\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Now that we've established a connection, create a peer, and it to the\n\t// set of currently active peers. Configure the peer with the incoming\n\t// and outgoing broadcast deltas to prevent htlcs from being accepted or\n\t// offered that would trigger channel closure. In case of outgoing\n\t// htlcs, an extra block is added to prevent the channel from being\n\t// closed when the htlc is outstanding and a new block comes in.\n\tpCfg := peer.Config{\n\t\tConn:                    brontideConn,\n\t\tConnReq:                 connReq,\n\t\tAddr:                    peerAddr,\n\t\tInbound:                 inbound,\n\t\tFeatures:                initFeatures,\n\t\tLegacyFeatures:          legacyFeatures,\n\t\tOutgoingCltvRejectDelta: lncfg.DefaultOutgoingCltvRejectDelta,\n\t\tChanActiveTimeout:       s.cfg.ChanEnableTimeout,\n\t\tErrorBuffer:             errBuffer,\n\t\tWritePool:               s.writePool,\n\t\tReadPool:                s.readPool,\n\t\tSwitch:                  s.htlcSwitch,\n\t\tInterceptSwitch:         s.interceptableSwitch,\n\t\tChannelDB:               s.chanStateDB,\n\t\tChannelGraph:            s.graphDB,\n\t\tChainArb:                s.chainArb,\n\t\tAuthGossiper:            s.authGossiper,\n\t\tChanStatusMgr:           s.chanStatusMgr,\n\t\tChainIO:                 s.cc.ChainIO,\n\t\tFeeEstimator:            s.cc.FeeEstimator,\n\t\tSigner:                  s.cc.Wallet.Cfg.Signer,\n\t\tSigPool:                 s.sigPool,\n\t\tWallet:                  s.cc.Wallet,\n\t\tChainNotifier:           s.cc.ChainNotifier,\n\t\tRoutingPolicy:           s.cc.RoutingPolicy,\n\t\tSphinx:                  s.sphinx,\n\t\tWitnessBeacon:           s.witnessBeacon,\n\t\tInvoices:                s.invoices,\n\t\tChannelNotifier:         s.channelNotifier,\n\t\tHtlcNotifier:            s.htlcNotifier,\n\t\tTowerClient:             s.towerClient,\n\t\tAnchorTowerClient:       s.anchorTowerClient,\n\t\tDisconnectPeer:          s.DisconnectPeer,\n\t\tGenNodeAnnouncement:     s.genNodeAnnouncement,\n\n\t\tPongBuf: s.pongBuf,\n\n\t\tPrunePersistentPeerConnection: s.prunePersistentPeerConnection,\n\n\t\tFetchLastChanUpdate: s.fetchLastChanUpdate(),\n\n\t\tFundingManager: s.fundingMgr,\n\n\t\tHodl:                    s.cfg.Hodl,\n\t\tUnsafeReplay:            s.cfg.UnsafeReplay,\n\t\tMaxOutgoingCltvExpiry:   s.cfg.MaxOutgoingCltvExpiry,\n\t\tMaxChannelFeeAllocation: s.cfg.MaxChannelFeeAllocation,\n\t\tCoopCloseTargetConfs:    s.cfg.CoopCloseTargetConfs,\n\t\tMaxAnchorsCommitFeeRate: chainfee.SatPerKVByte(\n\t\t\ts.cfg.MaxCommitFeeRateAnchors * 1000).FeePerKWeight(),\n\t\tChannelCommitInterval:  s.cfg.ChannelCommitInterval,\n\t\tPendingCommitInterval:  s.cfg.PendingCommitInterval,\n\t\tChannelCommitBatchSize: s.cfg.ChannelCommitBatchSize,\n\t\tHandleCustomMessage:    s.handleCustomMessage,\n\t\tGetAliases:             s.aliasMgr.GetAliases,\n\t\tRequestAlias:           s.aliasMgr.RequestAlias,\n\t\tAddLocalAlias:          s.aliasMgr.AddLocalAlias,\n\t\tQuit:                   s.quit,\n\t}\n\n\tcopy(pCfg.PubKeyBytes[:], peerAddr.IdentityKey.SerializeCompressed())\n\tcopy(pCfg.ServerPubKey[:], s.identityECDH.PubKey().SerializeCompressed())\n\n\tp := peer.NewBrontide(pCfg)\n\n\t// TODO(roasbeef): update IP address for link-node\n\t//  * also mark last-seen, do it one single transaction?\n\n\ts.addPeer(p)\n\n\t// Once we have successfully added the peer to the server, we can\n\t// delete the previous error buffer from the server's map of error\n\t// buffers.\n\tdelete(s.peerErrors, pkStr)\n\n\t// Dispatch a goroutine to asynchronously start the peer. This process\n\t// includes sending and receiving Init messages, which would be a DOS\n\t// vector if we held the server's mutex throughout the procedure.\n\ts.wg.Add(1)\n\tgo s.peerInitializer(p)\n}\n\n// addPeer adds the passed peer to the server's global state of all active\n// peers.",
      "length": 4517,
      "tokens": 401,
      "embedding": []
    },
    {
      "slug": "func (s *server) addPeer(p *peer.Brontide) {",
      "content": "func (s *server) addPeer(p *peer.Brontide) {\n\tif p == nil {\n\t\treturn\n\t}\n\n\t// Ignore new peers if we're shutting down.\n\tif s.Stopped() {\n\t\tp.Disconnect(ErrServerShuttingDown)\n\t\treturn\n\t}\n\n\t// Track the new peer in our indexes so we can quickly look it up either\n\t// according to its public key, or its peer ID.\n\t// TODO(roasbeef): pipe all requests through to the\n\t// queryHandler/peerManager\n\n\tpubSer := p.IdentityKey().SerializeCompressed()\n\tpubStr := string(pubSer)\n\n\ts.peersByPub[pubStr] = p\n\n\tif p.Inbound() {\n\t\ts.inboundPeers[pubStr] = p\n\t} else {\n\t\ts.outboundPeers[pubStr] = p\n\t}\n\n\t// Inform the peer notifier of a peer online event so that it can be reported\n\t// to clients listening for peer events.\n\tvar pubKey [33]byte\n\tcopy(pubKey[:], pubSer)\n\n\ts.peerNotifier.NotifyPeerOnline(pubKey)\n}\n\n// peerInitializer asynchronously starts a newly connected peer after it has\n// been added to the server's peer map. This method sets up a\n// peerTerminationWatcher for the given peer, and ensures that it executes even\n// if the peer failed to start. In the event of a successful connection, this\n// method reads the negotiated, local feature-bits and spawns the appropriate\n// graph synchronization method. Any registered clients of NotifyWhenOnline will\n// be signaled of the new peer once the method returns.\n//\n// NOTE: This MUST be launched as a goroutine.",
      "length": 1273,
      "tokens": 202,
      "embedding": []
    },
    {
      "slug": "func (s *server) peerInitializer(p *peer.Brontide) {",
      "content": "func (s *server) peerInitializer(p *peer.Brontide) {\n\tdefer s.wg.Done()\n\n\t// Avoid initializing peers while the server is exiting.\n\tif s.Stopped() {\n\t\treturn\n\t}\n\n\t// Create a channel that will be used to signal a successful start of\n\t// the link. This prevents the peer termination watcher from beginning\n\t// its duty too early.\n\tready := make(chan struct{})\n\n\t// Before starting the peer, launch a goroutine to watch for the\n\t// unexpected termination of this peer, which will ensure all resources\n\t// are properly cleaned up, and re-establish persistent connections when\n\t// necessary. The peer termination watcher will be short circuited if\n\t// the peer is ever added to the ignorePeerTermination map, indicating\n\t// that the server has already handled the removal of this peer.\n\ts.wg.Add(1)\n\tgo s.peerTerminationWatcher(p, ready)\n\n\t// Start the peer! If an error occurs, we Disconnect the peer, which\n\t// will unblock the peerTerminationWatcher.\n\tif err := p.Start(); err != nil {\n\t\tp.Disconnect(fmt.Errorf(\"unable to start peer: %v\", err))\n\t\treturn\n\t}\n\n\t// Otherwise, signal to the peerTerminationWatcher that the peer startup\n\t// was successful, and to begin watching the peer's wait group.\n\tclose(ready)\n\n\tpubStr := string(p.IdentityKey().SerializeCompressed())\n\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\t// Check if there are listeners waiting for this peer to come online.\n\tsrvrLog.Debugf(\"Notifying that peer %v is online\", p)\n\tfor _, peerChan := range s.peerConnectedListeners[pubStr] {\n\t\tselect {\n\t\tcase peerChan <- p:\n\t\tcase <-s.quit:\n\t\t\treturn\n\t\t}\n\t}\n\tdelete(s.peerConnectedListeners, pubStr)\n}\n\n// peerTerminationWatcher waits until a peer has been disconnected unexpectedly,\n// and then cleans up all resources allocated to the peer, notifies relevant\n// sub-systems of its demise, and finally handles re-connecting to the peer if\n// it's persistent. If the server intentionally disconnects a peer, it should\n// have a corresponding entry in the ignorePeerTermination map which will cause\n// the cleanup routine to exit early. The passed `ready` chan is used to\n// synchronize when WaitForDisconnect should begin watching on the peer's\n// waitgroup. The ready chan should only be signaled if the peer starts\n// successfully, otherwise the peer should be disconnected instead.\n//\n// NOTE: This MUST be launched as a goroutine.",
      "length": 2223,
      "tokens": 340,
      "embedding": []
    },
    {
      "slug": "func (s *server) peerTerminationWatcher(p *peer.Brontide, ready chan struct{}) {",
      "content": "func (s *server) peerTerminationWatcher(p *peer.Brontide, ready chan struct{}) {\n\tdefer s.wg.Done()\n\n\tp.WaitForDisconnect(ready)\n\n\tsrvrLog.Debugf(\"Peer %v has been disconnected\", p)\n\n\t// If the server is exiting then we can bail out early ourselves as all\n\t// the other sub-systems will already be shutting down.\n\tif s.Stopped() {\n\t\treturn\n\t}\n\n\t// Next, we'll cancel all pending funding reservations with this node.\n\t// If we tried to initiate any funding flows that haven't yet finished,\n\t// then we need to unlock those committed outputs so they're still\n\t// available for use.\n\ts.fundingMgr.CancelPeerReservations(p.PubKey())\n\n\tpubKey := p.IdentityKey()\n\n\t// We'll also inform the gossiper that this peer is no longer active,\n\t// so we don't need to maintain sync state for it any longer.\n\ts.authGossiper.PruneSyncState(p.PubKey())\n\n\t// Tell the switch to remove all links associated with this peer.\n\t// Passing nil as the target link indicates that all links associated\n\t// with this interface should be closed.\n\t//\n\t// TODO(roasbeef): instead add a PurgeInterfaceLinks function?\n\tlinks, err := s.htlcSwitch.GetLinksByInterface(p.PubKey())\n\tif err != nil && err != htlcswitch.ErrNoLinksFound {\n\t\tsrvrLog.Errorf(\"Unable to get channel links for %v: %v\", p, err)\n\t}\n\n\tfor _, link := range links {\n\t\ts.htlcSwitch.RemoveLink(link.ChanID())\n\t}\n\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\t// If there were any notification requests for when this peer\n\t// disconnected, we can trigger them now.\n\tsrvrLog.Debugf(\"Notifying that peer %v is offline\", p)\n\tpubStr := string(pubKey.SerializeCompressed())\n\tfor _, offlineChan := range s.peerDisconnectedListeners[pubStr] {\n\t\tclose(offlineChan)\n\t}\n\tdelete(s.peerDisconnectedListeners, pubStr)\n\n\t// If the server has already removed this peer, we can short circuit the\n\t// peer termination watcher and skip cleanup.\n\tif _, ok := s.ignorePeerTermination[p]; ok {\n\t\tdelete(s.ignorePeerTermination, p)\n\n\t\tpubKey := p.PubKey()\n\t\tpubStr := string(pubKey[:])\n\n\t\t// If a connection callback is present, we'll go ahead and\n\t\t// execute it now that previous peer has fully disconnected. If\n\t\t// the callback is not present, this likely implies the peer was\n\t\t// purposefully disconnected via RPC, and that no reconnect\n\t\t// should be attempted.\n\t\tconnCallback, ok := s.scheduledPeerConnection[pubStr]\n\t\tif ok {\n\t\t\tdelete(s.scheduledPeerConnection, pubStr)\n\t\t\tconnCallback()\n\t\t}\n\t\treturn\n\t}\n\n\t// First, cleanup any remaining state the server has regarding the peer\n\t// in question.\n\ts.removePeer(p)\n\n\t// Next, check to see if this is a persistent peer or not.\n\tif _, ok := s.persistentPeers[pubStr]; !ok {\n\t\treturn\n\t}\n\n\t// Get the last address that we used to connect to the peer.\n\taddrs := []net.Addr{\n\t\tp.NetAddress().Address,\n\t}\n\n\t// We'll ensure that we locate all the peers advertised addresses for\n\t// reconnection purposes.\n\tadvertisedAddrs, err := s.fetchNodeAdvertisedAddrs(pubKey)\n\tswitch {\n\t// We found advertised addresses, so use them.\n\tcase err == nil:\n\t\taddrs = advertisedAddrs\n\n\t// The peer doesn't have an advertised address.\n\tcase err == errNoAdvertisedAddr:\n\t\t// If it is an outbound peer then we fall back to the existing\n\t\t// peer address.\n\t\tif !p.Inbound() {\n\t\t\tbreak\n\t\t}\n\n\t\t// Fall back to the existing peer address if\n\t\t// we're not accepting connections over Tor.\n\t\tif s.torController == nil {\n\t\t\tbreak\n\t\t}\n\n\t\t// If we are, the peer's address won't be known\n\t\t// to us (we'll see a private address, which is\n\t\t// the address used by our onion service to dial\n\t\t// to lnd), so we don't have enough information\n\t\t// to attempt a reconnect.\n\t\tsrvrLog.Debugf(\"Ignoring reconnection attempt \"+\n\t\t\t\"to inbound peer %v without \"+\n\t\t\t\"advertised address\", p)\n\t\treturn\n\n\t// We came across an error retrieving an advertised\n\t// address, log it, and fall back to the existing peer\n\t// address.\n\tdefault:\n\t\tsrvrLog.Errorf(\"Unable to retrieve advertised \"+\n\t\t\t\"address for node %x: %v\", p.PubKey(),\n\t\t\terr)\n\t}\n\n\t// Make an easy lookup map so that we can check if an address\n\t// is already in the address list that we have stored for this peer.\n\texistingAddrs := make(map[string]bool)\n\tfor _, addr := range s.persistentPeerAddrs[pubStr] {\n\t\texistingAddrs[addr.String()] = true\n\t}\n\n\t// Add any missing addresses for this peer to persistentPeerAddr.\n\tfor _, addr := range addrs {\n\t\tif existingAddrs[addr.String()] {\n\t\t\tcontinue\n\t\t}\n\n\t\ts.persistentPeerAddrs[pubStr] = append(\n\t\t\ts.persistentPeerAddrs[pubStr],\n\t\t\t&lnwire.NetAddress{\n\t\t\t\tIdentityKey: p.IdentityKey(),\n\t\t\t\tAddress:     addr,\n\t\t\t\tChainNet:    p.NetAddress().ChainNet,\n\t\t\t},\n\t\t)\n\t}\n\n\t// Record the computed backoff in the backoff map.\n\tbackoff := s.nextPeerBackoff(pubStr, p.StartTime())\n\ts.persistentPeersBackoff[pubStr] = backoff\n\n\t// Initialize a retry canceller for this peer if one does not\n\t// exist.\n\tcancelChan, ok := s.persistentRetryCancels[pubStr]\n\tif !ok {\n\t\tcancelChan = make(chan struct{})\n\t\ts.persistentRetryCancels[pubStr] = cancelChan\n\t}\n\n\t// We choose not to wait group this go routine since the Connect\n\t// call can stall for arbitrarily long if we shutdown while an\n\t// outbound connection attempt is being made.\n\tgo func() {\n\t\tsrvrLog.Debugf(\"Scheduling connection re-establishment to \"+\n\t\t\t\"persistent peer %x in %s\",\n\t\t\tp.IdentityKey().SerializeCompressed(), backoff)\n\n\t\tselect {\n\t\tcase <-time.After(backoff):\n\t\tcase <-cancelChan:\n\t\t\treturn\n\t\tcase <-s.quit:\n\t\t\treturn\n\t\t}\n\n\t\tsrvrLog.Debugf(\"Attempting to re-establish persistent \"+\n\t\t\t\"connection to peer %x\",\n\t\t\tp.IdentityKey().SerializeCompressed())\n\n\t\ts.connectToPersistentPeer(pubStr)\n\t}()\n}\n\n// connectToPersistentPeer uses all the stored addresses for a peer to attempt\n// to connect to the peer. It creates connection requests if there are\n// currently none for a given address and it removes old connection requests\n// if the associated address is no longer in the latest address list for the\n// peer.",
      "length": 5604,
      "tokens": 811,
      "embedding": []
    },
    {
      "slug": "func (s *server) connectToPersistentPeer(pubKeyStr string) {",
      "content": "func (s *server) connectToPersistentPeer(pubKeyStr string) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\t// Create an easy lookup map of the addresses we have stored for the\n\t// peer. We will remove entries from this map if we have existing\n\t// connection requests for the associated address and then any leftover\n\t// entries will indicate which addresses we should create new\n\t// connection requests for.\n\taddrMap := make(map[string]*lnwire.NetAddress)\n\tfor _, addr := range s.persistentPeerAddrs[pubKeyStr] {\n\t\taddrMap[addr.String()] = addr\n\t}\n\n\t// Go through each of the existing connection requests and\n\t// check if they correspond to the latest set of addresses. If\n\t// there is a connection requests that does not use one of the latest\n\t// advertised addresses then remove that connection request.\n\tvar updatedConnReqs []*connmgr.ConnReq\n\tfor _, connReq := range s.persistentConnReqs[pubKeyStr] {\n\t\tlnAddr := connReq.Addr.(*lnwire.NetAddress).Address.String()\n\n\t\tswitch _, ok := addrMap[lnAddr]; ok {\n\t\t// If the existing connection request is using one of the\n\t\t// latest advertised addresses for the peer then we add it to\n\t\t// updatedConnReqs and remove the associated address from\n\t\t// addrMap so that we don't recreate this connReq later on.\n\t\tcase true:\n\t\t\tupdatedConnReqs = append(\n\t\t\t\tupdatedConnReqs, connReq,\n\t\t\t)\n\t\t\tdelete(addrMap, lnAddr)\n\n\t\t// If the existing connection request is using an address that\n\t\t// is not one of the latest advertised addresses for the peer\n\t\t// then we remove the connecting request from the connection\n\t\t// manager.\n\t\tcase false:\n\t\t\tsrvrLog.Info(\n\t\t\t\t\"Removing conn req:\", connReq.Addr.String(),\n\t\t\t)\n\t\t\ts.connMgr.Remove(connReq.ID())\n\t\t}\n\t}\n\n\ts.persistentConnReqs[pubKeyStr] = updatedConnReqs\n\n\tcancelChan, ok := s.persistentRetryCancels[pubKeyStr]\n\tif !ok {\n\t\tcancelChan = make(chan struct{})\n\t\ts.persistentRetryCancels[pubKeyStr] = cancelChan\n\t}\n\n\t// Any addresses left in addrMap are new ones that we have not made\n\t// connection requests for. So create new connection requests for those.\n\t// If there is more than one address in the address map, stagger the\n\t// creation of the connection requests for those.\n\tgo func() {\n\t\tticker := time.NewTicker(multiAddrConnectionStagger)\n\t\tdefer ticker.Stop()\n\n\t\tfor _, addr := range addrMap {\n\t\t\t// Send the persistent connection request to the\n\t\t\t// connection manager, saving the request itself so we\n\t\t\t// can cancel/restart the process as needed.\n\t\t\tconnReq := &connmgr.ConnReq{\n\t\t\t\tAddr:      addr,\n\t\t\t\tPermanent: true,\n\t\t\t}\n\n\t\t\ts.mu.Lock()\n\t\t\ts.persistentConnReqs[pubKeyStr] = append(\n\t\t\t\ts.persistentConnReqs[pubKeyStr], connReq,\n\t\t\t)\n\t\t\ts.mu.Unlock()\n\n\t\t\tsrvrLog.Debugf(\"Attempting persistent connection to \"+\n\t\t\t\t\"channel peer %v\", addr)\n\n\t\t\tgo s.connMgr.Connect(connReq)\n\n\t\t\tselect {\n\t\t\tcase <-s.quit:\n\t\t\t\treturn\n\t\t\tcase <-cancelChan:\n\t\t\t\treturn\n\t\t\tcase <-ticker.C:\n\t\t\t}\n\t\t}\n\t}()\n}\n\n// removePeer removes the passed peer from the server's state of all active\n// peers.",
      "length": 2810,
      "tokens": 391,
      "embedding": []
    },
    {
      "slug": "func (s *server) removePeer(p *peer.Brontide) {",
      "content": "func (s *server) removePeer(p *peer.Brontide) {\n\tif p == nil {\n\t\treturn\n\t}\n\n\tsrvrLog.Debugf(\"removing peer %v\", p)\n\n\t// As the peer is now finished, ensure that the TCP connection is\n\t// closed and all of its related goroutines have exited.\n\tp.Disconnect(fmt.Errorf(\"server: disconnecting peer %v\", p))\n\n\t// If this peer had an active persistent connection request, remove it.\n\tif p.ConnReq() != nil {\n\t\ts.connMgr.Remove(p.ConnReq().ID())\n\t}\n\n\t// Ignore deleting peers if we're shutting down.\n\tif s.Stopped() {\n\t\treturn\n\t}\n\n\tpKey := p.PubKey()\n\tpubSer := pKey[:]\n\tpubStr := string(pubSer)\n\n\tdelete(s.peersByPub, pubStr)\n\n\tif p.Inbound() {\n\t\tdelete(s.inboundPeers, pubStr)\n\t} else {\n\t\tdelete(s.outboundPeers, pubStr)\n\t}\n\n\t// Copy the peer's error buffer across to the server if it has any items\n\t// in it so that we can restore peer errors across connections.\n\tif p.ErrorBuffer().Total() > 0 {\n\t\ts.peerErrors[pubStr] = p.ErrorBuffer()\n\t}\n\n\t// Inform the peer notifier of a peer offline event so that it can be\n\t// reported to clients listening for peer events.\n\tvar pubKey [33]byte\n\tcopy(pubKey[:], pubSer)\n\n\ts.peerNotifier.NotifyPeerOffline(pubKey)\n}\n\n// ConnectToPeer requests that the server connect to a Lightning Network peer\n// at the specified address. This function will *block* until either a\n// connection is established, or the initial handshake process fails.\n//\n// NOTE: This function is safe for concurrent access.",
      "length": 1329,
      "tokens": 203,
      "embedding": []
    },
    {
      "slug": "func (s *server) ConnectToPeer(addr *lnwire.NetAddress,",
      "content": "func (s *server) ConnectToPeer(addr *lnwire.NetAddress,\n\tperm bool, timeout time.Duration) error {\n\n\ttargetPub := string(addr.IdentityKey.SerializeCompressed())\n\n\t// Acquire mutex, but use explicit unlocking instead of defer for\n\t// better granularity.  In certain conditions, this method requires\n\t// making an outbound connection to a remote peer, which requires the\n\t// lock to be released, and subsequently reacquired.\n\ts.mu.Lock()\n\n\t// Ensure we're not already connected to this peer.\n\tpeer, err := s.findPeerByPubStr(targetPub)\n\tif err == nil {\n\t\ts.mu.Unlock()\n\t\treturn &errPeerAlreadyConnected{peer: peer}\n\t}\n\n\t// Peer was not found, continue to pursue connection with peer.\n\n\t// If there's already a pending connection request for this pubkey,\n\t// then we ignore this request to ensure we don't create a redundant\n\t// connection.\n\tif reqs, ok := s.persistentConnReqs[targetPub]; ok {\n\t\tsrvrLog.Warnf(\"Already have %d persistent connection \"+\n\t\t\t\"requests for %v, connecting anyway.\", len(reqs), addr)\n\t}\n\n\t// If there's not already a pending or active connection to this node,\n\t// then instruct the connection manager to attempt to establish a\n\t// persistent connection to the peer.\n\tsrvrLog.Debugf(\"Connecting to %v\", addr)\n\tif perm {\n\t\tconnReq := &connmgr.ConnReq{\n\t\t\tAddr:      addr,\n\t\t\tPermanent: true,\n\t\t}\n\n\t\t// Since the user requested a permanent connection, we'll set\n\t\t// the entry to true which will tell the server to continue\n\t\t// reconnecting even if the number of channels with this peer is\n\t\t// zero.\n\t\ts.persistentPeers[targetPub] = true\n\t\tif _, ok := s.persistentPeersBackoff[targetPub]; !ok {\n\t\t\ts.persistentPeersBackoff[targetPub] = s.cfg.MinBackoff\n\t\t}\n\t\ts.persistentConnReqs[targetPub] = append(\n\t\t\ts.persistentConnReqs[targetPub], connReq,\n\t\t)\n\t\ts.mu.Unlock()\n\n\t\tgo s.connMgr.Connect(connReq)\n\n\t\treturn nil\n\t}\n\ts.mu.Unlock()\n\n\t// If we're not making a persistent connection, then we'll attempt to\n\t// connect to the target peer. If the we can't make the connection, or\n\t// the crypto negotiation breaks down, then return an error to the\n\t// caller.\n\terrChan := make(chan error, 1)\n\ts.connectToPeer(addr, errChan, timeout)\n\n\tselect {\n\tcase err := <-errChan:\n\t\treturn err\n\tcase <-s.quit:\n\t\treturn ErrServerShuttingDown\n\t}\n}\n\n// connectToPeer establishes a connection to a remote peer. errChan is used to\n// notify the caller if the connection attempt has failed. Otherwise, it will be\n// closed.",
      "length": 2294,
      "tokens": 330,
      "embedding": []
    },
    {
      "slug": "func (s *server) connectToPeer(addr *lnwire.NetAddress,",
      "content": "func (s *server) connectToPeer(addr *lnwire.NetAddress,\n\terrChan chan<- error, timeout time.Duration) {\n\n\tconn, err := brontide.Dial(\n\t\ts.identityECDH, addr, timeout, s.cfg.net.Dial,\n\t)\n\tif err != nil {\n\t\tsrvrLog.Errorf(\"Unable to connect to %v: %v\", addr, err)\n\t\tselect {\n\t\tcase errChan <- err:\n\t\tcase <-s.quit:\n\t\t}\n\t\treturn\n\t}\n\n\tclose(errChan)\n\n\tsrvrLog.Tracef(\"Brontide dialer made local=%v, remote=%v\",\n\t\tconn.LocalAddr(), conn.RemoteAddr())\n\n\ts.OutboundPeerConnected(nil, conn)\n}\n\n// DisconnectPeer sends the request to server to close the connection with peer\n// identified by public key.\n//\n// NOTE: This function is safe for concurrent access.",
      "length": 570,
      "tokens": 78,
      "embedding": []
    },
    {
      "slug": "func (s *server) DisconnectPeer(pubKey *btcec.PublicKey) error {",
      "content": "func (s *server) DisconnectPeer(pubKey *btcec.PublicKey) error {\n\tpubBytes := pubKey.SerializeCompressed()\n\tpubStr := string(pubBytes)\n\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\t// Check that were actually connected to this peer. If not, then we'll\n\t// exit in an error as we can't disconnect from a peer that we're not\n\t// currently connected to.\n\tpeer, err := s.findPeerByPubStr(pubStr)\n\tif err == ErrPeerNotConnected {\n\t\treturn fmt.Errorf(\"peer %x is not connected\", pubBytes)\n\t}\n\n\tsrvrLog.Infof(\"Disconnecting from %v\", peer)\n\n\ts.cancelConnReqs(pubStr, nil)\n\n\t// If this peer was formerly a persistent connection, then we'll remove\n\t// them from this map so we don't attempt to re-connect after we\n\t// disconnect.\n\tdelete(s.persistentPeers, pubStr)\n\tdelete(s.persistentPeersBackoff, pubStr)\n\n\t// Remove the peer by calling Disconnect. Previously this was done with\n\t// removePeer, which bypassed the peerTerminationWatcher.\n\tpeer.Disconnect(fmt.Errorf(\"server: DisconnectPeer called\"))\n\n\treturn nil\n}\n\n// OpenChannel sends a request to the server to open a channel to the specified\n// peer identified by nodeKey with the passed channel funding parameters.\n//\n// NOTE: This function is safe for concurrent access.",
      "length": 1111,
      "tokens": 155,
      "embedding": []
    },
    {
      "slug": "func (s *server) OpenChannel(",
      "content": "func (s *server) OpenChannel(\n\treq *funding.InitFundingMsg) (chan *lnrpc.OpenStatusUpdate, chan error) {\n\n\t// The updateChan will have a buffer of 2, since we expect a ChanPending\n\t// + a ChanOpen update, and we want to make sure the funding process is\n\t// not blocked if the caller is not reading the updates.\n\treq.Updates = make(chan *lnrpc.OpenStatusUpdate, 2)\n\treq.Err = make(chan error, 1)\n\n\t// First attempt to locate the target peer to open a channel with, if\n\t// we're unable to locate the peer then this request will fail.\n\tpubKeyBytes := req.TargetPubkey.SerializeCompressed()\n\ts.mu.RLock()\n\tpeer, ok := s.peersByPub[string(pubKeyBytes)]\n\tif !ok {\n\t\ts.mu.RUnlock()\n\n\t\treq.Err <- fmt.Errorf(\"peer %x is not online\", pubKeyBytes)\n\t\treturn req.Updates, req.Err\n\t}\n\treq.Peer = peer\n\ts.mu.RUnlock()\n\n\t// We'll wait until the peer is active before beginning the channel\n\t// opening process.\n\tselect {\n\tcase <-peer.ActiveSignal():\n\tcase <-peer.QuitSignal():\n\t\treq.Err <- fmt.Errorf(\"peer %x disconnected\", pubKeyBytes)\n\t\treturn req.Updates, req.Err\n\tcase <-s.quit:\n\t\treq.Err <- ErrServerShuttingDown\n\t\treturn req.Updates, req.Err\n\t}\n\n\t// If the fee rate wasn't specified, then we'll use a default\n\t// confirmation target.\n\tif req.FundingFeePerKw == 0 {\n\t\testimator := s.cc.FeeEstimator\n\t\tfeeRate, err := estimator.EstimateFeePerKW(6)\n\t\tif err != nil {\n\t\t\treq.Err <- err\n\t\t\treturn req.Updates, req.Err\n\t\t}\n\t\treq.FundingFeePerKw = feeRate\n\t}\n\n\t// Spawn a goroutine to send the funding workflow request to the funding\n\t// manager. This allows the server to continue handling queries instead\n\t// of blocking on this request which is exported as a synchronous\n\t// request to the outside world.\n\tgo s.fundingMgr.InitFundingWorkflow(req)\n\n\treturn req.Updates, req.Err\n}\n\n// Peers returns a slice of all active peers.\n//\n// NOTE: This function is safe for concurrent access.",
      "length": 1782,
      "tokens": 260,
      "embedding": []
    },
    {
      "slug": "func (s *server) Peers() []*peer.Brontide {",
      "content": "func (s *server) Peers() []*peer.Brontide {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\n\tpeers := make([]*peer.Brontide, 0, len(s.peersByPub))\n\tfor _, peer := range s.peersByPub {\n\t\tpeers = append(peers, peer)\n\t}\n\n\treturn peers\n}\n\n// computeNextBackoff uses a truncated exponential backoff to compute the next\n// backoff using the value of the exiting backoff. The returned duration is\n// randomized in either direction by 1/20 to prevent tight loops from\n// stabilizing.",
      "length": 406,
      "tokens": 61,
      "embedding": []
    },
    {
      "slug": "func computeNextBackoff(currBackoff, maxBackoff time.Duration) time.Duration {",
      "content": "func computeNextBackoff(currBackoff, maxBackoff time.Duration) time.Duration {\n\t// Double the current backoff, truncating if it exceeds our maximum.\n\tnextBackoff := 2 * currBackoff\n\tif nextBackoff > maxBackoff {\n\t\tnextBackoff = maxBackoff\n\t}\n\n\t// Using 1/10 of our duration as a margin, compute a random offset to\n\t// avoid the nodes entering connection cycles.\n\tmargin := nextBackoff / 10\n\n\tvar wiggle big.Int\n\twiggle.SetUint64(uint64(margin))\n\tif _, err := rand.Int(rand.Reader, &wiggle); err != nil {\n\t\t// Randomizing is not mission critical, so we'll just return the\n\t\t// current backoff.\n\t\treturn nextBackoff\n\t}\n\n\t// Otherwise add in our wiggle, but subtract out half of the margin so\n\t// that the backoff can tweaked by 1/20 in either direction.\n\treturn nextBackoff + (time.Duration(wiggle.Uint64()) - margin/2)\n}\n\n// errNoAdvertisedAddr is an error returned when we attempt to retrieve the\n// advertised address of a node, but they don't have one.\nvar errNoAdvertisedAddr = errors.New(\"no advertised address found\")\n\n// fetchNodeAdvertisedAddrs attempts to fetch the advertised addresses of a node.",
      "length": 999,
      "tokens": 155,
      "embedding": []
    },
    {
      "slug": "func (s *server) fetchNodeAdvertisedAddrs(pub *btcec.PublicKey) ([]net.Addr, error) {",
      "content": "func (s *server) fetchNodeAdvertisedAddrs(pub *btcec.PublicKey) ([]net.Addr, error) {\n\tvertex, err := route.NewVertexFromBytes(pub.SerializeCompressed())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tnode, err := s.graphDB.FetchLightningNode(vertex)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif len(node.Addresses) == 0 {\n\t\treturn nil, errNoAdvertisedAddr\n\t}\n\n\treturn node.Addresses, nil\n}\n\n// fetchLastChanUpdate returns a function which is able to retrieve our latest\n// channel update for a target channel.",
      "length": 397,
      "tokens": 58,
      "embedding": []
    },
    {
      "slug": "func (s *server) fetchLastChanUpdate() func(lnwire.ShortChannelID) (",
      "content": "func (s *server) fetchLastChanUpdate() func(lnwire.ShortChannelID) (\n\t*lnwire.ChannelUpdate, error) {\n\n\tourPubKey := s.identityECDH.PubKey().SerializeCompressed()\n\treturn func(cid lnwire.ShortChannelID) (*lnwire.ChannelUpdate, error) {\n\t\tinfo, edge1, edge2, err := s.chanRouter.GetChannelByID(cid)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\treturn netann.ExtractChannelUpdate(\n\t\t\tourPubKey[:], info, edge1, edge2,\n\t\t)\n\t}\n}\n\n// applyChannelUpdate applies the channel update to the different sub-systems of\n// the server. The useAlias boolean denotes whether or not to send an alias in\n// place of the real SCID.",
      "length": 525,
      "tokens": 68,
      "embedding": []
    },
    {
      "slug": "func (s *server) applyChannelUpdate(update *lnwire.ChannelUpdate,",
      "content": "func (s *server) applyChannelUpdate(update *lnwire.ChannelUpdate,\n\top *wire.OutPoint, useAlias bool) error {\n\n\tvar (\n\t\tpeerAlias    *lnwire.ShortChannelID\n\t\tdefaultAlias lnwire.ShortChannelID\n\t)\n\n\tchanID := lnwire.NewChanIDFromOutPoint(op)\n\n\t// Fetch the peer's alias from the lnwire.ChannelID so it can be used\n\t// in the ChannelUpdate if it hasn't been announced yet.\n\tif useAlias {\n\t\tfoundAlias, _ := s.aliasMgr.GetPeerAlias(chanID)\n\t\tif foundAlias != defaultAlias {\n\t\t\tpeerAlias = &foundAlias\n\t\t}\n\t}\n\n\terrChan := s.authGossiper.ProcessLocalAnnouncement(\n\t\tupdate, discovery.RemoteAlias(peerAlias),\n\t)\n\tselect {\n\tcase err := <-errChan:\n\t\treturn err\n\tcase <-s.quit:\n\t\treturn ErrServerShuttingDown\n\t}\n}\n\n// SendCustomMessage sends a custom message to the peer with the specified\n// pubkey.",
      "length": 694,
      "tokens": 90,
      "embedding": []
    },
    {
      "slug": "func (s *server) SendCustomMessage(peerPub [33]byte, msgType lnwire.MessageType,",
      "content": "func (s *server) SendCustomMessage(peerPub [33]byte, msgType lnwire.MessageType,\n\tdata []byte) error {\n\n\tpeer, err := s.FindPeerByPubStr(string(peerPub[:]))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// We'll wait until the peer is active.\n\tselect {\n\tcase <-peer.ActiveSignal():\n\tcase <-peer.QuitSignal():\n\t\treturn fmt.Errorf(\"peer %x disconnected\", peerPub)\n\tcase <-s.quit:\n\t\treturn ErrServerShuttingDown\n\t}\n\n\tmsg, err := lnwire.NewCustom(msgType, data)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Send the message as low-priority. For now we assume that all\n\t// application-defined message are low priority.\n\treturn peer.SendMessageLazy(true, msg)\n}\n\n// newSweepPkScriptGen creates closure that generates a new public key script\n// which should be used to sweep any funds into the on-chain wallet.\n// Specifically, the script generated is a version 0, pay-to-witness-pubkey-hash\n// (p2wkh) output.",
      "length": 775,
      "tokens": 112,
      "embedding": []
    },
    {
      "slug": "func newSweepPkScriptGen(",
      "content": "func newSweepPkScriptGen(\n\twallet lnwallet.WalletController) func() ([]byte, error) {\n\n\treturn func() ([]byte, error) {\n\t\tsweepAddr, err := wallet.NewAddress(\n\t\t\tlnwallet.TaprootPubkey, false,\n\t\t\tlnwallet.DefaultAccountName,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\treturn txscript.PayToAddrScript(sweepAddr)\n\t}\n}\n\n// shouldPeerBootstrap returns true if we should attempt to perform peer\n// bootstrapping to actively seek our peers using the set of active network\n// bootstrappers.",
      "length": 445,
      "tokens": 58,
      "embedding": []
    },
    {
      "slug": "func shouldPeerBootstrap(cfg *Config) bool {",
      "content": "func shouldPeerBootstrap(cfg *Config) bool {\n\tisSimnet := (cfg.Bitcoin.SimNet || cfg.Litecoin.SimNet)\n\tisSignet := (cfg.Bitcoin.SigNet || cfg.Litecoin.SigNet)\n\tisRegtest := (cfg.Bitcoin.RegTest || cfg.Litecoin.RegTest)\n\tisDevNetwork := isSimnet || isSignet || isRegtest\n\n\t// TODO(yy): remove the check on simnet/regtest such that the itest is\n\t// covering the bootstrapping process.\n\treturn !cfg.NoNetBootstrap && !isDevNetwork\n}\n",
      "length": 376,
      "tokens": 44,
      "embedding": []
    }
  ]
}