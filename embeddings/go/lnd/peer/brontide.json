{
  "filepath": "../implementations/go/lnd/peer/brontide.go",
  "package": "peer",
  "sections": [
    {
      "slug": "type outgoingMsg struct {",
      "content": "type outgoingMsg struct {\n\tpriority bool\n\tmsg      lnwire.Message\n\terrChan  chan error // MUST be buffered.\n}\n\n// newChannelMsg packages a channeldb.OpenChannel with a channel that allows\n// the receiver of the request to report when the channel creation process has\n// completed.",
      "length": 247,
      "tokens": 38,
      "embedding": []
    },
    {
      "slug": "type newChannelMsg struct {",
      "content": "type newChannelMsg struct {\n\tchannel *channeldb.OpenChannel\n\terr     chan error\n}\n",
      "length": 51,
      "tokens": 6,
      "embedding": []
    },
    {
      "slug": "type customMsg struct {",
      "content": "type customMsg struct {\n\tpeer [33]byte\n\tmsg  lnwire.Custom\n}\n\n// closeMsg is a wrapper struct around any wire messages that deal with the\n// cooperative channel closure negotiation process. This struct includes the\n// raw channel ID targeted along with the original message.",
      "length": 244,
      "tokens": 39,
      "embedding": []
    },
    {
      "slug": "type closeMsg struct {",
      "content": "type closeMsg struct {\n\tcid lnwire.ChannelID\n\tmsg lnwire.Message\n}\n\n// PendingUpdate describes the pending state of a closing channel.",
      "length": 107,
      "tokens": 15,
      "embedding": []
    },
    {
      "slug": "type PendingUpdate struct {",
      "content": "type PendingUpdate struct {\n\tTxid        []byte\n\tOutputIndex uint32\n}\n\n// ChannelCloseUpdate contains the outcome of the close channel operation.",
      "length": 113,
      "tokens": 15,
      "embedding": []
    },
    {
      "slug": "type ChannelCloseUpdate struct {",
      "content": "type ChannelCloseUpdate struct {\n\tClosingTxid []byte\n\tSuccess     bool\n}\n\n// TimestampedError is a timestamped error that is used to store the most recent\n// errors we have experienced with our peers.",
      "length": 162,
      "tokens": 27,
      "embedding": []
    },
    {
      "slug": "type TimestampedError struct {",
      "content": "type TimestampedError struct {\n\tError     error\n\tTimestamp time.Time\n}\n\n// Config defines configuration fields that are necessary for a peer object\n// to function.",
      "length": 127,
      "tokens": 20,
      "embedding": []
    },
    {
      "slug": "type Config struct {",
      "content": "type Config struct {\n\t// Conn is the underlying network connection for this peer.\n\tConn MessageConn\n\n\t// ConnReq stores information related to the persistent connection request\n\t// for this peer.\n\tConnReq *connmgr.ConnReq\n\n\t// PubKeyBytes is the serialized, compressed public key of this peer.\n\tPubKeyBytes [33]byte\n\n\t// Addr is the network address of the peer.\n\tAddr *lnwire.NetAddress\n\n\t// Inbound indicates whether or not the peer is an inbound peer.\n\tInbound bool\n\n\t// Features is the set of features that we advertise to the remote party.\n\tFeatures *lnwire.FeatureVector\n\n\t// LegacyFeatures is the set of features that we advertise to the remote\n\t// peer for backwards compatibility. Nodes that have not implemented\n\t// flat features will still be able to read our feature bits from the\n\t// legacy global field, but we will also advertise everything in the\n\t// default features field.\n\tLegacyFeatures *lnwire.FeatureVector\n\n\t// OutgoingCltvRejectDelta defines the number of blocks before expiry of\n\t// an htlc where we don't offer it anymore.\n\tOutgoingCltvRejectDelta uint32\n\n\t// ChanActiveTimeout specifies the duration the peer will wait to request\n\t// a channel reenable, beginning from the time the peer was started.\n\tChanActiveTimeout time.Duration\n\n\t// ErrorBuffer stores a set of errors related to a peer. It contains error\n\t// messages that our peer has recently sent us over the wire and records of\n\t// unknown messages that were sent to us so that we can have a full track\n\t// record of the communication errors we have had with our peer. If we\n\t// choose to disconnect from a peer, it also stores the reason we had for\n\t// disconnecting.\n\tErrorBuffer *queue.CircularBuffer\n\n\t// WritePool is the task pool that manages reuse of write buffers. Write\n\t// tasks are submitted to the pool in order to conserve the total number of\n\t// write buffers allocated at any one time, and decouple write buffer\n\t// allocation from the peer life cycle.\n\tWritePool *pool.Write\n\n\t// ReadPool is the task pool that manages reuse of read buffers.\n\tReadPool *pool.Read\n\n\t// Switch is a pointer to the htlcswitch. It is used to setup, get, and\n\t// tear-down ChannelLinks.\n\tSwitch messageSwitch\n\n\t// InterceptSwitch is a pointer to the InterceptableSwitch, a wrapper around\n\t// the regular Switch. We only export it here to pass ForwardPackets to the\n\t// ChannelLinkConfig.\n\tInterceptSwitch *htlcswitch.InterceptableSwitch\n\n\t// ChannelDB is used to fetch opened channels, and closed channels.\n\tChannelDB *channeldb.ChannelStateDB\n\n\t// ChannelGraph is a pointer to the channel graph which is used to\n\t// query information about the set of known active channels.\n\tChannelGraph *channeldb.ChannelGraph\n\n\t// ChainArb is used to subscribe to channel events, update contract signals,\n\t// and force close channels.\n\tChainArb *contractcourt.ChainArbitrator\n\n\t// AuthGossiper is needed so that the Brontide impl can register with the\n\t// gossiper and process remote channel announcements.\n\tAuthGossiper *discovery.AuthenticatedGossiper\n\n\t// ChanStatusMgr is used to set or un-set the disabled bit in channel\n\t// updates.\n\tChanStatusMgr *netann.ChanStatusManager\n\n\t// ChainIO is used to retrieve the best block.\n\tChainIO lnwallet.BlockChainIO\n\n\t// FeeEstimator is used to compute our target ideal fee-per-kw when\n\t// initializing the coop close process.\n\tFeeEstimator chainfee.Estimator\n\n\t// Signer is used when creating *lnwallet.LightningChannel instances.\n\tSigner input.Signer\n\n\t// SigPool is used when creating *lnwallet.LightningChannel instances.\n\tSigPool *lnwallet.SigPool\n\n\t// Wallet is used to publish transactions and generates delivery\n\t// scripts during the coop close process.\n\tWallet *lnwallet.LightningWallet\n\n\t// ChainNotifier is used to receive confirmations of a coop close\n\t// transaction.\n\tChainNotifier chainntnfs.ChainNotifier\n\n\t// RoutingPolicy is used to set the forwarding policy for links created by\n\t// the Brontide.\n\tRoutingPolicy htlcswitch.ForwardingPolicy\n\n\t// Sphinx is used when setting up ChannelLinks so they can decode sphinx\n\t// onion blobs.\n\tSphinx *hop.OnionProcessor\n\n\t// WitnessBeacon is used when setting up ChannelLinks so they can add any\n\t// preimages that they learn.\n\tWitnessBeacon contractcourt.WitnessBeacon\n\n\t// Invoices is passed to the ChannelLink on creation and handles all\n\t// invoice-related logic.\n\tInvoices *invoices.InvoiceRegistry\n\n\t// ChannelNotifier is used by the link to notify other sub-systems about\n\t// channel-related events and by the Brontide to subscribe to\n\t// ActiveLinkEvents.\n\tChannelNotifier *channelnotifier.ChannelNotifier\n\n\t// HtlcNotifier is used when creating a ChannelLink.\n\tHtlcNotifier *htlcswitch.HtlcNotifier\n\n\t// TowerClient is used by legacy channels to backup revoked states.\n\tTowerClient wtclient.Client\n\n\t// AnchorTowerClient is used by anchor channels to backup revoked\n\t// states.\n\tAnchorTowerClient wtclient.Client\n\n\t// DisconnectPeer is used to disconnect this peer if the cooperative close\n\t// process fails.\n\tDisconnectPeer func(*btcec.PublicKey) error\n\n\t// GenNodeAnnouncement is used to send our node announcement to the remote\n\t// on startup.\n\tGenNodeAnnouncement func(bool,\n\t\t...netann.NodeAnnModifier) (lnwire.NodeAnnouncement, error)\n\n\t// PrunePersistentPeerConnection is used to remove all internal state\n\t// related to this peer in the server.\n\tPrunePersistentPeerConnection func([33]byte)\n\n\t// FetchLastChanUpdate fetches our latest channel update for a target\n\t// channel.\n\tFetchLastChanUpdate func(lnwire.ShortChannelID) (*lnwire.ChannelUpdate,\n\t\terror)\n\n\t// FundingManager is an implementation of the funding.Controller interface.\n\tFundingManager funding.Controller\n\n\t// Hodl is used when creating ChannelLinks to specify HodlFlags as\n\t// breakpoints in dev builds.\n\tHodl *hodl.Config\n\n\t// UnsafeReplay is used when creating ChannelLinks to specify whether or\n\t// not to replay adds on its commitment tx.\n\tUnsafeReplay bool\n\n\t// MaxOutgoingCltvExpiry is used when creating ChannelLinks and is the max\n\t// number of blocks that funds could be locked up for when forwarding\n\t// payments.\n\tMaxOutgoingCltvExpiry uint32\n\n\t// MaxChannelFeeAllocation is used when creating ChannelLinks and is the\n\t// maximum percentage of total funds that can be allocated to a channel's\n\t// commitment fee. This only applies for the initiator of the channel.\n\tMaxChannelFeeAllocation float64\n\n\t// MaxAnchorsCommitFeeRate is the maximum fee rate we'll use as an\n\t// initiator for anchor channel commitments.\n\tMaxAnchorsCommitFeeRate chainfee.SatPerKWeight\n\n\t// CoopCloseTargetConfs is the confirmation target that will be used\n\t// to estimate the fee rate to use during a cooperative channel\n\t// closure initiated by the remote peer.\n\tCoopCloseTargetConfs uint32\n\n\t// ServerPubKey is the serialized, compressed public key of our lnd node.\n\t// It is used to determine which policy (channel edge) to pass to the\n\t// ChannelLink.\n\tServerPubKey [33]byte\n\n\t// ChannelCommitInterval is the maximum time that is allowed to pass between\n\t// receiving a channel state update and signing the next commitment.\n\t// Setting this to a longer duration allows for more efficient channel\n\t// operations at the cost of latency.\n\tChannelCommitInterval time.Duration\n\n\t// PendingCommitInterval is the maximum time that is allowed to pass\n\t// while waiting for the remote party to revoke a locally initiated\n\t// commitment state. Setting this to a longer duration if a slow\n\t// response is expected from the remote party or large number of\n\t// payments are attempted at the same time.\n\tPendingCommitInterval time.Duration\n\n\t// ChannelCommitBatchSize is the maximum number of channel state updates\n\t// that is accumulated before signing a new commitment.\n\tChannelCommitBatchSize uint32\n\n\t// HandleCustomMessage is called whenever a custom message is received\n\t// from the peer.\n\tHandleCustomMessage func(peer [33]byte, msg *lnwire.Custom) error\n\n\t// GetAliases is passed to created links so the Switch and link can be\n\t// aware of the channel's aliases.\n\tGetAliases func(base lnwire.ShortChannelID) []lnwire.ShortChannelID\n\n\t// RequestAlias allows the Brontide struct to request an alias to send\n\t// to the peer.\n\tRequestAlias func() (lnwire.ShortChannelID, error)\n\n\t// AddLocalAlias persists an alias to an underlying alias store.\n\tAddLocalAlias func(alias, base lnwire.ShortChannelID,\n\t\tgossip bool) error\n\n\t// PongBuf is a slice we'll reuse instead of allocating memory on the\n\t// heap. Since only reads will occur and no writes, there is no need\n\t// for any synchronization primitives. As a result, it's safe to share\n\t// this across multiple Peer struct instances.\n\tPongBuf []byte\n\n\t// Quit is the server's quit channel. If this is closed, we halt operation.\n\tQuit chan struct{}\n}\n\n// Brontide is an active peer on the Lightning Network. This struct is responsible\n// for managing any channel state related to this peer. To do so, it has\n// several helper goroutines to handle events such as HTLC timeouts, new\n// funding workflow, and detecting an uncooperative closure of any active\n// channels.\n// TODO(roasbeef): proper reconnection logic.",
      "length": 8843,
      "tokens": 1286,
      "embedding": []
    },
    {
      "slug": "type Brontide struct {",
      "content": "type Brontide struct {\n\t// MUST be used atomically.\n\tstarted    int32\n\tdisconnect int32\n\n\t// MUST be used atomically.\n\tbytesReceived uint64\n\tbytesSent     uint64\n\n\t// pingTime is a rough estimate of the RTT (round-trip-time) between us\n\t// and the connected peer. This time is expressed in microseconds.\n\t// To be used atomically.\n\t// TODO(roasbeef): also use a WMA or EMA?\n\tpingTime int64\n\n\t// pingLastSend is the Unix time expressed in nanoseconds when we sent\n\t// our last ping message. To be used atomically.\n\tpingLastSend int64\n\n\t// lastPingPayload stores an unsafe pointer wrapped as an atomic\n\t// variable which points to the last payload the remote party sent us\n\t// as their ping.\n\t//\n\t// MUST be used atomically.\n\tlastPingPayload atomic.Value\n\n\tcfg Config\n\n\t// activeSignal when closed signals that the peer is now active and\n\t// ready to process messages.\n\tactiveSignal chan struct{}\n\n\t// startTime is the time this peer connection was successfully established.\n\t// It will be zero for peers that did not successfully call Start().\n\tstartTime time.Time\n\n\t// sendQueue is the channel which is used to queue outgoing messages to be\n\t// written onto the wire. Note that this channel is unbuffered.\n\tsendQueue chan outgoingMsg\n\n\t// outgoingQueue is a buffered channel which allows second/third party\n\t// objects to queue messages to be sent out on the wire.\n\toutgoingQueue chan outgoingMsg\n\n\t// activeChanMtx protects access to the activeChannels and\n\t// addedChannels maps.\n\tactiveChanMtx sync.RWMutex\n\n\t// activeChannels is a map which stores the state machines of all\n\t// active channels. Channels are indexed into the map by the txid of\n\t// the funding transaction which opened the channel.\n\t//\n\t// NOTE: On startup, pending channels are stored as nil in this map.\n\t// Confirmed channels have channel data populated in the map. This means\n\t// that accesses to this map should nil-check the LightningChannel to\n\t// see if this is a pending channel or not. The tradeoff here is either\n\t// having two maps everywhere (one for pending, one for confirmed chans)\n\t// or having an extra nil-check per access.\n\tactiveChannels map[lnwire.ChannelID]*lnwallet.LightningChannel\n\n\t// addedChannels tracks any new channels opened during this peer's\n\t// lifecycle. We use this to filter out these new channels when the time\n\t// comes to request a reenable for active channels, since they will have\n\t// waited a shorter duration.\n\taddedChannels map[lnwire.ChannelID]struct{}\n\n\t// newChannels is used by the fundingManager to send fully opened\n\t// channels to the source peer which handled the funding workflow.\n\tnewChannels chan *newChannelMsg\n\n\t// activeMsgStreams is a map from channel id to the channel streams that\n\t// proxy messages to individual, active links.\n\tactiveMsgStreams map[lnwire.ChannelID]*msgStream\n\n\t// activeChanCloses is a map that keeps track of all the active\n\t// cooperative channel closures. Any channel closing messages are directed\n\t// to one of these active state machines. Once the channel has been closed,\n\t// the state machine will be deleted from the map.\n\tactiveChanCloses map[lnwire.ChannelID]*chancloser.ChanCloser\n\n\t// localCloseChanReqs is a channel in which any local requests to close\n\t// a particular channel are sent over.\n\tlocalCloseChanReqs chan *htlcswitch.ChanClose\n\n\t// linkFailures receives all reported channel failures from the switch,\n\t// and instructs the channelManager to clean remaining channel state.\n\tlinkFailures chan linkFailureReport\n\n\t// chanCloseMsgs is a channel that any message related to channel\n\t// closures are sent over. This includes lnwire.Shutdown message as\n\t// well as lnwire.ClosingSigned messages.\n\tchanCloseMsgs chan *closeMsg\n\n\t// remoteFeatures is the feature vector received from the peer during\n\t// the connection handshake.\n\tremoteFeatures *lnwire.FeatureVector\n\n\t// resentChanSyncMsg is a set that keeps track of which channels we\n\t// have re-sent channel reestablishment messages for. This is done to\n\t// avoid getting into loop where both peers will respond to the other\n\t// peer's chansync message with its own over and over again.\n\tresentChanSyncMsg map[lnwire.ChannelID]struct{}\n\n\t// channelEventClient is the channel event subscription client that's\n\t// used to assist retry enabling the channels. This client is only\n\t// created when the reenableTimeout is no greater than 1 minute. Once\n\t// created, it is canceled once the reenabling has been finished.\n\t//\n\t// NOTE: we choose to create the client conditionally to avoid\n\t// potentially holding lots of un-consumed events.\n\tchannelEventClient *subscribe.Client\n\n\tqueueQuit chan struct{}\n\tquit      chan struct{}\n\twg        sync.WaitGroup\n\n\t// log is a peer-specific logging instance.\n\tlog btclog.Logger\n}\n\n// A compile-time check to ensure that Brontide satisfies the lnpeer.Peer interface.\nvar _ lnpeer.Peer = (*Brontide)(nil)\n\n// NewBrontide creates a new Brontide from a peer.Config struct.",
      "length": 4784,
      "tokens": 724,
      "embedding": []
    },
    {
      "slug": "func NewBrontide(cfg Config) *Brontide {",
      "content": "func NewBrontide(cfg Config) *Brontide {\n\tlogPrefix := fmt.Sprintf(\"Peer(%x):\", cfg.PubKeyBytes)\n\n\tp := &Brontide{\n\t\tcfg:            cfg,\n\t\tactiveSignal:   make(chan struct{}),\n\t\tsendQueue:      make(chan outgoingMsg),\n\t\toutgoingQueue:  make(chan outgoingMsg),\n\t\taddedChannels:  make(map[lnwire.ChannelID]struct{}),\n\t\tactiveChannels: make(map[lnwire.ChannelID]*lnwallet.LightningChannel),\n\t\tnewChannels:    make(chan *newChannelMsg, 1),\n\n\t\tactiveMsgStreams:   make(map[lnwire.ChannelID]*msgStream),\n\t\tactiveChanCloses:   make(map[lnwire.ChannelID]*chancloser.ChanCloser),\n\t\tlocalCloseChanReqs: make(chan *htlcswitch.ChanClose),\n\t\tlinkFailures:       make(chan linkFailureReport),\n\t\tchanCloseMsgs:      make(chan *closeMsg),\n\t\tresentChanSyncMsg:  make(map[lnwire.ChannelID]struct{}),\n\t\tqueueQuit:          make(chan struct{}),\n\t\tquit:               make(chan struct{}),\n\t\tlog:                build.NewPrefixLog(logPrefix, peerLog),\n\t}\n\n\treturn p\n}\n\n// Start starts all helper goroutines the peer needs for normal operations.  In\n// the case this peer has already been started, then this function is a noop.",
      "length": 1038,
      "tokens": 82,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) Start() error {",
      "content": "func (p *Brontide) Start() error {\n\tif atomic.AddInt32(&p.started, 1) != 1 {\n\t\treturn nil\n\t}\n\n\tp.log.Tracef(\"starting with conn[%v->%v]\",\n\t\tp.cfg.Conn.LocalAddr(), p.cfg.Conn.RemoteAddr())\n\n\t// Fetch and then load all the active channels we have with this remote\n\t// peer from the database.\n\tactiveChans, err := p.cfg.ChannelDB.FetchOpenChannels(\n\t\tp.cfg.Addr.IdentityKey,\n\t)\n\tif err != nil {\n\t\tp.log.Errorf(\"Unable to fetch active chans \"+\n\t\t\t\"for peer: %v\", err)\n\t\treturn err\n\t}\n\n\tif len(activeChans) == 0 {\n\t\tp.cfg.PrunePersistentPeerConnection(p.cfg.PubKeyBytes)\n\t}\n\n\t// Quickly check if we have any existing legacy channels with this\n\t// peer.\n\thaveLegacyChan := false\n\tfor _, c := range activeChans {\n\t\tif c.ChanType.IsTweakless() {\n\t\t\tcontinue\n\t\t}\n\n\t\thaveLegacyChan = true\n\t\tbreak\n\t}\n\n\t// Exchange local and global features, the init message should be very\n\t// first between two nodes.\n\tif err := p.sendInitMsg(haveLegacyChan); err != nil {\n\t\treturn fmt.Errorf(\"unable to send init msg: %v\", err)\n\t}\n\n\t// Before we launch any of the helper goroutines off the peer struct,\n\t// we'll first ensure proper adherence to the p2p protocol. The init\n\t// message MUST be sent before any other message.\n\treadErr := make(chan error, 1)\n\tmsgChan := make(chan lnwire.Message, 1)\n\tp.wg.Add(1)\n\tgo func() {\n\t\tdefer p.wg.Done()\n\n\t\tmsg, err := p.readNextMessage()\n\t\tif err != nil {\n\t\t\treadErr <- err\n\t\t\tmsgChan <- nil\n\t\t\treturn\n\t\t}\n\t\treadErr <- nil\n\t\tmsgChan <- msg\n\t}()\n\n\tselect {\n\t// In order to avoid blocking indefinitely, we'll give the other peer\n\t// an upper timeout to respond before we bail out early.\n\tcase <-time.After(handshakeTimeout):\n\t\treturn fmt.Errorf(\"peer did not complete handshake within %v\",\n\t\t\thandshakeTimeout)\n\tcase err := <-readErr:\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to read init msg: %v\", err)\n\t\t}\n\t}\n\n\t// Once the init message arrives, we can parse it so we can figure out\n\t// the negotiation of features for this session.\n\tmsg := <-msgChan\n\tif msg, ok := msg.(*lnwire.Init); ok {\n\t\tif err := p.handleInitMsg(msg); err != nil {\n\t\t\tp.storeError(err)\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\treturn errors.New(\"very first message between nodes \" +\n\t\t\t\"must be init message\")\n\t}\n\n\t// Next, load all the active channels we have with this peer,\n\t// registering them with the switch and launching the necessary\n\t// goroutines required to operate them.\n\tp.log.Debugf(\"Loaded %v active channels from database\",\n\t\tlen(activeChans))\n\n\t// Conditionally subscribe to channel events before loading channels so\n\t// we won't miss events. This subscription is used to listen to active\n\t// channel event when reenabling channels. Once the reenabling process\n\t// is finished, this subscription will be canceled.\n\t//\n\t// NOTE: ChannelNotifier must be started before subscribing events\n\t// otherwise we'd panic here.\n\tif err := p.attachChannelEventSubscription(); err != nil {\n\t\treturn err\n\t}\n\n\tmsgs, err := p.loadActiveChannels(activeChans)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to load channels: %v\", err)\n\t}\n\n\tp.startTime = time.Now()\n\n\tp.wg.Add(5)\n\tgo p.queueHandler()\n\tgo p.writeHandler()\n\tgo p.readHandler()\n\tgo p.channelManager()\n\tgo p.pingHandler()\n\n\t// Signal to any external processes that the peer is now active.\n\tclose(p.activeSignal)\n\n\t// Now that the peer has started up, we send any channel sync messages\n\t// that must be resent for borked channels.\n\tif len(msgs) > 0 {\n\t\tp.log.Infof(\"Sending %d channel sync messages to peer after \"+\n\t\t\t\"loading active channels\", len(msgs))\n\t\tif err := p.SendMessage(true, msgs...); err != nil {\n\t\t\tp.log.Warnf(\"Failed sending channel sync \"+\n\t\t\t\t\"messages to peer: %v\", err)\n\t\t}\n\t}\n\n\t// Node announcements don't propagate very well throughout the network\n\t// as there isn't a way to efficiently query for them through their\n\t// timestamp, mostly affecting nodes that were offline during the time\n\t// of broadcast. We'll resend our node announcement to the remote peer\n\t// as a best-effort delivery such that it can also propagate to their\n\t// peers. To ensure they can successfully process it in most cases,\n\t// we'll only resend it as long as we have at least one confirmed\n\t// advertised channel with the remote peer.\n\t//\n\t// TODO(wilmer): Remove this once we're able to query for node\n\t// announcements through their timestamps.\n\tp.maybeSendNodeAnn(activeChans)\n\n\treturn nil\n}\n\n// initGossipSync initializes either a gossip syncer or an initial routing\n// dump, depending on the negotiated synchronization method.",
      "length": 4293,
      "tokens": 661,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) initGossipSync() {",
      "content": "func (p *Brontide) initGossipSync() {\n\t// If the remote peer knows of the new gossip queries feature, then\n\t// we'll create a new gossipSyncer in the AuthenticatedGossiper for it.\n\tif p.remoteFeatures.HasFeature(lnwire.GossipQueriesOptional) {\n\t\tp.log.Info(\"Negotiated chan series queries\")\n\n\t\t// Register the peer's gossip syncer with the gossiper.\n\t\t// This blocks synchronously to ensure the gossip syncer is\n\t\t// registered with the gossiper before attempting to read\n\t\t// messages from the remote peer.\n\t\t//\n\t\t// TODO(wilmer): Only sync updates from non-channel peers. This\n\t\t// requires an improved version of the current network\n\t\t// bootstrapper to ensure we can find and connect to non-channel\n\t\t// peers.\n\t\tp.cfg.AuthGossiper.InitSyncState(p)\n\t}\n}\n\n// taprootShutdownAllowed returns true if both parties have negotiated the\n// shutdown-any-segwit feature.",
      "length": 808,
      "tokens": 113,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) taprootShutdownAllowed() bool {",
      "content": "func (p *Brontide) taprootShutdownAllowed() bool {\n\treturn p.RemoteFeatures().HasFeature(lnwire.ShutdownAnySegwitOptional) &&\n\t\tp.LocalFeatures().HasFeature(lnwire.ShutdownAnySegwitOptional)\n}\n\n// QuitSignal is a method that should return a channel which will be sent upon\n// or closed once the backing peer exits. This allows callers using the\n// interface to cancel any processing in the event the backing implementation\n// exits.\n//\n// NOTE: Part of the lnpeer.Peer interface.",
      "length": 419,
      "tokens": 55,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) QuitSignal() <-chan struct{} {",
      "content": "func (p *Brontide) QuitSignal() <-chan struct{} {\n\treturn p.quit\n}\n\n// loadActiveChannels creates indexes within the peer for tracking all active\n// channels returned by the database. It returns a slice of channel reestablish\n// messages that should be sent to the peer immediately, in case we have borked\n// channels that haven't been closed yet.",
      "length": 291,
      "tokens": 49,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) loadActiveChannels(chans []*channeldb.OpenChannel) (",
      "content": "func (p *Brontide) loadActiveChannels(chans []*channeldb.OpenChannel) (\n\t[]lnwire.Message, error) {\n\n\t// Return a slice of messages to send to the peers in case the channel\n\t// cannot be loaded normally.\n\tvar msgs []lnwire.Message\n\n\tscidAliasNegotiated := p.hasNegotiatedScidAlias()\n\n\tfor _, dbChan := range chans {\n\t\thasScidFeature := dbChan.ChanType.HasScidAliasFeature()\n\t\tif scidAliasNegotiated && !hasScidFeature {\n\t\t\t// We'll request and store an alias, making sure that a\n\t\t\t// gossiper mapping is not created for the alias to the\n\t\t\t// real SCID. This is done because the peer and funding\n\t\t\t// manager are not aware of each other's states and if\n\t\t\t// we did not do this, we would accept alias channel\n\t\t\t// updates after 6 confirmations, which would be buggy.\n\t\t\t// We'll queue a funding_locked message with the new\n\t\t\t// alias. This should technically be done *after* the\n\t\t\t// reestablish, but this behavior is pre-existing since\n\t\t\t// the funding manager may already queue a\n\t\t\t// funding_locked before the channel_reestablish.\n\t\t\tif !dbChan.IsPending {\n\t\t\t\taliasScid, err := p.cfg.RequestAlias()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\n\t\t\t\terr = p.cfg.AddLocalAlias(\n\t\t\t\t\taliasScid, dbChan.ShortChanID(), false,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\n\t\t\t\tchanID := lnwire.NewChanIDFromOutPoint(\n\t\t\t\t\t&dbChan.FundingOutpoint,\n\t\t\t\t)\n\n\t\t\t\t// Fetch the second commitment point to send in\n\t\t\t\t// the funding_locked message.\n\t\t\t\tsecond, err := dbChan.SecondCommitmentPoint()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\n\t\t\t\tfundingLockedMsg := lnwire.NewFundingLocked(\n\t\t\t\t\tchanID, second,\n\t\t\t\t)\n\t\t\t\tfundingLockedMsg.AliasScid = &aliasScid\n\n\t\t\t\tmsgs = append(msgs, fundingLockedMsg)\n\t\t\t}\n\n\t\t\t// If we've negotiated the option-scid-alias feature\n\t\t\t// and this channel does not have ScidAliasFeature set\n\t\t\t// to true due to an upgrade where the feature bit was\n\t\t\t// turned on, we'll update the channel's database\n\t\t\t// state.\n\t\t\terr := dbChan.MarkScidAliasNegotiated()\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\n\t\tlnChan, err := lnwallet.NewLightningChannel(\n\t\t\tp.cfg.Signer, dbChan, p.cfg.SigPool,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tchanPoint := &dbChan.FundingOutpoint\n\n\t\tchanID := lnwire.NewChanIDFromOutPoint(chanPoint)\n\n\t\tp.log.Infof(\"loading ChannelPoint(%v)\", chanPoint)\n\n\t\t// Skip adding any permanently irreconcilable channels to the\n\t\t// htlcswitch.\n\t\tif !dbChan.HasChanStatus(channeldb.ChanStatusDefault) &&\n\t\t\t!dbChan.HasChanStatus(channeldb.ChanStatusRestored) {\n\n\t\t\tp.log.Warnf(\"ChannelPoint(%v) has status %v, won't \"+\n\t\t\t\t\"start.\", chanPoint, dbChan.ChanStatus())\n\n\t\t\t// To help our peer recover from a potential data loss,\n\t\t\t// we resend our channel reestablish message if the\n\t\t\t// channel is in a borked state. We won't process any\n\t\t\t// channel reestablish message sent from the peer, but\n\t\t\t// that's okay since the assumption is that we did when\n\t\t\t// marking the channel borked.\n\t\t\tchanSync, err := dbChan.ChanSyncMsg()\n\t\t\tif err != nil {\n\t\t\t\tp.log.Errorf(\"Unable to create channel \"+\n\t\t\t\t\t\"reestablish message for channel %v: \"+\n\t\t\t\t\t\"%v\", chanPoint, err)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tmsgs = append(msgs, chanSync)\n\n\t\t\t// Check if this channel needs to have the cooperative\n\t\t\t// close process restarted. If so, we'll need to send\n\t\t\t// the Shutdown message that is returned.\n\t\t\tif dbChan.HasChanStatus(\n\t\t\t\tchanneldb.ChanStatusCoopBroadcasted,\n\t\t\t) {\n\n\t\t\t\tshutdownMsg, err := p.restartCoopClose(lnChan)\n\t\t\t\tif err != nil {\n\t\t\t\t\tp.log.Errorf(\"Unable to restart \"+\n\t\t\t\t\t\t\"coop close for channel: %v\",\n\t\t\t\t\t\terr)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tif shutdownMsg == nil {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\t// Append the message to the set of messages to\n\t\t\t\t// send.\n\t\t\t\tmsgs = append(msgs, shutdownMsg)\n\t\t\t}\n\n\t\t\tcontinue\n\t\t}\n\n\t\t// Before we register this new link with the HTLC Switch, we'll\n\t\t// need to fetch its current link-layer forwarding policy from\n\t\t// the database.\n\t\tgraph := p.cfg.ChannelGraph\n\t\tinfo, p1, p2, err := graph.FetchChannelEdgesByOutpoint(chanPoint)\n\t\tif err != nil && err != channeldb.ErrEdgeNotFound {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// We'll filter out our policy from the directional channel\n\t\t// edges based whom the edge connects to. If it doesn't connect\n\t\t// to us, then we know that we were the one that advertised the\n\t\t// policy.\n\t\t//\n\t\t// TODO(roasbeef): can add helper method to get policy for\n\t\t// particular channel.\n\t\tvar selfPolicy *channeldb.ChannelEdgePolicy\n\t\tif info != nil && bytes.Equal(info.NodeKey1Bytes[:],\n\t\t\tp.cfg.ServerPubKey[:]) {\n\n\t\t\tselfPolicy = p1\n\t\t} else {\n\t\t\tselfPolicy = p2\n\t\t}\n\n\t\t// If we don't yet have an advertised routing policy, then\n\t\t// we'll use the current default, otherwise we'll translate the\n\t\t// routing policy into a forwarding policy.\n\t\tvar forwardingPolicy *htlcswitch.ForwardingPolicy\n\t\tif selfPolicy != nil {\n\t\t\tforwardingPolicy = &htlcswitch.ForwardingPolicy{\n\t\t\t\tMinHTLCOut:    selfPolicy.MinHTLC,\n\t\t\t\tMaxHTLC:       selfPolicy.MaxHTLC,\n\t\t\t\tBaseFee:       selfPolicy.FeeBaseMSat,\n\t\t\t\tFeeRate:       selfPolicy.FeeProportionalMillionths,\n\t\t\t\tTimeLockDelta: uint32(selfPolicy.TimeLockDelta),\n\t\t\t}\n\t\t} else {\n\t\t\tp.log.Warnf(\"Unable to find our forwarding policy \"+\n\t\t\t\t\"for channel %v, using default values\",\n\t\t\t\tchanPoint)\n\t\t\tforwardingPolicy = &p.cfg.RoutingPolicy\n\t\t}\n\n\t\tp.log.Tracef(\"Using link policy of: %v\",\n\t\t\tspew.Sdump(forwardingPolicy))\n\n\t\t// If the channel is pending, set the value to nil in the\n\t\t// activeChannels map. This is done to signify that the channel is\n\t\t// pending. We don't add the link to the switch here - it's the funding\n\t\t// manager's responsibility to spin up pending channels. Adding them\n\t\t// here would just be extra work as we'll tear them down when creating\n\t\t// + adding the final link.\n\t\tif lnChan.IsPending() {\n\t\t\tp.activeChanMtx.Lock()\n\t\t\tp.activeChannels[chanID] = nil\n\t\t\tp.activeChanMtx.Unlock()\n\n\t\t\tcontinue\n\t\t}\n\n\t\t// Subscribe to the set of on-chain events for this channel.\n\t\tchainEvents, err := p.cfg.ChainArb.SubscribeChannelEvents(\n\t\t\t*chanPoint,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\terr = p.addLink(\n\t\t\tchanPoint, lnChan, forwardingPolicy, chainEvents,\n\t\t\ttrue,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to add link %v to \"+\n\t\t\t\t\"switch: %v\", chanPoint, err)\n\t\t}\n\n\t\tp.activeChanMtx.Lock()\n\t\tp.activeChannels[chanID] = lnChan\n\t\tp.activeChanMtx.Unlock()\n\t}\n\n\treturn msgs, nil\n}\n\n// addLink creates and adds a new ChannelLink from the specified channel.",
      "length": 6165,
      "tokens": 843,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) addLink(chanPoint *wire.OutPoint,",
      "content": "func (p *Brontide) addLink(chanPoint *wire.OutPoint,\n\tlnChan *lnwallet.LightningChannel,\n\tforwardingPolicy *htlcswitch.ForwardingPolicy,\n\tchainEvents *contractcourt.ChainEventSubscription,\n\tsyncStates bool) error {\n\n\t// onChannelFailure will be called by the link in case the channel\n\t// fails for some reason.\n\tonChannelFailure := func(chanID lnwire.ChannelID,\n\t\tshortChanID lnwire.ShortChannelID,\n\t\tlinkErr htlcswitch.LinkFailureError) {\n\n\t\tfailure := linkFailureReport{\n\t\t\tchanPoint:   *chanPoint,\n\t\t\tchanID:      chanID,\n\t\t\tshortChanID: shortChanID,\n\t\t\tlinkErr:     linkErr,\n\t\t}\n\n\t\tselect {\n\t\tcase p.linkFailures <- failure:\n\t\tcase <-p.quit:\n\t\tcase <-p.cfg.Quit:\n\t\t}\n\t}\n\n\tupdateContractSignals := func(signals *contractcourt.ContractSignals) error {\n\t\treturn p.cfg.ChainArb.UpdateContractSignals(*chanPoint, signals)\n\t}\n\n\tnotifyContractUpdate := func(update *contractcourt.ContractUpdate) error {\n\t\treturn p.cfg.ChainArb.NotifyContractUpdate(*chanPoint, update)\n\t}\n\n\tchanType := lnChan.State().ChanType\n\n\t// Select the appropriate tower client based on the channel type. It's\n\t// okay if the clients are disabled altogether and these values are nil,\n\t// as the link will check for nilness before using either.\n\tvar towerClient htlcswitch.TowerClient\n\tif chanType.HasAnchors() {\n\t\ttowerClient = p.cfg.AnchorTowerClient\n\t} else {\n\t\ttowerClient = p.cfg.TowerClient\n\t}\n\n\t//nolint:lll\n\tlinkCfg := htlcswitch.ChannelLinkConfig{\n\t\tPeer:                   p,\n\t\tDecodeHopIterators:     p.cfg.Sphinx.DecodeHopIterators,\n\t\tExtractErrorEncrypter:  p.cfg.Sphinx.ExtractErrorEncrypter,\n\t\tFetchLastChannelUpdate: p.cfg.FetchLastChanUpdate,\n\t\tHodlMask:               p.cfg.Hodl.Mask(),\n\t\tRegistry:               p.cfg.Invoices,\n\t\tBestHeight:             p.cfg.Switch.BestHeight,\n\t\tCircuits:               p.cfg.Switch.CircuitModifier(),\n\t\tForwardPackets:         p.cfg.InterceptSwitch.ForwardPackets,\n\t\tFwrdingPolicy:          *forwardingPolicy,\n\t\tFeeEstimator:           p.cfg.FeeEstimator,\n\t\tPreimageCache:          p.cfg.WitnessBeacon,\n\t\tChainEvents:            chainEvents,\n\t\tUpdateContractSignals:  updateContractSignals,\n\t\tNotifyContractUpdate:   notifyContractUpdate,\n\t\tOnChannelFailure:       onChannelFailure,\n\t\tSyncStates:             syncStates,\n\t\tBatchTicker:            ticker.New(p.cfg.ChannelCommitInterval),\n\t\tFwdPkgGCTicker:         ticker.New(time.Hour),\n\t\tPendingCommitTicker: ticker.New(\n\t\t\tp.cfg.PendingCommitInterval,\n\t\t),\n\t\tBatchSize:               p.cfg.ChannelCommitBatchSize,\n\t\tUnsafeReplay:            p.cfg.UnsafeReplay,\n\t\tMinFeeUpdateTimeout:     htlcswitch.DefaultMinLinkFeeUpdateTimeout,\n\t\tMaxFeeUpdateTimeout:     htlcswitch.DefaultMaxLinkFeeUpdateTimeout,\n\t\tOutgoingCltvRejectDelta: p.cfg.OutgoingCltvRejectDelta,\n\t\tTowerClient:             towerClient,\n\t\tMaxOutgoingCltvExpiry:   p.cfg.MaxOutgoingCltvExpiry,\n\t\tMaxFeeAllocation:        p.cfg.MaxChannelFeeAllocation,\n\t\tMaxAnchorsCommitFeeRate: p.cfg.MaxAnchorsCommitFeeRate,\n\t\tNotifyActiveLink:        p.cfg.ChannelNotifier.NotifyActiveLinkEvent,\n\t\tNotifyActiveChannel:     p.cfg.ChannelNotifier.NotifyActiveChannelEvent,\n\t\tNotifyInactiveChannel:   p.cfg.ChannelNotifier.NotifyInactiveChannelEvent,\n\t\tNotifyInactiveLinkEvent: p.cfg.ChannelNotifier.NotifyInactiveLinkEvent,\n\t\tHtlcNotifier:            p.cfg.HtlcNotifier,\n\t\tGetAliases:              p.cfg.GetAliases,\n\t}\n\n\t// Before adding our new link, purge the switch of any pending or live\n\t// links going by the same channel id. If one is found, we'll shut it\n\t// down to ensure that the mailboxes are only ever under the control of\n\t// one link.\n\tchanID := lnwire.NewChanIDFromOutPoint(chanPoint)\n\tp.cfg.Switch.RemoveLink(chanID)\n\n\t// With the channel link created, we'll now notify the htlc switch so\n\t// this channel can be used to dispatch local payments and also\n\t// passively forward payments.\n\treturn p.cfg.Switch.CreateAndAddLink(linkCfg, lnChan)\n}\n\n// maybeSendNodeAnn sends our node announcement to the remote peer if at least\n// one confirmed public channel exists with them.",
      "length": 3857,
      "tokens": 316,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) maybeSendNodeAnn(channels []*channeldb.OpenChannel) {",
      "content": "func (p *Brontide) maybeSendNodeAnn(channels []*channeldb.OpenChannel) {\n\thasConfirmedPublicChan := false\n\tfor _, channel := range channels {\n\t\tif channel.IsPending {\n\t\t\tcontinue\n\t\t}\n\t\tif channel.ChannelFlags&lnwire.FFAnnounceChannel == 0 {\n\t\t\tcontinue\n\t\t}\n\n\t\thasConfirmedPublicChan = true\n\t\tbreak\n\t}\n\tif !hasConfirmedPublicChan {\n\t\treturn\n\t}\n\n\tourNodeAnn, err := p.cfg.GenNodeAnnouncement(false)\n\tif err != nil {\n\t\tp.log.Debugf(\"Unable to retrieve node announcement: %v\", err)\n\t\treturn\n\t}\n\n\tif err := p.SendMessageLazy(false, &ourNodeAnn); err != nil {\n\t\tp.log.Debugf(\"Unable to resend node announcement: %v\", err)\n\t}\n}\n\n// WaitForDisconnect waits until the peer has disconnected. A peer may be\n// disconnected if the local or remote side terminates the connection, or an\n// irrecoverable protocol error has been encountered. This method will only\n// begin watching the peer's waitgroup after the ready channel or the peer's\n// quit channel are signaled. The ready channel should only be signaled if a\n// call to Start returns no error. Otherwise, if the peer fails to start,\n// calling Disconnect will signal the quit channel and the method will not\n// block, since no goroutines were spawned.",
      "length": 1088,
      "tokens": 165,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) WaitForDisconnect(ready chan struct{}) {",
      "content": "func (p *Brontide) WaitForDisconnect(ready chan struct{}) {\n\tselect {\n\tcase <-ready:\n\tcase <-p.quit:\n\t}\n\n\tp.wg.Wait()\n}\n\n// Disconnect terminates the connection with the remote peer. Additionally, a\n// signal is sent to the server and htlcSwitch indicating the resources\n// allocated to the peer can now be cleaned up.",
      "length": 248,
      "tokens": 42,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) Disconnect(reason error) {",
      "content": "func (p *Brontide) Disconnect(reason error) {\n\tif !atomic.CompareAndSwapInt32(&p.disconnect, 0, 1) {\n\t\treturn\n\t}\n\n\terr := fmt.Errorf(\"disconnecting %s, reason: %v\", p, reason)\n\tp.storeError(err)\n\n\tp.log.Infof(err.Error())\n\n\t// Ensure that the TCP connection is properly closed before continuing.\n\tp.cfg.Conn.Close()\n\n\tclose(p.quit)\n}\n\n// String returns the string representation of this peer.",
      "length": 331,
      "tokens": 40,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) String() string {",
      "content": "func (p *Brontide) String() string {\n\treturn fmt.Sprintf(\"%x@%s\", p.cfg.PubKeyBytes, p.cfg.Conn.RemoteAddr())\n}\n\n// readNextMessage reads, and returns the next message on the wire along with\n// any additional raw payload.",
      "length": 180,
      "tokens": 23,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) readNextMessage() (lnwire.Message, error) {",
      "content": "func (p *Brontide) readNextMessage() (lnwire.Message, error) {\n\tnoiseConn := p.cfg.Conn\n\terr := noiseConn.SetReadDeadline(time.Time{})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tpktLen, err := noiseConn.ReadNextHeader()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// First we'll read the next _full_ message. We do this rather than\n\t// reading incrementally from the stream as the Lightning wire protocol\n\t// is message oriented and allows nodes to pad on additional data to\n\t// the message stream.\n\tvar (\n\t\tnextMsg lnwire.Message\n\t\tmsgLen  uint64\n\t)\n\terr = p.cfg.ReadPool.Submit(func(buf *buffer.Read) error {\n\t\t// Before reading the body of the message, set the read timeout\n\t\t// accordingly to ensure we don't block other readers using the\n\t\t// pool. We do so only after the task has been scheduled to\n\t\t// ensure the deadline doesn't expire while the message is in\n\t\t// the process of being scheduled.\n\t\treadDeadline := time.Now().Add(readMessageTimeout)\n\t\treadErr := noiseConn.SetReadDeadline(readDeadline)\n\t\tif readErr != nil {\n\t\t\treturn readErr\n\t\t}\n\n\t\t// The ReadNextBody method will actually end up re-using the\n\t\t// buffer, so within this closure, we can continue to use\n\t\t// rawMsg as it's just a slice into the buf from the buffer\n\t\t// pool.\n\t\trawMsg, readErr := noiseConn.ReadNextBody(buf[:pktLen])\n\t\tif readErr != nil {\n\t\t\treturn readErr\n\t\t}\n\t\tmsgLen = uint64(len(rawMsg))\n\n\t\t// Next, create a new io.Reader implementation from the raw\n\t\t// message, and use this to decode the message directly from.\n\t\tmsgReader := bytes.NewReader(rawMsg)\n\t\tnextMsg, err = lnwire.ReadMessage(msgReader, 0)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// At this point, rawMsg and buf will be returned back to the\n\t\t// buffer pool for re-use.\n\t\treturn nil\n\t})\n\tatomic.AddUint64(&p.bytesReceived, msgLen)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.logWireMessage(nextMsg, true)\n\n\treturn nextMsg, nil\n}\n\n// msgStream implements a goroutine-safe, in-order stream of messages to be\n// delivered via closure to a receiver. These messages MUST be in order due to\n// the nature of the lightning channel commitment and gossiper state machines.\n// TODO(conner): use stream handler interface to abstract out stream\n// state/logging.",
      "length": 2073,
      "tokens": 325,
      "embedding": []
    },
    {
      "slug": "type msgStream struct {",
      "content": "type msgStream struct {\n\tstreamShutdown int32 // To be used atomically.\n\n\tpeer *Brontide\n\n\tapply func(lnwire.Message)\n\n\tstartMsg string\n\tstopMsg  string\n\n\tmsgCond *sync.Cond\n\tmsgs    []lnwire.Message\n\n\tmtx sync.Mutex\n\n\tproducerSema chan struct{}\n\n\twg   sync.WaitGroup\n\tquit chan struct{}\n}\n\n// newMsgStream creates a new instance of a chanMsgStream for a particular\n// channel identified by its channel ID. bufSize is the max number of messages\n// that should be buffered in the internal queue. Callers should set this to a\n// sane value that avoids blocking unnecessarily, but doesn't allow an\n// unbounded amount of memory to be allocated to buffer incoming messages.",
      "length": 621,
      "tokens": 94,
      "embedding": []
    },
    {
      "slug": "func newMsgStream(p *Brontide, startMsg, stopMsg string, bufSize uint32,",
      "content": "func newMsgStream(p *Brontide, startMsg, stopMsg string, bufSize uint32,\n\tapply func(lnwire.Message)) *msgStream {\n\n\tstream := &msgStream{\n\t\tpeer:         p,\n\t\tapply:        apply,\n\t\tstartMsg:     startMsg,\n\t\tstopMsg:      stopMsg,\n\t\tproducerSema: make(chan struct{}, bufSize),\n\t\tquit:         make(chan struct{}),\n\t}\n\tstream.msgCond = sync.NewCond(&stream.mtx)\n\n\t// Before we return the active stream, we'll populate the producer's\n\t// semaphore channel. We'll use this to ensure that the producer won't\n\t// attempt to allocate memory in the queue for an item until it has\n\t// sufficient extra space.\n\tfor i := uint32(0); i < bufSize; i++ {\n\t\tstream.producerSema <- struct{}{}\n\t}\n\n\treturn stream\n}\n\n// Start starts the chanMsgStream.",
      "length": 638,
      "tokens": 88,
      "embedding": []
    },
    {
      "slug": "func (ms *msgStream) Start() {",
      "content": "func (ms *msgStream) Start() {\n\tms.wg.Add(1)\n\tgo ms.msgConsumer()\n}\n\n// Stop stops the chanMsgStream.",
      "length": 66,
      "tokens": 9,
      "embedding": []
    },
    {
      "slug": "func (ms *msgStream) Stop() {",
      "content": "func (ms *msgStream) Stop() {\n\t// TODO(roasbeef): signal too?\n\n\tclose(ms.quit)\n\n\t// Now that we've closed the channel, we'll repeatedly signal the msg\n\t// consumer until we've detected that it has exited.\n\tfor atomic.LoadInt32(&ms.streamShutdown) == 0 {\n\t\tms.msgCond.Signal()\n\t\ttime.Sleep(time.Millisecond * 100)\n\t}\n\n\tms.wg.Wait()\n}\n\n// msgConsumer is the main goroutine that streams messages from the peer's\n// readHandler directly to the target channel.",
      "length": 410,
      "tokens": 57,
      "embedding": []
    },
    {
      "slug": "func (ms *msgStream) msgConsumer() {",
      "content": "func (ms *msgStream) msgConsumer() {\n\tdefer ms.wg.Done()\n\tdefer peerLog.Tracef(ms.stopMsg)\n\tdefer atomic.StoreInt32(&ms.streamShutdown, 1)\n\n\tpeerLog.Tracef(ms.startMsg)\n\n\tfor {\n\t\t// First, we'll check our condition. If the queue of messages\n\t\t// is empty, then we'll wait until a new item is added.\n\t\tms.msgCond.L.Lock()\n\t\tfor len(ms.msgs) == 0 {\n\t\t\tms.msgCond.Wait()\n\n\t\t\t// If we woke up in order to exit, then we'll do so.\n\t\t\t// Otherwise, we'll check the message queue for any new\n\t\t\t// items.\n\t\t\tselect {\n\t\t\tcase <-ms.peer.quit:\n\t\t\t\tms.msgCond.L.Unlock()\n\t\t\t\treturn\n\t\t\tcase <-ms.quit:\n\t\t\t\tms.msgCond.L.Unlock()\n\t\t\t\treturn\n\t\t\tdefault:\n\t\t\t}\n\t\t}\n\n\t\t// Grab the message off the front of the queue, shifting the\n\t\t// slice's reference down one in order to remove the message\n\t\t// from the queue.\n\t\tmsg := ms.msgs[0]\n\t\tms.msgs[0] = nil // Set to nil to prevent GC leak.\n\t\tms.msgs = ms.msgs[1:]\n\n\t\tms.msgCond.L.Unlock()\n\n\t\tms.apply(msg)\n\n\t\t// We've just successfully processed an item, so we'll signal\n\t\t// to the producer that a new slot in the buffer. We'll use\n\t\t// this to bound the size of the buffer to avoid allowing it to\n\t\t// grow indefinitely.\n\t\tselect {\n\t\tcase ms.producerSema <- struct{}{}:\n\t\tcase <-ms.peer.quit:\n\t\t\treturn\n\t\tcase <-ms.quit:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// AddMsg adds a new message to the msgStream. This function is safe for\n// concurrent access.",
      "length": 1275,
      "tokens": 196,
      "embedding": []
    },
    {
      "slug": "func (ms *msgStream) AddMsg(msg lnwire.Message) {",
      "content": "func (ms *msgStream) AddMsg(msg lnwire.Message) {\n\t// First, we'll attempt to receive from the producerSema struct. This\n\t// acts as a semaphore to prevent us from indefinitely buffering\n\t// incoming items from the wire. Either the msg queue isn't full, and\n\t// we'll not block, or the queue is full, and we'll block until either\n\t// we're signalled to quit, or a slot is freed up.\n\tselect {\n\tcase <-ms.producerSema:\n\tcase <-ms.peer.quit:\n\t\treturn\n\tcase <-ms.quit:\n\t\treturn\n\t}\n\n\t// Next, we'll lock the condition, and add the message to the end of\n\t// the message queue.\n\tms.msgCond.L.Lock()\n\tms.msgs = append(ms.msgs, msg)\n\tms.msgCond.L.Unlock()\n\n\t// With the message added, we signal to the msgConsumer that there are\n\t// additional messages to consume.\n\tms.msgCond.Signal()\n}\n\n// waitUntilLinkActive waits until the target link is active and returns a\n// ChannelLink to pass messages to. It accomplishes this by subscribing to\n// an ActiveLinkEvent which is emitted by the link when it first starts up.",
      "length": 929,
      "tokens": 153,
      "embedding": []
    },
    {
      "slug": "func waitUntilLinkActive(p *Brontide,",
      "content": "func waitUntilLinkActive(p *Brontide,\n\tcid lnwire.ChannelID) htlcswitch.ChannelUpdateHandler {\n\n\t// Subscribe to receive channel events.\n\t//\n\t// NOTE: If the link is already active by SubscribeChannelEvents, then\n\t// GetLink will retrieve the link and we can send messages. If the link\n\t// becomes active between SubscribeChannelEvents and GetLink, then GetLink\n\t// will retrieve the link. If the link becomes active after GetLink, then\n\t// we will get an ActiveLinkEvent notification and retrieve the link. If\n\t// the call to GetLink is before SubscribeChannelEvents, however, there\n\t// will be a race condition.\n\tsub, err := p.cfg.ChannelNotifier.SubscribeChannelEvents()\n\tif err != nil {\n\t\t// If we have a non-nil error, then the server is shutting down and we\n\t\t// can exit here and return nil. This means no message will be delivered\n\t\t// to the link.\n\t\treturn nil\n\t}\n\tdefer sub.Cancel()\n\n\t// The link may already be active by this point, and we may have missed the\n\t// ActiveLinkEvent. Check if the link exists.\n\tlink := p.fetchLinkFromKeyAndCid(cid)\n\tif link != nil {\n\t\treturn link\n\t}\n\n\t// If the link is nil, we must wait for it to be active.\n\tfor {\n\t\tselect {\n\t\t// A new event has been sent by the ChannelNotifier. We first check\n\t\t// whether the event is an ActiveLinkEvent. If it is, we'll check\n\t\t// that the event is for this channel. Otherwise, we discard the\n\t\t// message.\n\t\tcase e := <-sub.Updates():\n\t\t\tevent, ok := e.(channelnotifier.ActiveLinkEvent)\n\t\t\tif !ok {\n\t\t\t\t// Ignore this notification.\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tchanPoint := event.ChannelPoint\n\n\t\t\t// Check whether the retrieved chanPoint matches the target\n\t\t\t// channel id.\n\t\t\tif !cid.IsChanPoint(chanPoint) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// The link shouldn't be nil as we received an\n\t\t\t// ActiveLinkEvent. If it is nil, we return nil and the\n\t\t\t// calling function should catch it.\n\t\t\treturn p.fetchLinkFromKeyAndCid(cid)\n\n\t\tcase <-p.quit:\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\n// newChanMsgStream is used to create a msgStream between the peer and\n// particular channel link in the htlcswitch. We utilize additional\n// synchronization with the fundingManager to ensure we don't attempt to\n// dispatch a message to a channel before it is fully active. A reference to the\n// channel this stream forwards to is held in scope to prevent unnecessary\n// lookups.",
      "length": 2212,
      "tokens": 361,
      "embedding": []
    },
    {
      "slug": "func newChanMsgStream(p *Brontide, cid lnwire.ChannelID) *msgStream {",
      "content": "func newChanMsgStream(p *Brontide, cid lnwire.ChannelID) *msgStream {\n\tvar chanLink htlcswitch.ChannelUpdateHandler\n\n\tapply := func(msg lnwire.Message) {\n\t\t// This check is fine because if the link no longer exists, it will\n\t\t// be removed from the activeChannels map and subsequent messages\n\t\t// shouldn't reach the chan msg stream.\n\t\tif chanLink == nil {\n\t\t\tchanLink = waitUntilLinkActive(p, cid)\n\n\t\t\t// If the link is still not active and the calling function\n\t\t\t// errored out, just return.\n\t\t\tif chanLink == nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\t// In order to avoid unnecessarily delivering message\n\t\t// as the peer is exiting, we'll check quickly to see\n\t\t// if we need to exit.\n\t\tselect {\n\t\tcase <-p.quit:\n\t\t\treturn\n\t\tdefault:\n\t\t}\n\n\t\tchanLink.HandleChannelUpdate(msg)\n\t}\n\n\treturn newMsgStream(p,\n\t\tfmt.Sprintf(\"Update stream for ChannelID(%x) created\", cid[:]),\n\t\tfmt.Sprintf(\"Update stream for ChannelID(%x) exiting\", cid[:]),\n\t\t1000,\n\t\tapply,\n\t)\n}\n\n// newDiscMsgStream is used to setup a msgStream between the peer and the\n// authenticated gossiper. This stream should be used to forward all remote\n// channel announcements.",
      "length": 1017,
      "tokens": 153,
      "embedding": []
    },
    {
      "slug": "func newDiscMsgStream(p *Brontide) *msgStream {",
      "content": "func newDiscMsgStream(p *Brontide) *msgStream {\n\tapply := func(msg lnwire.Message) {\n\t\t// TODO(yy): `ProcessRemoteAnnouncement` returns an error chan\n\t\t// and we need to process it.\n\t\tp.cfg.AuthGossiper.ProcessRemoteAnnouncement(msg, p)\n\t}\n\n\treturn newMsgStream(\n\t\tp,\n\t\t\"Update stream for gossiper created\",\n\t\t\"Update stream for gossiper exited\",\n\t\t1000,\n\t\tapply,\n\t)\n}\n\n// readHandler is responsible for reading messages off the wire in series, then\n// properly dispatching the handling of the message to the proper subsystem.\n//\n// NOTE: This method MUST be run as a goroutine.",
      "length": 512,
      "tokens": 75,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) readHandler() {",
      "content": "func (p *Brontide) readHandler() {\n\tdefer p.wg.Done()\n\n\t// We'll stop the timer after a new messages is received, and also\n\t// reset it after we process the next message.\n\tidleTimer := time.AfterFunc(idleTimeout, func() {\n\t\terr := fmt.Errorf(\"peer %s no answer for %s -- disconnecting\",\n\t\t\tp, idleTimeout)\n\t\tp.Disconnect(err)\n\t})\n\n\t// Initialize our negotiated gossip sync method before reading messages\n\t// off the wire. When using gossip queries, this ensures a gossip\n\t// syncer is active by the time query messages arrive.\n\t//\n\t// TODO(conner): have peer store gossip syncer directly and bypass\n\t// gossiper?\n\tp.initGossipSync()\n\n\tdiscStream := newDiscMsgStream(p)\n\tdiscStream.Start()\n\tdefer discStream.Stop()\nout:\n\tfor atomic.LoadInt32(&p.disconnect) == 0 {\n\t\tnextMsg, err := p.readNextMessage()\n\t\tif !idleTimer.Stop() {\n\t\t\tselect {\n\t\t\tcase <-idleTimer.C:\n\t\t\tdefault:\n\t\t\t}\n\t\t}\n\t\tif err != nil {\n\t\t\tp.log.Infof(\"unable to read message from peer: %v\", err)\n\n\t\t\t// If we could not read our peer's message due to an\n\t\t\t// unknown type or invalid alias, we continue processing\n\t\t\t// as normal. We store unknown message and address\n\t\t\t// types, as they may provide debugging insight.\n\t\t\tswitch e := err.(type) {\n\t\t\t// If this is just a message we don't yet recognize,\n\t\t\t// we'll continue processing as normal as this allows\n\t\t\t// us to introduce new messages in a forwards\n\t\t\t// compatible manner.\n\t\t\tcase *lnwire.UnknownMessage:\n\t\t\t\tp.storeError(e)\n\t\t\t\tidleTimer.Reset(idleTimeout)\n\t\t\t\tcontinue\n\n\t\t\t// If they sent us an address type that we don't yet\n\t\t\t// know of, then this isn't a wire error, so we'll\n\t\t\t// simply continue parsing the remainder of their\n\t\t\t// messages.\n\t\t\tcase *lnwire.ErrUnknownAddrType:\n\t\t\t\tp.storeError(e)\n\t\t\t\tidleTimer.Reset(idleTimeout)\n\t\t\t\tcontinue\n\n\t\t\t// If the NodeAnnouncement has an invalid alias, then\n\t\t\t// we'll log that error above and continue so we can\n\t\t\t// continue to read messages from the peer. We do not\n\t\t\t// store this error because it is of little debugging\n\t\t\t// value.\n\t\t\tcase *lnwire.ErrInvalidNodeAlias:\n\t\t\t\tidleTimer.Reset(idleTimeout)\n\t\t\t\tcontinue\n\n\t\t\t// If the error we encountered wasn't just a message we\n\t\t\t// didn't recognize, then we'll stop all processing as\n\t\t\t// this is a fatal error.\n\t\t\tdefault:\n\t\t\t\tbreak out\n\t\t\t}\n\t\t}\n\n\t\tvar (\n\t\t\ttargetChan   lnwire.ChannelID\n\t\t\tisLinkUpdate bool\n\t\t)\n\n\t\tswitch msg := nextMsg.(type) {\n\t\tcase *lnwire.Pong:\n\t\t\t// When we receive a Pong message in response to our\n\t\t\t// last ping message, we'll use the time in which we\n\t\t\t// sent the ping message to measure a rough estimate of\n\t\t\t// round trip time.\n\t\t\tpingSendTime := atomic.LoadInt64(&p.pingLastSend)\n\t\t\tdelay := (time.Now().UnixNano() - pingSendTime) / 1000\n\t\t\tatomic.StoreInt64(&p.pingTime, delay)\n\n\t\tcase *lnwire.Ping:\n\t\t\t// First, we'll store their latest ping payload within\n\t\t\t// the relevant atomic variable.\n\t\t\tp.lastPingPayload.Store(msg.PaddingBytes[:])\n\n\t\t\t// Next, we'll send over the amount of specified pong\n\t\t\t// bytes.\n\t\t\tpong := lnwire.NewPong(p.cfg.PongBuf[0:msg.NumPongBytes])\n\t\t\tp.queueMsg(pong, nil)\n\n\t\tcase *lnwire.OpenChannel,\n\t\t\t*lnwire.AcceptChannel,\n\t\t\t*lnwire.FundingCreated,\n\t\t\t*lnwire.FundingSigned,\n\t\t\t*lnwire.FundingLocked:\n\n\t\t\tp.cfg.FundingManager.ProcessFundingMsg(msg, p)\n\n\t\tcase *lnwire.Shutdown:\n\t\t\tselect {\n\t\t\tcase p.chanCloseMsgs <- &closeMsg{msg.ChannelID, msg}:\n\t\t\tcase <-p.quit:\n\t\t\t\tbreak out\n\t\t\t}\n\t\tcase *lnwire.ClosingSigned:\n\t\t\tselect {\n\t\t\tcase p.chanCloseMsgs <- &closeMsg{msg.ChannelID, msg}:\n\t\t\tcase <-p.quit:\n\t\t\t\tbreak out\n\t\t\t}\n\n\t\tcase *lnwire.Warning:\n\t\t\ttargetChan = msg.ChanID\n\t\t\tisLinkUpdate = p.handleWarning(msg)\n\n\t\tcase *lnwire.Error:\n\t\t\ttargetChan = msg.ChanID\n\t\t\tisLinkUpdate = p.handleError(msg)\n\n\t\tcase *lnwire.ChannelReestablish:\n\t\t\ttargetChan = msg.ChanID\n\t\t\tisLinkUpdate = p.isActiveChannel(targetChan)\n\n\t\t\t// If we failed to find the link in question, and the\n\t\t\t// message received was a channel sync message, then\n\t\t\t// this might be a peer trying to resync closed channel.\n\t\t\t// In this case we'll try to resend our last channel\n\t\t\t// sync message, such that the peer can recover funds\n\t\t\t// from the closed channel.\n\t\t\tif !isLinkUpdate {\n\t\t\t\terr := p.resendChanSyncMsg(targetChan)\n\t\t\t\tif err != nil {\n\t\t\t\t\t// TODO(halseth): send error to peer?\n\t\t\t\t\tp.log.Errorf(\"resend failed: %v\",\n\t\t\t\t\t\terr)\n\t\t\t\t}\n\t\t\t}\n\n\t\tcase LinkUpdater:\n\t\t\ttargetChan = msg.TargetChanID()\n\t\t\tisLinkUpdate = p.isActiveChannel(targetChan)\n\n\t\tcase *lnwire.ChannelUpdate,\n\t\t\t*lnwire.ChannelAnnouncement,\n\t\t\t*lnwire.NodeAnnouncement,\n\t\t\t*lnwire.AnnounceSignatures,\n\t\t\t*lnwire.GossipTimestampRange,\n\t\t\t*lnwire.QueryShortChanIDs,\n\t\t\t*lnwire.QueryChannelRange,\n\t\t\t*lnwire.ReplyChannelRange,\n\t\t\t*lnwire.ReplyShortChanIDsEnd:\n\n\t\t\tdiscStream.AddMsg(msg)\n\n\t\tcase *lnwire.Custom:\n\t\t\terr := p.handleCustomMessage(msg)\n\t\t\tif err != nil {\n\t\t\t\tp.storeError(err)\n\t\t\t\tp.log.Errorf(\"%v\", err)\n\t\t\t}\n\n\t\tdefault:\n\t\t\t// If the message we received is unknown to us, store\n\t\t\t// the type to track the failure.\n\t\t\terr := fmt.Errorf(\"unknown message type %v received\",\n\t\t\t\tuint16(msg.MsgType()))\n\t\t\tp.storeError(err)\n\n\t\t\tp.log.Errorf(\"%v\", err)\n\t\t}\n\n\t\tif isLinkUpdate {\n\t\t\t// If this is a channel update, then we need to feed it\n\t\t\t// into the channel's in-order message stream.\n\t\t\tchanStream, ok := p.activeMsgStreams[targetChan]\n\t\t\tif !ok {\n\t\t\t\t// If a stream hasn't yet been created, then\n\t\t\t\t// we'll do so, add it to the map, and finally\n\t\t\t\t// start it.\n\t\t\t\tchanStream = newChanMsgStream(p, targetChan)\n\t\t\t\tp.activeMsgStreams[targetChan] = chanStream\n\t\t\t\tchanStream.Start()\n\t\t\t\tdefer chanStream.Stop()\n\t\t\t}\n\n\t\t\t// With the stream obtained, add the message to the\n\t\t\t// stream so we can continue processing message.\n\t\t\tchanStream.AddMsg(nextMsg)\n\t\t}\n\n\t\tidleTimer.Reset(idleTimeout)\n\t}\n\n\tp.Disconnect(errors.New(\"read handler closed\"))\n\n\tp.log.Trace(\"readHandler for peer done\")\n}\n\n// handleCustomMessage handles the given custom message if a handler is\n// registered.",
      "length": 5693,
      "tokens": 733,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) handleCustomMessage(msg *lnwire.Custom) error {",
      "content": "func (p *Brontide) handleCustomMessage(msg *lnwire.Custom) error {\n\tif p.cfg.HandleCustomMessage == nil {\n\t\treturn fmt.Errorf(\"no custom message handler for \"+\n\t\t\t\"message type %v\", uint16(msg.MsgType()))\n\t}\n\n\treturn p.cfg.HandleCustomMessage(p.PubKey(), msg)\n}\n\n// isActiveChannel returns true if the provided channel id is active, otherwise\n// returns false.",
      "length": 284,
      "tokens": 36,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) isActiveChannel(chanID lnwire.ChannelID) bool {",
      "content": "func (p *Brontide) isActiveChannel(chanID lnwire.ChannelID) bool {\n\tp.activeChanMtx.RLock()\n\t_, ok := p.activeChannels[chanID]\n\tp.activeChanMtx.RUnlock()\n\treturn ok\n}\n\n// storeError stores an error in our peer's buffer of recent errors with the\n// current timestamp. Errors are only stored if we have at least one active\n// channel with the peer to mitigate a dos vector where a peer costlessly\n// connects to us and spams us with errors.",
      "length": 362,
      "tokens": 60,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) storeError(err error) {",
      "content": "func (p *Brontide) storeError(err error) {\n\tvar haveChannels bool\n\n\tp.activeChanMtx.RLock()\n\tfor _, channel := range p.activeChannels {\n\t\t// Pending channels will be nil in the activeChannels map.\n\t\tif channel == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\thaveChannels = true\n\t\tbreak\n\t}\n\tp.activeChanMtx.RUnlock()\n\n\t// If we do not have any active channels with the peer, we do not store\n\t// errors as a dos mitigation.\n\tif !haveChannels {\n\t\tp.log.Trace(\"no channels with peer, not storing err\")\n\t\treturn\n\t}\n\n\tp.cfg.ErrorBuffer.Add(\n\t\t&TimestampedError{Timestamp: time.Now(), Error: err},\n\t)\n}\n\n// handleWarning processes a warning message read from the remote peer. The\n// boolean return indicates whether the message should be delivered to a\n// targeted peer or not. The message gets stored in memory as an error if we\n// have open channels with the peer we received it from.\n//\n// NOTE: This method should only be called from within the readHandler.",
      "length": 860,
      "tokens": 139,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) handleWarning(msg *lnwire.Warning) bool {",
      "content": "func (p *Brontide) handleWarning(msg *lnwire.Warning) bool {\n\tswitch {\n\t// Connection wide messages should be forward the warning to all the\n\t// channels with this peer.\n\tcase msg.ChanID == lnwire.ConnectionWideID:\n\t\tfor _, chanStream := range p.activeMsgStreams {\n\t\t\tchanStream.AddMsg(msg)\n\t\t}\n\n\t\treturn false\n\n\t// If the channel ID for the warning message corresponds to a pending\n\t// channel, then the funding manager will handle the warning.\n\tcase p.cfg.FundingManager.IsPendingChannel(msg.ChanID, p):\n\t\tp.cfg.FundingManager.ProcessFundingMsg(msg, p)\n\t\treturn false\n\n\t// If not we hand the warning to the channel link for this channel.\n\tcase p.isActiveChannel(msg.ChanID):\n\t\treturn true\n\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// handleError processes an error message read from the remote peer. The boolean\n// returns indicates whether the message should be delivered to a targeted peer.\n// It stores the error we received from the peer in memory if we have a channel\n// open with the peer.\n//\n// NOTE: This method should only be called from within the readHandler.",
      "length": 971,
      "tokens": 148,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) handleError(msg *lnwire.Error) bool {",
      "content": "func (p *Brontide) handleError(msg *lnwire.Error) bool {\n\t// Store the error we have received.\n\tp.storeError(msg)\n\n\tswitch {\n\t// In the case of an all-zero channel ID we want to forward the error to\n\t// all channels with this peer.\n\tcase msg.ChanID == lnwire.ConnectionWideID:\n\t\tfor _, chanStream := range p.activeMsgStreams {\n\t\t\tchanStream.AddMsg(msg)\n\t\t}\n\t\treturn false\n\n\t// If the channel ID for the error message corresponds to a pending\n\t// channel, then the funding manager will handle the error.\n\tcase p.cfg.FundingManager.IsPendingChannel(msg.ChanID, p):\n\t\tp.cfg.FundingManager.ProcessFundingMsg(msg, p)\n\t\treturn false\n\n\t// If not we hand the error to the channel link for this channel.\n\tcase p.isActiveChannel(msg.ChanID):\n\t\treturn true\n\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// messageSummary returns a human-readable string that summarizes a\n// incoming/outgoing message. Not all messages will have a summary, only those\n// which have additional data that can be informative at a glance.",
      "length": 905,
      "tokens": 133,
      "embedding": []
    },
    {
      "slug": "func messageSummary(msg lnwire.Message) string {",
      "content": "func messageSummary(msg lnwire.Message) string {\n\tswitch msg := msg.(type) {\n\tcase *lnwire.Init:\n\t\t// No summary.\n\t\treturn \"\"\n\n\tcase *lnwire.OpenChannel:\n\t\treturn fmt.Sprintf(\"temp_chan_id=%x, chain=%v, csv=%v, amt=%v, \"+\n\t\t\t\"push_amt=%v, reserve=%v, flags=%v\",\n\t\t\tmsg.PendingChannelID[:], msg.ChainHash,\n\t\t\tmsg.CsvDelay, msg.FundingAmount, msg.PushAmount,\n\t\t\tmsg.ChannelReserve, msg.ChannelFlags)\n\n\tcase *lnwire.AcceptChannel:\n\t\treturn fmt.Sprintf(\"temp_chan_id=%x, reserve=%v, csv=%v, num_confs=%v\",\n\t\t\tmsg.PendingChannelID[:], msg.ChannelReserve, msg.CsvDelay,\n\t\t\tmsg.MinAcceptDepth)\n\n\tcase *lnwire.FundingCreated:\n\t\treturn fmt.Sprintf(\"temp_chan_id=%x, chan_point=%v\",\n\t\t\tmsg.PendingChannelID[:], msg.FundingPoint)\n\n\tcase *lnwire.FundingSigned:\n\t\treturn fmt.Sprintf(\"chan_id=%v\", msg.ChanID)\n\n\tcase *lnwire.FundingLocked:\n\t\treturn fmt.Sprintf(\"chan_id=%v, next_point=%x\",\n\t\t\tmsg.ChanID, msg.NextPerCommitmentPoint.SerializeCompressed())\n\n\tcase *lnwire.Shutdown:\n\t\treturn fmt.Sprintf(\"chan_id=%v, script=%x\", msg.ChannelID,\n\t\t\tmsg.Address[:])\n\n\tcase *lnwire.ClosingSigned:\n\t\treturn fmt.Sprintf(\"chan_id=%v, fee_sat=%v\", msg.ChannelID,\n\t\t\tmsg.FeeSatoshis)\n\n\tcase *lnwire.UpdateAddHTLC:\n\t\treturn fmt.Sprintf(\"chan_id=%v, id=%v, amt=%v, expiry=%v, hash=%x\",\n\t\t\tmsg.ChanID, msg.ID, msg.Amount, msg.Expiry, msg.PaymentHash[:])\n\n\tcase *lnwire.UpdateFailHTLC:\n\t\treturn fmt.Sprintf(\"chan_id=%v, id=%v, reason=%x\", msg.ChanID,\n\t\t\tmsg.ID, msg.Reason)\n\n\tcase *lnwire.UpdateFulfillHTLC:\n\t\treturn fmt.Sprintf(\"chan_id=%v, id=%v, pre_image=%x\",\n\t\t\tmsg.ChanID, msg.ID, msg.PaymentPreimage[:])\n\n\tcase *lnwire.CommitSig:\n\t\treturn fmt.Sprintf(\"chan_id=%v, num_htlcs=%v\", msg.ChanID,\n\t\t\tlen(msg.HtlcSigs))\n\n\tcase *lnwire.RevokeAndAck:\n\t\treturn fmt.Sprintf(\"chan_id=%v, rev=%x, next_point=%x\",\n\t\t\tmsg.ChanID, msg.Revocation[:],\n\t\t\tmsg.NextRevocationKey.SerializeCompressed())\n\n\tcase *lnwire.UpdateFailMalformedHTLC:\n\t\treturn fmt.Sprintf(\"chan_id=%v, id=%v, fail_code=%v\",\n\t\t\tmsg.ChanID, msg.ID, msg.FailureCode)\n\n\tcase *lnwire.Warning:\n\t\treturn fmt.Sprintf(\"%v\", msg.Warning())\n\n\tcase *lnwire.Error:\n\t\treturn fmt.Sprintf(\"%v\", msg.Error())\n\n\tcase *lnwire.AnnounceSignatures:\n\t\treturn fmt.Sprintf(\"chan_id=%v, short_chan_id=%v\", msg.ChannelID,\n\t\t\tmsg.ShortChannelID.ToUint64())\n\n\tcase *lnwire.ChannelAnnouncement:\n\t\treturn fmt.Sprintf(\"chain_hash=%v, short_chan_id=%v\",\n\t\t\tmsg.ChainHash, msg.ShortChannelID.ToUint64())\n\n\tcase *lnwire.ChannelUpdate:\n\t\treturn fmt.Sprintf(\"chain_hash=%v, short_chan_id=%v, \"+\n\t\t\t\"mflags=%v, cflags=%v, update_time=%v\", msg.ChainHash,\n\t\t\tmsg.ShortChannelID.ToUint64(), msg.MessageFlags,\n\t\t\tmsg.ChannelFlags, time.Unix(int64(msg.Timestamp), 0))\n\n\tcase *lnwire.NodeAnnouncement:\n\t\treturn fmt.Sprintf(\"node=%x, update_time=%v\",\n\t\t\tmsg.NodeID, time.Unix(int64(msg.Timestamp), 0))\n\n\tcase *lnwire.Ping:\n\t\treturn fmt.Sprintf(\"ping_bytes=%x\", msg.PaddingBytes[:])\n\n\tcase *lnwire.Pong:\n\t\treturn fmt.Sprintf(\"pong_bytes=%x\", msg.PongBytes[:])\n\n\tcase *lnwire.UpdateFee:\n\t\treturn fmt.Sprintf(\"chan_id=%v, fee_update_sat=%v\",\n\t\t\tmsg.ChanID, int64(msg.FeePerKw))\n\n\tcase *lnwire.ChannelReestablish:\n\t\treturn fmt.Sprintf(\"next_local_height=%v, remote_tail_height=%v\",\n\t\t\tmsg.NextLocalCommitHeight, msg.RemoteCommitTailHeight)\n\n\tcase *lnwire.ReplyShortChanIDsEnd:\n\t\treturn fmt.Sprintf(\"chain_hash=%v, complete=%v\", msg.ChainHash,\n\t\t\tmsg.Complete)\n\n\tcase *lnwire.ReplyChannelRange:\n\t\treturn fmt.Sprintf(\"start_height=%v, end_height=%v, \"+\n\t\t\t\"num_chans=%v, encoding=%v\", msg.FirstBlockHeight,\n\t\t\tmsg.LastBlockHeight(), len(msg.ShortChanIDs),\n\t\t\tmsg.EncodingType)\n\n\tcase *lnwire.QueryShortChanIDs:\n\t\treturn fmt.Sprintf(\"chain_hash=%v, encoding=%v, num_chans=%v\",\n\t\t\tmsg.ChainHash, msg.EncodingType, len(msg.ShortChanIDs))\n\n\tcase *lnwire.QueryChannelRange:\n\t\treturn fmt.Sprintf(\"chain_hash=%v, start_height=%v, \"+\n\t\t\t\"end_height=%v\", msg.ChainHash, msg.FirstBlockHeight,\n\t\t\tmsg.LastBlockHeight())\n\n\tcase *lnwire.GossipTimestampRange:\n\t\treturn fmt.Sprintf(\"chain_hash=%v, first_stamp=%v, \"+\n\t\t\t\"stamp_range=%v\", msg.ChainHash,\n\t\t\ttime.Unix(int64(msg.FirstTimestamp), 0),\n\t\t\tmsg.TimestampRange)\n\n\tcase *lnwire.Custom:\n\t\treturn fmt.Sprintf(\"type=%d\", msg.Type)\n\t}\n\n\treturn fmt.Sprintf(\"unknown msg type=%T\", msg)\n}\n\n// logWireMessage logs the receipt or sending of particular wire message. This\n// function is used rather than just logging the message in order to produce\n// less spammy log messages in trace mode by setting the 'Curve\" parameter to\n// nil. Doing this avoids printing out each of the field elements in the curve\n// parameters for secp256k1.",
      "length": 4362,
      "tokens": 321,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) logWireMessage(msg lnwire.Message, read bool) {",
      "content": "func (p *Brontide) logWireMessage(msg lnwire.Message, read bool) {\n\tsummaryPrefix := \"Received\"\n\tif !read {\n\t\tsummaryPrefix = \"Sending\"\n\t}\n\n\tp.log.Debugf(\"%v\", newLogClosure(func() string {\n\t\t// Debug summary of message.\n\t\tsummary := messageSummary(msg)\n\t\tif len(summary) > 0 {\n\t\t\tsummary = \"(\" + summary + \")\"\n\t\t}\n\n\t\tpreposition := \"to\"\n\t\tif read {\n\t\t\tpreposition = \"from\"\n\t\t}\n\n\t\tvar msgType string\n\t\tif msg.MsgType() < lnwire.CustomTypeStart {\n\t\t\tmsgType = msg.MsgType().String()\n\t\t} else {\n\t\t\tmsgType = \"custom\"\n\t\t}\n\n\t\treturn fmt.Sprintf(\"%v %v%s %v %s\", summaryPrefix,\n\t\t\tmsgType, summary, preposition, p)\n\t}))\n\n\tprefix := \"readMessage from peer\"\n\tif !read {\n\t\tprefix = \"writeMessage to peer\"\n\t}\n\n\tp.log.Tracef(prefix+\": %v\", newLogClosure(func() string {\n\t\treturn spew.Sdump(msg)\n\t}))\n}\n\n// writeMessage writes and flushes the target lnwire.Message to the remote peer.\n// If the passed message is nil, this method will only try to flush an existing\n// message buffered on the connection. It is safe to call this method again\n// with a nil message iff a timeout error is returned. This will continue to\n// flush the pending message to the wire.",
      "length": 1039,
      "tokens": 162,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) writeMessage(msg lnwire.Message) error {",
      "content": "func (p *Brontide) writeMessage(msg lnwire.Message) error {\n\t// Simply exit if we're shutting down.\n\tif atomic.LoadInt32(&p.disconnect) != 0 {\n\t\treturn lnpeer.ErrPeerExiting\n\t}\n\n\t// Only log the message on the first attempt.\n\tif msg != nil {\n\t\tp.logWireMessage(msg, false)\n\t}\n\n\tnoiseConn := p.cfg.Conn\n\n\tflushMsg := func() error {\n\t\t// Ensure the write deadline is set before we attempt to send\n\t\t// the message.\n\t\twriteDeadline := time.Now().Add(writeMessageTimeout)\n\t\terr := noiseConn.SetWriteDeadline(writeDeadline)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Flush the pending message to the wire. If an error is\n\t\t// encountered, e.g. write timeout, the number of bytes written\n\t\t// so far will be returned.\n\t\tn, err := noiseConn.Flush()\n\n\t\t// Record the number of bytes written on the wire, if any.\n\t\tif n > 0 {\n\t\t\tatomic.AddUint64(&p.bytesSent, uint64(n))\n\t\t}\n\n\t\treturn err\n\t}\n\n\t// If the current message has already been serialized, encrypted, and\n\t// buffered on the underlying connection we will skip straight to\n\t// flushing it to the wire.\n\tif msg == nil {\n\t\treturn flushMsg()\n\t}\n\n\t// Otherwise, this is a new message. We'll acquire a write buffer to\n\t// serialize the message and buffer the ciphertext on the connection.\n\terr := p.cfg.WritePool.Submit(func(buf *bytes.Buffer) error {\n\t\t// Using a buffer allocated by the write pool, encode the\n\t\t// message directly into the buffer.\n\t\t_, writeErr := lnwire.WriteMessage(buf, msg, 0)\n\t\tif writeErr != nil {\n\t\t\treturn writeErr\n\t\t}\n\n\t\t// Finally, write the message itself in a single swoop. This\n\t\t// will buffer the ciphertext on the underlying connection. We\n\t\t// will defer flushing the message until the write pool has been\n\t\t// released.\n\t\treturn noiseConn.WriteMessage(buf.Bytes())\n\t})\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn flushMsg()\n}\n\n// writeHandler is a goroutine dedicated to reading messages off of an incoming\n// queue, and writing them out to the wire. This goroutine coordinates with the\n// queueHandler in order to ensure the incoming message queue is quickly\n// drained.\n//\n// NOTE: This method MUST be run as a goroutine.",
      "length": 1973,
      "tokens": 322,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) writeHandler() {",
      "content": "func (p *Brontide) writeHandler() {\n\t// We'll stop the timer after a new messages is sent, and also reset it\n\t// after we process the next message.\n\tidleTimer := time.AfterFunc(idleTimeout, func() {\n\t\terr := fmt.Errorf(\"peer %s no write for %s -- disconnecting\",\n\t\t\tp, idleTimeout)\n\t\tp.Disconnect(err)\n\t})\n\n\tvar exitErr error\n\nout:\n\tfor {\n\t\tselect {\n\t\tcase outMsg := <-p.sendQueue:\n\t\t\t// If we're about to send a ping message, then log the\n\t\t\t// exact time in which we send the message so we can\n\t\t\t// use the delay as a rough estimate of latency to the\n\t\t\t// remote peer.\n\t\t\tif _, ok := outMsg.msg.(*lnwire.Ping); ok {\n\t\t\t\t// TODO(roasbeef): do this before the write?\n\t\t\t\t// possibly account for processing within func?\n\t\t\t\tnow := time.Now().UnixNano()\n\t\t\t\tatomic.StoreInt64(&p.pingLastSend, now)\n\t\t\t}\n\n\t\t\t// Record the time at which we first attempt to send the\n\t\t\t// message.\n\t\t\tstartTime := time.Now()\n\n\t\tretry:\n\t\t\t// Write out the message to the socket. If a timeout\n\t\t\t// error is encountered, we will catch this and retry\n\t\t\t// after backing off in case the remote peer is just\n\t\t\t// slow to process messages from the wire.\n\t\t\terr := p.writeMessage(outMsg.msg)\n\t\t\tif nerr, ok := err.(net.Error); ok && nerr.Timeout() {\n\t\t\t\tp.log.Debugf(\"Write timeout detected for \"+\n\t\t\t\t\t\"peer, first write for message \"+\n\t\t\t\t\t\"attempted %v ago\",\n\t\t\t\t\ttime.Since(startTime))\n\n\t\t\t\t// If we received a timeout error, this implies\n\t\t\t\t// that the message was buffered on the\n\t\t\t\t// connection successfully and that a flush was\n\t\t\t\t// attempted. We'll set the message to nil so\n\t\t\t\t// that on a subsequent pass we only try to\n\t\t\t\t// flush the buffered message, and forgo\n\t\t\t\t// reserializing or reencrypting it.\n\t\t\t\toutMsg.msg = nil\n\n\t\t\t\tgoto retry\n\t\t\t}\n\n\t\t\t// The write succeeded, reset the idle timer to prevent\n\t\t\t// us from disconnecting the peer.\n\t\t\tif !idleTimer.Stop() {\n\t\t\t\tselect {\n\t\t\t\tcase <-idleTimer.C:\n\t\t\t\tdefault:\n\t\t\t\t}\n\t\t\t}\n\t\t\tidleTimer.Reset(idleTimeout)\n\n\t\t\t// If the peer requested a synchronous write, respond\n\t\t\t// with the error.\n\t\t\tif outMsg.errChan != nil {\n\t\t\t\toutMsg.errChan <- err\n\t\t\t}\n\n\t\t\tif err != nil {\n\t\t\t\texitErr = fmt.Errorf(\"unable to write \"+\n\t\t\t\t\t\"message: %v\", err)\n\t\t\t\tbreak out\n\t\t\t}\n\n\t\tcase <-p.quit:\n\t\t\texitErr = lnpeer.ErrPeerExiting\n\t\t\tbreak out\n\t\t}\n\t}\n\n\t// Avoid an exit deadlock by ensuring WaitGroups are decremented before\n\t// disconnect.\n\tp.wg.Done()\n\n\tp.Disconnect(exitErr)\n\n\tp.log.Trace(\"writeHandler for peer done\")\n}\n\n// queueHandler is responsible for accepting messages from outside subsystems\n// to be eventually sent out on the wire by the writeHandler.\n//\n// NOTE: This method MUST be run as a goroutine.",
      "length": 2517,
      "tokens": 394,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) queueHandler() {",
      "content": "func (p *Brontide) queueHandler() {\n\tdefer p.wg.Done()\n\n\t// priorityMsgs holds an in order list of messages deemed high-priority\n\t// to be added to the sendQueue. This predominately includes messages\n\t// from the funding manager and htlcswitch.\n\tpriorityMsgs := list.New()\n\n\t// lazyMsgs holds an in order list of messages deemed low-priority to be\n\t// added to the sendQueue only after all high-priority messages have\n\t// been queued. This predominately includes messages from the gossiper.\n\tlazyMsgs := list.New()\n\n\tfor {\n\t\t// Examine the front of the priority queue, if it is empty check\n\t\t// the low priority queue.\n\t\telem := priorityMsgs.Front()\n\t\tif elem == nil {\n\t\t\telem = lazyMsgs.Front()\n\t\t}\n\n\t\tif elem != nil {\n\t\t\tfront := elem.Value.(outgoingMsg)\n\n\t\t\t// There's an element on the queue, try adding\n\t\t\t// it to the sendQueue. We also watch for\n\t\t\t// messages on the outgoingQueue, in case the\n\t\t\t// writeHandler cannot accept messages on the\n\t\t\t// sendQueue.\n\t\t\tselect {\n\t\t\tcase p.sendQueue <- front:\n\t\t\t\tif front.priority {\n\t\t\t\t\tpriorityMsgs.Remove(elem)\n\t\t\t\t} else {\n\t\t\t\t\tlazyMsgs.Remove(elem)\n\t\t\t\t}\n\t\t\tcase msg := <-p.outgoingQueue:\n\t\t\t\tif msg.priority {\n\t\t\t\t\tpriorityMsgs.PushBack(msg)\n\t\t\t\t} else {\n\t\t\t\t\tlazyMsgs.PushBack(msg)\n\t\t\t\t}\n\t\t\tcase <-p.quit:\n\t\t\t\treturn\n\t\t\t}\n\t\t} else {\n\t\t\t// If there weren't any messages to send to the\n\t\t\t// writeHandler, then we'll accept a new message\n\t\t\t// into the queue from outside sub-systems.\n\t\t\tselect {\n\t\t\tcase msg := <-p.outgoingQueue:\n\t\t\t\tif msg.priority {\n\t\t\t\t\tpriorityMsgs.PushBack(msg)\n\t\t\t\t} else {\n\t\t\t\t\tlazyMsgs.PushBack(msg)\n\t\t\t\t}\n\t\t\tcase <-p.quit:\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n}\n\n// pingHandler is responsible for periodically sending ping messages to the\n// remote peer in order to keep the connection alive and/or determine if the\n// connection is still active.\n//\n// NOTE: This method MUST be run as a goroutine.",
      "length": 1765,
      "tokens": 269,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) pingHandler() {",
      "content": "func (p *Brontide) pingHandler() {\n\tdefer p.wg.Done()\n\n\tpingTicker := time.NewTicker(pingInterval)\n\tdefer pingTicker.Stop()\n\n\t// TODO(roasbeef): make dynamic in order to create fake cover traffic\n\tconst numPongBytes = 16\n\n\tblockEpochs, err := p.cfg.ChainNotifier.RegisterBlockEpochNtfn(nil)\n\tif err != nil {\n\t\tp.log.Errorf(\"unable to establish block epoch \"+\n\t\t\t\"subscription: %v\", err)\n\t\treturn\n\t}\n\tdefer blockEpochs.Cancel()\n\n\tvar (\n\t\tpingPayload [wire.MaxBlockHeaderPayload]byte\n\t\tblockHeader *wire.BlockHeader\n\t)\nout:\n\tfor {\n\t\tselect {\n\t\t// Each time a new block comes in, we'll copy the raw header\n\t\t// contents over to our ping payload declared above. Over time,\n\t\t// we'll use this to disseminate the latest block header\n\t\t// between all our peers, which can later be used to\n\t\t// cross-check our own view of the network to mitigate various\n\t\t// types of eclipse attacks.\n\t\tcase epoch, ok := <-blockEpochs.Epochs:\n\t\t\tif !ok {\n\t\t\t\tp.log.Debugf(\"block notifications \" +\n\t\t\t\t\t\"canceled\")\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tblockHeader = epoch.BlockHeader\n\t\t\theaderBuf := bytes.NewBuffer(pingPayload[0:0])\n\t\t\terr := blockHeader.Serialize(headerBuf)\n\t\t\tif err != nil {\n\t\t\t\tp.log.Errorf(\"unable to encode header: %v\",\n\t\t\t\t\terr)\n\t\t\t}\n\n\t\tcase <-pingTicker.C:\n\n\t\t\tpingMsg := &lnwire.Ping{\n\t\t\t\tNumPongBytes: numPongBytes,\n\t\t\t\tPaddingBytes: pingPayload[:],\n\t\t\t}\n\n\t\t\tp.queueMsg(pingMsg, nil)\n\t\tcase <-p.quit:\n\t\t\tbreak out\n\t\t}\n\t}\n}\n\n// PingTime returns the estimated ping time to the peer in microseconds.",
      "length": 1391,
      "tokens": 184,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) PingTime() int64 {",
      "content": "func (p *Brontide) PingTime() int64 {\n\treturn atomic.LoadInt64(&p.pingTime)\n}\n\n// queueMsg adds the lnwire.Message to the back of the high priority send queue.\n// If the errChan is non-nil, an error is sent back if the msg failed to queue\n// or failed to write, and nil otherwise.",
      "length": 237,
      "tokens": 42,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) queueMsg(msg lnwire.Message, errChan chan error) {",
      "content": "func (p *Brontide) queueMsg(msg lnwire.Message, errChan chan error) {\n\tp.queue(true, msg, errChan)\n}\n\n// queueMsgLazy adds the lnwire.Message to the back of the low priority send\n// queue. If the errChan is non-nil, an error is sent back if the msg failed to\n// queue or failed to write, and nil otherwise.",
      "length": 231,
      "tokens": 43,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) queueMsgLazy(msg lnwire.Message, errChan chan error) {",
      "content": "func (p *Brontide) queueMsgLazy(msg lnwire.Message, errChan chan error) {\n\tp.queue(false, msg, errChan)\n}\n\n// queue sends a given message to the queueHandler using the passed priority. If\n// the errChan is non-nil, an error is sent back if the msg failed to queue or\n// failed to write, and nil otherwise.",
      "length": 226,
      "tokens": 42,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) queue(priority bool, msg lnwire.Message,",
      "content": "func (p *Brontide) queue(priority bool, msg lnwire.Message,\n\terrChan chan error) {\n\n\tselect {\n\tcase p.outgoingQueue <- outgoingMsg{priority, msg, errChan}:\n\tcase <-p.quit:\n\t\tp.log.Tracef(\"Peer shutting down, could not enqueue msg: %v.\",\n\t\t\tspew.Sdump(msg))\n\t\tif errChan != nil {\n\t\t\terrChan <- lnpeer.ErrPeerExiting\n\t\t}\n\t}\n}\n\n// ChannelSnapshots returns a slice of channel snapshots detailing all\n// currently active channels maintained with the remote peer.",
      "length": 383,
      "tokens": 53,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) ChannelSnapshots() []*channeldb.ChannelSnapshot {",
      "content": "func (p *Brontide) ChannelSnapshots() []*channeldb.ChannelSnapshot {\n\tp.activeChanMtx.RLock()\n\tdefer p.activeChanMtx.RUnlock()\n\n\tsnapshots := make([]*channeldb.ChannelSnapshot, 0, len(p.activeChannels))\n\tfor _, activeChan := range p.activeChannels {\n\t\t// If the activeChan is nil, then we skip it as the channel is pending.\n\t\tif activeChan == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\t// We'll only return a snapshot for channels that are\n\t\t// *immediately* available for routing payments over.\n\t\tif activeChan.RemoteNextRevocation() == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\tsnapshot := activeChan.StateSnapshot()\n\t\tsnapshots = append(snapshots, snapshot)\n\t}\n\n\treturn snapshots\n}\n\n// genDeliveryScript returns a new script to be used to send our funds to in\n// the case of a cooperative channel close negotiation.",
      "length": 690,
      "tokens": 96,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) genDeliveryScript() ([]byte, error) {",
      "content": "func (p *Brontide) genDeliveryScript() ([]byte, error) {\n\t// We'll send a normal p2wkh address unless we've negotiated the\n\t// shutdown-any-segwit feature.\n\taddrType := lnwallet.WitnessPubKey\n\tif p.taprootShutdownAllowed() {\n\t\taddrType = lnwallet.TaprootPubkey\n\t}\n\n\tdeliveryAddr, err := p.cfg.Wallet.NewAddress(\n\t\taddrType, false, lnwallet.DefaultAccountName,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tp.log.Infof(\"Delivery addr for channel close: %v\",\n\t\tdeliveryAddr)\n\n\treturn txscript.PayToAddrScript(deliveryAddr)\n}\n\n// channelManager is goroutine dedicated to handling all requests/signals\n// pertaining to the opening, cooperative closing, and force closing of all\n// channels maintained with the remote peer.\n//\n// NOTE: This method MUST be run as a goroutine.",
      "length": 686,
      "tokens": 90,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) channelManager() {",
      "content": "func (p *Brontide) channelManager() {\n\tdefer p.wg.Done()\n\n\t// reenableTimeout will fire once after the configured channel status\n\t// interval has elapsed. This will trigger us to sign new channel\n\t// updates and broadcast them with the \"disabled\" flag unset.\n\treenableTimeout := time.After(p.cfg.ChanActiveTimeout)\n\nout:\n\tfor {\n\t\tselect {\n\t\t// A new channel has arrived which means we've just completed a\n\t\t// funding workflow. We'll initialize the necessary local\n\t\t// state, and notify the htlc switch of a new link.\n\t\tcase newChanReq := <-p.newChannels:\n\t\t\tnewChan := newChanReq.channel\n\t\t\tchanPoint := &newChan.FundingOutpoint\n\t\t\tchanID := lnwire.NewChanIDFromOutPoint(chanPoint)\n\n\t\t\t// Only update RemoteNextRevocation if the channel is in the\n\t\t\t// activeChannels map and if we added the link to the switch.\n\t\t\t// Only active channels will be added to the switch.\n\t\t\tp.activeChanMtx.Lock()\n\t\t\tcurrentChan, ok := p.activeChannels[chanID]\n\t\t\tif ok && currentChan != nil {\n\t\t\t\tp.log.Infof(\"Already have ChannelPoint(%v), \"+\n\t\t\t\t\t\"ignoring.\", chanPoint)\n\n\t\t\t\tp.activeChanMtx.Unlock()\n\t\t\t\tclose(newChanReq.err)\n\n\t\t\t\t// If we're being sent a new channel, and our\n\t\t\t\t// existing channel doesn't have the next\n\t\t\t\t// revocation, then we need to update the\n\t\t\t\t// current existing channel.\n\t\t\t\tif currentChan.RemoteNextRevocation() != nil {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tp.log.Infof(\"Processing retransmitted \"+\n\t\t\t\t\t\"FundingLocked for ChannelPoint(%v)\",\n\t\t\t\t\tchanPoint)\n\n\t\t\t\tnextRevoke := newChan.RemoteNextRevocation\n\t\t\t\terr := currentChan.InitNextRevocation(nextRevoke)\n\t\t\t\tif err != nil {\n\t\t\t\t\tp.log.Errorf(\"unable to init chan \"+\n\t\t\t\t\t\t\"revocation: %v\", err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// If not already active, we'll add this channel to the\n\t\t\t// set of active channels, so we can look it up later\n\t\t\t// easily according to its channel ID.\n\t\t\tlnChan, err := lnwallet.NewLightningChannel(\n\t\t\t\tp.cfg.Signer, newChan, p.cfg.SigPool,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tp.activeChanMtx.Unlock()\n\t\t\t\terr := fmt.Errorf(\"unable to create \"+\n\t\t\t\t\t\"LightningChannel: %v\", err)\n\t\t\t\tp.log.Errorf(err.Error())\n\n\t\t\t\tnewChanReq.err <- err\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// This refreshes the activeChannels entry if the link was not in\n\t\t\t// the switch, also populates for new entries.\n\t\t\tp.activeChannels[chanID] = lnChan\n\t\t\tp.addedChannels[chanID] = struct{}{}\n\t\t\tp.activeChanMtx.Unlock()\n\n\t\t\tp.log.Infof(\"New channel active ChannelPoint(%v) \"+\n\t\t\t\t\"with peer\", chanPoint)\n\n\t\t\t// Next, we'll assemble a ChannelLink along with the\n\t\t\t// necessary items it needs to function.\n\t\t\t//\n\t\t\t// TODO(roasbeef): panic on below?\n\t\t\tchainEvents, err := p.cfg.ChainArb.SubscribeChannelEvents(\n\t\t\t\t*chanPoint,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\terr := fmt.Errorf(\"unable to subscribe to \"+\n\t\t\t\t\t\"chain events: %v\", err)\n\t\t\t\tp.log.Errorf(err.Error())\n\n\t\t\t\tnewChanReq.err <- err\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// We'll query the localChanCfg of the new channel to determine the\n\t\t\t// minimum HTLC value that can be forwarded. For the maximum HTLC\n\t\t\t// value that can be forwarded and fees we'll use the default\n\t\t\t// values, as they currently are always set to the default values\n\t\t\t// at initial channel creation. Note that the maximum HTLC value\n\t\t\t// defaults to the cap on the total value of outstanding HTLCs.\n\t\t\tfwdMinHtlc := lnChan.FwdMinHtlc()\n\t\t\tdefaultPolicy := p.cfg.RoutingPolicy\n\t\t\tforwardingPolicy := &htlcswitch.ForwardingPolicy{\n\t\t\t\tMinHTLCOut:    fwdMinHtlc,\n\t\t\t\tMaxHTLC:       newChan.LocalChanCfg.MaxPendingAmount,\n\t\t\t\tBaseFee:       defaultPolicy.BaseFee,\n\t\t\t\tFeeRate:       defaultPolicy.FeeRate,\n\t\t\t\tTimeLockDelta: defaultPolicy.TimeLockDelta,\n\t\t\t}\n\n\t\t\t// If we've reached this point, there are two possible scenarios.\n\t\t\t// If the channel was in the active channels map as nil, then it\n\t\t\t// was loaded from disk and we need to send reestablish. Else,\n\t\t\t// it was not loaded from disk and we don't need to send\n\t\t\t// reestablish as this is a fresh channel.\n\t\t\tshouldReestablish := ok\n\n\t\t\t// Create the link and add it to the switch.\n\t\t\terr = p.addLink(\n\t\t\t\tchanPoint, lnChan, forwardingPolicy,\n\t\t\t\tchainEvents, shouldReestablish,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\terr := fmt.Errorf(\"can't register new channel \"+\n\t\t\t\t\t\"link(%v) with peer\", chanPoint)\n\t\t\t\tp.log.Errorf(err.Error())\n\n\t\t\t\tnewChanReq.err <- err\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tclose(newChanReq.err)\n\n\t\t// We've just received a local request to close an active\n\t\t// channel. It will either kick of a cooperative channel\n\t\t// closure negotiation, or be a notification of a breached\n\t\t// contract that should be abandoned.\n\t\tcase req := <-p.localCloseChanReqs:\n\t\t\tp.handleLocalCloseReq(req)\n\n\t\t// We've received a link failure from a link that was added to\n\t\t// the switch. This will initiate the teardown of the link, and\n\t\t// initiate any on-chain closures if necessary.\n\t\tcase failure := <-p.linkFailures:\n\t\t\tp.handleLinkFailure(failure)\n\n\t\t// We've received a new cooperative channel closure related\n\t\t// message from the remote peer, we'll use this message to\n\t\t// advance the chan closer state machine.\n\t\tcase closeMsg := <-p.chanCloseMsgs:\n\t\t\tp.handleCloseMsg(closeMsg)\n\n\t\t// The channel reannounce delay has elapsed, broadcast the\n\t\t// reenabled channel updates to the network. This should only\n\t\t// fire once, so we set the reenableTimeout channel to nil to\n\t\t// mark it for garbage collection. If the peer is torn down\n\t\t// before firing, reenabling will not be attempted.\n\t\t// TODO(conner): consolidate reenables timers inside chan status\n\t\t// manager\n\t\tcase <-reenableTimeout:\n\t\t\tp.reenableActiveChannels()\n\n\t\t\t// Since this channel will never fire again during the\n\t\t\t// lifecycle of the peer, we nil the channel to mark it\n\t\t\t// eligible for garbage collection, and make this\n\t\t\t// explicitly ineligible to receive in future calls to\n\t\t\t// select. This also shaves a few CPU cycles since the\n\t\t\t// select will ignore this case entirely.\n\t\t\treenableTimeout = nil\n\n\t\t\t// Once the reenabling is attempted, we also cancel the\n\t\t\t// channel event subscription to free up the overflow\n\t\t\t// queue used in channel notifier.\n\t\t\t//\n\t\t\t// NOTE: channelEventClient will be nil if the\n\t\t\t// reenableTimeout is greater than 1 minute.\n\t\t\tif p.channelEventClient != nil {\n\t\t\t\tp.channelEventClient.Cancel()\n\t\t\t}\n\n\t\tcase <-p.quit:\n\t\t\t// As, we've been signalled to exit, we'll reset all\n\t\t\t// our active channel back to their default state.\n\t\t\tp.activeChanMtx.Lock()\n\t\t\tfor _, channel := range p.activeChannels {\n\t\t\t\t// If the channel is nil, continue as it's a pending channel.\n\t\t\t\tif channel == nil {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tchannel.ResetState()\n\t\t\t}\n\t\t\tp.activeChanMtx.Unlock()\n\n\t\t\tbreak out\n\t\t}\n\t}\n}\n\n// reenableActiveChannels searches the index of channels maintained with this\n// peer, and reenables each public, non-pending channel. This is done at the\n// gossip level by broadcasting a new ChannelUpdate with the disabled bit unset.\n// No message will be sent if the channel is already enabled.",
      "length": 6683,
      "tokens": 929,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) reenableActiveChannels() {",
      "content": "func (p *Brontide) reenableActiveChannels() {\n\t// First, filter all known channels with this peer for ones that are\n\t// both public and not pending.\n\tactivePublicChans := p.filterChannelsToEnable()\n\n\t// Create a map to hold channels that needs to be retried.\n\tretryChans := make(map[wire.OutPoint]struct{}, len(activePublicChans))\n\n\t// For each of the public, non-pending channels, set the channel\n\t// disabled bit to false and send out a new ChannelUpdate. If this\n\t// channel is already active, the update won't be sent.\n\tfor _, chanPoint := range activePublicChans {\n\t\terr := p.cfg.ChanStatusMgr.RequestEnable(chanPoint, false)\n\n\t\tswitch {\n\t\t// No error occurred, continue to request the next channel.\n\t\tcase err == nil:\n\t\t\tcontinue\n\n\t\t// Cannot auto enable a manually disabled channel so we do\n\t\t// nothing but proceed to the next channel.\n\t\tcase errors.Is(err, netann.ErrEnableManuallyDisabledChan):\n\t\t\tp.log.Debugf(\"Channel(%v) was manually disabled, \"+\n\t\t\t\t\"ignoring automatic enable request\", chanPoint)\n\n\t\t\tcontinue\n\n\t\t// If the channel is reported as inactive, we will give it\n\t\t// another chance. When handling the request, ChanStatusManager\n\t\t// will check whether the link is active or not. One of the\n\t\t// conditions is whether the link has been marked as\n\t\t// reestablished, which happens inside a goroutine(htlcManager)\n\t\t// after the link is started. And we may get a false negative\n\t\t// saying the link is not active because that goroutine hasn't\n\t\t// reached the line to mark the reestablishment. Thus we give\n\t\t// it a second chance to send the request.\n\t\tcase errors.Is(err, netann.ErrEnableInactiveChan):\n\t\t\t// If we don't have a client created, it means we\n\t\t\t// shouldn't retry enabling the channel.\n\t\t\tif p.channelEventClient == nil {\n\t\t\t\tp.log.Errorf(\"Channel(%v) request enabling \"+\n\t\t\t\t\t\"failed due to inactive link\",\n\t\t\t\t\tchanPoint)\n\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tp.log.Warnf(\"Channel(%v) cannot be enabled as \" +\n\t\t\t\t\"ChanStatusManager reported inactive, retrying\")\n\n\t\t\t// Add the channel to the retry map.\n\t\t\tretryChans[chanPoint] = struct{}{}\n\t\t}\n\t}\n\n\t// Retry the channels if we have any.\n\tif len(retryChans) != 0 {\n\t\tp.retryRequestEnable(retryChans)\n\t}\n}\n\n// fetchActiveChanCloser attempts to fetch the active chan closer state machine\n// for the target channel ID. If the channel isn't active an error is returned.\n// Otherwise, either an existing state machine will be returned, or a new one\n// will be created.",
      "length": 2330,
      "tokens": 348,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) fetchActiveChanCloser(chanID lnwire.ChannelID) (",
      "content": "func (p *Brontide) fetchActiveChanCloser(chanID lnwire.ChannelID) (\n\t*chancloser.ChanCloser, error) {\n\n\tchanCloser, found := p.activeChanCloses[chanID]\n\tif found {\n\t\t// An entry will only be found if the closer has already been\n\t\t// created for a non-pending channel or for a channel that had\n\t\t// previously started the shutdown process but the connection\n\t\t// was restarted.\n\t\treturn chanCloser, nil\n\t}\n\n\t// First, we'll ensure that we actually know of the target channel. If\n\t// not, we'll ignore this message.\n\tp.activeChanMtx.RLock()\n\tchannel, ok := p.activeChannels[chanID]\n\tp.activeChanMtx.RUnlock()\n\n\t// If the channel isn't in the map or the channel is nil, return\n\t// ErrChannelNotFound as the channel is pending.\n\tif !ok || channel == nil {\n\t\treturn nil, ErrChannelNotFound\n\t}\n\n\t// Optimistically try a link shutdown, erroring out if it failed.\n\tif err := p.tryLinkShutdown(chanID); err != nil {\n\t\tp.log.Errorf(\"failed link shutdown: %v\", err)\n\t\treturn nil, err\n\t}\n\n\t// We'll create a valid closing state machine in order to respond to\n\t// the initiated cooperative channel closure. First, we set the\n\t// delivery script that our funds will be paid out to. If an upfront\n\t// shutdown script was set, we will use it. Otherwise, we get a fresh\n\t// delivery script.\n\t//\n\t// TODO: Expose option to allow upfront shutdown script from watch-only\n\t// accounts.\n\tdeliveryScript := channel.LocalUpfrontShutdownScript()\n\tif len(deliveryScript) == 0 {\n\t\tvar err error\n\t\tdeliveryScript, err = p.genDeliveryScript()\n\t\tif err != nil {\n\t\t\tp.log.Errorf(\"unable to gen delivery script: %v\",\n\t\t\t\terr)\n\t\t\treturn nil, fmt.Errorf(\"close addr unavailable\")\n\t\t}\n\t}\n\n\t// In order to begin fee negotiations, we'll first compute our target\n\t// ideal fee-per-kw.\n\tfeePerKw, err := p.cfg.FeeEstimator.EstimateFeePerKW(\n\t\tp.cfg.CoopCloseTargetConfs,\n\t)\n\tif err != nil {\n\t\tp.log.Errorf(\"unable to query fee estimator: %v\", err)\n\t\treturn nil, fmt.Errorf(\"unable to estimate fee\")\n\t}\n\n\tchanCloser, err = p.createChanCloser(\n\t\tchannel, deliveryScript, feePerKw, nil, false,\n\t)\n\tif err != nil {\n\t\tp.log.Errorf(\"unable to create chan closer: %v\", err)\n\t\treturn nil, fmt.Errorf(\"unable to create chan closer\")\n\t}\n\n\tp.activeChanCloses[chanID] = chanCloser\n\n\treturn chanCloser, nil\n}\n\n// filterChannelsToEnable filters a list of channels to be enabled upon start.\n// The filtered channels are active channels that's neither private nor\n// pending.",
      "length": 2279,
      "tokens": 340,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) filterChannelsToEnable() []wire.OutPoint {",
      "content": "func (p *Brontide) filterChannelsToEnable() []wire.OutPoint {\n\tvar activePublicChans []wire.OutPoint\n\n\tp.activeChanMtx.RLock()\n\tdefer p.activeChanMtx.RUnlock()\n\n\tfor chanID, lnChan := range p.activeChannels {\n\t\t// If the lnChan is nil, continue as this is a pending channel.\n\t\tif lnChan == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\tdbChan := lnChan.State()\n\t\tisPublic := dbChan.ChannelFlags&lnwire.FFAnnounceChannel != 0\n\t\tif !isPublic || dbChan.IsPending {\n\t\t\tcontinue\n\t\t}\n\n\t\t// We'll also skip any channels added during this peer's\n\t\t// lifecycle since they haven't waited out the timeout. Their\n\t\t// first announcement will be enabled, and the chan status\n\t\t// manager will begin monitoring them passively since they exist\n\t\t// in the database.\n\t\tif _, ok := p.addedChannels[chanID]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tactivePublicChans = append(\n\t\t\tactivePublicChans, dbChan.FundingOutpoint,\n\t\t)\n\t}\n\n\treturn activePublicChans\n}\n\n// retryRequestEnable takes a map of channel outpoints and a channel event\n// client. It listens to the channel events and removes a channel from the map\n// if it's matched to the event. Upon receiving an active channel event, it\n// will send the enabling request again.",
      "length": 1076,
      "tokens": 159,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) retryRequestEnable(activeChans map[wire.OutPoint]struct{}) {",
      "content": "func (p *Brontide) retryRequestEnable(activeChans map[wire.OutPoint]struct{}) {\n\tp.log.Debugf(\"Retry enabling %v channels\", len(activeChans))\n\n\t// retryEnable is a helper closure that sends an enable request and\n\t// removes the channel from the map if it's matched.\n\tretryEnable := func(chanPoint wire.OutPoint) error {\n\t\t// If this is an active channel event, check whether it's in\n\t\t// our targeted channels map.\n\t\t_, found := activeChans[chanPoint]\n\n\t\t// If this channel is irrelevant, return nil so the loop can\n\t\t// jump to next iteration.\n\t\tif !found {\n\t\t\treturn nil\n\t\t}\n\n\t\t// Otherwise we've just received an active signal for a channel\n\t\t// that's previously failed to be enabled, we send the request\n\t\t// again.\n\t\t//\n\t\t// We only give the channel one more shot, so we delete it from\n\t\t// our map first to keep it from being attempted again.\n\t\tdelete(activeChans, chanPoint)\n\n\t\t// Send the request.\n\t\terr := p.cfg.ChanStatusMgr.RequestEnable(chanPoint, false)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"request enabling channel %v \"+\n\t\t\t\t\"failed: %w\", chanPoint, err)\n\t\t}\n\n\t\treturn nil\n\t}\n\n\tfor {\n\t\t// If activeChans is empty, we've done processing all the\n\t\t// channels.\n\t\tif len(activeChans) == 0 {\n\t\t\tp.log.Debug(\"Finished retry enabling channels\")\n\t\t\treturn\n\t\t}\n\n\t\tselect {\n\t\t// A new event has been sent by the ChannelNotifier. We now\n\t\t// check whether it's an active or inactive channel event.\n\t\tcase e := <-p.channelEventClient.Updates():\n\t\t\t// If this is an active channel event, try enable the\n\t\t\t// channel then jump to the next iteration.\n\t\t\tactive, ok := e.(channelnotifier.ActiveChannelEvent)\n\t\t\tif ok {\n\t\t\t\tchanPoint := *active.ChannelPoint\n\n\t\t\t\t// If we received an error for this particular\n\t\t\t\t// channel, we log an error and won't quit as\n\t\t\t\t// we still want to retry other channels.\n\t\t\t\tif err := retryEnable(chanPoint); err != nil {\n\t\t\t\t\tp.log.Errorf(\"Retry failed: %v\", err)\n\t\t\t\t}\n\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Otherwise check for inactive link event, and jump to\n\t\t\t// next iteration if it's not.\n\t\t\tinactive, ok := e.(channelnotifier.InactiveLinkEvent)\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Found an inactive link event, if this is our\n\t\t\t// targeted channel, remove it from our map.\n\t\t\tchanPoint := *inactive.ChannelPoint\n\t\t\t_, found := activeChans[chanPoint]\n\t\t\tif !found {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tdelete(activeChans, chanPoint)\n\t\t\tp.log.Warnf(\"Re-enable channel %v failed, received \"+\n\t\t\t\t\"inactive link event\", chanPoint)\n\n\t\tcase <-p.quit:\n\t\t\tp.log.Debugf(\"Peer shutdown during retry enabling\")\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// chooseDeliveryScript takes two optionally set shutdown scripts and returns\n// a suitable script to close out to. This may be nil if neither script is\n// set. If both scripts are set, this function will error if they do not match.",
      "length": 2605,
      "tokens": 400,
      "embedding": []
    },
    {
      "slug": "func chooseDeliveryScript(upfront,",
      "content": "func chooseDeliveryScript(upfront,\n\trequested lnwire.DeliveryAddress) (lnwire.DeliveryAddress, error) {\n\n\t// If no upfront shutdown script was provided, return the user\n\t// requested address (which may be nil).\n\tif len(upfront) == 0 {\n\t\treturn requested, nil\n\t}\n\n\t// If an upfront shutdown script was provided, and the user did not request\n\t// a custom shutdown script, return the upfront address.\n\tif len(requested) == 0 {\n\t\treturn upfront, nil\n\t}\n\n\t// If both an upfront shutdown script and a custom close script were\n\t// provided, error if the user provided shutdown script does not match\n\t// the upfront shutdown script (because closing out to a different script\n\t// would violate upfront shutdown).\n\tif !bytes.Equal(upfront, requested) {\n\t\treturn nil, chancloser.ErrUpfrontShutdownScriptMismatch\n\t}\n\n\t// The user requested script matches the upfront shutdown script, so we\n\t// can return it without error.\n\treturn upfront, nil\n}\n\n// restartCoopClose checks whether we need to restart the cooperative close\n// process for a given channel.",
      "length": 979,
      "tokens": 153,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) restartCoopClose(lnChan *lnwallet.LightningChannel) (",
      "content": "func (p *Brontide) restartCoopClose(lnChan *lnwallet.LightningChannel) (\n\t*lnwire.Shutdown, error) {\n\n\t// If this channel has status ChanStatusCoopBroadcasted and does not\n\t// have a closing transaction, then the cooperative close process was\n\t// started but never finished. We'll re-create the chanCloser state\n\t// machine and resend Shutdown. BOLT#2 requires that we retransmit\n\t// Shutdown exactly, but doing so would mean persisting the RPC\n\t// provided close script. Instead use the LocalUpfrontShutdownScript\n\t// or generate a script.\n\tc := lnChan.State()\n\t_, err := c.BroadcastedCooperative()\n\tif err != nil && err != channeldb.ErrNoCloseTx {\n\t\t// An error other than ErrNoCloseTx was encountered.\n\t\treturn nil, err\n\t} else if err == nil {\n\t\t// This channel has already completed the coop close\n\t\t// negotiation.\n\t\treturn nil, nil\n\t}\n\n\t// As mentioned above, we don't re-create the delivery script.\n\tdeliveryScript := c.LocalShutdownScript\n\tif len(deliveryScript) == 0 {\n\t\tvar err error\n\t\tdeliveryScript, err = p.genDeliveryScript()\n\t\tif err != nil {\n\t\t\tp.log.Errorf(\"unable to gen delivery script: %v\",\n\t\t\t\terr)\n\t\t\treturn nil, fmt.Errorf(\"close addr unavailable\")\n\t\t}\n\t}\n\n\t// Compute an ideal fee.\n\tfeePerKw, err := p.cfg.FeeEstimator.EstimateFeePerKW(\n\t\tp.cfg.CoopCloseTargetConfs,\n\t)\n\tif err != nil {\n\t\tp.log.Errorf(\"unable to query fee estimator: %v\", err)\n\t\treturn nil, fmt.Errorf(\"unable to estimate fee\")\n\t}\n\n\t// Determine whether we or the peer are the initiator of the coop\n\t// close attempt by looking at the channel's status.\n\tlocallyInitiated := c.HasChanStatus(\n\t\tchanneldb.ChanStatusLocalCloseInitiator,\n\t)\n\n\tchanCloser, err := p.createChanCloser(\n\t\tlnChan, deliveryScript, feePerKw, nil, locallyInitiated,\n\t)\n\tif err != nil {\n\t\tp.log.Errorf(\"unable to create chan closer: %v\", err)\n\t\treturn nil, fmt.Errorf(\"unable to create chan closer\")\n\t}\n\n\t// This does not need a mutex even though it is in a different\n\t// goroutine since this is done before the channelManager goroutine is\n\t// created.\n\tchanID := lnwire.NewChanIDFromOutPoint(&c.FundingOutpoint)\n\tp.activeChanCloses[chanID] = chanCloser\n\n\t// Create the Shutdown message.\n\tshutdownMsg, err := chanCloser.ShutdownChan()\n\tif err != nil {\n\t\tp.log.Errorf(\"unable to create shutdown message: %v\", err)\n\t\tdelete(p.activeChanCloses, chanID)\n\t\treturn nil, err\n\t}\n\n\treturn shutdownMsg, nil\n}\n\n// createChanCloser constructs a ChanCloser from the passed parameters and is\n// used to de-duplicate code.",
      "length": 2322,
      "tokens": 328,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) createChanCloser(channel *lnwallet.LightningChannel,",
      "content": "func (p *Brontide) createChanCloser(channel *lnwallet.LightningChannel,\n\tdeliveryScript lnwire.DeliveryAddress, fee chainfee.SatPerKWeight,\n\treq *htlcswitch.ChanClose,\n\tlocallyInitiated bool) (*chancloser.ChanCloser, error) {\n\n\t_, startingHeight, err := p.cfg.ChainIO.GetBestBlock()\n\tif err != nil {\n\t\tp.log.Errorf(\"unable to obtain best block: %v\", err)\n\t\treturn nil, fmt.Errorf(\"cannot obtain best block\")\n\t}\n\n\t// The req will only be set if we initaited the co-op closing flow.\n\tvar maxFee chainfee.SatPerKWeight\n\tif req != nil {\n\t\tmaxFee = req.MaxFee\n\t}\n\n\tchanCloser := chancloser.NewChanCloser(\n\t\tchancloser.ChanCloseCfg{\n\t\t\tChannel:      channel,\n\t\t\tFeeEstimator: &chancloser.SimpleCoopFeeEstimator{},\n\t\t\tBroadcastTx:  p.cfg.Wallet.PublishTransaction,\n\t\t\tDisableChannel: func(op wire.OutPoint) error {\n\t\t\t\treturn p.cfg.ChanStatusMgr.RequestDisable(\n\t\t\t\t\top, false,\n\t\t\t\t)\n\t\t\t},\n\t\t\tMaxFee: maxFee,\n\t\t\tDisconnect: func() error {\n\t\t\t\treturn p.cfg.DisconnectPeer(p.IdentityKey())\n\t\t\t},\n\t\t\tChainParams: &p.cfg.Wallet.Cfg.NetParams,\n\t\t\tQuit:        p.quit,\n\t\t},\n\t\tdeliveryScript,\n\t\tfee,\n\t\tuint32(startingHeight),\n\t\treq,\n\t\tlocallyInitiated,\n\t)\n\n\treturn chanCloser, nil\n}\n\n// handleLocalCloseReq kicks-off the workflow to execute a cooperative or\n// forced unilateral closure of the channel initiated by a local subsystem.",
      "length": 1203,
      "tokens": 128,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) handleLocalCloseReq(req *htlcswitch.ChanClose) {",
      "content": "func (p *Brontide) handleLocalCloseReq(req *htlcswitch.ChanClose) {\n\tchanID := lnwire.NewChanIDFromOutPoint(req.ChanPoint)\n\n\tp.activeChanMtx.RLock()\n\tchannel, ok := p.activeChannels[chanID]\n\tp.activeChanMtx.RUnlock()\n\n\t// Though this function can't be called for pending channels, we still\n\t// check whether channel is nil for safety.\n\tif !ok || channel == nil {\n\t\terr := fmt.Errorf(\"unable to close channel, ChannelID(%v) is \"+\n\t\t\t\"unknown\", chanID)\n\t\tp.log.Errorf(err.Error())\n\t\treq.Err <- err\n\t\treturn\n\t}\n\n\tswitch req.CloseType {\n\t// A type of CloseRegular indicates that the user has opted to close\n\t// out this channel on-chain, so we execute the cooperative channel\n\t// closure workflow.\n\tcase contractcourt.CloseRegular:\n\t\t// First, we'll choose a delivery address that we'll use to send the\n\t\t// funds to in the case of a successful negotiation.\n\n\t\t// An upfront shutdown and user provided script are both optional,\n\t\t// but must be equal if both set  (because we cannot serve a request\n\t\t// to close out to a script which violates upfront shutdown). Get the\n\t\t// appropriate address to close out to (which may be nil if neither\n\t\t// are set) and error if they are both set and do not match.\n\t\tdeliveryScript, err := chooseDeliveryScript(\n\t\t\tchannel.LocalUpfrontShutdownScript(), req.DeliveryScript,\n\t\t)\n\t\tif err != nil {\n\t\t\tp.log.Errorf(\"cannot close channel %v: %v\", req.ChanPoint, err)\n\t\t\treq.Err <- err\n\t\t\treturn\n\t\t}\n\n\t\t// If neither an upfront address or a user set address was\n\t\t// provided, generate a fresh script.\n\t\tif len(deliveryScript) == 0 {\n\t\t\tdeliveryScript, err = p.genDeliveryScript()\n\t\t\tif err != nil {\n\t\t\t\tp.log.Errorf(err.Error())\n\t\t\t\treq.Err <- err\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\t// Optimistically try a link shutdown, erroring out if it\n\t\t// failed.\n\t\tif err := p.tryLinkShutdown(chanID); err != nil {\n\t\t\tp.log.Errorf(\"failed link shutdown: %v\", err)\n\t\t\treq.Err <- err\n\t\t\treturn\n\t\t}\n\n\t\tchanCloser, err := p.createChanCloser(\n\t\t\tchannel, deliveryScript, req.TargetFeePerKw, req, true,\n\t\t)\n\t\tif err != nil {\n\t\t\tp.log.Errorf(err.Error())\n\t\t\treq.Err <- err\n\t\t\treturn\n\t\t}\n\n\t\tp.activeChanCloses[chanID] = chanCloser\n\n\t\t// Finally, we'll initiate the channel shutdown within the\n\t\t// chanCloser, and send the shutdown message to the remote\n\t\t// party to kick things off.\n\t\tshutdownMsg, err := chanCloser.ShutdownChan()\n\t\tif err != nil {\n\t\t\tp.log.Errorf(err.Error())\n\t\t\treq.Err <- err\n\t\t\tdelete(p.activeChanCloses, chanID)\n\n\t\t\t// As we were unable to shutdown the channel, we'll\n\t\t\t// return it back to its normal state.\n\t\t\tchannel.ResetState()\n\t\t\treturn\n\t\t}\n\n\t\tp.queueMsg(shutdownMsg, nil)\n\n\t// A type of CloseBreach indicates that the counterparty has breached\n\t// the channel therefore we need to clean up our local state.\n\tcase contractcourt.CloseBreach:\n\t\t// TODO(roasbeef): no longer need with newer beach logic?\n\t\tp.log.Infof(\"ChannelPoint(%v) has been breached, wiping \"+\n\t\t\t\"channel\", req.ChanPoint)\n\t\tp.WipeChannel(req.ChanPoint)\n\t}\n}\n\n// linkFailureReport is sent to the channelManager whenever a link reports a\n// link failure, and is forced to exit. The report houses the necessary\n// information to clean up the channel state, send back the error message, and\n// force close if necessary.",
      "length": 3050,
      "tokens": 442,
      "embedding": []
    },
    {
      "slug": "type linkFailureReport struct {",
      "content": "type linkFailureReport struct {\n\tchanPoint   wire.OutPoint\n\tchanID      lnwire.ChannelID\n\tshortChanID lnwire.ShortChannelID\n\tlinkErr     htlcswitch.LinkFailureError\n}\n\n// handleLinkFailure processes a link failure report when a link in the switch\n// fails. It facilitates the removal of all channel state within the peer,\n// force closing the channel depending on severity, and sending the error\n// message back to the remote party.",
      "length": 391,
      "tokens": 54,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) handleLinkFailure(failure linkFailureReport) {",
      "content": "func (p *Brontide) handleLinkFailure(failure linkFailureReport) {\n\t// Retrieve the channel from the map of active channels. We do this to\n\t// have access to it even after WipeChannel remove it from the map.\n\tchanID := lnwire.NewChanIDFromOutPoint(&failure.chanPoint)\n\tp.activeChanMtx.Lock()\n\tlnChan := p.activeChannels[chanID]\n\tp.activeChanMtx.Unlock()\n\n\t// We begin by wiping the link, which will remove it from the switch,\n\t// such that it won't be attempted used for any more updates.\n\t//\n\t// TODO(halseth): should introduce a way to atomically stop/pause the\n\t// link and cancel back any adds in its mailboxes such that we can\n\t// safely force close without the link being added again and updates\n\t// being applied.\n\tp.WipeChannel(&failure.chanPoint)\n\n\t// If the error encountered was severe enough, we'll now force close the\n\t// channel to prevent reading it to the switch in the future.\n\tif failure.linkErr.ForceClose {\n\t\tp.log.Warnf(\"Force closing link(%v)\",\n\t\t\tfailure.shortChanID)\n\n\t\tcloseTx, err := p.cfg.ChainArb.ForceCloseContract(\n\t\t\tfailure.chanPoint,\n\t\t)\n\t\tif err != nil {\n\t\t\tp.log.Errorf(\"unable to force close \"+\n\t\t\t\t\"link(%v): %v\", failure.shortChanID, err)\n\t\t} else {\n\t\t\tp.log.Infof(\"channel(%v) force \"+\n\t\t\t\t\"closed with txid %v\",\n\t\t\t\tfailure.shortChanID, closeTx.TxHash())\n\t\t}\n\t}\n\n\t// If this is a permanent failure, we will mark the channel borked.\n\tif failure.linkErr.PermanentFailure && lnChan != nil {\n\t\tp.log.Warnf(\"Marking link(%v) borked due to permanent \"+\n\t\t\t\"failure\", failure.shortChanID)\n\n\t\tif err := lnChan.State().MarkBorked(); err != nil {\n\t\t\tp.log.Errorf(\"Unable to mark channel %v borked: %v\",\n\t\t\t\tfailure.shortChanID, err)\n\t\t}\n\t}\n\n\t// Send an error to the peer, why we failed the channel.\n\tif failure.linkErr.ShouldSendToPeer() {\n\t\t// If SendData is set, send it to the peer. If not, we'll use\n\t\t// the standard error messages in the payload. We only include\n\t\t// sendData in the cases where the error data does not contain\n\t\t// sensitive information.\n\t\tdata := []byte(failure.linkErr.Error())\n\t\tif failure.linkErr.SendData != nil {\n\t\t\tdata = failure.linkErr.SendData\n\t\t}\n\t\terr := p.SendMessage(true, &lnwire.Error{\n\t\t\tChanID: failure.chanID,\n\t\t\tData:   data,\n\t\t})\n\t\tif err != nil {\n\t\t\tp.log.Errorf(\"unable to send msg to \"+\n\t\t\t\t\"remote peer: %v\", err)\n\t\t}\n\t}\n}\n\n// tryLinkShutdown attempts to fetch a target link from the switch, calls\n// ShutdownIfChannelClean to optimistically trigger a link shutdown, and\n// removes the link from the switch. It returns an error if any step failed.",
      "length": 2390,
      "tokens": 346,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) tryLinkShutdown(cid lnwire.ChannelID) error {",
      "content": "func (p *Brontide) tryLinkShutdown(cid lnwire.ChannelID) error {\n\t// Fetch the appropriate link and call ShutdownIfChannelClean to ensure\n\t// no other updates can occur.\n\tchanLink := p.fetchLinkFromKeyAndCid(cid)\n\n\t// If the link happens to be nil, return ErrChannelNotFound so we can\n\t// ignore the close message.\n\tif chanLink == nil {\n\t\treturn ErrChannelNotFound\n\t}\n\n\t// Else, the link exists, so attempt to trigger shutdown. If this\n\t// fails, we'll send an error message to the remote peer.\n\tif err := chanLink.ShutdownIfChannelClean(); err != nil {\n\t\treturn err\n\t}\n\n\t// Next, we remove the link from the switch to shut down all of the\n\t// link's goroutines and remove it from the switch's internal maps. We\n\t// don't call WipeChannel as the channel must still be in the\n\t// activeChannels map to process coop close messages.\n\tp.cfg.Switch.RemoveLink(cid)\n\n\treturn nil\n}\n\n// fetchLinkFromKeyAndCid fetches a link from the switch via the remote's\n// public key and the channel id.",
      "length": 892,
      "tokens": 148,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) fetchLinkFromKeyAndCid(",
      "content": "func (p *Brontide) fetchLinkFromKeyAndCid(\n\tcid lnwire.ChannelID) htlcswitch.ChannelUpdateHandler {\n\n\tvar chanLink htlcswitch.ChannelUpdateHandler\n\n\t// We don't need to check the error here, and can instead just loop\n\t// over the slice and return nil.\n\tlinks, _ := p.cfg.Switch.GetLinksByInterface(p.cfg.PubKeyBytes)\n\tfor _, link := range links {\n\t\tif link.ChanID() == cid {\n\t\t\tchanLink = link\n\t\t\tbreak\n\t\t}\n\t}\n\n\treturn chanLink\n}\n\n// finalizeChanClosure performs the final clean up steps once the cooperative\n// closure transaction has been fully broadcast. The finalized closing state\n// machine should be passed in. Once the transaction has been sufficiently\n// confirmed, the channel will be marked as fully closed within the database,\n// and any clients will be notified of updates to the closing state.",
      "length": 743,
      "tokens": 113,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) finalizeChanClosure(chanCloser *chancloser.ChanCloser) {",
      "content": "func (p *Brontide) finalizeChanClosure(chanCloser *chancloser.ChanCloser) {\n\tcloseReq := chanCloser.CloseRequest()\n\n\t// First, we'll clear all indexes related to the channel in question.\n\tchanPoint := chanCloser.Channel().ChannelPoint()\n\tp.WipeChannel(chanPoint)\n\n\t// Also clear the activeChanCloses map of this channel.\n\tcid := lnwire.NewChanIDFromOutPoint(chanPoint)\n\tdelete(p.activeChanCloses, cid)\n\n\t// Next, we'll launch a goroutine which will request to be notified by\n\t// the ChainNotifier once the closure transaction obtains a single\n\t// confirmation.\n\tnotifier := p.cfg.ChainNotifier\n\n\t// If any error happens during waitForChanToClose, forward it to\n\t// closeReq. If this channel closure is not locally initiated, closeReq\n\t// will be nil, so just ignore the error.\n\terrChan := make(chan error, 1)\n\tif closeReq != nil {\n\t\terrChan = closeReq.Err\n\t}\n\n\tclosingTx, err := chanCloser.ClosingTx()\n\tif err != nil {\n\t\tif closeReq != nil {\n\t\t\tp.log.Error(err)\n\t\t\tcloseReq.Err <- err\n\t\t}\n\t}\n\n\tclosingTxid := closingTx.TxHash()\n\n\t// If this is a locally requested shutdown, update the caller with a\n\t// new event detailing the current pending state of this request.\n\tif closeReq != nil {\n\t\tcloseReq.Updates <- &PendingUpdate{\n\t\t\tTxid: closingTxid[:],\n\t\t}\n\t}\n\n\tgo WaitForChanToClose(chanCloser.NegotiationHeight(), notifier, errChan,\n\t\tchanPoint, &closingTxid, closingTx.TxOut[0].PkScript, func() {\n\t\t\t// Respond to the local subsystem which requested the\n\t\t\t// channel closure.\n\t\t\tif closeReq != nil {\n\t\t\t\tcloseReq.Updates <- &ChannelCloseUpdate{\n\t\t\t\t\tClosingTxid: closingTxid[:],\n\t\t\t\t\tSuccess:     true,\n\t\t\t\t}\n\t\t\t}\n\t\t})\n}\n\n// WaitForChanToClose uses the passed notifier to wait until the channel has\n// been detected as closed on chain and then concludes by executing the\n// following actions: the channel point will be sent over the settleChan, and\n// finally the callback will be executed. If any error is encountered within\n// the function, then it will be sent over the errChan.",
      "length": 1849,
      "tokens": 263,
      "embedding": []
    },
    {
      "slug": "func WaitForChanToClose(bestHeight uint32, notifier chainntnfs.ChainNotifier,",
      "content": "func WaitForChanToClose(bestHeight uint32, notifier chainntnfs.ChainNotifier,\n\terrChan chan error, chanPoint *wire.OutPoint,\n\tclosingTxID *chainhash.Hash, closeScript []byte, cb func()) {\n\n\tpeerLog.Infof(\"Waiting for confirmation of close of ChannelPoint(%v) \"+\n\t\t\"with txid: %v\", chanPoint, closingTxID)\n\n\t// TODO(roasbeef): add param for num needed confs\n\tconfNtfn, err := notifier.RegisterConfirmationsNtfn(\n\t\tclosingTxID, closeScript, 1, bestHeight,\n\t)\n\tif err != nil {\n\t\tif errChan != nil {\n\t\t\terrChan <- err\n\t\t}\n\t\treturn\n\t}\n\n\t// In the case that the ChainNotifier is shutting down, all subscriber\n\t// notification channels will be closed, generating a nil receive.\n\theight, ok := <-confNtfn.Confirmed\n\tif !ok {\n\t\treturn\n\t}\n\n\t// The channel has been closed, remove it from any active indexes, and\n\t// the database state.\n\tpeerLog.Infof(\"ChannelPoint(%v) is now closed at \"+\n\t\t\"height %v\", chanPoint, height.BlockHeight)\n\n\t// Finally, execute the closure call back to mark the confirmation of\n\t// the transaction closing the contract.\n\tcb()\n}\n\n// WipeChannel removes the passed channel point from all indexes associated with\n// the peer and the switch.",
      "length": 1043,
      "tokens": 154,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) WipeChannel(chanPoint *wire.OutPoint) {",
      "content": "func (p *Brontide) WipeChannel(chanPoint *wire.OutPoint) {\n\tchanID := lnwire.NewChanIDFromOutPoint(chanPoint)\n\n\tp.activeChanMtx.Lock()\n\tdelete(p.activeChannels, chanID)\n\tp.activeChanMtx.Unlock()\n\n\t// Instruct the HtlcSwitch to close this link as the channel is no\n\t// longer active.\n\tp.cfg.Switch.RemoveLink(chanID)\n}\n\n// handleInitMsg handles the incoming init message which contains global and\n// local feature vectors. If feature vectors are incompatible then disconnect.",
      "length": 403,
      "tokens": 47,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) handleInitMsg(msg *lnwire.Init) error {",
      "content": "func (p *Brontide) handleInitMsg(msg *lnwire.Init) error {\n\t// First, merge any features from the legacy global features field into\n\t// those presented in the local features fields.\n\terr := msg.Features.Merge(msg.GlobalFeatures)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to merge legacy global features: %v\",\n\t\t\terr)\n\t}\n\n\t// Then, finalize the remote feature vector providing the flattened\n\t// feature bit namespace.\n\tp.remoteFeatures = lnwire.NewFeatureVector(\n\t\tmsg.Features, lnwire.Features,\n\t)\n\n\t// Now that we have their features loaded, we'll ensure that they\n\t// didn't set any required bits that we don't know of.\n\terr = feature.ValidateRequired(p.remoteFeatures)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"invalid remote features: %v\", err)\n\t}\n\n\t// Ensure the remote party's feature vector contains all transitive\n\t// dependencies. We know ours are correct since they are validated\n\t// during the feature manager's instantiation.\n\terr = feature.ValidateDeps(p.remoteFeatures)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"invalid remote features: %v\", err)\n\t}\n\n\t// Now that we know we understand their requirements, we'll check to\n\t// see if they don't support anything that we deem to be mandatory.\n\tif !p.remoteFeatures.HasFeature(lnwire.DataLossProtectRequired) {\n\t\treturn fmt.Errorf(\"data loss protection required\")\n\t}\n\n\treturn nil\n}\n\n// LocalFeatures returns the set of global features that has been advertised by\n// the local node. This allows sub-systems that use this interface to gate their\n// behavior off the set of negotiated feature bits.\n//\n// NOTE: Part of the lnpeer.Peer interface.",
      "length": 1496,
      "tokens": 219,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) LocalFeatures() *lnwire.FeatureVector {",
      "content": "func (p *Brontide) LocalFeatures() *lnwire.FeatureVector {\n\treturn p.cfg.Features\n}\n\n// RemoteFeatures returns the set of global features that has been advertised by\n// the remote node. This allows sub-systems that use this interface to gate\n// their behavior off the set of negotiated feature bits.\n//\n// NOTE: Part of the lnpeer.Peer interface.",
      "length": 280,
      "tokens": 47,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) RemoteFeatures() *lnwire.FeatureVector {",
      "content": "func (p *Brontide) RemoteFeatures() *lnwire.FeatureVector {\n\treturn p.remoteFeatures\n}\n\n// hasNegotiatedScidAlias returns true if we've negotiated the\n// option-scid-alias feature bit with the peer.",
      "length": 134,
      "tokens": 18,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) hasNegotiatedScidAlias() bool {",
      "content": "func (p *Brontide) hasNegotiatedScidAlias() bool {\n\tpeerHas := p.remoteFeatures.HasFeature(lnwire.ScidAliasOptional)\n\tlocalHas := p.cfg.Features.HasFeature(lnwire.ScidAliasOptional)\n\treturn peerHas && localHas\n}\n\n// sendInitMsg sends the Init message to the remote peer. This message contains\n// our currently supported local and global features.",
      "length": 289,
      "tokens": 32,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) sendInitMsg(legacyChan bool) error {",
      "content": "func (p *Brontide) sendInitMsg(legacyChan bool) error {\n\tfeatures := p.cfg.Features.Clone()\n\tlegacyFeatures := p.cfg.LegacyFeatures.Clone()\n\n\t// If we have a legacy channel open with a peer, we downgrade static\n\t// remote required to optional in case the peer does not understand the\n\t// required feature bit. If we do not do this, the peer will reject our\n\t// connection because it does not understand a required feature bit, and\n\t// our channel will be unusable.\n\tif legacyChan && features.RequiresFeature(lnwire.StaticRemoteKeyRequired) {\n\t\tp.log.Infof(\"Legacy channel open with peer, \" +\n\t\t\t\"downgrading static remote required feature bit to \" +\n\t\t\t\"optional\")\n\n\t\t// Unset and set in both the local and global features to\n\t\t// ensure both sets are consistent and merge able by old and\n\t\t// new nodes.\n\t\tfeatures.Unset(lnwire.StaticRemoteKeyRequired)\n\t\tlegacyFeatures.Unset(lnwire.StaticRemoteKeyRequired)\n\n\t\tfeatures.Set(lnwire.StaticRemoteKeyOptional)\n\t\tlegacyFeatures.Set(lnwire.StaticRemoteKeyOptional)\n\t}\n\n\tmsg := lnwire.NewInitMessage(\n\t\tlegacyFeatures.RawFeatureVector,\n\t\tfeatures.RawFeatureVector,\n\t)\n\n\treturn p.writeMessage(msg)\n}\n\n// resendChanSyncMsg will attempt to find a channel sync message for the closed\n// channel and resend it to our peer.",
      "length": 1173,
      "tokens": 150,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) resendChanSyncMsg(cid lnwire.ChannelID) error {",
      "content": "func (p *Brontide) resendChanSyncMsg(cid lnwire.ChannelID) error {\n\t// If we already re-sent the mssage for this channel, we won't do it\n\t// again.\n\tif _, ok := p.resentChanSyncMsg[cid]; ok {\n\t\treturn nil\n\t}\n\n\t// Check if we have any channel sync messages stored for this channel.\n\tc, err := p.cfg.ChannelDB.FetchClosedChannelForID(cid)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to fetch channel sync messages for \"+\n\t\t\t\"peer %v: %v\", p, err)\n\t}\n\n\tif c.LastChanSyncMsg == nil {\n\t\treturn fmt.Errorf(\"no chan sync message stored for channel %v\",\n\t\t\tcid)\n\t}\n\n\tif !c.RemotePub.IsEqual(p.IdentityKey()) {\n\t\treturn fmt.Errorf(\"ignoring channel reestablish from \"+\n\t\t\t\"peer=%x\", p.IdentityKey().SerializeCompressed())\n\t}\n\n\tp.log.Debugf(\"Re-sending channel sync message for channel %v to \"+\n\t\t\"peer\", cid)\n\n\tif err := p.SendMessage(true, c.LastChanSyncMsg); err != nil {\n\t\treturn fmt.Errorf(\"failed resending channel sync \"+\n\t\t\t\"message to peer %v: %v\", p, err)\n\t}\n\n\tp.log.Debugf(\"Re-sent channel sync message for channel %v to peer \",\n\t\tcid)\n\n\t// Note down that we sent the message, so we won't resend it again for\n\t// this connection.\n\tp.resentChanSyncMsg[cid] = struct{}{}\n\n\treturn nil\n}\n\n// SendMessage sends a variadic number of high-priority messages to the remote\n// peer. The first argument denotes if the method should block until the\n// messages have been sent to the remote peer or an error is returned,\n// otherwise it returns immediately after queuing.\n//\n// NOTE: Part of the lnpeer.Peer interface.",
      "length": 1394,
      "tokens": 214,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) SendMessage(sync bool, msgs ...lnwire.Message) error {",
      "content": "func (p *Brontide) SendMessage(sync bool, msgs ...lnwire.Message) error {\n\treturn p.sendMessage(sync, true, msgs...)\n}\n\n// SendMessageLazy sends a variadic number of low-priority messages to the\n// remote peer. The first argument denotes if the method should block until\n// the messages have been sent to the remote peer or an error is returned,\n// otherwise it returns immediately after queueing.\n//\n// NOTE: Part of the lnpeer.Peer interface.",
      "length": 362,
      "tokens": 59,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) SendMessageLazy(sync bool, msgs ...lnwire.Message) error {",
      "content": "func (p *Brontide) SendMessageLazy(sync bool, msgs ...lnwire.Message) error {\n\treturn p.sendMessage(sync, false, msgs...)\n}\n\n// sendMessage queues a variadic number of messages using the passed priority\n// to the remote peer. If sync is true, this method will block until the\n// messages have been sent to the remote peer or an error is returned, otherwise\n// it returns immediately after queueing.",
      "length": 314,
      "tokens": 53,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) sendMessage(sync, priority bool, msgs ...lnwire.Message) error {",
      "content": "func (p *Brontide) sendMessage(sync, priority bool, msgs ...lnwire.Message) error {\n\t// Add all incoming messages to the outgoing queue. A list of error\n\t// chans is populated for each message if the caller requested a sync\n\t// send.\n\tvar errChans []chan error\n\tif sync {\n\t\terrChans = make([]chan error, 0, len(msgs))\n\t}\n\tfor _, msg := range msgs {\n\t\t// If a sync send was requested, create an error chan to listen\n\t\t// for an ack from the writeHandler.\n\t\tvar errChan chan error\n\t\tif sync {\n\t\t\terrChan = make(chan error, 1)\n\t\t\terrChans = append(errChans, errChan)\n\t\t}\n\n\t\tif priority {\n\t\t\tp.queueMsg(msg, errChan)\n\t\t} else {\n\t\t\tp.queueMsgLazy(msg, errChan)\n\t\t}\n\t}\n\n\t// Wait for all replies from the writeHandler. For async sends, this\n\t// will be a NOP as the list of error chans is nil.\n\tfor _, errChan := range errChans {\n\t\tselect {\n\t\tcase err := <-errChan:\n\t\t\treturn err\n\t\tcase <-p.quit:\n\t\t\treturn lnpeer.ErrPeerExiting\n\t\tcase <-p.cfg.Quit:\n\t\t\treturn lnpeer.ErrPeerExiting\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// PubKey returns the pubkey of the peer in compressed serialized format.\n//\n// NOTE: Part of the lnpeer.Peer interface.",
      "length": 993,
      "tokens": 171,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) PubKey() [33]byte {",
      "content": "func (p *Brontide) PubKey() [33]byte {\n\treturn p.cfg.PubKeyBytes\n}\n\n// IdentityKey returns the public key of the remote peer.\n//\n// NOTE: Part of the lnpeer.Peer interface.",
      "length": 128,
      "tokens": 21,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) IdentityKey() *btcec.PublicKey {",
      "content": "func (p *Brontide) IdentityKey() *btcec.PublicKey {\n\treturn p.cfg.Addr.IdentityKey\n}\n\n// Address returns the network address of the remote peer.\n//\n// NOTE: Part of the lnpeer.Peer interface.",
      "length": 134,
      "tokens": 21,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) Address() net.Addr {",
      "content": "func (p *Brontide) Address() net.Addr {\n\treturn p.cfg.Addr.Address\n}\n\n// AddNewChannel adds a new channel to the peer. The channel should fail to be\n// added if the cancel channel is closed.\n//\n// NOTE: Part of the lnpeer.Peer interface.",
      "length": 191,
      "tokens": 34,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) AddNewChannel(channel *channeldb.OpenChannel,",
      "content": "func (p *Brontide) AddNewChannel(channel *channeldb.OpenChannel,\n\tcancel <-chan struct{}) error {\n\n\terrChan := make(chan error, 1)\n\tnewChanMsg := &newChannelMsg{\n\t\tchannel: channel,\n\t\terr:     errChan,\n\t}\n\n\tselect {\n\tcase p.newChannels <- newChanMsg:\n\tcase <-cancel:\n\t\treturn errors.New(\"canceled adding new channel\")\n\tcase <-p.quit:\n\t\treturn lnpeer.ErrPeerExiting\n\t}\n\n\t// We pause here to wait for the peer to recognize the new channel\n\t// before we close the channel barrier corresponding to the channel.\n\tselect {\n\tcase err := <-errChan:\n\t\treturn err\n\tcase <-p.quit:\n\t\treturn lnpeer.ErrPeerExiting\n\t}\n}\n\n// StartTime returns the time at which the connection was established if the\n// peer started successfully, and zero otherwise.",
      "length": 641,
      "tokens": 95,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) StartTime() time.Time {",
      "content": "func (p *Brontide) StartTime() time.Time {\n\treturn p.startTime\n}\n\n// handleCloseMsg is called when a new cooperative channel closure related\n// message is received from the remote peer. We'll use this message to advance\n// the chan closer state machine.",
      "length": 205,
      "tokens": 34,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) handleCloseMsg(msg *closeMsg) {",
      "content": "func (p *Brontide) handleCloseMsg(msg *closeMsg) {\n\t// We'll now fetch the matching closing state machine in order to continue,\n\t// or finalize the channel closure process.\n\tchanCloser, err := p.fetchActiveChanCloser(msg.cid)\n\tif err != nil {\n\t\t// If the channel is not known to us, we'll simply ignore this message.\n\t\tif err == ErrChannelNotFound {\n\t\t\treturn\n\t\t}\n\n\t\tp.log.Errorf(\"Unable to respond to remote close msg: %v\", err)\n\n\t\terrMsg := &lnwire.Error{\n\t\t\tChanID: msg.cid,\n\t\t\tData:   lnwire.ErrorData(err.Error()),\n\t\t}\n\t\tp.queueMsg(errMsg, nil)\n\t\treturn\n\t}\n\n\t// Next, we'll process the next message using the target state machine.\n\t// We'll either continue negotiation, or halt.\n\tmsgs, closeFin, err := chanCloser.ProcessCloseMsg(\n\t\tmsg.msg,\n\t)\n\tif err != nil {\n\t\terr := fmt.Errorf(\"unable to process close msg: %v\", err)\n\t\tp.log.Error(err)\n\n\t\t// As the negotiations failed, we'll reset the channel state machine to\n\t\t// ensure we act to on-chain events as normal.\n\t\tchanCloser.Channel().ResetState()\n\n\t\tif chanCloser.CloseRequest() != nil {\n\t\t\tchanCloser.CloseRequest().Err <- err\n\t\t}\n\t\tdelete(p.activeChanCloses, msg.cid)\n\t\treturn\n\t}\n\n\t// Queue any messages to the remote peer that need to be sent as a part of\n\t// this latest round of negotiations.\n\tfor _, msg := range msgs {\n\t\tp.queueMsg(msg, nil)\n\t}\n\n\t// If we haven't finished close negotiations, then we'll continue as we\n\t// can't yet finalize the closure.\n\tif !closeFin {\n\t\treturn\n\t}\n\n\t// Otherwise, we've agreed on a closing fee! In this case, we'll wrap up\n\t// the channel closure by notifying relevant sub-systems and launching a\n\t// goroutine to wait for close tx conf.\n\tp.finalizeChanClosure(chanCloser)\n}\n\n// HandleLocalCloseChanReqs accepts a *htlcswitch.ChanClose and passes it onto\n// the channelManager goroutine, which will shut down the link and possibly\n// close the channel.",
      "length": 1743,
      "tokens": 263,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) HandleLocalCloseChanReqs(req *htlcswitch.ChanClose) {",
      "content": "func (p *Brontide) HandleLocalCloseChanReqs(req *htlcswitch.ChanClose) {\n\tselect {\n\tcase p.localCloseChanReqs <- req:\n\t\tp.log.Info(\"Local close channel request delivered to \" +\n\t\t\t\"peer\")\n\tcase <-p.quit:\n\t\tp.log.Info(\"Unable to deliver local close channel request \" +\n\t\t\t\"to peer\")\n\t}\n}\n\n// NetAddress returns the network of the remote peer as an lnwire.NetAddress.",
      "length": 282,
      "tokens": 42,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) NetAddress() *lnwire.NetAddress {",
      "content": "func (p *Brontide) NetAddress() *lnwire.NetAddress {\n\treturn p.cfg.Addr\n}\n\n// Inbound is a getter for the Brontide's Inbound boolean in cfg.",
      "length": 84,
      "tokens": 15,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) Inbound() bool {",
      "content": "func (p *Brontide) Inbound() bool {\n\treturn p.cfg.Inbound\n}\n\n// ConnReq is a getter for the Brontide's connReq in cfg.",
      "length": 79,
      "tokens": 14,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) ConnReq() *connmgr.ConnReq {",
      "content": "func (p *Brontide) ConnReq() *connmgr.ConnReq {\n\treturn p.cfg.ConnReq\n}\n\n// ErrorBuffer is a getter for the Brontide's errorBuffer in cfg.",
      "length": 87,
      "tokens": 14,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) ErrorBuffer() *queue.CircularBuffer {",
      "content": "func (p *Brontide) ErrorBuffer() *queue.CircularBuffer {\n\treturn p.cfg.ErrorBuffer\n}\n\n// SetAddress sets the remote peer's address given an address.",
      "length": 88,
      "tokens": 13,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) SetAddress(address net.Addr) {",
      "content": "func (p *Brontide) SetAddress(address net.Addr) {\n\tp.cfg.Addr.Address = address\n}\n\n// ActiveSignal returns the peer's active signal.",
      "length": 79,
      "tokens": 11,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) ActiveSignal() chan struct{} {",
      "content": "func (p *Brontide) ActiveSignal() chan struct{} {\n\treturn p.activeSignal\n}\n\n// Conn returns a pointer to the peer's connection struct.",
      "length": 81,
      "tokens": 13,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) Conn() net.Conn {",
      "content": "func (p *Brontide) Conn() net.Conn {\n\treturn p.cfg.Conn\n}\n\n// BytesReceived returns the number of bytes received from the peer.",
      "length": 87,
      "tokens": 14,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) BytesReceived() uint64 {",
      "content": "func (p *Brontide) BytesReceived() uint64 {\n\treturn atomic.LoadUint64(&p.bytesReceived)\n}\n\n// BytesSent returns the number of bytes sent to the peer.",
      "length": 102,
      "tokens": 14,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) BytesSent() uint64 {",
      "content": "func (p *Brontide) BytesSent() uint64 {\n\treturn atomic.LoadUint64(&p.bytesSent)\n}\n\n// LastRemotePingPayload returns the last payload the remote party sent as part\n// of their ping.",
      "length": 136,
      "tokens": 19,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) LastRemotePingPayload() []byte {",
      "content": "func (p *Brontide) LastRemotePingPayload() []byte {\n\tpingPayload := p.lastPingPayload.Load()\n\tif pingPayload == nil {\n\t\treturn []byte{}\n\t}\n\n\tpingBytes, ok := pingPayload.(lnwire.PingPayload)\n\tif !ok {\n\t\treturn nil\n\t}\n\n\treturn pingBytes\n}\n\n// attachChannelEventSubscription creates a channel event subscription and\n// attaches to client to Brontide if the reenableTimeout is no greater than 1\n// minute.",
      "length": 335,
      "tokens": 48,
      "embedding": []
    },
    {
      "slug": "func (p *Brontide) attachChannelEventSubscription() error {",
      "content": "func (p *Brontide) attachChannelEventSubscription() error {\n\t// If the timeout is greater than 1 minute, it's unlikely that the link\n\t// hasn't yet finished its reestablishment. Return a nil without\n\t// creating the client to specify that we don't want to retry.\n\tif p.cfg.ChanActiveTimeout > 1*time.Minute {\n\t\treturn nil\n\t}\n\n\t// When the reenable timeout is less than 1 minute, it's likely the\n\t// channel link hasn't finished its reestablishment yet. In that case,\n\t// we'll give it a second chance by subscribing to the channel update\n\t// events. Upon receiving the `ActiveLinkEvent`, we'll then request\n\t// enabling the channel again.\n\tsub, err := p.cfg.ChannelNotifier.SubscribeChannelEvents()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"SubscribeChannelEvents failed: %w\", err)\n\t}\n\n\tp.channelEventClient = sub\n\n\treturn nil\n}\n",
      "length": 744,
      "tokens": 116,
      "embedding": []
    }
  ]
}